[{"content":"Giá»›i thiá»‡u Ta biáº¿t ráº±ng, viá»‡c táº¡o ra cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n (nhiá»u layer) chÆ°a cháº¯c Ä‘Ã£ mang láº¡i hiá»‡u quáº£ tá»‘t hÆ¡n nhá»¯ng mÃ´ hÃ¬nh â€œcáº¡nâ€ hÆ¡n. VÃ­ dá»¥, vá»›i táº­p CIFAR10 thÃ¬ ta cÃ³ má»™t káº¿t quáº£ thá»­ nghiá»‡m cho tháº¥y ráº±ng mÃ´ hÃ¬nh sÃ¢u hÆ¡n láº¡i cÃ³ Ä‘á»™ hiá»‡u quáº£ kÃ©m hÆ¡n:\nÄá»‘i vá»›i viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n thÃ¬ ta cÃ³ thá»ƒ sáº½ bá»‹ gáº·p pháº£i cÃ¡c váº¥n Ä‘á» sau:\nOverfitting Vanishing/exploding gradient. Váº¥n Ä‘á» nÃ y thÃ¬ ta Ä‘Ã£ cÃ³ má»™t sá»‘ cÃ¡ch giáº£i quyáº¿t phá»• biáº¿n nhÆ° thay Ä‘á»•i activation function, cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o trá»ng sá»‘ Äáº·c biÃªt hÆ¡n lÃ  váº¥n Ä‘á» degradation: Accuracy tÄƒng dáº§n cho Ä‘áº¿n má»™t Ä‘á»™ sÃ¢u nháº¥t Ä‘á»‹nh thÃ¬ ngá»«ng tÄƒng (bÃ£o hÃ²a) rá»“i sau Ä‘Ã³ sáº½ giáº£m dáº§n.\nLÆ°u Ã½. Nhiá»u trÆ°á»ng há»£p degradation khÃ´ng pháº£i do overfitting gÃ¢y ra.\nDegradation cho chÃºng ta tháº¥y ráº±ng viá»‡c tá»‘i Æ°u cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n lÃ  khÃ´ng há» dá»… dÃ ng.\nTrong quÃ¡ trÃ¬nh xÃ¢y dá»±ng mÃ´ hÃ¬nh, tá»« má»™t mÃ´ hÃ¬nh ban Ä‘áº§u, ta thÃªm má»™t sá»‘ layer vÃ o thÃ¬ táº¥t nhiÃªn lÃ  ta mong ráº±ng mÃ´ hÃ¬nh má»›i pháº£i cÃ³ Ä‘á»™ hiá»‡u quáº£ Ã­t nháº¥t lÃ  ngang mÃ´ hÃ¬nh gá»‘c. Tuy nhiÃªn, khi xáº£y ra degradation thÃ¬ mong muá»‘n Ä‘Ã³ Ä‘Ã£ khÃ´ng thá»ƒ thÃ nh sá»± tháº­t Ä‘Æ°á»£c. ğŸ˜€\nResNet Ä‘Æ°á»£c cÃ´ng bá»‘ nháº±m giáº£i quyáº¿t váº¥n Ä‘á» degradation Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n. Khi nháº¯c Ä‘áº¿n ResNet thÃ¬ ta sáº½ láº­p tá»©c nghÄ© Ä‘áº¿n nhá»¯ng mÃ´ hÃ¬nh vá»›i Ä‘á»™ sÃ¢u ráº¥t khá»§ng, tháº­m chá»‰ lÃ  lÃªn Ä‘áº¿n 100, 200 layer.\nHÃ m pháº§n dÆ° vÃ  skip connection Nháº¯c láº¡i vá» cÃ¡i mong muá»‘n á»Ÿ trÃªn, ráº±ng mÃ´ hÃ¬nh má»›i pháº£i cÃ³ Ä‘á»™ hiá»‡u quáº£ Ã­t nháº¥t lÃ  ngang mÃ´ hÃ¬nh gá»‘c, ta cÃ³ thá»ƒ nghÄ© ngay Ä‘áº¿n má»™t phÆ°Æ¡ng phÃ¡p cá»±c kÃ¬ Ä‘Æ¡n giáº£n: cÃ¡c layer phÃ­a sau sáº½ lÃ  identity mapping, tá»©c lÃ  input vÃ  output cá»§a nÃ³ sáº½ giá»‘ng há»‡t nhau. Vá»›i cÃ¡ch lÃ m nÃ y thÃ¬ hiá»ƒn nhiÃªn lÃ  ta Ä‘áº¡t Ä‘Æ°á»£c mong muá»‘n rá»“i, vÃ¬ Ä‘á»™ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh má»›i vÃ  mÃ´ hÃ¬nh gá»‘c sáº½ y há»‡t nhau.\nTuy nhiÃªn, náº¿u chá»‰ muá»‘n ngang Ä‘Ã³ thÃ´i thÃ¬ thÃªm layer vÃ o lÃ m gÃ¬ :v Ta muá»‘n Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n! CÃ¡c tÃ¡c giáº£ cá»§a paper ResNet giá»›i thiá»‡u má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  deep residual learning (há»c pháº§n dÆ°).\nGiáº£ sá»­ ta cÃ³ má»™t block B cÃ¡c layer, input cá»§a nÃ³ lÃ  $x$. Vá»›i má»™t mÃ´ hÃ¬nh thÃ´ng thÆ°á»ng, ta sáº½ â€œhá»câ€ má»™t hÃ m sá»‘ Ä‘áº§u ra mong muá»‘n lÃ  $f(x)$. LÃºc nÃ y, ban Ä‘áº§u thÃ¬ ta hoÃ n toÃ n chÆ°a cÃ³ má»™t thÃ´ng tin gÃ¬ vá» $f(x)$ cáº£, viá»‡c â€œhá»câ€ sáº½ xuáº¥t phÃ¡t tá»« má»™t Ä‘áº¡i lÆ°á»£ng ngáº«u nhiÃªn. Äá»‘i vá»›i phÆ°Æ¡ng phÃ¡p deep residual learning, output cá»§a block B sáº½ cÃ³ dáº¡ng $h(x) = x + f(x)$, vÃ  ta sáº½ Ä‘i há»c $f(x)$. LÃºc nÃ y, ta gá»i $f(x)$ lÃ  residual function (hÃ m pháº§n dÆ°) Ta cÃ³ cÃ¡c nháº­n xÃ©t sau: Trong deep residual learning, ta Ä‘Ã£ cÃ³ sá»± â€œgá»£i Ã½â€ cho hÃ m mong muá»‘n thÃ´ng qua giÃ¡ trá»‹ input $x$. Äá»‘i vá»›i thÃ´ng thÆ°á»ng thÃ¬ khÃ´ng cÃ³ sá»± gá»£i Ã½ nÃ o, há»c tá»« má»™t cÃ¡i hoÃ n toÃ n ngáº«u nhiÃªn. Viá»‡c há»c $f(x)$ nhÆ° lÃ  má»™t hÃ m pháº§n dÆ° lÃ  dá»… hÆ¡n so vá»›i viá»‡c há»c hÃ m mong muá»‘n. Náº¿u identity mapping lÃ  káº¿t quáº£ tá»‘i Æ°u thÃ¬ ta cÃ³ luÃ´n $f(x)=0$ Vá» máº·t báº£n cháº¥t, vá»›i phÆ°Æ¡ng phÃ¡p deep residual learning, ta Ä‘Ã£ tÃ¡c Ä‘á»™ng vÃ o lá»›p hÃ m chá»©a hÃ m mong muá»‘n, sao cho lá»›p má»›i lÃ  lá»›n hÆ¡n (bao gá»“m) lá»›p cÅ©. Lá»›p hÃ m chá»©a hÃ m mong muá»‘n cá»§a phÆ°Æ¡ng phÃ¡p thÃ´ng thÆ°á»ng vÃ  phÆ°Æ¡ng phÃ¡p deep residual learning Nguá»“n: Dive into AI NhÆ° váº­y, Ä‘iá»ƒm nháº¥n cá»§a ResNet lÃ  ta Ä‘i há»c cÃ¡c hÃ m pháº§n dÆ°, thÃ´ng qua viá»‡c â€œgá»£i Ã½â€ cho hÃ m sá»‘ mong muá»‘n má»™t giÃ¡ trá»‹ báº±ng vá»›i giÃ¡ trá»‹ input. Trong cÃ i Ä‘áº·t, thao tÃ¡c nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua má»™t káº¿t ná»‘i gá»i lÃ  skip connection. Ta sáº½ cá»™ng input $x$ vÃ o output cá»§a block thÃ´ng thÆ°á»ng.\nÄá»‘i vá»›i viá»‡c cá»™ng nhÆ° váº­y thÃ¬ thá»±c cháº¥t lÃ  ta Ä‘ang Ä‘i cá»™ng hai ma tráº­n. Khi Ä‘Ã³ thÃ¬ cÃ³ náº£y sinh má»™t váº¥n Ä‘á» vá» kÃ­ch thÆ°á»›c cá»§a chÃºng, mÃ  thÆ°á»ng lÃ  sá»‘ channel. Náº¿u sá»‘ channel khÃ´ng khá»›p thÃ¬ ta cáº§n pháº£i tiáº¿n hÃ nh â€œÄ‘iá»u chá»‰nhâ€. Chi tiáº¿t vá» pháº§n nÃ y sáº½ Ä‘Æ°á»£c Ä‘á» cáº­p trong pháº§n vá» Residual block. VÃ¬ cÃ¡c block Ä‘Æ°á»£c sá»­ dá»¥ng thÆ°á»ng giá»¯ nguyÃªn kÃ­ch thÆ°á»›c cá»§a input (width x height) nÃªn ta chá»‰ nháº¯c Ä‘áº¿n sá»‘ channel. Khi width vÃ  height Ä‘Æ°á»£c giá»¯ nguyÃªn thÃ¬ skip connection Ä‘Æ°á»£c gá»i lÃ  identity skip connection. Trong trÆ°á»ng há»£p width vÃ  height cá»§a output cÅ©ng khÃ¡c input $x$ thÃ¬ ta cáº§n pháº£i biáº¿n Ä‘á»•i $x$ trÆ°á»›c. PhÃ©p biáº¿n Ä‘á»•i nÃ y thá»±c cháº¥t lÃ  dÃ¹ng conv layer $1 \\times 1$ Ä‘á»ƒ thay Ä‘á»•i sá»‘ channel. LÃºc nÃ y, skip connection Ä‘Æ°á»£c gá»i lÃ  projection skip connection. Sá»­ dá»¥ng skip connection (identity) Nguá»“n: Dive into AI BÃ¢y giá», khi quan sÃ¡t vÃ o hÃ¬nh trÃªn thÃ¬ ta cÃ²n nháº­n tháº¥y má»™t lá»£i Ã­ch khÃ¡c cá»§a skip connection lÃ  giÃºp cho gradient Ä‘Æ°á»£c lan truyá»n tá»‘t hÆ¡n trong quÃ¡ trÃ¬nh backpropagation, tá»« Ä‘Ã³ gÃ³p pháº§n lÃ m giáº£m hiá»‡n tÆ°á»£ng vanishing gradient.\nResidual block Residual block (khá»‘i pháº§n dÆ°) Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch thÃªm má»™t skip connection vÃ o má»™t block thÃ´ng thÆ°á»ng trong cÃ¡c mÃ´ hÃ¬nh CNN nhÆ° VGG block. Ta cÃ³ hai loáº¡i residual block nhÆ° sau:\nBasic: CÃ¡c conv layer trong block nÃ y cÃ³ filter $3 \\times 3$. Block dáº¡ng nÃ y thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng cho cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u vá»«a pháº£i. Bottleneck: Nháº±m giáº£m bá»›t sá»‘ lÆ°á»£ng tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n, cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng dáº¡ng block nÃ y vá»›i hai conv layer $1 \\times 1$ vá»›i vai trÃ² lÃ  giáº£m/tÄƒng sá»‘ channel, táº¡o ra má»™t hÃ¬nh dÃ¡ng giá»‘ng nhÆ° nÃºt tháº¯t cá»• chai. Hai layer conv $1 \\times 1$ nhÆ° váº­y Ä‘Æ°á»£c gá»i lÃ  bottleneck layer. Äá»ƒ cho dá»… hÃ¬nh dÃ¹ng thÃ¬ ta cÃ³ thá»ƒ xem Ä‘Ã¢y nhÆ° lÃ  thao tÃ¡c â€œcÃ´ Ä‘á»ng kiáº¿n thá»©câ€ cá»§a mÃ´ hÃ¬nh, hay nÃ³i rÃµ hÆ¡n lÃ  nÃ©n lÆ°á»£ng thÃ´ng tin láº¡i sao cho vá»«a giá»¯ Ä‘Æ°á»£c thÃ´ng tin vÃ  vá»«a tiáº¿t kiá»‡m tÃ i nguyÃªn (bá»™ nhá»›, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n). Basic Residual Block vÃ  Bottleneck Residual Block LÆ°u Ã½. CÃ³ má»™t chi tiáº¿t nÃ y khÃ¡ quan trá»ng trong cÃ i Ä‘áº·t lÃ  ta sáº½ tiáº¿n hÃ nh cá»™ng ma tráº­n trÆ°á»›c rá»“i má»›i Ä‘Æ°a káº¿t quáº£ qua activation function.\nNgoÃ i ra, ta cÃ²n cÃ³ 2 loáº¡i skip connection lÃ  identity vÃ  projection. LÃºc nÃ y, tÃ¹y vÃ o sá»‘ channels cá»§a input vÃ  output ban Ä‘áº§u cÃ³ khá»›p hay khÃ´ng mÃ  block tÆ°Æ¡ng á»©ng sáº½ chá»©a loáº¡i skip connection phÃ¹ há»£p.\nBasic residual block vá»›i identity vÃ  projection skip connection Nguá»“n: Dive into AI Kiáº¿n trÃºc ResNet ResNet Ä‘Æ°á»£c táº¡o nÃªn báº±ng cÃ¡ch sá»­ dá»¥ng nhiá»u residual block liÃªn tiáº¿p nhau, tÆ°Æ¡ng tá»± nhÆ° nhá»¯ng gÃ¬ mÃ  GoogLeNet hay VGG Ä‘Ã£ thá»±c hiá»‡n, CÃ¡c tÃ¡c giáº£ cá»§a paper ResNet táº¡o ra nhiá»u phiÃªn báº£n ResNet khÃ¡c nhau vá»›i Ä‘á»™ sÃ¢u tÄƒng dáº§n.\nDá»±a vÃ o sá»‘ lÆ°á»£ng layer thÃ¬ ta sáº½ cÃ³ tÃªn cÃ¡c mÃ´ hÃ¬nh nhÆ° ResNet18 (18 layer cÃ³ trá»ng sá»‘), ResNet34,â€¦ CÃ¡c phiÃªn báº£n ResNet Ta cÃ³ má»™t sá»‘ nháº­n xÃ©t nhÆ° sau:\nCÃ¡c phiÃªn báº£n cÃ³ Ä‘á»™ sÃ¢u lá»›n nhÆ° 50, 101 vÃ  152 sá»­ dá»¥ng Bottleneck Residual Block. Vá»›i 18 vÃ  34 thÃ¬ chÃºng dÃ¹ng Basic block Trong cÃ¡c kiáº¿n trÃºc trÃªn thÃ¬ ta sá»­ dá»¥ng cáº£ 2 loáº¡i skip connection: identity vÃ  projection VÃ­ dá»¥, vá»›i ResNet18 thÃ¬ trong cá»¥m Basic block Ä‘áº§u tiÃªn cá»§a cá»¥m conv3_x sáº½ cÃ³ sá»‘ channel lÃ  64, cÃ²n output ban Ä‘áº§u cá»§a block nÃ y thÃ¬ lÃ  128 nÃªn ta pháº£i Ã¡p dá»¥ng projection vÃ o input Äá»‘i vá»›i height vÃ  width thÃ¬ cÃ¡c tÃ¡c giáº£ cho biáº¿t viá»‡c downsampling input sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n táº¡i conv layer Ä‘áº§u tiÃªn cá»§a cÃ¡c cá»¥m conv3_x, conv4_x, conv5_x (vá»›i stride lÃ  2). CÃ¡c conv layer khÃ¡c thÃ¬ Ä‘á»u cÃ³ stride 1. Ta sáº½ cáº§n chÃº Ã½ Ä‘áº¿n chi tiáº¿t nÃ y khi tiáº¿n hÃ nh cÃ i Ä‘áº·t ResNet. CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t ResNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau\nTÃ i liá»‡u tham kháº£o Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into AI, ResNet ","date":"2023-02-08T17:41:09+07:00","permalink":"http://example.org/post/resnet/","title":"Resnet (2015)"},{"content":" CÃ¡ nhÃ¢n mÃ¬nh tháº¥y GoogLeNet lÃ  má»™t paper khÃ³ Ä‘á»c. Khi viáº¿t ra bÃ i nÃ y thÃ¬ mÃ¬nh váº«n Ä‘ang cáº£m tháº¥y hÆ¡i lÃº vá» ná»™i dung cá»§a nÃ³ ğŸ˜€\nGiá»›i thiá»‡u Tá»« khi AlexNet Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o nÄƒm 2012 vÃ  Ä‘áº·t ná»n táº£ng cho cÃ¡c máº¡ng Deep CNN, GoogLeNet, hay Inception V1 (2014), lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc cÃ³ cÃ¡ch thiáº¿t káº¿ ráº¥t thÃº vá»‹ khi nÃ³ táº­n dá»¥ng hiá»‡u quáº£ cÃ¡c conv layer, Ä‘áº·t ná»n mÃ³ng cho nhiá»u mÃ´ hÃ¬nh sau nÃ y.\nâ€œInceptionâ€ cÃ³ thá»ƒ dá»‹ch lÃ  â€œsá»± khá»Ÿi Ä‘áº§uâ€, nghe cÃ³ váº» ráº¥t há»£p lÃ½ ğŸ˜€. NgoÃ i ra, á»Ÿ trong bÃ i viáº¿t vá» VGG, mÃ¬nh cÃ³ Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» thiáº¿t káº¿ kiáº¿n trÃºc mÃ´ hÃ¬nh theo hÆ°á»›ng cÃ³ sá»± láº·p láº¡i cÃ¡c khuÃ´n máº«u. GoogLeNet cÅ©ng sáº½ Ä‘Æ°á»£c thiáº¿t káº¿ nhÆ° váº­y.\nLÆ°u Ã½. TÃªn cá»§a mÃ´ hÃ¬nh nÃ y lÃ  GoogLeNet, khÃ´ng pháº£i GoogleNet =)) TÃ¡c giáº£ cho biáº¿t Ã½ nghÄ©a cá»§a cÃ¡i tÃªn nÃ y lÃ  â€œThis name is a homage to Yann LeCuns pioneering LeNet 5 network.â€\nCÃ¡c tÃ¡c giáº£ cá»§a paper GoogLeNet cÃ³ tÃ³m táº¯t cÃ¡c má»¥c tiÃªu cá»§a nghiÃªn cá»©u nhÆ° sau:\nNÃ¢ng cao kháº£ nÄƒng táº­n dá»¥ng tÃ i nguyÃªn tÃ­nh toÃ¡n Cho phÃ©p tÄƒng chiá»u rá»™ng vÃ  chiá»u sÃ¢u cá»§a kiáº¿n trÃºc mÃ´ hÃ¬nh mÃ  váº«n Ä‘áº£m báº£o Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n lÃ  á»Ÿ má»©c cháº¥p nháº­n Ä‘Æ°á»£c. GoogLeNet tháº­t sá»± Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng Ä‘iá»u Ä‘Ã³, vÃ  nÃ³ Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn nguyÃªn lÃ½ Hebbian: â€œneurons that fire together, wire togetherâ€. Paper GoogLeNet Ä‘Ã£ Ä‘Æ°a ra cÃ¡c ná»n táº£ng lÃ½ thuyáº¿t ráº¥t â€œcÄƒng tháº³ngâ€ Ä‘á»ƒ cho tháº¥y ráº±ng mÃ´ hÃ¬nh CNN cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng â€œÄ‘á»§ tá»‘tâ€, Ä‘iá»u mÃ  ta chÆ°a Ä‘Æ°á»£c biáº¿t Ä‘áº¿n á»Ÿ trong cÃ¡c paper trÆ°á»›c Ä‘Ã³ ğŸ˜€\nNgoÃ i ra, cÃ³ má»™t quan Ä‘iá»ƒm khÃ¡ thÃº vá»‹ khi mÃ´ táº£ vá» kiáº¿n trÃºc cá»§a GoogLeNet nhÆ° sau: Khi xÃ¢y dá»±ng kiáº¿n trÃºc CNN, thay vÃ¬ pháº£i suy nghÄ© xem trong cÃ¡c máº¡ng CNN ta nÃªn Ã¡p dá»¥ng filter vá»›i kÃ­ch thÆ°á»›c bao nhiÃªu, hÃ£y Ã¡p dá»¥ng luÃ´n nhiá»u filter vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau vÃ  tá»•ng há»£p káº¿t quáº£ láº¡i ğŸ˜œ\nMÃ¬nh cÅ©ng cÃ³ Ä‘á»“ng tÃ¬nh vá»›i quan Ä‘iá»ƒm, nhÆ°ng khi Ä‘á»c paper thÃ¬ cÃ³ váº» lÃ  cÃ¡c tÃ¡c giáº£ cÅ©ng Ä‘Ã£ tiáº¿n hÃ nh phÃ¢n tÃ­ch vÃ  thá»­ nghiá»‡m nhiá»u láº¯m má»›i cho ra Ä‘Æ°á»£c cÃ¡i kiáº¿n trÃºc nhÆ° váº­y. Káº¿t ná»‘i thÆ°a vÃ  CNN Äáº§u tiÃªn, táº¡i thá»i Ä‘iá»ƒm trÆ°á»›c khi Inception Ä‘Æ°á»£c cÃ´ng bá»‘, chÃºng ta cÃ³ thá»ƒ cáº£i thiá»‡n má»™t mÃ´ hÃ¬nh DNN báº±ng nhá»¯ng cÃ¡ch nhÆ° sau:\nTÄƒng chiá»u sÃ¢u cá»§a mÃ´ hÃ¬nh (tá»©c lÃ  sá»‘ layer) TÄƒng chiá»u rá»™ng (sá»‘ channel trong má»—i layer) Tuy nhiÃªn, vá»›i hai cÃ¡ch trÃªn thÃ¬ sáº½ cÃ³ nhá»¯ng váº¥n Ä‘á» mÃ  ta cáº§n lÆ°u tÃ¢m lÃ  hiá»‡n tÆ°á»£ng overfitting vÃ  sá»± gia tÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh.\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n má»™t hÆ°á»›ng Ä‘i cÃ³ thá»ƒ giáº£m bá»›t hai váº¥n Ä‘á» trÃªn lÃ  sá»­ dá»¥ng kiáº¿n trÃºc káº¿t ná»‘i thÆ°a (sparsely connected architectures). Ta cÃ³ thá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° sau:\nVá»›i hai fully connected layer liÃªn tiáº¿p nhau, má»—i neuron trong layer sau sáº½ káº¿t ná»‘i vá»›i táº¥t cáº£ cÃ¡c unit trong layer trÆ°á»›c. Náº¿u nhÆ° ta thay Ä‘á»•i Ä‘i má»™t chÃºt, má»—i neuron trong layer sau sáº½ chá»‰ káº¿t ná»‘i Ä‘áº¿n má»™t vÃ i unit trong layer trÆ°á»›c, kiáº¿n trÃºc cÃ³ Ä‘Æ°á»£c sáº½ trá»Ÿ nÃªn â€œthÆ°aâ€ hÆ¡n. Káº¿t ná»‘i thÆ°a trong fully connected layer Äá»‘i vá»›i conv layer thÃ¬ ta Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c tÃ­nh cháº¥t thÆ°a nhÆ° váº­y. Ta biáº¿t ráº±ng, vá»›i má»—i pháº§n tá»­ trÃªn feature map cá»§a layer hiá»‡n táº¡i thÃ¬ ta tÃ­nh nÃ³ dá»±a vÃ o má»™t vÃ¹ng nhá» trÃªn feature map output cá»§a layer trÆ°á»›c Ä‘Ã³. Giáº£ sá»­ hai layer nÃ y Ä‘á»u chá»‰ cÃ³ 1 channel thÃ¬ ta cÃ³ thá»ƒ biá»ƒu diá»…n nÃ³ nhÆ° hÃ¬nh bÃªn dÆ°á»›i: Káº¿t ná»‘i thÆ°a trong conv layer Kiáº¿n trÃºc thÆ°a Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ mÃ´ táº£ lÃ  mÃ´ phá»ng láº¡i há»‡ thá»‘ng sinh há»c (vÃ­ dá»¥ nhÆ° khi ta nhÃ¬n vÃ o má»™t Ä‘á»‘i tÆ°á»£ng thÃ¬ ta thÆ°á»ng chá»‰ chÃº Ã½ má»™t sá»‘ Ä‘iá»ƒm trÃªn Ä‘á»‘i tÆ°á»£ng Ä‘Ã³ thÃ´i, khÃ³ mÃ  chÃº Ã½ táº¥t cáº£ Ä‘Æ°á»£c).\nNgoÃ i ra, cÃ³ má»™t ná»n tÃ ng lÃ½ thuyáº¿t ráº¥t trÃ¢u bÃ² vá» kiáº¿n trÃºc thÆ°a cá»§a Arora nhÆ° sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Tháº­t sá»± mÃ¬nh cÅ©ng chÆ°a hiá»ƒu háº¿t Ã½ cá»§a cÃ¢u trÃªnâ€¦ NhÆ°ng Ä‘áº¡i Ã½ cá»§a nÃ³ cÃ³ thá»ƒ lÃ  náº¿u ta biá»ƒu diá»…n Ä‘Æ°á»£c phÃ¢n phá»‘i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cá»§a má»™t dataset báº±ng má»™t kiáº¿n trÃºc DNN lá»›n vÃ  thÆ°a thÃ¬ tá»« Ä‘Ã³ ta cÃ³ thá»ƒ xÃ¢y nÃªn Ä‘Æ°á»£c má»™t kiáº¿n trÃºc tá»‘i Æ°u (vá» cáº£ Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ phá»©c táº¡p), báº±ng cÃ¡ch xÃ¢y tá»«ng layer má»™t ğŸ˜€\nTuy nhiÃªn, vá»›i cÃ¡c tÃ i nguyÃªn pháº§n cá»©ng trong thá»i gian nÃ y thÃ¬ ráº¥t khÃ³ Ä‘á»ƒ ta cÃ³ thá»ƒ tÃ­nh toÃ¡n trÃªn cÃ¡c máº¡ng DNN thÆ°a. Do Ä‘Ã³, cÃ¡c tÃ¡c giáº£ Ä‘i theo hÆ°á»›ng tÃ¬m má»™t mÃ´ hÃ¬nh lÃ  cÃ³ táº­n dá»¥ng má»™t sá»‘ thÃ´ng tin vá» tÃ­nh cháº¥t thÆ°a nhÆ°ng váº«n thá»±c hiá»‡n tÃ­nh toÃ¡n trÃªn cÃ¡c ma tráº­n Ä‘áº§y Ä‘á»§. Äáº¥y chÃ­nh lÃ  hÆ°á»›ng sá»­ dá»¥ng cÃ¡c phÃ©p toÃ¡n convolution!.\nCÅ©ng vÃ¬ lÃ½ do nÃ y, á»Ÿ pháº§n kiáº¿n trÃºc cá»§a GoogLeNet thÃ¬ ta sáº½ tháº¥y nÃ³ chá»‰ cÃ³ duy nháº¥t má»™t fully connected layer Ä‘á»ƒ sinh ra output, cÃ²n láº¡i chá»‰ toÃ n conv layer thÃ´i ğŸ˜— NÃ³i Ä‘áº¿n viá»‡c Ã¡p dá»¥ng cÃ¡c filter trong conv layer, cÃ¡c tÃ¡c giáº£ cho ráº±ng:\nMá»—i pháº§n tá»­ trong feature map cá»§a má»™t layer sáº½ cÃ³ má»‘i tÆ°Æ¡ng quan vá»›i má»™t vÃ¹ng nÃ o Ä‘Ã³ trÃªn áº£nh input (receptive field). Ta sáº½ gom cÃ¡c pháº§n tá»­ cÃ¹ng tÆ°Æ¡ng quan vá»›i má»™t vÃ¹ng vÃ o cÃ¹ng má»™t cá»¥m. Äá»ƒ Ã½ ráº±ng, trong nhá»¯ng layer á»Ÿ gáº§n áº£nh input thÃ¬ ta sáº½ cÃ³ nhiá»u cá»¥m vÃ  kÃ­ch thÆ°á»›c má»—i cá»¥m lÃ  nhá». CÃ ng Ä‘i qua cÃ¡c layer CNN thÃ¬ sá»‘ lÆ°á»£ng cá»¥m sáº½ Ã­t láº¡i vÃ  kÃ­ch thÆ°á»›c cá»¥m sáº½ lan rá»™ng ra. Táº¥t nhiÃªn lÃ  váº«n cÃ³ thá»ƒ cÃ³ nhá»¯ng cá»¥m cÃ³ kÃ­ch thÆ°á»›c nhá» táº¡i nhá»¯ng layer Ä‘Ã³. Do váº­y, ta nÃªn cÃ³ cÃ¡c filter vá»›i kÃ­ch thÆ°á»›c lá»›n hÆ¡n Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng táº¡i cÃ¡c cá»¥m lá»›n, Ä‘á»“ng thá»i cÅ©ng cáº§n cÃ³ filter kÃ­ch thÆ°á»›c nhá» Ä‘á»‘i vá»›i cÃ¡c cá»¥m nhá» hÆ¡n. Qua má»™t loáº¡t cÃ¡c thá»­ nghiá»‡m, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ chá»n 3 filter lÃ  $1 \\times 1$, $3 \\times 3$ vÃ  $5 \\times 5$. Má»™t khá»‘i chá»©a 3 filter trÃªn Ä‘Æ°á»£c Ä‘áº·t tÃªn lÃ  Inception module.\nBÃ¢y giá» quay láº¡i vá»›i lÃ½ thuyáº¿t cá»§a Arora, á»Ÿ Ã½ xÃ¢y dá»±ng máº¡ng tá»‘i Æ°u qua tá»«ng layer má»™t. GoogLeNet Ä‘Æ°á»£c táº¡o nÃªn báº±ng Ä‘Ãºng Ã½ tÆ°á»Ÿng nhÆ° váº­y, ta ná»‘i Inception module - by - Inception module ğŸ˜€\nP/s: CÃ¡c báº¡n cÃ³ thá»ƒ tÃ¬m Ä‘á»c pháº§n Motivation trong paper gá»‘c vÃ  tá»± cáº£m nháº­n nÃ³ nhÃ©.\nInception module Inception module Qua cÃ¡c mÃ´ táº£ á»Ÿ pháº§n trÃªn, ta cÃ³ thá»ƒ liÃªn tÆ°á»Ÿng Ä‘áº¿n kiáº¿n trÃºc cá»§a Inception module lÃ  má»™t thá»© gÃ¬ Ä‘Ã³ giá»‘ng vá»›i cÃ¡i á»Ÿ hÃ¬nh (a), vá»›i Ä‘á»§ 3 loáº¡i filter lÃ  $1 \\times 1$, $3 \\times 3$ vÃ  $5 \\times 5$. CÃ¡c tÃ¡c giáº£ dÃ¹ng thÃªm cáº£ má»™t layer max pooling trong Ä‘Ã³ ná»¯a, vá»›i lÃ­ do ráº¥t Ä‘Æ¡n giáº£n lÃ  vÃ¬ á»Ÿ thá»i Ä‘iá»ƒm Ä‘Ã³ thÃ¬ max pooling thÆ°á»ng mang láº¡i hiá»‡u quáº£ tá»‘t trong cÃ¡c kiáº¿n trÃºc máº¡ng CNN =))\nTruy nhiÃªn, cÃ¡ch cÃ i Ä‘áº·t nhÆ° hÃ¬nh (a) sáº½ dáº«n Ä‘áº¿n sá»‘ lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh lÃ  ráº¥t lá»›n. Thay vÃ o Ä‘Ã³, ta cÃ³ thá»ƒ táº¡o ra kiáº¿n trÃºc dáº¡ng â€œbottleneckâ€ báº±ng cÃ¡ch sá»­ dá»¥ng thÃªm cÃ¡c layer convolution $1 \\times 1$ nhÆ° hÃ¬nh (b), nháº±m má»¥c Ä‘Ã­ch chÃ­nh lÃ  giáº£m sá»‘ channel. Sá»‘ lÆ°á»£ng phÃ©p tÃ­nh lÃºc nÃ y sáº½ Ä‘Æ°á»£c giáº£m má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ.\nLÆ°u Ã½ ráº±ng, á»Ÿ cuá»‘i module ta cÃ³ phÃ©p toÃ¡n concatenate, tá»©c lÃ  cÃ¡c feature-maps cá»§a toÃ n bá»™ layer convolution Ä‘á»u pháº£i cÃ³ cÃ¹ng size (tá»©c lÃ  width vÃ  height)\nKiáº¿n trÃºc GoogLeNet GoogLeNet sá»­ dá»¥ng tá»•ng cá»™ng 9 Inception module, gá»“m cÃ³ 22 layer cÃ³ trá»ng sá»‘ (tÃ­nh cáº£ pooling lÃ  27), Ä‘Æ°á»£c tÃ³m táº¯t qua báº£ng sau:\nMá»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t khi train GoogLeNet lÃ  cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng cÃ¡c auxiliary classifiers (xem hÃ¬nh bÃªn dÆ°á»›i). CÃ¡c thÃ nh pháº§n nÃ y sáº½ khÃ¡ giá»‘ng nhÆ° giá»‘ng há»‡t vá»›i pháº§n cuá»‘i cá»§a mÃ´ hÃ¬nh (bá»™ classifier). NhÆ° váº­y, ta cÃ³ thá»ƒ xem GoogLeNet cÃ³ 3 bá»™ classifier. Auxiliary classifiers Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i cÃ¡c Ã½ nghÄ©a nhÆ° sau:\nHáº¡n cháº¿ vanishing gradient TÄƒng regularization LÆ°u Ã½:\nLoss khi huáº¥n luyá»‡n sáº½ cá»™ng loss cá»§a cáº£ ba láº¡i vá»›i nhau. Khi test thÃ¬ ta thÆ°á»ng chá»‰ quan tÃ¢m Ä‘áº¿n bá»™ classifer cuá»‘i cÃ¹ng. Má»™t cÃ¡ch lÃ m khÃ¡c lÃ  ta xÃ i cáº£ 3, sau Ä‘Ã³ láº¥y káº¿t quáº£ trung bÃ¬nh. Nguá»“n: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t GoogLeNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau. Trong cÃ¡ch cÃ i Ä‘áº·t nÃ y, mÃ¬nh sáº½ bá» qua auxiliary classifiers.\nTÃ i liá»‡u tham kháº£o Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"http://example.org/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giá»›i thiá»‡u Dá»±a trÃªn sá»± thÃ nh cÃ´ng cá»§a AlexNet vÃ o nÄƒm 2012, nhiá»u nghiÃªn cá»©u Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh nháº±m tÃ¬m ra cÃ¡c phÆ°Æ¡ng phÃ¡p hay kiáº¿n trÃºc má»›i Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n, vÃ­ dá»¥ nhÆ°:\nThay Ä‘á»•i (tÄƒng, giáº£m) kÃ­ch thÆ°á»›c cá»§a conv filter Thay Ä‘á»•i stride, padding cá»§a conv layer Train vÃ  test trÃªn cÃ¡c input vá»›i nhiá»u kÃ­ch thÆ°á»›c áº£nh khÃ¡c nhau Trong nÄƒm 2014, VGG lÃ  má»™t trong nhá»¯ng káº¿t quáº£ nghiÃªn cá»©u ná»•i báº­t nháº¥t, vÃ  nÃ³ táº­p trung vÃ o má»™t váº¥n Ä‘á» khÃ¡c vá»›i cÃ¡c hÆ°á»›ng trÃªn lÃ  Ä‘á»™ sÃ¢u (depth, hay lÃ  sá»‘ layer) cá»§a mÃ´ hÃ¬nh. VGG Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c káº¿t quáº£ ráº¥t tá»‘t trÃªn táº­p ImageNet vÃ  cÃ¡c dataset khÃ¡c, trong cÃ¡c task nhÆ° classification, localization,â€¦\nNgoÃ i ra, ta cÃ³ thá»ƒ Ä‘Æ°a ra nháº­n xÃ©t nhÆ° sau vá» AlexNet:\nDÃ¹ AlexNet Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c CNN cÃ³ thá»ƒ Ä‘áº¡t Ä‘á»™ hiá»‡u quáº£ tá»‘t, nÃ³ láº¡i khÃ´ng cung cáº¥p má»™t khuÃ´n máº«u nÃ o cho viá»‡c nghiÃªn cá»©u, thiáº¿t káº¿ cÃ¡c máº¡ng má»›i. Theo thá»i gian, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ thay Ä‘á»•i suy nghÄ© tá»« quy mÃ´ nhá»¯ng neuron riÃªng láº» sang cÃ¡c táº§ng, rá»“i sau Ä‘Ã³ lÃ  cÃ¡c khá»‘i gá»“m cÃ¡c táº§ng láº·p láº¡i theo khuÃ´n máº«u. Kiáº¿n trÃºc cá»§a VGG lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc phá»• biáº¿n Ä‘áº§u tiÃªn Ä‘Æ°á»£c xÃ¢y dá»±ng theo tiÃªu chÃ­ nhÆ° váº­y.\nVGG block Äiá»ƒm ná»•i báº­t cá»§a VGG lÃ  ta chá»‰ dÃ¹ng duy nháº¥t má»™t kÃ­ch thÆ°á»›c filter trong má»i conv layer lÃ  3 x 3, vÃ  ta dáº§n tÄƒng Ä‘á»™ sÃ¢u cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡c conv layer. HÆ¡n ná»¯a, ta cÃ²n Ã¡p dá»¥ng nhiá»u conv layer liá»n nhau rá»“i má»›i dÃ¹ng Ä‘áº¿n max pooling (ta cÃ³ thá»ƒ gá»i Ä‘Ã¢y lÃ  VGG block)\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n má»™t váº¥n Ä‘á» cho cÃ¡ch Ã¡p dá»¥ng nÃ y nhÆ° sau: Viá»‡c dÃ¹ng nhiá»u conv layer 3 x 3 liá»n nhau nhÆ° váº­y so vá»›i dÃ¹ng má»™t conv layer vá»›i filter lá»›n hÆ¡n (vÃ­ dá»¥ 7 x 7) nhÆ° háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o thá»i Ä‘iá»ƒm trÆ°á»›c Ä‘Ã³ thÃ¬ cÃ³ gÃ¬ â€œtá»‘tâ€ hÆ¡n, khi mÃ  features map sau cÃ¹ng ta thu Ä‘Æ°á»£c cÃ³ thá»ƒ cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c? Äá»ƒ tráº£ lá»i, ta cÃ³ 2 Ã½ chÃ­nh nhÆ° sau: Giáº£m sá»‘ lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh (Ä‘áº·t tÃ­nh lÃ  biáº¿t nhaa ğŸ˜œ) DÃ¹ng nhiá»u conv layer thÃ¬ ta cÃ³ kháº£ nÄƒng sáº½ phÃ¡t hiá»‡n Ä‘Æ°á»£c nhiá»u feature cÃ³ Ã­ch hÆ¡n, tá»« Ä‘Ã³ decision function sáº½ ok hÆ¡n. NgoÃ i ra, VGG block sá»­ dá»¥ng padding 1 (giá»¯ nguyÃªn kÃ­ch thÆ°á»›c input), theo sau Ä‘Ã³ lÃ  max pooling vá»›i pool size 2 x 2 vÃ  stride 2 (giáº£m kÃ­ch thÆ°á»›c input Ä‘i má»™t ná»­a). Kiáº¿n trÃºc cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ nhÆ° hÃ¬nh bÃªn dÆ°á»›i:\nVGG block Nguá»“n: Dive intro AI Trong cÃ¡c bÃ i toÃ¡n Ã¡p dá»¥ng VGG, Ä‘Ã´i khi ta cÃ³ thá»ƒ gáº·p VGG block vá»›i má»™t conv layer 1 x 1 á»Ÿ trÆ°á»›c max pooling. Block dáº¡ng nÃ y thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i má»¥c Ä‘Ã­ch chÃ­nh lÃ  bá»• sung thÃªm má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh ná»¯a. Tuy nhiÃªn, trong thá»±c nghiá»‡m thÃ¬ cÃ¡c tÃ¡c giáº£ Ä‘Ã£ cho tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng block dáº¡ng nÃ y khÃ´ng hiá»‡u quáº£ hÆ¡n so vá»›i toÃ n cÃ¡c conv layer vá»›i filter 3 x 3 (cÃ¹ng sá»‘ lÆ°á»£ng layer).\nKiáº¿n trÃºc VGG Báº±ng cÃ¡ch káº¿t há»£p nhiá»u VGG block vá»›i nhau, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ táº¡o ra nhiá»u phiÃªn báº£n VGG khÃ¡c nhau, vá»›i sá»‘ layer cÃ³ trá»ng sá»‘ lÃ  trong Ä‘oáº¡n 11 - 19. Trong paper VGG, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ Ä‘Æ°a ra 6 kiáº¿n trÃºc vá»›i Ä‘á»™ sÃ¢u tÄƒng dáº§n vÃ  tiáº¿n hÃ nh so sÃ¡nh vá»›i nhau. Äiá»ƒm chung cá»§a cÃ¡c kiáº¿n trÃºc nÃ y lÃ  pháº§n fully connected Ä‘á»u cÃ³ 3 layer, vÃ  toÃ n bá»™ layer Ä‘á»u sá»­ dá»¥ng activation ReLU.\nCÃ¡c phiÃªn báº£n VGG Ta cÃ³ má»™t sá»‘ nháº­n xÃ©t nhÆ° sau:\nSá»‘ lÆ°á»£ng conv layer trong cÃ¡c VGG block cá»§a cÃ¡c phiÃªn báº£n lÃ  tÄƒng dáº§n. Äá»ƒ Ã½ Ä‘áº¿n B, C vÃ  D thÃ¬: C = B + conv layer 1 x 1 D = C + Ä‘á»•i conv layer 1 x 1 thÃ nh 3 x 3 Khi test, ta cÃ³ B \u0026lt; C \u0026lt; D. Do Ä‘Ã³, viá»‡c thÃªm phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh báº±ng conv layer 1 x 1 giÃºp mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n, nhÆ°ng nÃ³ váº«n khÃ´ng báº±ng vá»›i viá»‡c lÃ  ta thÃªm luÃ´n conv layer 3 x 3. Äá»™ rá»™ng (sá»‘ channel) trong tá»«ng block Ä‘Æ°á»£c tÄƒng theo bá»™i 2. Ã tÆ°á»Ÿng nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t rá»™ng rÃ£i trong thá»i Ä‘iá»ƒm nÃ y vÃ  cáº£ vá» sau Äá»ƒ háº¡n cháº¿ overfitting, ta cÃ³ thá»ƒ sá»­ dá»¥ng thÃªm dropout cho hai táº§ng fully connected Ä‘áº§u tiÃªn. Hai kiáº¿n trÃºc phá»• biáº¿n nháº¥t trong viá»‡c Ã¡p dá»¥ng VGG vÃ o cÃ¡c bÃ i toÃ¡n khÃ¡c lÃ  D (VGG16) vÃ  E (VGG19). Äá»ƒ trá»±c quan hÆ¡n, ta cÃ³ thá»ƒ thá»ƒ biá»ƒu diá»…n VGG16 nhÆ° sau:\nNguá»“n: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t VGG báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau\nTÃ i liá»‡u tham kháº£o Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"http://example.org/post/vgg/","title":"VGG (2014)"}]
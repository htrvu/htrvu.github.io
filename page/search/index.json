[{"content":"Giới thiệu Qua các mô hình đã được giới thiệu như VGG, GoogLeNet hay ResNet thì ta thấy rằng chúng đều được phát triển theo hướng tăng dần độ sâu và độ phức tạp tính toán của mô hình để đạt được độ chính xác cao hơn, kể từ khi AlexNet được công bố. Số lượng tham số của chúng là rất lớn.\nTuy nhiên, các ứng dụng AI trong thực tế như robotics, xe tự hành thì các phép tính toán của mô hình cần được thực hiện trong một khoảng thời gian giới hạn, cùng với tài nguyên phần cứng hạn chế. Do đó, ta phải đối mặt với một trade-off giữa độ chính xác và độ trễ, kích thước mô hình.\nVào thời điểm này, có 2 hướng giải pháp chính để có thể đưa các mô hình vào ứng dụng thực tế như sau:\nNén các mô hình phức tạp lại thông qua các phương pháp như lượng tử hóa (quantization), hashing, cắt tỉa mô hình Xây dựng và huấn luyên các mô hình nhỏ, độ phức tạp thấp ngay từ đầu. MobileNet được phát triển theo hướng thứ 2, trong đó, nó tập trung vào các yếu tố:\nVừa đảm bảo kích thước mô hình đủ nhỏ, tốc độ suy diễn đủ nhanh (độ trễ thấp) và với độ chính xác đủ cao. Cung cấp hai siêu tham số cho phép ta điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước mô hình: width multiplier (liên quan đến số channel trong từng layer) và resolution multiplier (width và height trong từng layer) Depthwise separable convolutions MobileNet được xây dựng từ các layer convolution khá đặc biệt, chúng được gọi là depthwise separable convolutions.\nDạng layer này đã được giới thiệu trước đó và ứng dụng vào mô hình Xception, một phiên bản cải tiến của InceptionV3. Depthwise separable convolution được tạo ra từ hai phép toán:\nDepthwise convolution: Áp dụng từng filter cho từng channel của input. Nếu input có bao nhiêu channel thì ta sẽ có bấy nhiêu filter. Pointwise convolution: Đây thực chất là convolution layer thông thường với filter 1 x 1. Nó được dùng để tổng hợp các kết quả từ phép toán depthwise convolution và tính ra output, thông qua các phép toán tổ hợp tuyến tính. Nguồn: Research Gate Ta có thể thấy ngay sự khác biệt giữa depthwise separable convolution và convolution thông thường như sau:\nConvolution thông thường: Mỗi filter sẽ tương tác với toàn bộ channel của input. Giả sử input của ta là $D_F \\times D_F \\times M$, một filter $3 \\times 3$ được áp dụng thì filter sẽ trở thành $3 \\times 3 \\times M$, ta thực hiện convolution trên từng channel và sau đó cộng $M$ ma trận lại với nhau, thu được kết quả $D_F \\times D_F$. Nếu dùng $N$ filter thì ta sẽ có kết quả cuối cùng là $D_F \\times D_F \\times N$ Depthwise separable convolution: Ban đầu, các channel được tính toán độc lập với từng filter riêng, sau đó mới kết hợp lại sau nhờ vào pointwise convolution. Với input $D_F \\times D_F \\times M$ thì khi đưa qua depthwise convotution, ta sẽ có kết quả là $D_F \\times D_F \\times M$. Nếu pointwise convolution sử dụng $N$ filter $1 \\times 1$ thì ta có kết quả cuối cùng là $D_F \\times D_F \\times N$ Vấn đề đặt ra là tại sao sử dụng depthwise separable convolution lại có thể giúp cho MobileNet gọn nhẹ hơn, tính toán nhanh hơn và có độ chính xác đủ tốt, không hề kém cạnh các mô hình to lớn khác. Ta sẽ đặt tính một chút:\nGiả sử input của ta là feature maps $\\bold{F}: D_F \\times D_F \\times M$, output cuối cùng là $\\bold{G}: D_F \\times D_F \\times N$.\nVới convolution thông thường: Giả sử ta dùng $N$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding sao cho kích thước width và height không đổi. Khi đó, độ phức tạp tính toán sẽ là\n$$ O_1 = D_K \\times D_K \\times D_F \\times D_F \\times M \\times N $$\n, vì với mỗi filter thì: mỗi lần tính toán ta phải thực hiện $D_K \\times D_K$ phép toán nhân, sau đó cộng chúng lại; ta tính tại $D_F \\times D_F$ vị trí trên $M$ channel của input, và ta sử dụng $N$ filter.\nVới depthwise separable convolution: Ở bước depthwise convolution thì ta dùng $M$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding phù hợp. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times M $$\n, vì ta chỉ đơn giản là áp dụng đơn lẻ từng filter cho từng channels\nVới pointwise convolution thì ta dùng $N$ filter $\\bold{K}: 1 \\times 1$, stride là 1, padding 0. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_F \\times D_F \\times M \\times N $$\nDo đó, ta có độ phức tạp tính toán là\n$$ O_2 = D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N $$\nLúc này, đem chia cho nhau thì ta có tỉ lệ\n$$ \\frac{O_2}{O_1}=\\frac{D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N}{D_K \\times D_K \\times D_F \\times D_F \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_K^2} $$\nNhư vậy, độ phức tạp tính toán khi sử dụng depthwise separable convolution đã giảm khoảng $D_K^2$ lần so với convolution thông thường. MobileNet sử dụng các filter $3 \\times 3$, từ đó giảm được độ phức tạp tính toán đi khoảng 8 đến 9 lần, trong khi độ chính xác chỉ giảm đi một phần nhỏ.\nSiêu tham số điều chỉnh trade-off Để có thể hỗ trợ tốt hơn việc áp dụng MobileNet vào các thiết bị biên trong các ứng dụng thực tế, các tác giả còn cung cấp thêm cho ta hai siêu tham số để điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước của mô hình\nWidth multiplier Tham số width multiplier (kí hiệu là $\\alpha$) sẽ tác động lên giá trị số channel của các layer. Với những công thức ở trên thì số channel chính là $M$ và $N$. Giá trị $\\alpha \\in (0, 1]$ và ta thường đặt là $1, 0.75, 0.5, 0.25.$ Khi đó, thứ thật sự được thay đổi chính là số lượng filter mà ta dùng trong các phép toán pointwise convolution.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng width multiplier $\\alpha$ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times \\alpha M + D_F \\times D_F \\times \\alpha M \\times \\alpha N $$\nResolution multiplier Tham số resolution muiltiplier (kí hiệu là $\\rho$ ) liên quan đến kích thước width và height (chính là $D_F$ trong các công thức trên). Miền giá trị của nó cũng sẽ tương tự như $\\alpha$. Thực chất thì ta sẽ chỉ áp dụng nó vào input ban đầu của mô hình (ảnh). Các kích thước input mà ta thường sử dụng với mô hình MobileNet là 224, 192, 160 hoặc 128.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng thêm resolution multiplier $\\rho$ là\n$$ D_K \\times D_K \\times \\rho D_F \\times \\rho D_F \\times \\alpha M + \\rho D_F \\times \\rho D_F \\times \\alpha M \\times \\alpha N $$\nKiến trúc mô hình Depthwise Separable block Mô hình MobileNet V1 được tạo thành bởi các thành phần chính là depthwise separable block. Chúng bao gồm hai phép toán như ta đã đề cập là depthwise convolution và pointwise convolution. Đi kèm với các layer đó là batch norm và activation ReLU.\nNguồn: Research Gate Kiến trúc MobileNet MobileNet sử dụng tất cả gồm 13 depthwise separable block. Tổng thể kiến trúc của MobileNet được thể hiện ở bảng sau:\nTrong đó:\nConv dw là depthwise convolution. Ta có thể thấy các filter shape của chúng luôn có cùng số channel trong input size. Các conv layer với filter shape $1 \\times 1$ chính là pointwise convolution. s1 tức là stride = 1, tương tự với s2. Toàn bộ các conv layer trong mô hình đều có padding sao cho kích thước width và height của input và output của layer đó là như nhau. Note:\nTrong bảng trên có vẻ có một chỗ gõ nhầm. Để ý đến layer “Conv dw / s2” đầu tiên từ dưới lên, nếu đây là s2 thì input size của layer “Conv / s1” tiếp theo phải bị giảm size chứ không phải $7 \\times 7$. Do đó, trong cài đặt thì mình đổi lại thành “Conv dw / s1”. Cài đặt Các bạn có thể tham khảo phần cài đặt MobileNet bằng Tensorflow và Pytorch tại repo sau\nTài liệu tham khảo Paper MobileNet: https://arxiv.org/abs/1704.04861 ","date":"2023-02-08T18:13:00+07:00","permalink":"http://example.org/post/mobilenet/","title":"MobileNet (2017)"},{"content":"Giới thiệu Các mô hình thuộc họ Inception-ResNet được phát triển dựa trên ý tưởng là kết hợp skip connection vào các Inception block (các ý tưởng từ ResNet và GoogLeNet). Vì paper này chỉ mang tính thực nghiệm là chính nên mình sẽ không trình bày chi tiết 👀.\nKiến trúc mô hình Inception-ResNet V1 Về mặt tổng quan, Inception-ResNet V1 có kiến trúc như sau:\nKiến trúc Inception-ResNet V1\nStem của Inception-ResNet V1\nTa sẽ đề cập đến các loại Inception-ResNet block và Reduction:\nInception-ResNet block: Ta thấy rằng phần “Inception” trong các block này là đơn giản hơn khá nhiều so với các Inception block nguyên mẫu. Inception-ResNet-A V1\nInception-ResNet-B V1\nInception-ResNet-C V1\nReduction: Chúng thực hiện nhiệm vụ giảm kích thước (width, height) của các tensor đi một nửa. Kiến trúc của chúng rất giống với các Inception block Reduction-A\nReduction-B\nInception-ResNet V2 Phiên bản thứ hai của Inception-ResNet có kiến trúc tổng thể giống hệt với phiên bản đầu tiên, ta chỉ có một số thay đổi ở các block Inception-ResNet và Reduction\nTài liệu tham khảo Paper Inception-ResNet: https://arxiv.org/abs/1602.07261 ","date":"2023-02-08T18:03:46+07:00","permalink":"http://example.org/post/inception-resnet/","title":"Inception-Reset (2016)"},{"content":"Giới thiệu Ta biết rằng, việc tạo ra các mô hình có độ sâu lớn (nhiều layer) chưa chắc đã mang lại hiệu quả tốt hơn những mô hình “cạn” hơn. Ví dụ, với tập CIFAR10 thì ta có một kết quả thử nghiệm cho thấy rằng mô hình sâu hơn lại có độ hiệu quả kém hơn:\nĐối với việc huấn luyện các mô hình có độ sâu lớn thì ta có thể sẽ bị gặp phải các vấn đề sau:\nOverfitting Vanishing/exploding gradient. Vấn đề này thì ta đã có một số cách giải quyết phổ biến như thay đổi activation function, các phương pháp khởi tạo trọng số Đặc biêt hơn là vấn đề degradation: Accuracy tăng dần cho đến một độ sâu nhất định thì ngừng tăng (bão hòa) rồi sau đó sẽ giảm dần.\nLưu ý. Nhiều trường hợp degradation không phải do overfitting gây ra.\nDegradation cho chúng ta thấy rằng việc tối ưu các mô hình có độ sâu lớn là không hề dễ dàng.\nTrong quá trình xây dựng mô hình, từ một mô hình ban đầu, ta thêm một số layer vào thì tất nhiên là ta mong rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc. Tuy nhiên, khi xảy ra degradation thì mong muốn đó đã không thể thành sự thật được. 😀\nResNet được công bố nhằm giải quyết vấn đề degradation đối với các mô hình có độ sâu lớn. Khi nhắc đến ResNet thì ta sẽ lập tức nghĩ đến những mô hình với độ sâu rất khủng, thậm chỉ là lên đến 100, 200 layer.\nHàm phần dư và skip connection Nhắc lại về cái mong muốn ở trên, rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc, ta có thể nghĩ ngay đến một phương pháp cực kì đơn giản: các layer phía sau sẽ là identity mapping, tức là input và output của nó sẽ giống hệt nhau. Với cách làm này thì hiển nhiên là ta đạt được mong muốn rồi, vì độ hiệu quả của mô hình mới và mô hình gốc sẽ y hệt nhau.\nTuy nhiên, nếu chỉ muốn ngang đó thôi thì thêm layer vào làm gì :v Ta muốn đạt được kết quả tốt hơn! Các tác giả của paper ResNet giới thiệu một phương pháp gọi là deep residual learning (học phần dư).\nGiả sử ta có một block B các layer, input của nó là $x$. Với một mô hình thông thường, ta sẽ “học” một hàm số đầu ra mong muốn là $f(x)$. Lúc này, ban đầu thì ta hoàn toàn chưa có một thông tin gì về $f(x)$ cả, việc “học” sẽ xuất phát từ một đại lượng ngẫu nhiên. Đối với phương pháp deep residual learning, output của block B sẽ có dạng $h(x) = x + f(x)$, và ta sẽ đi học $f(x)$. Lúc này, ta gọi $f(x)$ là residual function (hàm phần dư) Ta có các nhận xét sau: Trong deep residual learning, ta đã có sự “gợi ý” cho hàm mong muốn thông qua giá trị input $x$. Đối với thông thường thì không có sự gợi ý nào, học từ một cái hoàn toàn ngẫu nhiên. Việc học $f(x)$ như là một hàm phần dư là dễ hơn so với việc học hàm mong muốn. Nếu identity mapping là kết quả tối ưu thì ta có luôn $f(x)=0$ Về mặt bản chất, với phương pháp deep residual learning, ta đã tác động vào lớp hàm chứa hàm mong muốn, sao cho lớp mới là lớn hơn (bao gồm) lớp cũ. Lớp hàm chứa hàm mong muốn của phương pháp thông thường và phương pháp deep residual learning Nguồn: Dive into AI Như vậy, điểm nhấn của ResNet là ta đi học các hàm phần dư, thông qua việc “gợi ý” cho hàm số mong muốn một giá trị bằng với giá trị input. Trong cài đặt, thao tác này được thực hiện thông qua một kết nối gọi là skip connection. Ta sẽ cộng input $x$ vào output của block thông thường.\nĐối với việc cộng như vậy thì thực chất là ta đang đi cộng hai ma trận. Khi đó, một vấn đề có thể nảy sinh là về shape của chúng, mà thường là số channel. Nếu số channel không khớp thì ta cần phải tiến hành “điều chỉnh”. Vì các block được sử dụng thường gồm các conv layer với stride và padding phù hợp để giữ nguyên width và height nên ta sẽ chỉ xét đến số channel. Nếu số channel của $x$ và output ban đầu là như nhau thì ta gọi skip connection này là identity skip connection Ngược lại, ta sẽ dùng conv layer $1 \\times 1$ để điều chỉnh số channel của $x$. Lúc này, skip connection được gọi là projection skip connection. Sử dụng skip connection (identity) Nguồn: Dive into AI Ngoài ra, ta còn có một điểm mạnh quan trọng của skip connection là nó giúp cho gradient được lan truyền tốt hơn trong quá trình backpropagation, từ đó góp phần làm giảm hiện tượng vanishing gradient\nQuan sát hình phía trên, ta thấy rằng khi backpropagation thì layer ở ngay trước block nhận được gradient từ hai layer phía sau nó (một layer liền trước nó và một layer được kết nối thông qua skip connection). Residual block Residual block (khối phần dư) được tạo ra bằng cách thêm một skip connection vào một block thông thường trong các mô hình CNN như VGG block. Ta có hai loại residual block như sau:\nBasic: Các conv layer trong block này có filter $3 \\times 3$. Block dạng này thường được dùng cho các mô hình có độ sâu vừa phải. Bottleneck: Nhằm giảm bớt số lượng tham số của các mô hình có độ sâu lớn, các tác giả sử dụng dạng block này với hai conv layer $1 \\times 1$ với vai trò là giảm/tăng số channel, tạo ra một hình dáng giống như nút thắt cổ chai. Hai layer conv $1 \\times 1$ như vậy được gọi là bottleneck layer. Để cho dễ hình dùng thì ta có thể xem đây như là thao tác “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán). Basic Residual Block và Bottleneck Residual Block Lưu ý. Có một chi tiết này khá quan trọng trong cài đặt là ta sẽ tiến hành cộng ma trận trước rồi mới đưa kết quả qua activation function.\nNgoài ra, ta còn có 2 loại skip connection là identity và projection. Lúc này, tùy vào số channels của input và output ban đầu có khớp hay không mà block tương ứng sẽ chứa loại skip connection phù hợp.\nBasic residual block với identity và projection skip connection Nguồn: Dive into AI Kiến trúc ResNet ResNet được tạo nên bằng cách sử dụng nhiều residual block liên tiếp nhau, tương tự như những gì mà GoogLeNet hay VGG đã thực hiện, Các tác giả của paper ResNet tạo ra nhiều phiên bản ResNet khác nhau với độ sâu tăng dần.\nDựa vào số lượng layer thì ta sẽ có tên các mô hình như ResNet18 (18 layer có trọng số), ResNet34,… Các phiên bản ResNet Ta có một số nhận xét như sau:\nCác phiên bản có độ sâu lớn như 50, 101 và 152 sử dụng Bottleneck Residual Block. Với 18 và 34 thì chúng dùng Basic block Trong các kiến trúc trên thì ta sử dụng cả 2 loại skip connection: identity và projection Ví dụ, với ResNet18 thì trong cụm Basic block đầu tiên của cụm conv3_x sẽ có số channel là 64, còn output ban đầu của block này thì là 128 nên ta phải áp dụng projection vào input Đối với height và width thì các tác giả cho biết việc downsampling input sẽ được thực hiện tại conv layer đầu tiên của các cụm conv3_x, conv4_x, conv5_x (với stride là 2). Các conv layer khác thì đều có stride 1. Ta sẽ cần chú ý đến chi tiết này khi tiến hành cài đặt ResNet. Cài đặt Các bạn có thể tham khảo phần cài đặt ResNet bằng Tensorflow và Pytorch tại repo sau\nTài liệu tham khảo Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into AI, ResNet ","date":"2023-02-08T17:41:09+07:00","permalink":"http://example.org/post/resnet/","title":"Resnet (2015)"},{"content":" Cá nhân mình thấy GoogLeNet là một paper khó đọc. Khi viết ra bài này thì mình vẫn đang cảm thấy hơi lú về nội dung của nó 😀\nGiới thiệu Từ khi AlexNet được công bố vào năm 2012 và đặt nền tảng cho các mạng Deep CNN, GoogLeNet, hay Inception V1 (2014), là một trong những kiến trúc có cách thiết kế rất thú vị khi nó tận dụng hiệu quả các conv layer, đặt nền móng cho nhiều mô hình sau này.\n“Inception” có thể dịch là “sự khởi đầu”, nghe có vẻ rất hợp lý 😀. Ngoài ra, ở trong bài viết về VGG, mình có đề cập đến vấn đề thiết kế kiến trúc mô hình theo hướng có sự lặp lại các khuôn mẫu. GoogLeNet cũng sẽ được thiết kế như vậy.\nLưu ý. Tên của mô hình này là GoogLeNet, không phải GoogleNet =)) Tác giả cho biết ý nghĩa của cái tên này là “This name is a homage to Yann LeCuns pioneering LeNet 5 network.”\nCác tác giả của paper GoogLeNet có tóm tắt các mục tiêu của nghiên cứu như sau:\nNâng cao khả năng tận dụng tài nguyên tính toán Cho phép tăng chiều rộng và chiều sâu của kiến trúc mô hình mà vẫn đảm bảo được độ phức tạp tính toán là ở mức chấp nhận được. GoogLeNet thật sự đã đạt được những điều đó, và nó được xây dựng dựa trên nguyên lý Hebbian: “neurons that fire together, wire together”. Paper GoogLeNet đã đưa ra các nền tảng lý thuyết rất “căng thẳng” để cho thấy rằng mô hình CNN có thể hoạt động “đủ tốt”, điều mà ta chưa được biết đến ở trong các paper trước đó 😀\nNgoài ra, có một quan điểm khá thú vị khi mô tả về kiến trúc của GoogLeNet như sau: Khi xây dựng kiến trúc CNN, thay vì phải suy nghĩ xem trong các mạng CNN ta nên áp dụng filter với kích thước bao nhiêu, hãy áp dụng luôn nhiều filter với kích thước khác nhau và tổng hợp kết quả lại 😜\nMình cũng có đồng tình với quan điểm, nhưng khi đọc paper thì có vẻ là các tác giả cũng đã tiến hành phân tích và thử nghiệm nhiều lắm mới cho ra được cái kiến trúc như vậy. Kết nối thưa và CNN Đầu tiên, tại thời điểm trước khi Inception được công bố, chúng ta có thể cải thiện một mô hình DNN bằng những cách như sau:\nTăng chiều sâu của mô hình (tức là số layer) Tăng chiều rộng (số channel trong mỗi layer) Tuy nhiên, với hai cách trên thì sẽ có những vấn đề mà ta cần lưu tâm là hiện tượng overfitting và sự gia tăng độ phức tạp của mô hình.\nCác tác giả có đề cập đến một hướng đi có thể giảm bớt hai vấn đề trên là sử dụng kiến trúc kết nối thưa (sparsely connected architectures). Ta có thể hiểu đơn giản như sau:\nVới hai fully connected layer liên tiếp nhau, mỗi neuron trong layer sau sẽ kết nối với tất cả các unit trong layer trước. Nếu như ta thay đổi đi một chút, mỗi neuron trong layer sau sẽ chỉ kết nối đến một vài unit trong layer trước, kiến trúc có được sẽ trở nên “thưa” hơn. Kết nối thưa trong fully connected layer Đối với conv layer thì ta đã đạt được tính chất thưa như vậy. Ta biết rằng, với mỗi phần tử trên feature map của layer hiện tại thì ta tính nó dựa vào một vùng nhỏ trên feature map output của layer trước đó. Giả sử hai layer này đều chỉ có 1 channel thì ta có thể biểu diễn nó như hình bên dưới: Kết nối thưa trong conv layer Kiến trúc thưa được các tác giả mô tả là mô phỏng lại hệ thống sinh học (ví dụ như khi ta nhìn vào một đối tượng thì ta thường chỉ chú ý một số điểm trên đối tượng đó thôi, khó mà chú ý tất cả được).\nNgoài ra, có một nền tàng lý thuyết rất trâu bò về kiến trúc thưa của Arora như sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Thật sự mình cũng chưa hiểu hết ý của câu trên… Nhưng đại ý của nó có thể là nếu ta biểu diễn được phân phối các điểm dữ liệu của một dataset bằng một kiến trúc DNN lớn và thưa thì từ đó ta có thể xây nên được một kiến trúc tối ưu (về cả độ chính xác và độ phức tạp), bằng cách xây từng layer một 😀\nTuy nhiên, với các tài nguyên phần cứng trong thời gian này thì rất khó để ta có thể tính toán trên các mạng DNN thưa. Do đó, các tác giả đi theo hướng tìm một mô hình là có tận dụng một số thông tin về tính chất thưa nhưng vẫn thực hiện tính toán trên các ma trận đầy đủ. Đấy chính là hướng sử dụng các phép toán convolution!.\nCũng vì lý do này, ở phần kiến trúc của GoogLeNet thì ta sẽ thấy nó chỉ có duy nhất một fully connected layer để sinh ra output, còn lại chỉ toàn conv layer thôi 😗 Nói đến việc áp dụng các filter trong conv layer, các tác giả cho rằng:\nMỗi phần tử trong feature map của một layer sẽ có mối tương quan với một vùng nào đó trên ảnh input (receptive field). Ta sẽ gom các phần tử cùng tương quan với một vùng vào cùng một cụm. Để ý rằng, trong những layer ở gần ảnh input thì ta sẽ có nhiều cụm và kích thước mỗi cụm là nhỏ. Càng đi qua các layer CNN thì số lượng cụm sẽ ít lại và kích thước cụm sẽ lan rộng ra. Tất nhiên là vẫn có thể có những cụm có kích thước nhỏ tại những layer đó. Do vậy, ta nên có các filter với kích thước lớn hơn để học các đặc trưng tại các cụm lớn, đồng thời cũng cần có filter kích thước nhỏ đối với các cụm nhỏ hơn. Qua một loạt các thử nghiệm, các tác giả đã chọn 3 filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Một khối chứa 3 filter trên được đặt tên là Inception module.\nBây giờ quay lại với lý thuyết của Arora, ở ý xây dựng mạng tối ưu qua từng layer một. GoogLeNet được tạo nên bằng đúng ý tưởng như vậy, ta nối Inception module - by - Inception module 😀\nP/s: Các bạn có thể tìm đọc phần Motivation trong paper gốc và tự cảm nhận nó nhé.\nInception module Inception module Qua các mô tả ở phần trên, ta có thể liên tưởng đến kiến trúc của Inception module là một thứ gì đó giống với cái ở hình (a), với đủ 3 loại filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Các tác giả dùng thêm cả một layer max pooling trong đó nữa, với lí do rất đơn giản là vì ở thời điểm đó thì max pooling thường mang lại hiệu quả tốt trong các kiến trúc mạng CNN =))\nTruy nhiên, cách cài đặt như hình (a) sẽ dẫn đến số lượng tham số của mô hình là rất lớn. Thay vào đó, ta có thể tạo ra kiến trúc dạng “bottleneck” bằng cách sử dụng thêm các layer convolution $1 \\times 1$ như hình (b), nhằm mục đích chính là giảm số channel. Số lượng phép tính lúc này sẽ được giảm một cách đáng kể.\nLưu ý rằng, ở cuối module ta có phép toán concatenate, tức là các feature-maps của toàn bộ layer convolution đều phải có cùng size (tức là width và height)\nKiến trúc GoogLeNet GoogLeNet sử dụng tổng cộng 9 Inception module, gồm có 22 layer có trọng số (tính cả pooling là 27), được tóm tắt qua bảng sau:\nMột điểm đặc biệt khi train GoogLeNet là các tác giả sử dụng các auxiliary classifiers (xem hình bên dưới). Các thành phần này sẽ khá giống như giống hệt với phần cuối của mô hình (bộ classifier). Như vậy, ta có thể xem GoogLeNet có 3 bộ classifier. Auxiliary classifiers được sử dụng với các ý nghĩa như sau:\nHạn chế vanishing gradient Tăng regularization Lưu ý:\nLoss khi huấn luyện sẽ cộng loss của cả ba lại với nhau. Khi test thì ta thường chỉ quan tâm đến bộ classifer cuối cùng. Một cách làm khác là ta xài cả 3, sau đó lấy kết quả trung bình. Nguồn: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png Cài đặt Các bạn có thể tham khảo phần cài đặt GoogLeNet bằng Tensorflow và Pytorch tại repo sau. Trong cách cài đặt này, mình sẽ bỏ qua auxiliary classifiers.\nTài liệu tham khảo Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"http://example.org/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giới thiệu Dựa trên sự thành công của AlexNet vào năm 2012, nhiều nghiên cứu đã được tiến hành nhằm tìm ra các phương pháp hay kiến trúc mới để đạt được kết quả tốt hơn, ví dụ như:\nThay đổi (tăng, giảm) kích thước của conv filter Thay đổi stride, padding của conv layer Train và test trên các input với nhiều kích thước ảnh khác nhau Trong năm 2014, VGG là một trong những kết quả nghiên cứu nổi bật nhất, và nó tập trung vào một vấn đề khác với các hướng trên là độ sâu (depth, hay là số layer) của mô hình. VGG đã đạt được các kết quả rất tốt trên tập ImageNet và các dataset khác, trong các task như classification, localization,…\nNgoài ra, ta có thể đưa ra nhận xét như sau về AlexNet:\nDù AlexNet đã chứng minh được CNN có thể đạt độ hiệu quả tốt, nó lại không cung cấp một khuôn mẫu nào cho việc nghiên cứu, thiết kế các mạng mới. Theo thời gian, các nhà nghiên cứu đã thay đổi suy nghĩ từ quy mô những neuron riêng lẻ sang các tầng, rồi sau đó là các khối gồm các tầng lặp lại theo khuôn mẫu. Kiến trúc của VGG là một trong những kiến trúc phổ biến đầu tiên được xây dựng theo tiêu chí như vậy.\nVGG block Điểm nổi bật của VGG là ta chỉ dùng duy nhất một kích thước filter trong mọi conv layer là 3 x 3, và ta dần tăng độ sâu của mô hình bằng các conv layer. Hơn nữa, ta còn áp dụng nhiều conv layer liền nhau rồi mới dùng đến max pooling (ta có thể gọi đây là VGG block)\nCác tác giả có đề cập đến một vấn đề cho cách áp dụng này như sau: Việc dùng nhiều conv layer 3 x 3 liền nhau như vậy so với dùng một conv layer với filter lớn hơn (ví dụ 7 x 7) như hầu hết các mô hình đã được công bố vào thời điểm trước đó thì có gì “tốt” hơn, khi mà features map sau cùng ta thu được có thể có cùng kích thước? Để trả lời, ta có 2 ý chính như sau: Giảm số lượng tham số của mô hình (đặt tính là biết nhaa 😜) Dùng nhiều conv layer thì ta có khả năng sẽ phát hiện được nhiều feature có ích hơn, từ đó decision function sẽ ok hơn. Ngoài ra, VGG block sử dụng padding 1 (giữ nguyên kích thước input), theo sau đó là max pooling với pool size 2 x 2 và stride 2 (giảm kích thước input đi một nửa). Kiến trúc của nó có thể được mô tả như hình bên dưới:\nVGG block Nguồn: Dive intro AI Trong các bài toán áp dụng VGG, đôi khi ta có thể gặp VGG block với một conv layer 1 x 1 ở trước max pooling. Block dạng này thường được sử dụng với mục đích chính là bổ sung thêm một phép biến đổi tuyến tính nữa. Tuy nhiên, trong thực nghiệm thì các tác giả đã cho thấy rằng việc sử dụng block dạng này không hiệu quả hơn so với toàn các conv layer với filter 3 x 3 (cùng số lượng layer).\nKiến trúc VGG Bằng cách kết hợp nhiều VGG block với nhau, các tác giả đã tạo ra nhiều phiên bản VGG khác nhau, với số layer có trọng số là trong đoạn 11 - 19. Trong paper VGG, các tác giả đã đưa ra 6 kiến trúc với độ sâu tăng dần và tiến hành so sánh với nhau. Điểm chung của các kiến trúc này là phần fully connected đều có 3 layer, và toàn bộ layer đều sử dụng activation ReLU.\nCác phiên bản VGG Ta có một số nhận xét như sau:\nSố lượng conv layer trong các VGG block của các phiên bản là tăng dần. Để ý đến B, C và D thì: C = B + conv layer 1 x 1 D = C + đổi conv layer 1 x 1 thành 3 x 3 Khi test, ta có B \u0026lt; C \u0026lt; D. Do đó, việc thêm phép biến đổi tuyến tính bằng conv layer 1 x 1 giúp mô hình hoạt động tốt hơn, nhưng nó vẫn không bằng với việc là ta thêm luôn conv layer 3 x 3. Độ rộng (số channel) trong từng block được tăng theo bội 2. Ý tưởng này được sử dụng rất rộng rãi trong thời điểm này và cả về sau Để hạn chế overfitting, ta có thể sử dụng thêm dropout cho hai tầng fully connected đầu tiên. Hai kiến trúc phổ biến nhất trong việc áp dụng VGG vào các bài toán khác là D (VGG16) và E (VGG19). Để trực quan hơn, ta có thể thể biểu diễn VGG16 như sau:\nNguồn: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png Cài đặt Các bạn có thể tham khảo phần cài đặt VGG bằng Tensorflow và Pytorch tại repo sau\nTài liệu tham khảo Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"http://example.org/post/vgg/","title":"VGG (2014)"}]
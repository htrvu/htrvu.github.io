[{"content":"Note. Ná»™i dung bÃ i viáº¿t nÃ y pháº§n lá»›n Ä‘Æ°á»£c trÃ­ch tá»« bÃ¡o cÃ¡o Ä‘á»“ Ã¡n mÃ´n há»c Há»c thá»‘ng kÃª cá»§a nhÃ³m mÃ¬nh vÃ  báº¡n Tráº§n Há»¯u ThiÃªn táº¡i HCMUS, vá»›i chá»§ Ä‘á» lÃ  English-Vietnamese Machine Translation. CÃ¡c báº¡n cÃ³ thá»ƒ xem mÃ£ nguá»“n cá»§a Ä‘á»“ Ã¡n nÃ y táº¡i Ä‘Ã¢y ğŸš€\nTrong bÃ i viáº¿t trÆ°á»›c, mÃ¬nh Ä‘Ã£ giá»›i thiá»‡u vá» bÃ i toÃ¡n Machine Translation cÃ¹ng má»™t mÃ´ hÃ¬nh khÃ¡ quen thuá»™c trong bÃ i toÃ¡n Ä‘Ã³ lÃ  Seq2seq. Trong giai Ä‘oáº¡n trÆ°á»›c nÄƒm 2017, cÃ¡c mÃ´ hÃ¬nh Deep Learning Ä‘Æ°á»£c xÃ¢y dá»±ng cho bÃ i toÃ¡n Machine Translation thÆ°á»ng Ä‘Æ°á»£c tá»• chá»©c theo kiáº¿n trÃºc Encoder-Decoder nhÆ° Seq2seq vá»›i pháº§n lÃµi lÃ  cÃ¡c biáº¿n thá»ƒ cáº£i tiáº¿n hÆ¡n cá»§a Recurrent Neural Network (RNN). Tuy nhiÃªn, nhá»¯ng mÃ´ hÃ¬nh NTM Ä‘Ã³ Ä‘á»u cÃ³ má»™t sá»‘ háº¡n cháº¿ nháº¥t Ä‘á»‹nh khi thá»±c hiá»‡n dá»‹ch nhá»¯ng cÃ¢u cÃ³ cÃ¢u trÃºc phá»©c táº¡p hoáº·c cÃ³ Ä‘á»™ dÃ i lá»›n, mÃ  thÆ°á»ng tháº¥y nháº¥t lÃ  háº¡n cháº¿ vá» Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n vÃ  viá»‡c ghi nhá»› quan há»‡ phá»¥ thuá»™c giá»¯a cÃ¡c tá»« trong cÃ¢u vá» máº·t ngá»¯ nghÄ©a. LÃ½ do cá»§a nhá»¯ng háº¡n cháº¿ nÃ y pháº§n lá»›n Ä‘áº¿n tá»« thao tÃ¡c tÃ­nh toÃ¡n, xá»­ lÃ½ tuáº§n tá»± cÃ¡c tá»« (hay lÃ  token) cá»§a RNN.\nTransformer, má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o nÄƒm 2017 Ä‘Ã£ táº¡o ra sá»± Ä‘á»™t phÃ¡ lá»›n trong lÄ©nh vá»±c NLP vá»›i Ä‘á»™ chÃ­nh xÃ¡c cÅ©ng nhÆ° tá»‘c Ä‘á»™ xá»­ lÃ½ Ä‘á»u vÆ°á»£t xa nhá»¯ng mÃ´ hÃ¬nh Deep Learning trÆ°á»›c Ä‘Ã³. DÃ¹ váº«n tá»• chá»©c mÃ´ hÃ¬nh theo kiáº¿n trÃºc Encoder-Decoder thÃ´ng dá»¥ng nhÆ°ng Transformer Ä‘Ã£ kháº¯c phá»¥c Ä‘Æ°á»£c nhá»¯ng nhÆ°á»£c Ä‘iá»ƒm cá»§a cÃ¡c mÃ´ hÃ¬nh trÆ°á»›c nhá» vÃ o viá»‡c táº­n dá»¥ng tá»‘i Ä‘a sá»©c máº¡nh cá»§a cÆ¡ cháº¿ Attention vá»›i hai phÃ©p toÃ¡n Self-Attention vÃ  Cross-Attention. Ká»ƒ tá»« Ä‘Ã³, mÃ´ hÃ¬nh Transformer Ä‘Ã£ trá»Ÿ thÃ nh má»™t tiÃªu chuáº©n má»›i trong bÃ i toÃ¡n Machine Translation nÃ³i riÃªng vÃ  trong nhiá»u bÃ i toÃ¡n thuá»™c lÄ©nh vá»±c NLP nÃ³i chung. KhÃ´ng dá»«ng láº¡i á»Ÿ Ä‘Ã³, Ã½ tÆ°á»Ÿng tá»« kiáº¿n trÃºc cá»§a Transformer cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng má»™t cÃ¡ch ráº¥t thÃ nh cÃ´ng á»Ÿ cÃ¡c bÃ i toÃ¡n thuá»™c lÄ©nh vá»±c khÃ¡c nhÆ° Computer Vision.\nTrong bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ trÃ¬nh bÃ y vá» Transformer cÃ¹ng cÆ¡ cháº¿ Attention (tá»•ng quan vÃ  sau Ä‘Ã³ Ä‘i vÃ o cá»¥ thá»ƒ vá»›i Self-Attention vÃ  Cross-Attention). Ná»™i dung cá»§a bÃ i nÃ y sáº½ lÃ  ráº¥t dÃ i.\nTá»•ng quan vá» kiáº¿n trÃºc Transformer MÃ´ hÃ¬nh Transformer cÃ³ kiáº¿n trÃºc Ä‘Æ°á»£c tá»• chá»©c dáº¡ng Encoder-Decoder vá»›i pháº§n lÃµi lÃ  cÆ¡ cháº¿ Attention. Tá»•ng quan kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i, trong Ä‘Ã³:\nEncoder cÃ³ vai trÃ² chÃ­nh lÃ  há»c má»‘i tÆ°Æ¡ng quan giá»¯a cÃ¡c tá»« vá»›i nhau trong cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n (cá»¥ thá»ƒ hÆ¡n lÃ  giá»¯a tá»«ng tá»« vá»›i cÃ¡c tá»« cÃ²n láº¡i trong cÃ¢u). Ta cÃ³ thá»ƒ hiá»ƒu Ä‘Ã¢y nhÆ° lÃ  viá»‡c ta Ä‘á»c má»™t cÃ¢u Tiáº¿ng Anh vÃ  cá»‘ gáº¯ng hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¢u Ä‘Ã³. Decoder sáº½ láº§n lÆ°á»£t sinh ra cÃ¡c tá»« cho cÃ¢u thuá»™c ngÃ´n ngá»¯ Ä‘Ã­ch. Trong quÃ¡ trÃ¬nh sinh tá»« nÃ y thÃ¬ Decoder sáº½ vá»«a Ä‘á»ƒ Ã½ Ä‘áº¿n cÃ¡c tá»« nÃ³ Ä‘Ã£ sinh ra trÆ°á»›c Ä‘Ã³ vÃ  vá»«a Ä‘á»ƒ Ã½ Ä‘áº¿n má»™t sá»‘ tá»« liÃªn quan trong cÃ¢u ngÃ´n ngá»¯ nguá»“n Ä‘á»ƒ dá»‹ch cho Ä‘Ãºng. ÄÃ¢y cÅ©ng cÃ³ thá»ƒ xem lÃ  sau khi Ä‘Ã£ hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¢u Tiáº¿ng Anh rá»“i thÃ¬ ta tá»«ng bÆ°á»›c dá»‹ch cÃ¢u Ä‘Ã³ ra Tiáº¿ng Viá»‡t sao cho trÃ´i cháº£y, máº¡ch láº¡c. Tá»•ng quan kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh Transformer\nTa sáº½ láº§n lÆ°á»£t Ä‘á» cáº­p Ä‘áº¿n nhá»¯ng thÃ nh pháº§n quan trá»ng trong kiáº¿n trÃºc cá»§a Transformer, mÃ  Ä‘áº·c biá»‡t lÃ  Scale Dot-Product Attention vÃ  Multi-Head Attention, thÃ nh pháº§n Ä‘Ã³ng vai trÃ² ráº¥t lá»›n trong viá»‡c táº¡o nÃªn sá»©c máº¡nh cá»§a mÃ´ hÃ¬nh nÃ y.\nWord Embedding vÃ  Positional Encoding Pháº§n Ä‘áº§u tiÃªn trong kiáº¿n trÃºc cá»§a Transformer lÃ  embedding, vá»›i hai thÃ nh pháº§n lÃ  Word Embedding vÃ  Positional Encoding.\nWord Embedding lÃ  má»™t phÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n cÃ¡c tá»« trong cÃ¢u thÃ nh cÃ¡c vector Ä‘áº·c trÆ°ng má»™t cÃ¡ch há»£p lÃ½, sao cho cÃ¡c vector nÃ y thá»ƒ hiá»‡n Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau. MÃ¬nh Ä‘Ã£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n nÃ³ á»Ÿ trong bÃ i viáº¿t nÃ y.\nÄá»‘i vá»›i bÃ i toÃ¡n Machine Translation, ta cáº§n lÆ°u Ã½ ráº±ng táº­p tá»« Ä‘iá»ƒn cá»§a ngÃ´n ngá»¯ nguá»“n vÃ  Ä‘Ã­ch cÃ³ thá»ƒ cÃ³ sá»‘ tá»« khÃ¡c nhau nÃªn vector one-hot biá»ƒu diá»…n cÃ¡c tá»« trong cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n vÃ  Ä‘Ã­ch cÅ©ng cÃ³ thá»ƒ cÃ³ sá»‘ chiá»u khÃ¡c nhau. Tuy nhiÃªn, ta sáº½ Ä‘á»u Ä‘Æ°a chÃºng vá» cÃ¡c word embedding vectors vá»›i cÃ¹ng sá»‘ chiá»u vÃ  giÃ¡ trá»‹ nÃ y Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  lÃ  $d_{model}$. Káº¿t quáº£ khi sá»­ dá»¥ng thuáº­t toÃ¡n t-SNE Ä‘á»ƒ trá»±c quan hÃ³a cÃ¡c word embedding vector trÃªn khÃ´ng gian 2 chiá»u cá»§a táº­p tá»« vá»±ng Tiáº¿ng Anh\nVÃ¬ mÃ´ hÃ¬nh Transformer khÃ´ng tÃ­nh toÃ¡n tuáº§n tá»± theo thá»© tá»± cá»§a cÃ¡c tá»« trong cÃ¢u nhÆ° nhá»¯ng mÃ´ hÃ¬nh dá»±a trÃªn pháº§n lÃµi lÃ  RNN (RNN-based) nÃªn Ä‘á»ƒ cÃ³ thá»ƒ cung cáº¥p thÃ´ng tin vá» vá»‹ trÃ­ cá»§a cÃ¡c tá»« trong cÃ¢u cho mÃ´ hÃ¬nh, cÃ¡c tÃ¡c giáº£ cá»§a Transformer Ä‘Ã£ Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  Positional Encoding. Pháº§n thÃ´ng tin cÃ³ Ä‘Æ°á»£c tá»« Positional Encoding sáº½ Ä‘Æ°á»£c cá»™ng vÃ o word embedding vectors cá»§a cÃ¡c tá»« trong cÃ¢u ngÃ´n ngá»¯ nguá»“n vÃ  ngÃ´n ngá»¯ Ä‘Ã­ch. Thao tÃ¡c cá»™ng nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n á»Ÿ pháº§n ngay sau \u0026ldquo;Input Embedding\u0026rdquo; vÃ  \u0026ldquo;Output Embedding\u0026rdquo; cá»§a hÃ¬nh mÃ´ táº£ kiáº¿n trÃºc á»Ÿ pháº§n 1.\nCÃ´ng thá»©c cá»§a Positional Encoding Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh thÃ´ng qua hÃ m Sinusoid. Táº¥t nhiÃªn lÃ  Ä‘á»ƒ cá»™ng Ä‘Æ°á»£c positional encoding vector (PE) vá»›i word embedding vector (WE) thÃ¬ hai vector nÃ y pháº£i cÃ³ cÃ¹ng sá»‘ chiá»u lÃ  $d_{model}$. Vá»›i tá»« á»Ÿ vá»‹ trÃ­ thá»© $pos$ trong cÃ¢u, giÃ¡ trá»‹ cá»§a pháº§n tá»­ táº¡i vÃ­ trÃ­ $2i$ vÃ  $(2i+1)$ ($0 \\leq 2i, 2i+1 \\leq d_{model} - 1)$ trong PE Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi: $$ \\begin{align*} PE(pos, 2i) \u0026amp;= \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right) \\\\ PE(pos, 2i+1) \u0026amp;= \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right) \\end{align*} $$\nMinh há»a cho káº¿t quáº£ cá»§a PE vá»›i cÃ¡c tá»« trong cÃ¢u Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i vá»›i má»™t cÃ¢u gá»“m 20 tá»«, má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t WE 128 chiá»u. Ta tháº¥y ráº±ng PE cá»§a cÃ¡c tá»« trong cÃ¢u Ä‘á»u Ä‘Ã´i má»™t khÃ¡c nhau, tá»©c lÃ  nÃ³ Ä‘Ã£ giÃºp ta mÃ£ hÃ³a Ä‘Æ°á»£c thÃ´ng tin vá»‹ trÃ­ cá»§a cÃ¡c tá»«.\nMinh há»a káº¿t quáº£ cá»§a cÃ¡c positional encoding vectors\nNgoÃ i ra, cÃ¡c tÃ¡c giáº£ cá»§a Transformer cÃ²n cho biáº¿t ráº±ng viá»‡c sá»­ dá»¥ng hÃ m Sinusoid Ä‘á»ƒ tÃ­nh PE cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c tá»«, khi mÃ  $PE(pos)$ vÃ  $PE(pos + k)$ cÃ³ thá»ƒ Ä‘Æ°á»£c biáº¿n Ä‘á»•i qua láº¡i thÃ´ng qua má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh, tá»©c lÃ  tá»“n táº¡i ma tráº­n $M$ sao cho $$ \\begin{equation*} M \\cdot \\begin{bmatrix} \\sin(pos \\cdot \\omega_i) \\\\ \\cos(pos \\cdot \\omega_i) \\end{bmatrix} = \\begin{bmatrix} \\sin((pos + k) \\cdot \\omega_i) \\\\ \\cos((pos + k) \\cdot \\omega_i) \\end{bmatrix} \\end{equation*} $$ , vá»›i $\\omega_i = \\left (10000^{\\frac{2i}{d_{model}}} \\right)^{-1}$.\nNhÆ° váº­y, vá»›i $x_{pos}$ lÃ  vector one-hot cá»§a tá»« thá»© $pos$ trong cÃ¢u, ta cÃ³ thá»ƒ biá»ƒu diá»…n káº¿t quáº£ sau viá»‡c káº¿t há»£p word embedding vector vÃ  positional encoding vector nhÆ° sau: $$ \\begin{equation*} z_{pos} = WE(x_{pos}) * \\sqrt{d_{model}} + PE(pos) \\end{equation*} $$ , trong Ä‘Ã³ $*$ lÃ  phÃ©p nhÃ¢n element-wise.\nLÆ°u Ã½ 1. Má»™t cÃ¡ch giáº£i thÃ­ch cho viá»‡c nhÃ¢n WE vá»›i $\\sqrt{d_{model}}$ trÆ°á»›c khi cá»™ng vá»›i PE lÃ  ta muá»‘n lÆ°á»£ng thÃ´ng tin nháº­n Ä‘Æ°á»£c tá»« PE sáº½ khÃ´ng tÃ¡c Ä‘á»™ng quÃ¡ nhiá»u Ä‘áº¿n nhá»¯ng thÃ´ng tin mÃ  WE Ä‘Ã£ cung cáº¥p vá» cÃ¡c tá»«.\nLÆ°u Ã½ 2. Ta cÃ²n cÃ³ thá»ƒ hÃ¬nh dung Ã½ nghÄ©a positional encoding vector thÃ´ng qua vÃ­ dá»¥ vá» cÃ¡ch biá»ƒu diá»…n nhá»‹ phÃ¢n cá»§a cÃ¡c sá»‘ nguyÃªn. XÃ©t hÃ¬nh minh há»a cho 16 sá»‘ nguyÃªn Ä‘áº§u tiÃªn biÃªn dÆ°á»›i thÃ¬:\nMá»—i sá»‘ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng duy nháº¥t má»™t chuá»—i nhá»‹ phÃ¢n vÃ  cÃ¡c chuá»—i nÃ y Ä‘Ã´i má»™t phÃ¢n biá»‡t. Äi tá»« bit tháº¥p Ä‘áº¿n bit cap thÃ¬ \u0026ldquo;táº§n suáº¥t thay Ä‘á»•i\u0026rdquo; cá»§a cÃ¡c bit tÄƒng dáº§n. VÃ­ dá»¥, tá»« 0 tá»›i 7 thÃ¬ bit 0 giá»¯ nguyÃªn, bit 1 thay Ä‘á»•i 1 láº§n, bit 2 thay Ä‘á»•i 3 láº§n, v.v. Náº¿u ta quan sÃ¡t láº¡i hÃ¬nh minh há»a káº¿t quáº£ cÃ¡c positional encoding vectors á»Ÿ phÃ­a trÃªn thÃ¬ cÅ©ng tháº¥y tÃ­nh cháº¥t tÆ°Æ¡ng tá»±. LiÃªn há»‡ giá»¯a Positional Encoding vÃ  cÃ¡ch biá»ƒu diá»…n nhá»‹ phÃ¢n cá»§a cÃ¡c sá»‘ nguyÃªn\nCÆ¡ cháº¿ Attention vÃ  sá»± truy xuáº¥t thÃ´ng tin Attention lÃ  má»™t Ã½ tÆ°á»Ÿng ráº¥t thÃº vá»‹ trong Deep Learning khi mÃ  nÃ³ Ä‘Ã£ mÃ´ phá»ng láº¡i cÃ¡ch bá»™ nÃ£o cá»§a con ngÆ°á»i hoáº¡t Ä‘á»™ng khi chÃºng ta phÃ¢n tÃ­ch, nhÃ¬n nháº­n má»™t Ä‘á»‘i tÆ°á»£ng. VÃ­ dá»¥, máº¯t chÃºng ta cÃ³ táº§m nhÃ¬n ráº¥t rá»™ng nhÆ°ng táº¡i má»—i thá»i Ä‘iá»ƒm thÃ¬ ta chá»‰ táº­p trung vÃ o má»™t vÃ¹ng nháº¥t Ä‘á»‹nh trong táº§m nhÃ¬n Ä‘á»ƒ láº¥y thÃ´ng tin. Attention Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ nh cÃ´ng vÃ o nhiá»u lÄ©nh vá»±c khÃ¡c nhau, nhiá»u bÃ i toÃ¡n khÃ¡c nhau trong Deep Learning.\nÄá»ƒ mÃ´ táº£ sÆ¡ lÆ°á»£c vá» quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a cÆ¡ cháº¿ Attention, ta xÃ©t cÃ¡ch Attention Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh trong bÃ i toÃ¡n Machine Translation, á»Ÿ giai Ä‘oáº¡n trÆ°á»›c khi Transformer Ä‘Æ°á»£c cÃ´ng bá»‘ (hÃ¬nh bÃªn dÆ°á»›i). Giáº£ sá»­ input cá»§a Encoder lÃ  cÃ¡c vector $x_i$ vÃ  output vector tÆ°Æ¡ng á»©ng lÃ  $h_i$, ta Ä‘ang tÃ­nh toÃ¡n cho tá»« Ä‘áº§u tiÃªn trong output cá»§a Decoder vá»›i input cá»§a Decoder lÃ  vector $y_1$. LÃºc nÃ y, ta thá»±c hiá»‡n Attention tá»« $y_1$ Ä‘áº¿n cÃ¡c vector $x_i$, vá»›i Ã½ nghÄ©a lÃ  khi ta dá»‹ch tá»« Ä‘áº§u tiÃªn nÃ y thÃ¬ ta nÃªn chÃº Ã½ vÃ o cÃ¡c tá»« nÃ o á»Ÿ trong cÃ¢u nguá»“n.\nMÃ´ táº£ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a cÆ¡ cháº¿ Attention\nTrong Attention, ta cÃ³ cÃ¡c khÃ¡i niá»‡m vá» context vector $c_i$ vÃ  attention weight $\\alpha_{ij}$. CÃ¡c giÃ¡ trá»‹ attention weight $\\alpha_{ij}$ sáº½ náº±m trong Ä‘oáº¡n [0, 1] vÃ  cho biáº¿t má»©c Ä‘á»™ chÃº Ã½ cá»§a vector $y_i$ vÃ o vector $x_j$ vÃ  context vector $c_i$ lÃ  káº¿t quáº£ thu Ä‘Æ°á»£c khi thá»±c hiá»‡n Attention tá»« vector $y_i$.\nTa tÃ­nh context vector $c_i$ theo cÃ´ng thá»©c $$ \\begin{equation*} c_i = \\sum_{j=1}^N \\left( \\alpha_{ij} \\cdot h_j \\right ) \\end{equation*} $$ , trong Ä‘Ã³ $N$ lÃ  Ä‘á»™ dÃ i cá»§a cÃ¢u nguá»“n vÃ  attention weight $\\alpha_{ij}$ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh nhÆ° sau: $$ \\begin{equation} \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{t=1}^N \\exp(e_{it})} % \\label{eq:attn_weights_1} \\end{equation} $$ , vá»›i $e_{ij} = f(y_i, h_j)$ vÃ  $f$ lÃ  má»™t mÃ´ hÃ¬nh há»c, nÃ³ cÃ³ thá»ƒ Ä‘Æ¡n giáº£n chá»‰ lÃ  Neural Network vá»›i má»™t layer, input vector sáº½ lÃ  vector Ä‘Æ°á»£c ná»‘i giá»¯a $\\alpha_{ij}$ vÃ  $h_j$, output lÃ  vector 1 chiá»u.\nTá»« cÃ´ng thá»©c $(1)$, ta cÃ³ thá»ƒ viáº¿t gá»n báº±ng cÃ¡ch biá»ƒu diá»…n báº±ng cÃ¡c vector $N$ chiá»u $\\alpha_i$ vÃ  $e_i$ nhÆ° sau: $$ \\begin{equation*} \\alpha_i = \\text{softmax}(e_i) \\end{equation*} $$\nNhÆ° váº­y, $\\alpha_{ij}$ cÃ ng lá»›n thÃ¬ $y_i$ cÃ ng chÃº Ã½ vÃ o $x_j$ vÃ  rÃµ rÃ ng ta cÃ³ $\\sum_{j=1}^N \\alpha_{ij} = 1$. Vá» máº·t trá»±c quan, ngÆ°á»i ta thÆ°á»ng biá»ƒu diá»…n attention weights báº±ng má»™t ma tráº­n nhÆ° hÃ¬nh bÃªn dÆ°á»›i, trong Ä‘Ã³ Ã´ cÃ³ mÃ u cÃ ng sÃ¡ng thÃ¬ sáº½ á»©ng vá»›i attention weights cÃ ng lá»›n.\nMinh há»a trá»±c quan vá» attention weights\nTrong Transformer, Attention cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° lÃ  má»™t cÆ¡ cháº¿ truy xuáº¥t thÃ´ng tin, khi mÃ  attention weights $\\alpha_{ij}$ cÃ ng lá»›n thÃ¬ cÃ ng thá»ƒ hiá»‡n sá»± \u0026ldquo;liÃªn quan Ä‘áº¿n nhau\u0026rdquo; giá»¯a cá»§a $y_i$ vÃ  $x_i$. Khi nÃ³i Ä‘áº¿n sá»± truy xuáº¥t thÃ´ng tin, ta cÃ³ 3 khÃ¡i niá»‡m Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  query, key vÃ  value. Má»™t cÃ¡ch mÃ´ táº£ ráº¥t dá»… hiá»ƒu vá» 3 khÃ¡i niá»‡m nÃ y vá»›i vÃ­ dá»¥ Google Search trong bÃ i viáº¿t cá»§a anh Quá»‘c nhÆ° sau:\nQuery: Vector dÃ¹ng Ä‘á»ƒ chá»©a thÃ´ng tin cá»§a tá»« Ä‘Æ°á»£c tÃ¬m kiáº¿m, so sÃ¡nh (hoáº·c lÃ  ma tráº­n vá»›i má»—i dÃ²ng lÃ  vector á»©ng vá»›i má»™t tá»«). VÃ­ dá»¥, tá»« khÃ³a mÃ  ta nháº­p vÃ o Ã´ tÃ¬m kiáº¿m cá»§a Google Search lÃ  query. Key: Ma tráº­n dÃ¹ng Ä‘á»ƒ biá»ƒu diá»…n thÃ´ng tin chÃ­nh cá»§a cÃ¡c tá»« Ä‘Æ°á»£c so sÃ¡nh vá»›i tá»« cáº§n tÃ¬m kiáº¿m á»Ÿ trÃªn, má»—i dÃ²ng lÃ  vector á»©ng vá»›i má»™t tá»«. VÃ­ dá»¥, tiÃªu Ä‘á» cÃ¡c trang web mÃ  Google sáº½ so sÃ¡nh vá»›i tá»« khÃ³a ta Ä‘Ã£ nháº­p lÃ  cÃ¡c key. Value: Ma tráº­n biá»ƒu diá»…n Ä‘áº§y Ä‘á»§ ná»™i dung, Ã½ nghÄ©a cá»§a cÃ¡c tá»« cÃ³ trong key, má»—i dÃ²ng lÃ  vector á»©ng vá»›i má»™t tá»«. NÃ³ nhÆ° lÃ  ná»™i dung cÃ¡c trang web Ä‘Æ°á»£c hiá»ƒn thá»‹ cho ngÆ°á»i dÃ¹ng sau khi tÃ¬m kiáº¿m. \\end{itemize} Trong vÃ­ dá»¥ vá» attention á»Ÿ hÃ¬nh trÃªn, query sáº½ lÃ  vector $y_1$, keys vÃ  values sáº½ Ä‘á»u lÃ  ma tráº­n $H$ táº¡o bá»Ÿi cÃ¡c vector $h_i$. QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n giá»¯a query, keys vÃ  values trong vÃ­ dá»¥ Ä‘Ã³ Ä‘Æ°á»£c mÃ´ táº£ láº¡i trong hÃ¬nh bÃªn dÆ°á»›i, vá»›i:\nAttention weights Ä‘Æ°á»£c tÃ­nh tá»« query vÃ  keys. Context vector Ä‘Æ°á»£c tÃ­nh tá»« attention weights vÃ  values. Attention vá»›i query, key vÃ  value. Trong Ä‘Ã³, vector key vÃ  value cá»§a cÃ¡c tá»« trong háº§u háº¿t cÃ¡c trÆ°á»ng há»£p lÃ  giá»‘ng nhau.\nLÆ°u Ã½. Vá»›i Ã½ nghÄ©a cá»§a tá»«ng ma tráº­n, trong má»™t sá»‘ tÃ¬nh huá»‘ng thÃ¬ keys vÃ  values cÃ³ thá»ƒ cÃ³ giÃ¡ trá»‹ khÃ¡c nhau chá»© khÃ´ng nháº¥t thiáº¿t lÃ  luÃ´n giá»‘ng nhau.\nScale Dot-Product Attention Trong mÃ´ hÃ¬nh Transformer, Attention Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘Æ¡n giáº£n vÃ  nhanh hÆ¡n so vá»›i háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng Attention trong Machine Translation trÆ°á»›c Ä‘Ã³. LÃ½ do lÃ  vÃ¬ mÃ´ hÃ¬nh há»c $f$ dÃ¹ng Ä‘á»ƒ tÃ­nh attention weights $\\alpha_{ij}$ sáº½ chá»‰ Ä‘Æ¡n giáº£n lÃ  phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng (Dot-Product). NgoÃ i ra, ta cÃ²n cÃ³ thÃªm thao tÃ¡c scale cÃ¡c giÃ¡ trá»‹ trong ma tráº­n káº¿t quáº£ vá»›i má»™t giÃ¡ trá»‹ háº±ng sá»‘. Do Ä‘Ã³, quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Attention trong Transformer Ä‘Æ°á»£c gá»i lÃ  Scale Dot-Product Attention.\nTáº¡i sao láº¡i chá»‰ Ä‘Æ¡n giáº£n lÃ  dÃ¹ng phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng?\nÄá»ƒ Ã½ ráº±ng, káº¿t quáº£ tÃ­ch vÃ´ hÆ°á»›ng cá»§a hai vector cÃ ng lá»›n thÃ¬ hai vector Ä‘Ã³ cÃ ng \u0026ldquo;liÃªn quan Ä‘áº¿n nhau\u0026rdquo;. Náº¿u xÃ©t hai vector cÃ³ chuáº©n báº±ng 1 thÃ¬ Ä‘iá»u nÃ y Ä‘ang cho biáº¿t ráº±ng gÃ³c giá»¯a hai vector Ä‘Ã³ Ä‘ang ráº¥t nhá». NhÆ° váº­y, phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c nhiá»‡m vá»¥ cá»§a mÃ´ hÃ¬nh há»c $f$ má»™t cÃ¡ch ráº¥t nhanh gá»n. TrÆ°á»›c háº¿t, kÃ½ hiá»‡u cÃ¡c ma tráº­n queries, keys vÃ  values láº§n lÆ°á»£t lÃ  $Q$, $K$ vÃ  $V$, vá»›i $Q \\in \\mathbb{R}^{length_q \\times d_q}$, $K \\in \\mathbb{R}^{length_k \\times d_k}$ vÃ  $V \\in \\mathbb{R}^{length_v \\times d_v}$. Gá»i cÃ¡c vector query trong ma tráº­n $Q$ lÃ  $q_i$ (á»©ng vá»›i tá»«ng dÃ²ng cá»§a ma tráº­n), tÆ°Æ¡ng tá»± vá»›i $k_i$ trong $K$ vÃ  $v_i$ trong $V$. Ta cÃ³ má»™t sá»‘ lÆ°u Ã½ sau:\nÄá»ƒ dá»… hÃ¬nh dung, vá»›i vÃ­ dá»¥ vá» Attention á»Ÿ hÃ¬nh nÃ y thÃ¬ $length_k = length_v = 5$ vÃ  $length_q$ sáº½ báº±ng vá»›i sá»‘ tá»« á»Ÿ phÃ­a Decoder (vÃ  trong hÃ¬nh Ä‘Ã³ thÃ¬ lÃ  1). Ta xÃ©t ma tráº­n $Q$ vÃ¬ tá»« pháº§n vá» Attention thÃ¬ ta tháº¥y ráº±ng vá»›i má»—i vector $y_i$, quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Attention tá»« nÃ³ Ä‘áº¿n cÃ¡c vector $x_j$ lÃ  hoÃ n toÃ n Ä‘á»™c láº­p vá»›i cÃ¡c vector $y_t$ khÃ¡c, tá»©c lÃ  ta cÃ³ thá»ƒ thá»±c hiá»‡n tÃ­nh toÃ¡n Attention song song vá»›i táº¥t cáº£ cÃ¡c vector query. VÃ¬ $f$ lÃ  phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng nÃªn sá»‘ chiá»u cá»§a vector query vÃ  vector key pháº£i giá»‘ng nhau, tá»©c lÃ  $d_q = d_k$. NgoÃ i ra, ta thÆ°á»ng cÃ³ $length_q = length_k = length_v$. QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Scale Dot-Product Attention Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. Vá» máº·t cÃ´ng thá»©c, ma tráº­n output sáº½ thuá»™c $\\mathbb{R}^{length_q \\times d_v}$ vÃ  nÃ³ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng $$ \\begin{equation*} \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V \\end{equation*} $$ , trong Ä‘Ã³ phÃ©p toÃ¡n $\\text{softmax}$ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn tá»«ng dÃ²ng cá»§a ma tráº­n $QK^\\top$.\nCÃ¡c bÆ°á»›c tÃ­nh toÃ¡n trong Scale Dot-Product Attention.\nLÆ°u Ã½. Äá»ƒ giáº£i thÃ­ch cho sá»± cáº§n thiáº¿t cá»§a viá»‡c scale cÃ¡c pháº§n tá»­ trong ma tráº­n $QK^\\top$ báº±ng cÃ¡ch chia cho $\\sqrt{d_k}$ trÆ°á»›c khi tÃ­nh softmax thÃ¬ ta cáº§n quan tÃ¢m Ä‘áº¿n phÆ°Æ¡ng sai cá»§a cÃ¡c giÃ¡ trá»‹ nÃ y. Náº¿u ban Ä‘áº§u cÃ¡c vector $q_i$ vÃ  $k_j$ cÃ³ phÆ°Æ¡ng sai lÃ  1 thÃ¬ cÃ¡c pháº§n tá»­ trong $QK^\\top$ sáº½ cÃ³ phÆ°Æ¡ng sai lÃ  $d_k$ (hoáº·c lÃ  $d_q$ vÃ¬ ta cÃ³ $d_q = d_k$). ÄÃ³ lÃ  má»™t giÃ¡ trá»‹ ráº¥t lá»›n vÃ  nÃ³ sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n quÃ¡ trÃ¬nh tÃ­nh attention weights (phÃ©p toÃ¡n softmax): giáº£m Ä‘á»™ chÃ­nh xÃ¡c vÃ  thá»i gian tÃ­nh tÄƒng lÃªn khÃ¡ nhiá»u.\nSelf-Attention vÃ  Cross-Attention KhÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh RNN-based, Transformer Ä‘Ã£ thay tháº¿ toÃ n bá»™ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n há»“i quy báº±ng cÃ¡c phÃ©p toÃ¡n Attention vÃ  má»™t sá»‘ fully connected layer. NÃ³i cÃ¡ch khÃ¡c, Transformer Ä‘Ã£ sá»­ dá»¥ng Attention Ä‘á»ƒ há»c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong cÃ¢u nguá»“n vÃ  trong cÃ¢u Ä‘Ã­ch.\nNgoÃ i ra, quÃ¡ trÃ­nh tÃ­nh toÃ¡n nÃ y hoÃ n toÃ n cÃ³ thá»ƒ diá»…n ra song song chá»© khÃ´ng cáº§n pháº£i tuáº§n tá»± tá»«ng vá»‹ trÃ­ nhÆ° RNN-based. Äá»ƒ ngáº¯n gá»n hÆ¡n, tá»« pháº§n nÃ y trá»Ÿ Ä‘i, khi nÃ³i Ä‘áº¿n Attention trong Transformer thÃ¬ ta hiá»ƒu Ä‘Ã³ lÃ  Scale Dot-Product Attention. Trong Transformer, Attention Ä‘Æ°á»£c sá»­ dá»¥ng theo hai dáº¡ng lÃ  Self-Attention vÃ  Cross-Attention. Sá»± khÃ¡c nhau giá»¯a Self-Attention vÃ  Cross-Attention náº±m á»Ÿ cÃ¡c cÃ¡ch xÃ¡c Ä‘á»‹nh giÃ¡ trá»‹ ma tráº­n $Q$, $K$ vÃ  $V$ Ä‘á»ƒ tÃ­nh Scale Dot-Product Attention, trong Ä‘Ã³:\nSelf-Attention: Ba ma tráº­n $Q$, $K$ vÃ  $V$ Ä‘á»u Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« input cá»§a Encoder hoáº·c cá»§a Decoder, tá»©c lÃ  cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n hoáº·c ngÃ´n ngá»¯ Ä‘Ã­ch. NhÆ° váº­y, Self-Attention sáº½ diá»…n ra trong ná»™i bá»™ cÃ¢u nguá»“n vÃ  ná»™i bá»™ cÃ¢u Ä‘Ã­ch. Cross-Attention: Ma tráº­n $Q$ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« input cá»§a Decoder, tá»©c lÃ  cÃ¢u ngÃ´n ngá»¯ Ä‘Ã­ch. Trong khi Ä‘Ã³, $K$ vÃ  $V$ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« cÃ¢u nguá»“n á»Ÿ phÃ­a Encoder. Äiá»u nÃ y nghÄ©a lÃ  Cross-Attention thá»±c hiá»‡n Attention tá»« cÃ¢u Ä‘Ã­ch vÃ o cÃ¢u nguá»“n (Ä‘Ã¢y tháº­t lÃ  Ã½ tÆ°á»Ÿng sá»­ dá»¥ng Attention trong nhá»¯ng mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³). Self-Attention TrÆ°á»›c tiÃªn, ta xÃ©t Ä‘áº¿n Self-Attention. Self-Attention sáº½ thá»±c hiá»‡n nhiá»‡m vá»¥ há»c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau trong cÃ¹ng má»™t cÃ¢u (thay tháº¿ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n há»“i quy trong cÃ¡c mÃ´ hÃ¬nh RNN-based). Do Ä‘Ã³ mÃ  cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$ Ä‘Æ°á»£c tÃ­nh tá»« cÃ¡c tá»« trong cÃ¹ng má»™t cÃ¢u. HÆ¡n ná»¯a, ta cÅ©ng cÃ³ $length_q = length_k = length_v = n$ vá»›i $n$ lÃ  sá»‘ tá»« cá»§a cÃ¢u Ä‘Ã³.\nGiáº£ sá»­ ráº±ng, cÃ¢u input sau khi qua cÃ¡c pháº§n Embedding (Word Embedding vÃ  Positional Encoding) thÃ¬ ta thu Ä‘Æ°á»£c ma tráº­n $X \\in \\mathbb{R}^{n \\times d_{model}}$. Khi Ä‘Ã³:\nTa sá»­ dá»¥ng 3 fully connected layer Ä‘á»ƒ biáº¿n Ä‘á»•i $X$ thÃ nh cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$. Gá»i 3 ma tráº­n trá»ng sá»‘ á»©ng vá»›i cÃ¡c layer Ä‘Ã³ lÃ  $W_Q$, $W_K$ vÃ  $W_V$. ÄÃ¢y chÃ­nh lÃ  cÃ¡c ma tráº­n tham sá»‘ mÃ  Transformer cáº§n há»c cho quÃ¡ trÃ¬nh Self-Attention. Tá»« $Q$, $K$ vÃ  $V$, qua phÃ©p toÃ¡n Scale Dot-Product Attention, ta thu Ä‘Æ°á»£c ma tráº­n káº¿t quáº£ $Z \\in \\mathbb{R}^{n \\times d_v}$. QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n nÃ y cá»§a Self-Attention Ä‘Æ°á»£c mÃ´ táº£ trong hÃ¬nh bÃªn dÆ°á»›i:\nQuÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Self-Attention vá»›i d = d_model\nMinh há»a cho káº¿t quáº£ cá»§a phÃ©p toÃ¡n Self-Attention trong má»™t cÃ¢u Tiáº¿ng Anh Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. Trong Ä‘Ã³, mÃ u cÃ ng Ä‘áº­m thá»ƒ hiá»‡n cho attention weights tá»« tá»« \u0026ldquo;it\u0026rdquo; vÃ o tá»« tÆ°Æ¡ng á»©ng lÃ  cÃ ng lá»›n.\nMinh há»a káº¿t quáº£ cá»§a Self-Attention táº¡i tá»« \"it\" trong cÃ¢u Ä‘áº§u vÃ o\nNhÆ° váº­y, Self-Attention Ä‘Ã£ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong cÃ¹ng má»™t cÃ¢u vá» máº·t ngá»¯ nghÄ©a má»™t cÃ¡ch Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ hÆ¡n so vá»›i quÃ¡ trÃ¬nh tÃ­nh toÃ¡n há»“i quy trong cÃ¡c mÃ´ hÃ¬nh RNN-based.\nCross-Attention Cross-Attention thá»±c ra chÃ­nh lÃ  cÃ¡ch sá»­ dá»¥ng Attention trong nhiá»u mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³ vá»›i bÃ i toÃ¡n Machine Translation. Nhiá»‡m vá»¥ cá»§a nÃ³ trong bÃ i toÃ¡n Machine Translation lÃ  thá»±c hiá»‡n Attention tá»« cÃ¢u Ä‘Ã­ch vÃ o cÃ¢u nguá»“n. Äiá»u nÃ y cÃ³ thá»ƒ xem nhÆ° lÃ  viá»‡c ta sá»­ dá»¥ng nhá»¯ng gÃ¬ Ä‘Ã£ hiá»ƒu Ä‘Æ°á»£c á»Ÿ cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n Ä‘á»ƒ dá»‹ch dáº§n cÃ¢u Ä‘Ã³ ra cÃ¢u ngÃ´n ngá»¯ Ä‘Ã­ch.\nCá»¥ thá»ƒ hÆ¡n, ta cÃ³:\nGiÃ¡ trá»‹ cá»§a ma tráº­n $K$ vÃ  $V$ sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« cÃ¢u nguá»“n theo má»™t cÃ¡ch nÃ o Ä‘Ã³ (cÃ¡c pháº§n vá» Encoder vÃ  Decoder sáº½ Ä‘á» cáº­p kÄ© hÆ¡n vá» chi tiáº¿t nÃ y). Äá»‘i vá»›i ma tráº­n $Q$ thÃ¬ nÃ³ cÅ©ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tá»« cÃ¡c tá»« hiá»‡n cÃ³ trong cÃ¢u ngÃ´n ngá»¯ Ä‘Ã­ch tÃ­nh Ä‘áº¿n vá»‹ trÃ­ Ä‘ang xÃ©t. Ta váº«n sáº½ Ä‘áº£m báº£o ráº±ng $d_q = d_k$ vÃ  quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Cross-Attention hoÃ n toÃ n tÆ°Æ¡ng tá»± nhÆ° Self-Attention. MÃ´ hÃ¬nh Transformer cÅ©ng cáº§n há»c ba ma tráº­n tham sá»‘ tÆ°Æ¡ng á»©ng Ä‘á»ƒ tÃ­nh ra $Q$, $K$ vÃ  $V$ trÆ°á»›c khi thá»±c hiá»‡n Scale Dot-Product Attention). Äá»ƒ dá»… hÃ¬nh dung hÆ¡n vá» sá»± khÃ¡c nhau giá»¯a $Q$, $K$ vÃ  $V$ trong Self-Attention vÃ  Cross-Attention, ta cÃ³ minh há»a trong hÃ¬nh bÃªn dÆ°á»›i, vá»›i cÃ¡c hÃ m $f$, $g$ vÃ  $h$ Ä‘áº¡i diá»‡n cho quÃ¡ trÃ¬nh tÃ­nh toÃ¡n ra ba ma tráº­n Ä‘Ã³.\nSá»± khÃ¡c nhau vá» cÃ¡ch tÃ­nh toÃ¡n $Q$, $K$ vÃ  $V$ trong Self-Attention vÃ  Cross-Attention\nLÆ°u Ã½. Cross-Attention lÃ  má»™t thÃ nh pháº§n ráº¥t thÃº vá»‹. Nhá» cÃ³ Cross-Attention mÃ  kiáº¿n trÃºc Transformer cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng hiá»‡u quáº£ vÃ o nhiá»u bÃ i toÃ¡n khÃ¡c nhau. VÃ­ dá»¥, vá»›i bÃ i toÃ¡n Image Captioning, ta cÃ³ thá»ƒ sá»­ dá»¥ng Encoder Ä‘á»ƒ \u0026ldquo;hiá»ƒu\u0026rdquo; Ã½ nghÄ©a cá»§a áº£nh Ä‘áº§u vÃ o, sau Ä‘Ã³, trong quÃ¡ trÃ¬nh sinh cÃ¢u mÃ´ táº£ cho áº£nh, ta cÃ³ thá»ƒ sá»­ dá»¥ng Cross-Attention Ä‘áº¿n cÃ¡c Ä‘áº·c trÆ°ng rÃºt ra tá»« áº£nh Ä‘Ã³ Ä‘á»ƒ thu Ä‘Æ°á»£c cÃ¡c cÃ¢u mÃ´ táº£ tá»‘t hÆ¡n.\nMulti-Head Attention vÃ  Masked Multi-Head Attention Mutli-Head Attention Äá»ƒ Ã½ ráº±ng, Self-Attention Ä‘ang thá»±c hiá»‡n Attention tá»« má»™t tá»« trong cÃ¢u Ä‘áº¿n toÃ n bá»™ cÃ¡c tá»« cÃ²n láº¡i trong cÃ¢u (ká»ƒ cáº£ chÃ­nh tá»« Ä‘Ã³). TÆ°Æ¡ng tá»± nhÆ° vá»›i Cross-Attention. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  ta Ä‘ang thá»±c hiá»‡n Global Attention. Tuy nhiÃªn, trong ngÃ´n ngá»¯, Ä‘Ã´i khi tá»« X cÃ³ thá»ƒ cÃ³ quan há»‡ \u0026ldquo;máº¡nh\u0026rdquo; vá»›i má»™t tá»« Y theo má»™t phÆ°Æ¡ng diá»‡n nÃ o Ä‘Ã³, vÃ  nÃ³ cÅ©ng cÃ³ thá»ƒ cÃ³ quan há»‡ máº¡nh vá»›i tá»« Z theo má»™t phÆ°Æ¡ng diá»‡n khÃ¡c ná»¯a. Do Ä‘Ã³, náº¿u thá»±c hiá»‡n Global Attention thÃ¬ cÃ¡c quan há»‡ nÃ y cÃ³ thá»ƒ bá»‹ trung hÃ²a láº«n nhau vÃ  khiáº¿n ta máº¥t Ä‘i nhá»¯ng Ä‘áº·c trÆ°ng cÃ³ Ã­ch.\nTa xÃ©t má»™t vÃ­ dá»¥ Ä‘Æ°á»£c Ä‘á» cáº­p trong bÃ i giáº£ng cá»§a ProtonX. Vá»›i cÃ¢u \u0026ldquo;TÃ´i Ä‘i há»c á»Ÿ HÃ  Ná»™i\u0026rdquo; vÃ  ta Ä‘ang xÃ©t tá»« \u0026ldquo;TÃ´i\u0026rdquo; nhÆ° lÃ  query vector. Náº¿u xÃ©t theo 3 phÆ°Æ¡ng diá»‡n, hay lÃ  3 cÃ¢u há»i, sau Ä‘Ã¢y: \u0026ldquo;Ai?\u0026rdquo;, \u0026ldquo;LÃ m gÃ¬?\u0026rdquo;, \u0026ldquo;á» Ä‘Ã¢u?\u0026rdquo;, thÃ¬ vá»›i má»—i phÆ°Æ¡ng diá»‡n, má»‘i quan há»‡ giá»¯a tá»« \u0026ldquo;TÃ´i\u0026rdquo; vá»›i cÃ¡c tá»« con láº¡i Ä‘Æ°á»£c thá»ƒ hiá»‡n máº¡nh nháº¥t á»Ÿ láº§n lÆ°á»£t cÃ¡c tá»« \u0026ldquo;TÃ´i\u0026rdquo;, \u0026ldquo;Ä‘i\u0026rdquo; vÃ  \u0026ldquo;há»c\u0026rdquo;, \u0026ldquo;á»Ÿ\u0026rdquo; vÃ  \u0026ldquo;HÃ \u0026rdquo; vÃ  \u0026ldquo;Ná»™i\u0026rdquo; (minh há»a á»Ÿ hÃ¬nh bÃªn dÆ°á»›i).\nXÃ©t tá»« \"TÃ´i\" trong ba phÆ°Æ¡ng diá»‡n khÃ¡c nhau khi thá»±c hiá»‡n Self-Attention\nChi tiáº¿t nÃ y lÃ  xuáº¥t phÃ¡t cho Ã½ tÆ°á»Ÿng cá»§a Multi-Head Attention. NÃ³ sáº½ háº¡n cháº¿ áº£nh hÆ°á»Ÿng cá»§a Global Attention trong viá»‡c trung hÃ²a má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau trong nhiá»u phÆ°Æ¡ng diá»‡n.\nÄá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, Multi-Head Attention sáº½ chia nhá» tá»«ng ma tráº­n trong cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$ thÃ nh $h$ pháº§n. VÃ­ dá»¥, vá»›i $Q \\in \\mathbb{R}^{length_q \\times d_q}$ thÃ¬ ta sáº½ cÃ³ $h$ ma tráº­n $Q_i \\in \\mathbb{R}^{length_q \\times (d_k / h)}$ (hay lÃ  chia thÃ nh $h$ heads). Sau Ä‘Ã³, Scale Dot-Product Attention sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn $h$ bá»™ ba ma tráº­n $(Q_i, K_i, V_i)$ vÃ  cÃ¡c káº¿t quáº£ sáº½ Ä‘Æ°á»£c tá»•ng há»£p láº¡i. Ta cÃ³ thá»ƒ hÃ¬nh dung ráº±ng má»—i pháº§n nhá» cá»§a cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$ sáº½ cá»‘ gáº¯ng biá»ƒu diá»…n Ä‘áº·c trÆ°ng cá»§a cÃ¡c tá»« á»Ÿ má»™t phÆ°Æ¡ng diá»‡n nÃ o Ä‘Ã³.\nQuÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Multi-Head Attention Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. CÃ³ má»™t sá»‘ lÆ°u Ã½ nhÆ° sau:\nVá»›i Multi-Head Attention trong Transformer, ta sáº½ xÃ©t $length_q = length_k = length_v = n$ vá»›i $n$ lÃ  Ä‘á»™ dÃ i cá»§a cÃ¢u Ä‘áº§u vÃ o $d_q = d_k = d_v = d_{model}$ (Ä‘Ã¢y lÃ  sá»‘ chiá»u cá»§a embedding vector cá»§a cÃ¡c tá»«, sau khi bá»• sung thÃ´ng tin vá» Positional Encoding trong pháº§n \\ref{sec:pos-enc}). QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n trong Multi-Head Attention\nÄáº·t $d_x = d_{model} / h$. Vá» máº·t cÃ´ng thá»©c, ma tráº­n káº¿t quáº£ cá»§a Multi-Head Attention vá»›i $h$ head cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh báº±ng $$ \\begin{equation*} \\begin{aligned} \\text{MultiHead}(Q, K, V) \u0026amp;= \\text{Concat}(\\text{head}_1; \\dots; \\text{head}_h)W^O \\\\ \\text{vá»›i head}_i \u0026amp;= \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i) \\end{aligned} \\end{equation*} $$ , trong Ä‘Ã³ $Q, K, V \\in \\mathbb{R}^{n \\times d_{model}}$, cÃ¡c ma tráº­n trá»ng sá»‘ $W^Q_i, W^V_i, W^K_i \\in \\mathbb{R}^{d_{model} \\times d_x}$, káº¿t quáº£ Scale Dot-Product Attention $\\text{head}_i \\in \\mathbb{R}^{n \\times d_x}$ vÃ  $W^O \\in \\mathbb{R}^{d_{model} \\times d_{model}}$. NhÆ° váº­y, ma tráº­n káº¿t quáº£ cá»§a Multi-Head Attention thuá»™c $\\mathbb{R}^{n \\times d_{model}}$.\nMasked-Multi Head Attention Masked-Multi Head Attention lÃ  má»™t biáº¿n thá»ƒ cá»§a Multi-Head Attention vÃ  nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng trong Decoder cá»§a Transformer. Má»¥c Ä‘Ã­ch cá»§a Masked-Multi Head Attention lÃ  ngÄƒn cháº·n quÃ¡ trÃ¬nh Attention táº¡i má»™t sá»‘ vá»‹ trÃ­ trong cÃ¢u. Ta cÃ³ thá»ƒ gá»i phÃ©p tÃ­nh Attention á»Ÿ trong Masked-Multi Head Attention lÃ  Masked Scale Dot-Product Attention, vÃ  Self-Attention táº¡i Ä‘Ã¢y cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i lÃ  Masked Self-Attention.\nLÃ½ do ta cáº§n Ä‘áº¿n thÃ nh pháº§n nÃ y á»Ÿ Decoder lÃ  Ä‘á»ƒ trÃ¡nh viá»‡c mÃ´ hÃ¬nh \u0026ldquo;nhÃ¬n tháº¥y\u0026rdquo; Ä‘Æ°á»£c cÃ¡c tá»« á»Ÿ phÃ­a sau tá»« hiá»‡n táº¡i khi nÃ³ Ä‘ang Ä‘Æ°a ra dá»± Ä‘oÃ¡n khi ta thá»±c hiá»‡n Self-Attention táº¡i Decoder. Minh há»a cho Ã½ tÆ°á»Ÿng nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i.\nMinh há»a Ã½ tÆ°á»Ÿng cá»§a Masked Self-Attention\nLÆ°u Ã½. Báº£n cháº¥t cá»§a phÃ©p toÃ¡n lÃ  ta sáº½ lÃ m cho giÃ¡ trá»‹ attention weights Ä‘áº¿n cÃ¡c tá»« nÃªn Ä‘Æ°á»£c bá» qua thÃ nh má»™t giÃ¡ trá»‹ ráº¥t nhá» Ä‘á»ƒ nÃ³ háº§u nhÆ° khÃ´ng thá»ƒ tÃ¡c Ä‘á»™ng gÃ¬ Ä‘áº¿n káº¿t quáº£ chung cá»§a quÃ¡ trÃ¬nh Attention.\nÄá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³, ta sáº½ dÃ¹ng má»™t máº·t náº¡ (mask) Ä‘á»ƒ Ä‘Ã¡nh dáº¥u nhá»¯ng tá»« bá»‹ bá» qua khi Ä‘ang xÃ©t Ä‘áº¿n má»™t tá»« nÃ o Ä‘Ã³ trong cÃ¢u. VÃ­ dá»¥, vá»›i cÃ¢u \u0026ldquo;TÃ´i Ä‘i há»c á»Ÿ HÃ  Ná»™i\u0026rdquo;, ta xÃ©t tá»« \u0026ldquo;Ä‘i\u0026rdquo; thÃ¬ giÃ¡ trá»‹ cá»§a mask sáº½ lÃ  [0, 0, 1, 1, 1, 1], vá»›i pháº§n tá»­ báº±ng 1 cÃ³ nghÄ©a lÃ  giÃ¡ trá»‹ táº¡i thÃ nh pháº§n Ä‘Ã³ bá»‹ bá» qua. LÃºc Ä‘Ã³, trÆ°á»›c khi tÃ­nh attention weights, ta kiá»ƒm tra mask xem giÃ¡ trá»‹ nÃ o báº±ng 1 thÃ¬ gÃ¡n cho pháº§n tá»­ tÆ°Æ¡ng á»©ng á»Ÿ Ä‘Ã³ lÃ  $-\\infty$. Khi Ä‘Ã³, sau phÃ©p toÃ¡n softmax, giÃ¡ trá»‹ attention weight táº¡i Ä‘Ã³ sáº½ xáº¥p xá»‰ 0. NhÆ° váº­y, trong Masked Multi-Head Attention, quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Scale Dot-Product Attention sáº½ bá»• sung thÃªm má»™t pháº§n kiá»ƒm tra mask vÃ  gÃ¡n láº¡i giÃ¡ trá»‹ trÆ°á»›c khi tÃ­nh softmax. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i.\nQuÃ¡ trÃ¬nh tÃ­nh toÃ¡n trong Scale Dot-Product Attention khi thá»±c hiá»‡n thÃªm thao tÃ¡c kiá»ƒm tra mask\nLayer Normalization Trong cÃ¡c mÃ´ hÃ¬nh Deep Learning, ta thÆ°á»ng tháº¥y sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c normalization layer nháº±m cáº£i thiá»‡n kháº£ nÄƒng há»™i tá»¥ cá»§a mÃ´ hÃ¬nh. Hai loáº¡i normalization layer thÆ°á»ng tháº¥y nháº¥t lÃ  Batch Normalization (chuáº©n hÃ³a theo batch) vÃ  Layer Normalization (chuáº©n hÃ³a theo tá»«ng máº«u). Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh thuá»™c bÃ i toÃ¡n NLP nÃ³i chung vÃ  Transformer nÃ³i riÃªng thÃ¬ loáº¡i layer Ä‘Æ°á»£c sá»­ dá»¥ng thÆ°á»ng lÃ  Layer Normalization. Sá»± khÃ¡c nhau giá»¯a hai loáº¡i layer nÃ y khi Ã¡p dá»¥ng vÃ o bÃ i toÃ¡n ngÃ´n ngá»¯ Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i, trong Ä‘Ã³:\nLayer Normalization: Ta chuáº©n hÃ³a vector Ä‘áº·c trÆ°ng cá»§a tá»«ng vá»‹ trÃ­ trÃªn tá»«ng máº«u má»™t (vá»›i sá»‘ vá»‹ trÃ­ báº±ng vá»›i Ä‘á»™ dÃ i cá»§a cÃ¢u). Má»i vá»‹ trÃ­ cá»§a cÃ¹ng má»™t máº«u sáº½ sá»­ dá»¥ng chung má»™t bá»™ tham sá»‘ gain vÃ  bias. NhÆ° váº­y, viá»‡c chuáº©n hÃ³a cá»§a tá»«ng máº«u sáº½ Ä‘á»™c láº­p vá»›i nhau. Batch Normalization: Äá»‘i vá»›i loáº¡i layer nÃ y, ta sáº½ chuáº©n hÃ³a tá»«ng pháº§n tá»­ cá»§a vector Ä‘áº·c trÆ°ng táº¡i tá»«ng vá»‹ trÃ­ trong cÃ¢u Ä‘áº§u vÃ o vÃ  viá»‡c tÃ­nh toÃ¡n Ä‘Æ°á»£c dá»±a theo toÃ n bá»™ cÃ¡c máº«u trong cÃ¹ng batch. Layer Normalization vÃ  Batch Normalization khi Ã¡p dá»¥ng vÃ o bÃ i toÃ¡n ngÃ´n ngá»¯\nQua sá»± khÃ¡c biá»‡t Ä‘Ã³, ta cÃ³ thá»ƒ tháº¥y ráº±ng Batch Normalization khÃ´ng nÃªn Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o cÃ¡c bÃ i toÃ¡n NLP vÃ¬ váº¥n Ä‘á» sá»± khÃ¡c nhau vá» Ä‘á»™ dÃ i tháº­t sá»± cá»§a cÃ¡c cÃ¢u trong cÃ¹ng má»™t batch sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ chuáº©n hÃ³a (dÃ¹ ta Ä‘Ã£ sá»­ dá»¥ng kÄ© thuáº­t padding Ä‘á»ƒ Ä‘Æ°a cÃ¡c cÃ¢u Ä‘Ã³ vá» cÃ¹ng má»™t Ä‘á»™ dÃ i nhÆ°ng cÃ¡c vá»‹ trÃ­ Ä‘Æ°á»£c padding láº¡i cÃ³ vector Ä‘áº·c trÆ°ng vá»›i giÃ¡ trá»‹ khÃ¡ vÃ´ nghÄ©a).\nCá»¥ thá»ƒ hÆ¡n, náº¿u trong cÃ¹ng má»™t batch, cÃ³ má»™t cÃ¢u cÃ³ Ä‘á»™ dÃ i lá»›n vÃ  nhiá»u cÃ¢u cÃ³ Ä‘á»™ dÃ i nhá» thÃ¬ Ä‘áº·c trÆ°ng cá»§a cÃ¡c tá»« á»Ÿ vá»‹ trÃ­ phÃ­a sau cá»§a cÃ¢u dÃ i hÆ¡n Ä‘Ã³ sáº½ cÃ³ kháº£ nÄƒng cao bá»‹ máº¥t Ä‘i khi ta Ã¡p dá»¥ng Batch Normalization. Feed Forward Network vÃ  skip connection BÃªn cáº¡nh cÃ¡c quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Attention, cÃ¡c tÃ¡c giáº£ cá»§a Transformer sá»­ dá»¥ng thÃªm má»™t sá»‘ Feed Forward Network (gá»“m cÃ¡c fully connected layer) vÃ  ká»¹ thuáº­t Skip connection Ä‘á»ƒ tÄƒng thÃªm kháº£ nÄƒng há»c cÃ¡c Ä‘áº·c trÆ°ng cá»§a mÃ´ hÃ¬nh (cÃ¡c Ã´ mÃ u xanh dÆ°Æ¡ng trong hÃ¬nh á»Ÿ pháº§n nÃ y).\nÄáº§u tiÃªn, Feed Forward Network (FFN) trong Transformer sá»­ dá»¥ng 2 fully connected layers vá»›i sá»‘ unit láº§n lÆ°á»£t lÃ  $d_{ff}$ vÃ  $d_{model}$ cÃ¹ng activation function ReLU trong layer Ä‘áº§u tiÃªn. Input cá»§a FFN lÃ  má»™t ma tráº­n thuá»™c $\\mathbb{R}^{n \\times d_{model}}$. NhÆ° váº­y, ta cÃ³ thá»ƒ biá»ƒu diá»…n káº¿t quáº£ Ä‘áº§u ra cá»§a FFN theo cÃ´ng thá»©c $$ \\begin{equation*} FFN(X) = \\max(0, XW_1 + b_1) W_2 + b_2 \\end{equation*} $$ , vá»›i hai ma tráº­n trá»ng sá»‘ $W_1 \\in \\mathbb{R}^{d_{model \\times d_{ff}}}$ vÃ  $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$ vÃ  $b_1$, $b_2$ lÃ  cÃ¡c bias vector. Do Ä‘Ã³ $FFN(X) \\in \\mathbb{R}^{n \\times d_{model}}$.\nNgoÃ i ra, skip connection lÃ  má»™t Ã½ tÆ°á»Ÿng ráº¥t hay vÃ  phá»• biáº¿n trong Deep Learning ká»ƒ tá»« khi nÃ³ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ nh cÃ´ng trong mÃ´ hÃ¬nh ResNet. Skip connection cÃ³ thá»ƒ giÃºp cho gradient Ä‘Æ°á»£c lan truyá»n tá»‘t hÆ¡n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh, tá»« Ä‘Ã³ gÃ³p pháº§n lÃ m giáº£m hiá»‡n tÆ°á»£ng vanishing gradient. MÃ¬nh Ä‘Ã£ Ä‘á» cáº­p Ä‘áº¿n Ã½ tÆ°á»Ÿng nÃ y trong bÃ i viáº¿t vá» ResNet.\nTrong kiáº¿n trÃºc Transformer, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ Ã¡p dá»¥ng skip connection cÃ¹ng vá»›i Layer Normalization á»Ÿ ráº¥t nhiá»u vá»‹ trÃ­ (cÃ¡c khá»‘i \u0026ldquo;Add \u0026amp; Norm\u0026rdquo; trong hÃ¬nh á»Ÿ pháº§n nÃ y). Ta cÃ³ thá»ƒ biá»ƒu diá»…n output cá»§a cÃ¡c khá»‘i Ä‘Ã³ á»Ÿ dáº¡ng $$\\text{LayerNorm}(X + \\text{Sublayer}(X)))$$ , vá»›i $\\text{Sublayer}$ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘áº¡i diá»‡n cho nhá»¯ng thÃ nh pháº§n á»Ÿ phÃ­a trÆ°á»›c cÃ¡c khá»‘i Ä‘Ã³.\nLÆ°u Ã½. Má»™t hÆ°á»›ng giáº£i thÃ­ch cho chi tiáº¿t Feed Forward Network chá»‰ Ã¡p dá»¥ng activation function ReLU cho layer Ä‘áº§u tiÃªn lÃ  vÃ¬ ngay sau Ä‘Ã³ ta Ä‘Ã£ sá»­ dá»¥ng skip connection. ÄÃ¢y lÃ  má»™t kÄ© thuáº­t thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng khi lÃ m viá»‡c vá»›i skip connection.\nNáº¿u ta sá»­ dá»¥ng activation function ReLU rá»“i sau Ä‘Ã³ Ã¡p dá»¥ng skip connection thÃ¬ cÃ³ thá»ƒ nÃ³i lÃ  giÃ¡ trá»‹ cÃ¡c pháº§n tá»­ trong ma tráº­n Ä‘áº§u vÃ o sáº½ luÃ´n khÃ´ng giáº£m, vÃ  hiá»‡u á»©ng nÃ y cÃ³ thá»ƒ sáº½ cÃ³ áº£nh hÆ°á»Ÿng khÃ´ng tá»‘t Ä‘áº¿n mÃ´ hÃ¬nh. Encoder Sau khi Ä‘Ã£ trÃ¬nh bÃ y vá» cÃ¡c thÃ nh pháº§n quan trá»ng trong kiáº¿n trÃºc cá»§a Transformer, ta sáº½ Ä‘i vÃ o cÃ¡c nhÃ¡nh chÃ­nh cá»§a kiáº¿n trÃºc vÃ  nhÃ¡nh Ä‘áº§u tiÃªn lÃ  Encoder. Kiáº¿n trÃºc cá»§a Encoder Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i:\nLayer Kiáº¿n trÃºc cá»§a Transformer Encoder\nThÃ nh pháº§n Encoder trong Transformer lÃ  sá»± káº¿t há»£p cá»§a Word Embedding, Positional Encoding vÃ  má»™t dÃ£y gá»“m $N$ Encoder Layer liÃªn tiáº¿p nhau. Trong Ä‘Ã³, Encoder Layer bao gá»“m nhiá»u thÃ nh pháº§n nhÆ° Multi-Head Attention, Feed Forward, skip connection vÃ  Layer Normalization.\nCá»¥ thá»ƒ hÆ¡n, trong bÃ i toÃ¡n Machine Translation, input cá»§a Encoder sáº½ lÃ  cÃ¢u vÄƒn thuá»™c ngÃ´n ngá»¯ nguá»“n gá»“m $L$ tá»«. Trong Ä‘Ã³, cÃ¡c \u0026ldquo;tá»«\u0026rdquo; trong cÃ¢u lÃºc nÃ y lÃ  cÃ¡c con sá»‘ á»©ng vá»›i vá»‹ trÃ­ cá»§a tá»« Ä‘Ã³ trong tá»« Ä‘iá»ƒn cá»§a ngÃ´n ngá»¯ nguá»“n. Khi Ä‘Ã³:\nSau khi qua Word Embedding vÃ  Positional Encoding, ta Ä‘Æ°á»£c má»™t ma tráº­n $X \\in \\mathbb{R}^{L \\times d_{model}}$. Táº¡i má»—i Encoder Layer, tá»« ma tráº­n input $X\u0026rsquo; \\in \\mathbb{R}^{N \\times d_{model}}$, ta sáº½ sá»­ dá»¥ng ba ma tráº­n trá»ng sá»‘ $W_Q, W_K, W_V$ Ä‘á»ƒ tÃ­nh ra $Q$ (Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ pháº§n , $K$, $V$ tá»« $X\u0026rsquo;$ vÃ  sau Ä‘Ã³ báº¯t Ä‘áº§u Ä‘i qua cÃ¡c thÃ nh pháº§n trong Ä‘Ã³. Ma tráº­n output cá»§a Encoder Layer cÅ©ng sáº½ lÃ  má»™t ma tráº­n thuá»™c $\\mathbb{R}^{L \\times d_{model}}$. LÆ°u Ã½. Encoder output sáº½ lÃ  má»™t ma tráº­n thuá»™c $\\mathbb{R}^{L \\times d_{model}}$, má»™t ma tráº­n cÃ³ shape giá»‘ng vá»›i ma tráº­n Ä‘áº§u vÃ o $X$ cá»§a Encoder. NhÆ° váº­y, ta cÃ³ thá»ƒ hÃ¬nh dung ráº±ng Encoder Ä‘ang lÃ m nhiá»‡m vá»¥ lÃ  bá»• sung thÃªm nhá»¯ng Ä‘áº·c trÆ°ng quan trá»ng vÃ o embedding vector ban Ä‘áº§u cá»§a cÃ¡c tá»« trong cÃ¢u.\nÄá»‘i vá»›i Encoder output, ta sáº½ sá»­ dá»¥ng ma tráº­n nÃ y Ä‘á»ƒ tham gia vÃ o phÃ©p tÃ­nh Cross-Attention trong Decoder.\nDecoder Tá»•ng quan vá» kiáº¿n trÃºc Kiáº¿n trÃºc cá»§a Decoder Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. Decoder cÅ©ng cÃ³ cÃ¡ch tá»• chá»©c khÃ¡ tÆ°Æ¡ng tá»± Encoder khi ta báº¯t Ä‘áº§u báº±ng Word Embedding vÃ  Positional Encoding, sau Ä‘Ã³ lÃ  dÃ£y gá»“m $N$ Decoder Layer liÃªn tiáº¿p nhau. Pháº§n cuá»‘i cá»§a Decoder lÃ  má»™t fully connected layer kÃ¨m theo hÃ m softmax Ä‘á»ƒ ta chá»n ra tá»« phÃ¹ há»£p nháº¥t lÃ m output cá»§a mÃ´ hÃ¬nh Ä‘á»‘i vá»›i vá»‹ trÃ­ hiá»‡n táº¡i trong cÃ¢u thuá»™c ngÃ´n ngá»¯ Ä‘Ã­ch.\nKiáº¿n trÃºc cá»§a Transformer Decoder\nTa cáº§n Ä‘á»ƒ Ã½ ráº±ng, trong Decoder Layer, ta sáº½ thá»±c hiá»‡n cáº£ hai phÃ©p tÃ­nh Self-Attention vÃ  Cross-Attention. Trong Ä‘Ã³:\nSelf-Attention Ä‘Æ°á»£c thá»±c hiá»‡n trÆ°á»›c Ä‘á»ƒ tiáº¿n hÃ nh Attention Ä‘áº¿n cÃ¡c vá»‹ trÃ­ á»Ÿ phÃ­a trÆ°á»›c vá»‹ trÃ­ hiá»‡n táº¡i trong cÃ¢u, tá»©c lÃ  ta Ä‘ang sá»­ dá»¥ng Masked Multi-Head Attention. Sau Ä‘Ã³, Cross-Attention Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i hai thÃ nh pháº§n $K$ vÃ  $V$ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« Encoder output. LÃºc nÃ y thÃ¬ ta Ä‘ang thá»±c hiá»‡n Attention Ä‘áº¿n toÃ n bá»™ cÃ¡c vá»‹ trÃ­ trong Encoder output nÃªn Multi-Head Attention Ä‘Æ°á»£c sá»­ dá»¥ng. NgoÃ i ra, ta sáº½ Ä‘áº·t sá»‘ lÆ°á»£ng tá»« trong cÃ¢u nguá»“n vÃ  cÃ¢u Ä‘Ã­ch cá»§a mÃ´ hÃ¬nh lÃ  báº±ng nhau vÃ  báº±ng $L$ (trong trÆ°á»ng há»£p cÃ¢u nÃ o ngáº¯n hÆ¡n thÃ¬ ta sáº½ sá»­ dá»¥ng ká»¹ thuáº­t padding Ä‘á»ƒ thÃªm cÃ¡c tá»« vÃ o). Do Ä‘Ã³:\nTrÆ°á»›c khi Ä‘áº¿n vá»›i fully connected layer cuá»‘i cÃ¹ng cá»§a Decoder Ä‘á»ƒ tiáº¿n hÃ nh phÃ¢n lá»›p, ma tráº­n output ta nháº­n Ä‘Æ°á»£c cÅ©ng sáº½ thuá»™c $\\mathbb{R}^{L \\times d_{model}}$. Sá»‘ chiá»u cá»§a output vector cá»§a fully connected layer cuá»‘i cÃ¹ng sáº½ báº±ng vá»›i kÃ­ch thÆ°á»›c táº­p tá»« Ä‘iá»ƒn cá»§a ngÃ´n ngá»¯ Ä‘Ã­ch. Decoder trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh, ta sáº½ táº¡o ra cÃ¡c cáº·p (input, ground truth) cá»§a Decoder theo cÃ¡ch khÃ¡ dáº·c biá»‡t. Äáº§u tiÃªn, ta sáº½ thÃªm cÃ¡c tá»« Ä‘Ã¡nh dáº¥u cho viá»‡c \u0026ldquo;báº¯t Ä‘áº§u\u0026rdquo; vÃ  \u0026ldquo;káº¿t thÃºc\u0026rdquo; cá»§a quÃ¡ trÃ¬nh dá»‹ch. Ta gá»i Ä‘Ã¢y lÃ  cÃ¡c tá»« \u0026ldquo;\u0026rdquo; vÃ  \u0026ldquo;\u0026rdquo;. CÃ¡c cáº·p (input, ground truth) dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n Decoder sáº½ Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch \u0026ldquo;shifted right\u0026rdquo; má»™t cÃ¢u thuá»™c ngÃ´n ngá»¯ Ä‘Ã­ch. VÃ­ dá»¥, vá»›i cÃ¢u Tiáº¿ng Viá»‡t lÃ  \u0026ldquo;tÃ´i Ä‘i há»c\u0026rdquo;, ta sáº½ cÃ³ cÃ¡c cáº·p (input, ground truth) tÆ°Æ¡ng á»©ng nhÆ° sau:\nInput: \u0026ldquo;\u0026rdquo;, \u0026ldquo;tÃ´i\u0026rdquo;, \u0026ldquo;Ä‘i\u0026rdquo;, \u0026ldquo;há»c\u0026rdquo;. Ground truth: \u0026ldquo;tÃ´i\u0026rdquo;, \u0026ldquo;Ä‘i\u0026rdquo;, \u0026ldquo;há»c\u0026rdquo;, \u0026ldquo;\u0026rdquo; NgoÃ i ra, cho dÃ¹ Decoder cÃ³ dá»± Ä‘oÃ¡n ra tá»« nÃ o á»Ÿ vá»‹ trÃ­ $t$ cá»§a cÃ¢u Ä‘Ã­ch Ä‘i ná»¯a thÃ¬ input cá»§a Decoder á»Ÿ vá»‹ trÃ­ $t+1$ cÅ©ng luÃ´n lÃ  má»™t tá»« Ä‘Ãºng (tá»©c lÃ  ground truth cá»§a vá»‹ trÃ­ $t$). Ká»¹ thuáº­t nÃ y gá»i lÃ  Teacher Forcing vÃ  nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t nhiá»u trong cÃ¡c bÃ i toÃ¡n NLP. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c minh há»a trong hÃ¬nh bÃªn dÆ°á»›i.\nKá»¹ thuáº­t Teacher Forcing vá»›i Decoder trong huáº¥n luyá»‡n Transformer\nDecoder trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n Äá»‘i vá»›i quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n (hay lÃ  dá»‹ch) cá»§a Transformer, Decoder sáº½ báº¯t Ä‘áº§u vá»›i má»™t tá»« lÃ  \u0026ldquo;\u0026lt;start\u0026gt;\u0026rdquo; vÃ  quÃ¡ trÃ¬nh dá»‹ch sáº½ tiáº¿p tá»¥c cho Ä‘áº¿n khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ra tá»« \u0026ldquo;\u0026lt;end\u0026gt;\u0026rdquo;. HÆ¡n ná»¯a, tá»« Ä‘Æ°á»£c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ra á»Ÿ vá»‹ trÃ­ $t$ sáº½ Ä‘Æ°á»£c dÃ¹ng lÃ m input cá»§a Decoder cho vá»‹ trÃ­ $t+1$. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i.\nDecoder trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n\nKáº¿t luáº­n NhÆ° váº­y, sau má»™t bÃ i viáº¿t ráº¥t dÃ i thÃ¬ mÃ¬nh Ä‘Ã£ trÃ¬nh bÃ y vá» Transformer vá»›i khÃ¡ nhiá»u phÃ¢n tÃ­ch vÃ o Ã½ tÆ°á»Ÿng vÃ  báº£n cháº¥t cá»§a cÃ¡c thÃ nh pháº§n. Tá»« sá»± thÃ nh cÃ´ng cá»§a Transformer trong Machine Translation, má»™t ká»· nguyÃªn má»›i tháº­t sá»± Ä‘Ã£ má»Ÿ ra Ä‘á»‘i vá»›i NLP nÃ³i riÃªng vÃ  Deep Learning nÃ³i chung:\nCÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° ChatGPT, Bard,\u0026hellip; Ä‘á»u dá»±a trÃªn ná»n táº£ng kiáº¿n trÃºc cá»§a Transformer. RÃµ hÆ¡n má»™t chÃºt thÃ¬ chÃºng sáº½ sá»­ dá»¥ng Transformer Decoder vÃ  sáº½ khÃ´ng dÃ¹ng Ä‘áº¿n Cross Attention ğŸ˜‰ Äá»‘i vá»›i cÃ¡c bÃ i toÃ¡n thuá»™c lÄ©nh vá»±c CV, hay lÃ  multi-model nhÆ° text-to-image (Stable Diffusion,\u0026hellip;) thÃ¬ cÅ©ng Ä‘á»u Ä‘ang táº­n dá»¥ng sá»©c máº¡nh cá»§a Transformer vÃ  cÃ¡c thÃ nh pháº§n cá»§a chÃºng, Ä‘áº·c biá»‡t lÃ  Cross Attention. TÃ i liá»‡u tham kháº£o Vaswani, Ashish, et al. \u0026ldquo;Attention is all you need.\u0026rdquo; Advances in neural information processing systems 30 (2017). Weng, Lilian, \u0026ldquo;Attention? Attention!\u0026rdquo; Pháº¡m BÃ¡ CÆ°á»ng Quá»‘c, \u0026ldquo;TÃ¬m hiá»ƒu mÃ´ hÃ¬nh Transformer - NgÆ°Æ¡i KhÃ´ng Pháº£i LÃ  Anh HÃ¹ng, NgÆ°Æ¡i LÃ  QuÃ¡i Váº­t Nhiá»u Äáº§u\u0026rdquo; Shen, Sheng, et al. \u0026ldquo;Powernorm: Rethinking batch normalization in transformers.\u0026rdquo; International Conference on Machine Learning. PMLR, 2020. ProtonX, Transformer - Encoder ProtonX, Transformer - Decoder Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \u0026ldquo;Effective approaches to attention-based neural machine translation.\u0026rdquo;, arXiv preprint arXiv:1508.04025 (2015). Notes On AI, \u0026ldquo;Attention Machenism\u0026rdquo; Sebastian Raschka, \u0026ldquo;Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\u0026rdquo; Jay Alammar, \u0026ldquo;The Illustrated Transformer\u0026rdquo; Dive Into Deep Learning, \u0026ldquo;Beam Search\u0026rdquo; ","date":"2023-07-17T21:58:29+07:00","permalink":"https://htrvu.github.io/post/transformer_01/","title":"CÆ¡ cháº¿ Attention vÃ  mÃ´ hÃ¬nh Transformer"},{"content":"Note. VÃ¬ gáº§n Ä‘Ã¢y mÃ¬nh báº­n khÃ¡ nhiá»u viá»‡c nÃªn blog Ä‘ang bá»‹ â€œÄ‘Ã³ng bÄƒngâ€ ğŸ¥² BÃ i viáº¿t nÃ y chá»‰ mang tÃ­nh cháº¥t chá»¯a chÃ¡y sau hÆ¡n 1 thÃ¡ng khÃ´ng cÃ³ bÃ i má»›i (Ä‘Ã¢y lÃ  bÃ i Ä‘Ã£ Ä‘Æ°á»£c viáº¿t sáºµn tá»« trÆ°á»›c) =))\nTuy nhiÃªn, MLP Mixer cÅ©ng lÃ  má»™t mÃ´ hÃ¬nh nÃ y cÅ©ng ráº¥t thÃº vá»‹. Chá»‰ hÆ¡i tiáº¿c má»™t chÃºt lÃ  mÃ¬nh láº¡i post nÃ³ trÆ°á»›c khi viáº¿t nhá»¯ng bÃ i vá» Transformer vÃ  Vision Transformerâ€¦\nGiá»›i thiá»‡u VÃ o thá»i Ä‘iá»ƒm nÄƒm 2021, cÃ³ thá»ƒ nÃ³i ráº±ng nháº¯c tá»›i cÃ¡c mÃ´ hÃ¬nh trong Computer Vision thÃ¬ ai ai cÅ©ng nghÄ© Ä‘áº¿n má»™t lÃ  CNN, hai lÃ  cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Transformer Ä‘ang lÃ m mÆ°a lÃ m giÃ³ vá»›i nhá»¯ng mÃ´ hÃ¬nh máº¡nh máº½ nhÆ° Vision Transformer hay Swin Transformer.\nGiá»¯a tÃ¬nh hÃ¬nh Ä‘Ã³, má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c cÃ´ng bá»‘ mÃ  khi nghe qua lÃ  ta Ä‘Ã£ tháº¥y khÃ³ mÃ  tin Ä‘Æ°á»£c lÃ  nÃ³ tá»‘t Ä‘áº¿n tháº¿. MLP-Mixer, mÃ´ hÃ¬nh chá»‰ sá»­ dá»¥ng cÃ¡c fully connected layer, há»‡t nhÆ° thá»i â€œxa xÆ°aâ€ ta dÃ¹ng Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c Deep Neural Network (DNN), hay cÃ²n láº¡i lÃ  Multi-layer Perceptrons (MLP) ğŸ˜€\nMulti-layer Perceptron TrÆ°á»›c háº¿t, ta cÃ¹ng nháº¯c láº¡i Ä‘áº¿n nhá»¯ng háº¡n cháº¿ khiáº¿n DNN trá»Ÿ nÃªn khÃ´ng phÃ¹ há»£p vá»›i computer vision vÃ  gáº§n nhÆ° Ä‘Ã£ bá»‹ bá» quÃªn khi CNN Ä‘Æ°á»£c phÃ¡t triá»ƒn:\nDNN dá»… bá»‹ overfitting Sá»‘ lÆ°á»£ng tham sá»‘ cá»§a má»™t mÃ´ hÃ¬nh DNN lÃ  ráº¥t lá»›n NÃ³ khÃ³ há»c Ä‘Æ°á»£c nhá»¯ng Ä‘áº·c trÆ°ng liÃªn quan sá»± dá»‹ch chuyá»ƒn vá»‹ trÃ­ trong khÃ´ng gian VÃ­ dá»¥, xÃ©t 4 bá»©c áº£nh chá»©a cÃ¹ng má»™t con chÃ³ nhÆ° á»Ÿ bÃªn dÆ°á»›i thÃ¬ khi cÃ¡c áº£nh nÃ y Ä‘Æ°á»£c Ä‘Æ°a vÃ o DNN Ä‘á»ƒ tÃ­nh toÃ¡n, ta sáº½ cÃ³ cÃ¡c vector ráº¥t khÃ¡c nhau. Trong khi Ä‘Ã³, vá»›i CNN, qua cÃ¡c láº§n sá»­ dá»¥ng filter thÃ¬ ta sáº½ thu vá» Ä‘Æ°á»£c cÃ¡c feature map tÆ°Æ¡ng tá»± nhau. VÃ­ dá»¥ vá» sá»± quan trá»ng cá»§a cÃ¡c Ä‘áº·c trÆ°ng liÃªn quan Ä‘áº¿n sá»± dá»‹ch chuyá»ƒn vá»‹ trÃ­ trong khÃ´ng quan NhÆ° váº­y, ta dá»… hiá»ƒu ráº±ng mÃ´ hÃ¬nh MLP-Mixer mÃ  bÃ i viáº¿t nÃ y giá»›i thiá»‡u sáº½ cÃ³ cÃ¡c cÃ¡ch Ä‘á»ƒ kháº¯c phá»¥c Ä‘Æ°á»£c nhá»¯ng váº¥n Ä‘á» Ä‘Ã³. LÆ°u Ã½ ráº±ng, MLP-Mixer khÃ´ng pháº£i lÃ  SOTA khi nÃ³ Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ  nÃ³ váº«n cÃ²n kÃ©m má»™t chÃºt so vá»›i Vision Transformer. Tuy nhiÃªn, xÃ©t Ä‘áº¿n tá»‘c Ä‘á»™ thÃ¬ MLP-Mixer nhanh hÆ¡n khÃ¡ nhiá»u.\nSo sÃ¡nh MLP-Mixer vÃ  Vision Transformer trÃªn táº­p ImageNet Chia áº£nh input thÃ nh cÃ¡c pháº§n nhá» Náº¿u Ä‘Ã£ tá»«ng xem qua cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Transformer trong Computer Vision thÃ¬ ta cÅ©ng Ä‘Ã£ ráº¥t quen thuá»™c vá»›i bÆ°á»›c nÃ y. áº¢nh input $H \\times W \\times C$ cá»§a mÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c chia thÃ nh cÃ¡c patch (hay lÃ  token) nhá» cÃ³ kÃ­ch thÆ°á»›c báº±ng nhau lÃ  $P \\times P \\times C$. GiÃ¡ trá»‹ cá»§a $H$, $W$ vÃ  $P$ thÆ°á»ng Ä‘Æ°á»£c chá»n sao cho ta chia vá»«a Ä‘á»§ sá»‘ patch, vÃ  $H$ thÆ°á»ng báº±ng $W$. NhÆ° váº­y sá»‘ patch mÃ  ta cÃ³ Ä‘Æ°á»£c lÃ \n$$ S = \\frac{H \\times W}{P^2} $$\nChia áº£nh input thÃ nh cÃ¡c tokens (hay patches) Sau Ä‘Ã³, ta sáº½ duá»—i nhá»¯ng patch cÃ³ Ä‘Æ°á»£c nÃ y Ä‘á»ƒ lÃ m input cho mÃ´ hÃ¬nh. LÃºc nÃ y, má»™t ma tráº­n input cá»§a ta sáº½ cÃ³ shape lÃ  $(S, P \\times P \\times C)$, vá»›i má»—i hÃ ng lÃ  â€œÄ‘áº·c trÆ°ngâ€ ban Ä‘áº§u cá»§a má»—i patch. á» cÃ¡c pháº§n tiáº¿p theo, ta gá»i $P \\times P \\times C$ lÃ  sá»‘ â€œchannelâ€ cá»§a má»™t token, vÃ  ta kÃ­ hiá»‡u nÃ³ lÃ  $C$ (hÆ¡i lÃº chÃºt ğŸ˜…)\nMinh há»a ma tráº­n input Channel-mixing vÃ  token-mixing Ã nghÄ©a Trong MLP-Mixer, cÃ¡c tÃ¡c giáº£ giá»›i thiá»‡u hai block Ä‘áº·c biá»‡t lÃ  channel-mixing vÃ  token-mixing. ÄÃ¢y cÅ©ng chÃ­nh lÃ  nhá»¯ng gÃ¬ tinh hoa nháº¥t trong MLP-Mixer.\nTa cÃ³ thá»ƒ mÃ´ táº£ hai block nÃ y nhÆ° sau:\nChannel-mixing: LiÃªn quan Ä‘áº¿n quan há»‡ giá»¯a cÃ¡c channel trong cÃ¹ng má»™t token, tá»©c lÃ  cÃ¡c hÃ ng cá»§a ma tráº­n input. NÃ³ thá»±c hiá»‡n phÃ©p toÃ¡n káº¿t há»£p nhá»¯ng giÃ¡ trá»‹ trÃªn cÃ¡c channel cá»§a cÃ¹ng má»™t patch vÃ  cÃ³ sá»± Ä‘á»™c láº­p giá»¯a cÃ¡c patch, do Ä‘Ã³ ta gá»i nÃ³ lÃ  channel-mixing. Token-mixing: LiÃªn quan Ä‘áº¿n quan há»‡ giá»¯a cÃ¡c giÃ¡ trá»‹ á»Ÿ cÃ¹ng má»™t vá»‹ trÃ­ trong cÃ¡c token, tá»©c lÃ  cÃ¡c cá»™t cá»§a ma tráº­n input. Ta tháº¥y ráº±ng cÃ¡c giÃ¡ trá»‹ táº¡i cÃ¹ng má»™t vá»‹ trÃ­ trÃªn cÃ¡c patch (token) Ä‘Æ°á»£c káº¿t há»£p vá»›i nhau vÃ  nÃ³ Ä‘á»™c láº­p giá»¯a cÃ¡c channel, do Ä‘Ã³ ta gá»i nÃ³ lÃ  token-mixing. Sá»± khÃ¡c biá»‡t giá»¯a channel-mixing vÃ  token-mixing trong khi cÃ i Ä‘áº·t chá»‰ Ä‘Æ¡n giáº£n lÃ  input cá»§a chÃºng. Vá»›i channel-mixing, ta dÃ¹ng luÃ´n ma tráº­n input cÃ³ shape lÃ  $(S, C)$, cÃ²n vá»›i token-mixing thÃ¬ ta cáº§n chuyá»ƒn vá»‹ ma tráº­n input thÃ nh shape $(C, S)$.\nMinh há»a input cá»§a Channel-mixing Minh há»a input cá»§a Token-mixing Block channel-mixing vÃ  token-mixing sáº½ bao gá»“m má»™t MLP vá»›i hai fully connected layer vÃ  chÃºng sá»­ dá»¥ng GELU activation function (sáº½ Ä‘Æ°á»£c á»Ÿ pháº§n tiáº¿p theo). TrÆ°á»›c khi tiáº¿n hÃ nh tÃ­nh toÃ¡n, ta sáº½ Ä‘Æ°a ma tráº­n input qua má»™t layer chuáº©n hÃ³a Ä‘á»ƒ Ä‘Æ°a ma tráº­n nÃ y vá» phÃ¢n phá»‘i chuáº©n táº¯c. HÆ¡n ná»¯a, ta cÅ©ng Ã¡p dá»¥ng skip connection Ä‘á»ƒ tÄƒng hiá»‡u quáº£ training.\nKiáº¿n trÃºc cá»§a vá» channel-mixing vÃ  token-mixing nhÆ° sau:\nKiáº¿n trÃºc cá»§a Channel-mixing Kiáº¿n trÃºc cá»§a Token-mixing CÃ³ má»™t sá»‘ Ä‘iá»ƒm cáº§n lÆ°u Ã½ nhÆ° sau:\nCÃ¡c pháº§n â€œMLP1â€ vÃ  â€œMLP2â€ á»Ÿ hai hÃ¬nh trÃªn Ä‘Æ°á»£c ghi theo tá»«ng khá»‘i dá»c cÃ³ nghÄ©a lÃ  ta Ä‘Ã£ chia sáº» trá»ng sá»‘ giá»¯a cÃ¡c vector input. Yáº¿u tá»‘ nÃ y sáº½ Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ nhá»¯ng pháº§n sauSS Trong token-mixing, trÆ°á»›c khi tÃ­nh toÃ¡n thÃ¬ ta cáº§n chuyá»ƒn vá»‹ ma tráº­n input Váº¥n Ä‘á» tiáº¿p theo ta cáº§n quan tÃ¢m lÃ  channel-mixing vÃ  token-mixing giÃºp cho MLP-Mixer Ä‘áº¡t Æ°á»£c nhá»¯ng gÃ¬.\nKáº¿t há»£p Ä‘áº·c trÆ°ng CÃ¡c mÃ´ hÃ¬nh trong Computer Vision thÆ°á»ng káº¿t há»£p cÃ¡c Ä‘áº·c trÆ°ng nhÆ° sau vá»›i nhau:\nÄáº·c trÆ°ng táº¡i má»™t vá»‹ trÃ­ nÃ o Ä‘Ã³ trong khÃ´ng gian Äáº·c trÆ°ng giá»¯a cÃ¡c vá»‹ trÃ­ khÃ¡c nhau trong khÃ´ng gian VÃ­ dá»¥:\nVá»›i CNN Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n nhá» vÃ o cÃ¡c phÃ©p toÃ¡n convolution vá»›i filter $K \\times K$ vÃ  pooling, mÃ  cá»¥ thá»ƒ hÆ¡n thÃ¬ ta thá»±c hiá»‡n Ä‘Æ°á»£c (2) khi $K \u0026gt; 1$, vÃ  thá»±c hiá»‡n Ä‘Æ°á»£c (1) khi $K = 1$. Vá»›i mÃ´ hÃ¬nh dá»±a trÃªn Transformer thÃ¬ cáº£ 2 Ä‘á»u Ä‘Æ°á»£c thá»±c hiá»‡n nhá» cÃ¡c self-attention layers. Khi xÃ©t Ä‘áº¿n MLP thÃ´ng thÆ°á»ng, ta chá»‰ thá»±c hiá»‡n Ä‘Æ°á»£c (1) Ã tÆ°á»Ÿng Ä‘áº±ng sau MLP-Mixer lÃ  sá»­ dá»¥ng channel-mixing cho yáº¿u tá»‘ (1), tháº­t sá»± lÃ  váº­y vÃ¬ channel-mixing tÃ­nh toÃ¡n trÃªn tá»«ng patch Ä‘á»™c láº­p; vÃ  sá»­ dá»¥ng token-mixing cho yáº¿u tá»‘ (2), token-mixing Ä‘Ã£ tÃ­nh toÃ¡n trÃªn cÃ¡c patch.\nChia sáº» trá»ng sá»‘ Chia sáº» trá»ng sá»‘ (typing weights) lÃ  má»™t yáº¿u tá»‘ giÃºp cho MLP-Mixer giáº£m Ä‘Æ°á»£c Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh, Ä‘á»“ng thá»i gÃ³p pháº§n há»— trá»£ viá»‡c há»c cÃ¡c Ä‘áº·c trÆ°ng liÃªn quan Ä‘áº¿n tÃ­nh khÃ´ng gian.\nXÃ©t phÃ©p toÃ¡n convolution, ta sáº½ tháº¥y ráº±ng phÃ©p toÃ¡n nÃ y cÃ³ sá»± chia sáº» trá»ng sá»‘ trong tá»«ng channel nhÆ° sau:\nVá»›i má»—i channel cá»§a input, ta sáº½ dÃ¹ng má»™t filter $K \\times K$ (cÃ³ $K^2$ trá»ng sá»‘) vÃ  láº§n lÆ°á»£t di chuyá»ƒn nÃ³ qua cÃ¡c vá»‹ trÃ­ trÃªn channel nÃ y Ä‘á»ƒ tÃ­nh toÃ¡n ra má»™t feature map. NhÆ° váº­y, toÃ n bá»™ cÃ¡c vÃ¹ng trÃªn input Ä‘á»u Ä‘Æ°á»£c Ã¡p dá»¥ng cÃ¹ng má»™t bá»™ trá»ng sá»‘ Ä‘á»ƒ tÃ­nh ra feature map. Nhá» vÃ o viá»‡c chia sáº» trá»ng sá»‘, náº¿u má»™t Ä‘á»‘i tÆ°á»£ng xuáº¥t hiá»‡n á»Ÿ nhá»¯ng vá»‹ trÃ­ khÃ¡c nhau trÃªn áº£nh input thÃ¬ feature map ta thu Ä‘Æ°á»£c cÅ©ng sáº½ ráº¥t tÆ°Æ¡ng tá»± nhau. PhÃ©p toÃ¡n convolution Äá»ƒ Ã½ ráº±ng, ta sáº½ khÃ´ng tÃ¬m tháº¥y sá»± chia sáº» trá»ng sá»‘ á»Ÿ MLP thÃ´ng thÆ°á»ng. LÃ½ do lÃ  vÃ¬ má»—i neuron trong má»™t layer cá»§a mÃ´ hÃ¬nh sáº½ ná»‘i vá»›i toÃ n bá»™ cÃ¡c neuron á»Ÿ layer phÃ­a trÆ°á»›c, do Ä‘Ã³ ta cÃ³ má»™t ma tráº­n trá»ng sá»‘ vá»›i má»—i dÃ²ng lÃ  má»™t vector trá»ng sá»‘ á»©ng vá»›i má»™t neuron.\nVáº­y trong MLP-Mixer thÃ¬ chia sáº» trá»ng sá»‘ xáº£y ra á»Ÿ Ä‘Ã¢u?\nVá»›i Ã½ tÆ°á»Ÿng ráº¥t giá»‘ng vá»›i filter trong CNN, channel-mixing sáº½ Ä‘Æ°á»£c Ã¡p dá»¥ng chia sáº» trá»ng sá»‘. Viá»‡c káº¿t há»£p Ä‘áº·c trÆ°ng trÃªn tá»«ng patch sáº½ Ä‘Æ°á»£c tiáº¿n hÃ nh bá»Ÿi cÃ¹ng má»™t bá»™ trá»ng sá»‘ (á»Ÿ Ä‘Ã¢y lÃ  vector). Äá»‘i vá»›i token-mixing thÃ¬ MLP-Mixer cÅ©ng Ã¡p dá»¥ng chia sáº» trá»ng sá»‘. Tuy nhiÃªn, yáº¿u tá»‘ nÃ y lÃ  ráº¥t hiáº¿m tháº¥y trong cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã³. LÃ­ do chÃ­nh cá»§a Ä‘iá»u nÃ y lÃ  nháº±m giáº£m sá»‘ lÆ°á»£ng trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh. CÃ¡c tÃ¡c giáº£ cugx cho biáº¿t ráº±ng há» Ä‘Ã£ thá»­ nghiá»‡m cáº£ hÆ°á»›ng khÃ´ng Ã¡p dá»¥ng sá»± chia sáº» trá»ng sá»‘ cho token-mixing vÃ  káº¿t quáº£ thÃ¬ ráº¥t xáº¥p xá»‰ nhau. Do Ä‘Ã³, á»Ÿ pháº§n giá»›i thiá»‡u channel-mixing vÃ  token-mixing thÃ¬ ta quan sÃ¡t tháº¥y cÃ¡ch biá»ƒu diá»…n pháº§n MLP theo tá»«ng khá»‘i â€œMLP2â€ vÃ  â€œMLP1â€ nhÆ° váº­y.\nGELU activation function Má»™t Ä‘iá»ƒm Ä‘Ã¡ng chÃº Ã½ á»Ÿ trong MLP-Mixer lÃ  mÃ´ hÃ¬nh nÃ y cÃ³ sá»­ dá»¥ng activation function GELU. ÄÃ¢y lÃ  má»™t hÃ m sá»­ dá»¥ng hÃ m phÃ¢n phá»‘i tÃ­ch lÅ©y chuáº©n táº¯c $N(0, 1)$. CÃ³ thá»ƒ nÃ³i GELU lÃ  má»™t phiÃªn báº£n â€œtrÆ¡n hÆ¡nâ€ cá»§a ReLU.\nVá»›i giáº£ sá»­ input $X$ tuÃ¢n theo phÃ¢n phá»‘i $N(0, 1)$, ta cÃ³\n$$ \\text{GELU}\\left(x\\right) = x{P}\\left(X\\leq{x}\\right) = x\\Phi\\left(x\\right) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}(x/\\sqrt{2})\\right], $$\nSo sÃ¡nh ReLU, ELU vÃ  GELU Mixer block Mixer block (hay Mixer Layer) lÃ  thÃ nh pháº§n chÃ­nh trong kiáº¿n trÃºc cá»§a MLP-Mixer. Trong block nÃ y, ta sáº½ sá»­ dá»¥ng cáº£ channel-mixing vÃ  token-mixing, táº¡o nÃªn kiáº¿n trÃºc nhÆ° sau:\nKiáº¿n trÃºc cá»§a Mixer block NhÆ° váº­y, ban Ä‘áº§u ta sáº½ Ã¡p dá»¥ng token-mixing vÃ  sau Ä‘Ã³ dÃ¹ng káº¿t quáº£ Ä‘á»ƒ lÃ m input cho channel-mixing.\nKiáº¿n trÃºc MLP-Mixer MÃ´ hÃ¬nh MLP-Mixer Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡ch Ã¡p dá»¥ng nhiá»u Mixer Block (hay Mixer Layer). TrÆ°á»›c Ä‘Ã³, ta chia áº£nh input thÃ nh cÃ¡c patch vÃ  Ä‘Æ°a cÃ¡c patch nÃ y qua má»™t fully connected layer Ä‘á»ƒ giáº£m sá»‘ channel má»—i patch. LÆ°u Ã½ ráº±ng, á»Ÿ layer nÃ y thÃ¬ ta cÅ©ng Ã¡p dá»¥ng sá»± chia sáº» trá»ng sá»‘, tá»©c lÃ  toÃ n bá»™ patch Ä‘á»u Ä‘Æ°á»£c giáº£m sá»‘ channel vá»›i cÃ¹ng má»™t bá»™ trá»ng sá»‘.\nKiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh MLP-Mixer NgoÃ i ra, tÃ¹y vÃ o kÃ­ch thÆ°á»›c cá»§a má»™t batch vÃ  sá»‘ lÆ°á»£ng Mixer Block Ä‘Æ°á»£c sá»­ dá»¥ng mÃ  ta sáº½ cÃ³ cÃ¡c phiÃªn báº£n MLP-Mixer khÃ¡c nhau. BÃªn cáº¡nh cÃ¡c siÃªu tham sá»‘ Ä‘Ã³ thÃ¬ ta cÅ©ng cÃ³ má»™t vÃ i siÃªu tham sá»‘ khÃ¡c nhÆ° trong báº£ng dÆ°á»›i Ä‘Ã¢y:\nCÃ¡c phiÃªn báº£n MLP-Mixer. Trong Ä‘Ã³, S, B, L vÃ  H láº§n lÆ°á»£t lÃ  Small, Base, Large vÃ  Huge. CÃ¡c siÃªu tham sá»‘ nÃ y Ä‘Æ°á»£c gÃ¡n giÃ¡ trá»‹ cho áº£nh input vá»›i Ä‘á»™ phÃ¢n giáº£i lÃ  224 x 224. TÃ i liá»‡u tham kháº£o ProtonX: AI Papers Reading and Coding - MLP-Mixer: An all-MLP Architecture for Vision Paper MLP-Mixer: https://arxiv.org/pdf/2105.01601.pdf MLP-Mixer - HÆ°á»›ng giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n Computer Vision má»›i bÃªn cáº¡nh CNN vÃ  Transformer ","date":"2023-05-16T11:37:12+07:00","permalink":"https://htrvu.github.io/post/mlp_mixer/","title":"MLP Mixer"},{"content":"Note. VÃ¬ mÃ¬nh cÅ©ng Ä‘ang trong quÃ¡ trÃ¬nh tÃ¬m hiá»ƒu vá» diffusion models nÃªn táº¡m thá»i blog sáº½ ngá»«ng cÃ¡c bÃ i viáº¿t trong chá»§ Ä‘á» NLP láº¡i má»™t thá»i gian Ä‘á»ƒ táº­p trung cho diffusion models nhÃ© ğŸ˜€\nGiá»›i thiá»‡u vá» diffusion models Trong thá»i gian gáº§n Ä‘Ã¢y, xu hÆ°á»›ng â€œAI váº½ tranh\u0026quot; Ä‘ang ráº¥t lÃ  hot vÃ  cÃ¡c mÃ´ hÃ¬nh sinh áº£nh ná»•i tiáº¿ng Ä‘Ã³ háº§u háº¿t lÃ  dá»±a trÃªn diffusion models, Ä‘áº·c biá»‡t lÃ  Stable Diffusion.\nMinh há»a áº£nh sinh bá»Ÿi Stable Diffusion\nNguá»“n: Stable Diffusion Online BÃ i toÃ¡n Image Generation, hay Image Synthesis, khÃ´ng pháº£i lÃ  bÃ i toÃ¡n má»›i mÃ  ta Ä‘Ã£ cÃ³ khÃ¡ nhiá»u há» mÃ´ hÃ¬nh Ä‘Æ°á»£c nghiÃªn cá»©u vÃ  cÃ´ng bá»‘. CÆ¡ báº£n nháº¥t lÃ  Autoencoder, Variational Autoencoder (VAE), Normalizing Flow vÃ  ná»•i tiáº¿ng nháº¥t lÃ  Generative Adversarial Network. Trong image generation thÃ¬ ta cÃ³ má»™t cÃ¡i gá»i lÃ  generative trilemma. Ã nghÄ©a cá»§a cÃ¡i nÃ y lÃ  cÃ¡c mÃ´ hÃ¬nh sinh áº£nh sáº½ chá»‰ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u nháº¥t lÃ  2 trong 3 tiÃªu chÃ­ sau: Thá»i gian sinh áº£nh nhanh, cháº¥t lÆ°á»£ng áº£nh rÃµ nÃ©t vÃ  ná»™i dung áº£nh Ä‘a dáº¡ng.\nThe generative trilemma\nNguá»“n: Tanishq Abraham GAN thÃ¬ sinh áº£nh nhanh vÃ  cháº¥t lÆ°á»£ng áº£nh rÃµ nÃ©t nhÆ°ng cÃ¡c áº£nh nÃ³ sinh ra thÆ°á»ng trÃ´ng khÃ¡ giá»‘ng nhau, tá»©c lÃ  thiáº¿u sá»± Ä‘a dáº¡ng VAE vÃ  Normalizing Flow thÃ¬ Ä‘áº¡t Ä‘Æ°á»£c máº·t tá»‘c Ä‘á»™ vÃ  Ä‘a dáº¡ng nhÆ°ng cháº¥t lÆ°á»£ng áº£nh thÃ¬ khÃ´ng tá»‘t láº¯m. Vá»›i diffusion models, ta thÆ°á»ng Ä‘áº¡t Ä‘Æ°á»£c tiÃªu chÃ­ cháº¥t lÆ°á»£ng vÃ  sá»± Ä‘a dáº¡ng nhÆ°ng tá»‘c Ä‘á»™ thÃ¬ láº¡i khÃ¡ cháº­m. Do Ä‘Ã³, háº§u háº¿t cÃ¡c cáº£i tiáº¿n trong difusion models lÃ  liÃªn quan Ä‘áº¿n viá»‡c tÄƒng tá»‘c quÃ¡ trÃ¬nh sinh áº£nh.\nÃ tÆ°á»Ÿng chung cá»§a diffusion models lÃ  ta tá»«ng bÆ°á»›c thÃªm nhiá»…u vÃ o áº£nh ban Ä‘áº§u Ä‘á»ƒ â€œphÃ¡ há»§yâ€ phÃ¢n phá»‘i cá»§a dá»¯ liá»‡u, sau Ä‘Ã³ há»c cÃ¡ch khÃ´i phá»¥c láº¡i cáº¥u trÃºc cá»§a áº£nh gá»‘c. Sau Ä‘Ã³, Ä‘á»ƒ sinh áº£nh thÃ¬ ta xuáº¥t phÃ¡t tá»« má»™t áº£nh nhiá»…u hoÃ n toÃ n vÃ  tá»« tá»« khá»­ nhiá»…u Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c áº£nh káº¿t quáº£.\nÃ tÆ°á»Ÿng chung cá»§a diffusion models MÃ´ hÃ¬nh Diffusion Probability Model Ä‘Æ°á»£c giá»›i thiá»‡u Ä‘áº§u tiÃªn trong paper nÄƒm 2015 lÃ  Deep Unsupervised Learning using Nonequilibrium Thermodynamics. Pháº§n toÃ¡n cá»§a mÃ´ hÃ¬nh nÃ y ráº¥t lÃ  náº·ng vÃ  nÃ³ Ã­t Ä‘Æ°á»£c cá»™ng Ä‘á»“ng Ä‘á»ƒ Ã½ tá»›i khi mÃ  káº¿t quáº£ lÃºc Ä‘Æ°á»£c cÃ´ng bá»‘ thÃ¬ cÅ©ng khÃ´ng cÃ³ gÃ¬ ná»•i báº­t. MÃ£i Ä‘áº¿n nÄƒm 2020, Denoising Diffusion Probability Model (DDPM) Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ  cÃ³ thá»ƒ nÃ³i Ä‘Ã¢y lÃ  sá»± kiá»‡n quan trá»ng khi nhá» paper nÃ y mÃ  diffusion models má»›i trá»Ÿ thÃ nh má»™t chá»§ Ä‘á» nghiÃªn cá»©u Ä‘Æ°á»£c nhiá»u ngÆ°á»i quan tÃ¢m.\nNÃ³i Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh áº£o diá»‡u nhÆ° Stable Diffusion thÃ¬ ta cÃ²n cÃ³ nhiá»u chi tiáº¿t khÃ¡c ná»¯a nhÆ°ng nÃ³ sáº½ khÃ´ng náº±m trong pháº¡m vi bÃ i viáº¿t nÃ y. MÃ¬nh sáº½ táº­p trung vÃ o pháº§n lÃ½ thuyáº¿t toÃ¡n cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ lÃ m ná»n táº£ng cho cÃ¡c bÃ i viáº¿t sau.\nForward diffusion Giáº£ sá»­ data sample (â€œáº£nhâ€ ban Ä‘áº§u) Ä‘Æ°á»£c láº¥y máº«u tá»« phÃ¢n phá»‘i dá»¯ liá»‡u tháº­t sá»± $\\bold{x}_0 \\sim q(\\bold{x})$. Feed forward lÃ  quÃ¡ trÃ¬nh ta thÃªm má»™t lÆ°á»£ng nhá» Gaussian noise vÃ o data sample $\\bold{x}_0$ thÃ´ng qua $T$ bÆ°á»›c, tá»« Ä‘Ã³ cÃ³ cÃ¡c noisy samples $\\bold{x}_1, \\bold{x}_2,\u0026hellip;, \\bold{x}_T$. PhÃ¢n bá»‘ cá»§a data sample $\\bold{x}_t$ chá»‰ phá»¥ thuá»™c vÃ o $\\bold{x}_{t-1}$ nhÆ° sau:\n$$ \\begin{equation} q(\\bold{x}_t | \\bold{x}_{t-1}) = \\mathcal{N}\\left(\\bold{x}_{t-1}; \\sqrt{1-\\beta_t} \\bold{x}_{t-1}, \\beta_t \\bold{I} \\right) \\end{equation} $$\nCÃ´ng thá»©c $(1)$ cÃ³ nghÄ©a lÃ  â€œáº£nhâ€ táº¡i bÆ°á»›c thá»© $t$ Ä‘Æ°á»£c sample tá»« má»™t conditional Gaussian distribution vá»›i mean $\\mu_t = \\sqrt{1 - \\beta_t} \\bold{x}_{t-1}$ vÃ  variance $\\sigma^2 = \\beta_t$. Vá»›i giáº£ thiáº¿t sá»± phá»¥ thuá»™c, ta cÅ©ng cÃ³ thá»ƒ xem Ä‘Ã¢y lÃ  má»™t xÃ­ch Markov.\nLÆ°u Ã½. $q$ lÃ  probability density fuction (hÃ m máº­t Ä‘á»™ xÃ¡c suáº¥t) cá»§a phÃ¢n phá»‘i chuáº©n. Minh há»a quÃ¡ trÃ¬nh forward\nNguá»“n: Steins Báº±ng re-parameterization trick, ta cÃ³ thá»ƒ sample $\\bold{x}_t$ nhÆ° sau:\n$$ \\begin{equation} \\bold{x}_t = \\sqrt{1 - \\beta_t} \\bold{x}_{t-1} + \\epsilon_{t-1} \\sqrt {\\beta_t} \\end{equation} $$\nvá»›i $\\epsilon_{t-1} \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$.\nCÃ¡c giÃ¡ trá»‹ phÆ°Æ¡ng sai $\\beta_t$ lÃ  Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trÆ°á»›c vÃ  nÃ³ Ä‘Æ°á»£c gá»i lÃ  variance scheduler. Ta sáº½ Ä‘á» cáº­p kÄ© hÆ¡n vá» scheduler á»Ÿ pháº§n 4. TrÆ°á»›c háº¿t thÃ¬ cÃ³ má»™t sá»‘ nháº­n xÃ©t nhÆ° sau:\n$0 \u0026lt; \\beta_1 \u0026lt; \\beta_2 \u0026lt; \u0026hellip; \u0026lt; \\beta_T \u0026lt; 1$ CÃ³ thá»ƒ hiá»ƒu ráº±ng cÃ ng Ä‘áº¿n cÃ¡c bÆ°á»›c sau thÃ¬ ta thÃªm cÃ ng nhiá»u nhiá»…u vÃ o data sample (vÃ¬ variance ngÃ y cÃ ng lá»›n). Khi $t \\to \\infty$, phÃ¢n bá»‘ cá»§a $\\bold{x}_T$ sáº½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i isotropic Gaussian distribution, tá»©c lÃ  $\\bold{x}_T \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$. Quay trá»Ÿ láº¡i vá»›i cÃ´ng thá»©c xÃ¡c Ä‘á»‹nh phÃ¢n phá»‘i cá»§a $\\bold{x}_t$. Äá»ƒ xÃ¡c Ä‘á»‹nh phÃ¢n phá»‘i cá»§a $\\bold{x}_T$ thÃ¬ ta sáº½ tÃ­nh dáº§n tá»«ng bÆ°á»›c nhÆ° sau:\n$$ \\begin{equation} q(\\bold{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\bold{x}_t \\vert \\bold{x}_{t-1}) \\end{equation} $$\nTÃ­nh nhÆ° trÃªn thÃ¬ trÃ´ng cÃ³ váº» lÃ  khÃ¡ lÃ¢u!\nMá»™t tÃ­nh cháº¥t thÃº vá»‹ cá»§a quÃ¡ trÃ¬nh forward lÃ  ta cÃ³ thá»ƒ sample Ä‘Æ°á»£c ngay $\\bold{x}_t$ vá»›i báº¥t kÃ¬ timestep $t$ nÃ o. Äáº·t $\\alpha_t = 1 - \\beta_t$ vÃ  $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$. VÃ¬ $\\beta_t$ Ä‘á»u Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh sáºµn nÃªn ta cÅ©ng sáº½ tÃ­nh trÆ°á»›c Ä‘Æ°á»£c $\\bar{\\alpha_t}$. Tá»« cÃ´ng thá»©c $(2)$ ta cÃ³\n$$ \\begin{aligned} \\mathbf{x}_t \u0026amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \\\\ \u0026amp;= \\sqrt{\\alpha_t \\alpha_{t-1}} \\bold{x}_{t-2} + \\sqrt{\\alpha_t - \\alpha_t \\alpha_{t-1}} \\epsilon_{t-2} + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1} \\end{aligned} $$\nKhi ta gá»™p hai Gaussion distribution $\\mathcal{N}(\\bold{0}, \\sigma_1^2 \\bold{I})$ vÃ  $\\mathcal{N}(\\bold{0}, \\sigma_2^2 \\bold{I})$ (Ã½ nÃ³i Ä‘áº¿n hai Ä‘áº¡i lÆ°á»£ng chá»©a $\\epsilon_{t-2}$ vÃ  $\\epsilon_{t-1}$ á»Ÿ trÃªn) thÃ¬ phÃ¢n phá»‘i thu Ä‘Æ°á»£c sáº½ lÃ  $\\mathcal{N}(\\bold{0}, (\\sigma_1^2 + \\sigma_2^2) \\bold{I})$. Do Ä‘Ã³, tá»« cÃ´ng thá»©c trÃªn ta cÃ³ thá»ƒ viáº¿t láº¡i thÃ nh\n$$ \\mathbf{x}_t = \\sqrt{\\alpha_t \\alpha_{t-1}} \\bold{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\epsilon}_{t-2} $$\nCá»© tiáº¿p tá»¥c biáº¿n Ä‘á»•i thÃ¬ ta sáº½ cÃ³\n$$ \\mathbf{x}_t = \\sqrt{\\bar{\\alpha_t}} \\bold{x}_{0} + \\sqrt{1 - \\bar{\\alpha_t}} \\epsilon $$\nvá»›i $\\epsilon \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$.\nTÃ³m láº¡i, trong quÃ¡ trÃ¬nh forward, ta cÃ³ thá»ƒ sample $\\bold{x}_t$ dá»±a vÃ o phÃ¢n phá»‘i sau:\n$$ q(\\mathbf{x}_t \\vert \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I}) $$\nReverse diffusion Äáº·t váº¥n Ä‘á» QuÃ¡ trÃ¬nh forward diffusion dáº§n dáº§n thÃªm cÃ¡c Gaussian noise vÃ o sample dá»±a theo cÃ¡c phÃ¢n phá»‘i $q(\\bold{x}_t|\\bold{x}_{t-1})$. Náº¿u ta cÃ³ thá»ƒ Ä‘i ngÆ°á»£c láº¡i, tá»©c lÃ  xuáº¥t phÃ¡t tá»« má»™t sample $\\bold{x}_{T} \\sim q(\\bold{x}_T) = \\mathcal{N}(\\bold{0}, \\bold{I})$ vÃ  dáº§n sample $\\bold{x}_{t-1}$ theo phÃ¢n phá»‘i $q(\\bold{x}_{t-1} | \\bold{x}_t)$ nÃ o Ä‘Ã³ thÃ¬ khi thá»±c hiá»‡n cho Ä‘áº¿n $\\bold{x}_0$, ta Ä‘Ã£ cÃ³ thá»ƒ denoise (khá»­ nhiá»…u) Ä‘á»ƒ thu láº¡i Ä‘Æ°á»£c sample xáº¥p xá»‰ sample ban Ä‘áº§u cá»§a quÃ¡ trÃ¬nh forward.\nMinh há»a quÃ¡ trÃ¬nh forward vÃ  reverse\nTuy nhiÃªn, viá»‡c xÃ¡c Ä‘á»‹nh $q(\\bold{x}_{t-1} | \\bold{x}_t)$ lÃ  ráº¥t khÃ³ vÃ¬ nÃ³ liÃªn quan Ä‘áº¿n phÃ¢n phá»‘i cá»§a toÃ n bá»™ dá»¯ liá»‡u. Do Ä‘Ã³, ta sáº½ tÃ¬m cÃ¡ch xáº¥p xá»‰ phÃ¢n phá»‘i nÃ y.\nKÄ© hÆ¡n vá» lÃ½ do khÃ³ xÃ¡c Ä‘á»‹nh $q(\\bold{x}_{t-1} | \\bold{x}_t)$: VÃ¬ cÃ³ ráº¥t nhiá»u kháº£ nÄƒng cÃ³ thá»ƒ xáº£y ra Ä‘á»‘i vá»›i $\\bold{x}_{t-1}$ nÃªn phÆ°Æ¡ng sai cá»§a phÃ¢n phá»‘i nÃ y cÅ©ng sáº½ ráº¥t lá»›n. Giáº£ sá»­ ta cáº§n xÃ¡c Ä‘á»‹nh $q(\\bold{x}_{t-1} | \\bold{x}_t, \\bold{x}_0)$ thÃ¬ má»i chuyá»‡n sáº½ dá»… dÃ ng hÆ¡n. Khi biáº¿t trÆ°á»›c thÃªm $\\bold{x}_0$ ná»¯a thÃ¬ ta cÃ³ thá»ƒ hÃ¬nh dung Ä‘Æ°á»£c $\\bold{x}_{t-1}$ nÃªn trÃ´ng nhÆ° tháº¿ nÃ o. KÄ© thuáº­t nÃ y gá»i lÃ  variance reduction step. Má»™t cÃ¡ch trá»±c giÃ¡c, ta Ä‘oÃ¡n Ä‘Æ°á»£c nhá»¯ng gÃ¬ cáº§n lÃ m khi biáº¿t thÃªm $\\bold{x}_0$ lÃ  thá»±c hiá»‡n ná»™i suy $\\bold{x}_{t-1}$ dá»±a vÃ o $\\bold{x}_0$ vÃ  $\\bold{x}_{t}$. Ta sáº½ quay láº¡i vá»›i nháº­n xÃ©t nÃ y sau. Äá»ƒ Ã½ ráº±ng vá»›i $\\bold{x}_T \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$ vÃ  $\\beta_T$ Ä‘á»§ nhá» thÃ¬ $q(\\bold{x}_{T-1}|\\bold{x}_T)$ cÅ©ng lÃ  má»™t Gaussian distribution. Má»™t cÃ¡ch Ä‘á»‡ quy, ta cÃ³ thá»ƒ xem toÃ n bá»™ $q(\\bold{x}_{t-1} | \\bold{x}_t)$ lÃ  Gaussian distribution luÃ´n ğŸ˜€ Äá»ƒ xáº¥p xá»‰ $q(\\bold{x}_{t-1} | \\bold{x}_t)$, ta sáº½ sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh $p_{\\theta}$ Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ mean $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$ vÃ  variance $\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)$. Tá»« Ä‘Ã³ xáº¥p xá»‰ Ä‘Æ°á»£c\n$$ p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)) $$\nLÆ°u Ã½ ráº±ng cÃ¡c giÃ¡ trá»‹ Ä‘áº§u vÃ o cá»§a mÃ´ hÃ¬nh bao gá»“m cáº£ $\\bold{x}_t$ vÃ  timestep $t$ (cho biáº¿t má»©c Ä‘á»™ noise Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ táº¡o ra $\\bold{x}_t$ trong quÃ¡ trÃ¬nh forward). MÃ´ hÃ¬nh $p_\\theta$ thÆ°á»ng lÃ  U-Net. Táº¡m thá»i ta sáº½ bá» qua chi tiáº¿t vá» kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh nÃ y. Minh há»a mÃ´ hÃ¬nh U-Net trong quÃ¡ trÃ¬nh reverse\nNguá»“n: Steins Trong quÃ¡ trÃ¬nh forward, ta Ä‘Ã£ Ä‘á» cáº­p lÃ  cÃ¡c giÃ¡ trá»‹ variance Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c dá»±a vÃ o scheluder nÃªn á»Ÿ Ä‘Ã¢y thÃ¬ ta khÃ´ng cáº§n dá»± Ä‘oÃ¡n variance. Khi Ä‘Ã³ $$ p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\beta_t \\bold{I}) $$\nGiáº£ sá»­ ta Ä‘Ã£ há»c Ä‘Æ°á»£c mÃ´ hÃ¬nh $p_{\\theta}$ nhÆ° trÃªn thÃ¬ tá»« $\\bold{x}_T$ ta cÃ³ thá»ƒ xáº¥p xá»‰ Ä‘Æ°á»£c phÃ¢n phá»‘i cá»§a $\\bold{x}_0$ nhÆ° sau:\n$$ p_\\theta (\\bold{x}_{0:T}) = q (\\bold{x}_T) \\prod_{t=1}^T p_\\theta (\\bold{x}_{t-1} | \\bold{x}_t) $$\nVáº¥n Ä‘á» Ä‘áº·t ra lÃ  ta huáº¥n luyá»‡n mÃ´ hÃ¬nh $p_\\theta$ nhÆ° tháº¿ nÃ o. Váº­y thÃ¬ ta cáº§n pháº£i tÃ¬m loss function!\nXÃ¡c Ä‘á»‹nh loss function Má»¥c tiÃªu cá»§a ta lÃ  minimize the negative log likelihood $-\\log p_\\theta (\\bold{x}_0)$. Biáº¿n Ä‘á»•i má»™t chÃºt nhÆ° sau:\n$$ \\begin{aligned} -\\log p_\\theta(\\mathbf{x}_0) \u0026amp;\\leq - \\log p_\\theta(\\mathbf{x}_0) + D_\\text{KL}(q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\| \\color{red}p_\\theta(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\color{default} ) \\\\ \u0026amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_{\\mathbf{x}_{1:T}\\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{\\color{red}p_\\theta(\\mathbf{x}_{0:T}) / p_\\theta(\\mathbf{x}_0)} \\Big] \\\\ \u0026amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_q \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} + \\log p_\\theta(\\mathbf{x}_0) \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\end{aligned} $$\nÄáº·t $L_{VLB} = \\mathbb{E}_q \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big]$ (variational lower bound). Nháº­n xÃ©t ráº±ng náº¿u minimize Ä‘Æ°á»£c $L_{VLB}$ thÃ¬ giÃ¡ trá»‹ cá»§a negative log likelihood cÅ©ng sáº½ nhá» ğŸ˜€ Do Ä‘Ã³, hÃ m má»¥c tiÃªu cá»§a ta sáº½ lÃ  $L_{VLB}$.\nTiáº¿p tá»¥c biáº¿n Ä‘á»•i $L_{VLB}$, ta cÃ³\n$$ \\begin{aligned} L_\\text{VLB} \u0026amp;= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log\\frac{\\color{red}q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{\\color{blue}p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\log\\frac{\\color{red}\\prod_{t=1}^T q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{\\color{blue} p_\\theta(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t) } \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{\\color{green}q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\Big( \\frac{\\color{green}q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)}\\cdot \\frac{\\color{green}q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{\\color{green}q(\\mathbf{x}_{t-1}\\vert\\mathbf{x}_0)} \\Big) + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\end{aligned} $$\nTrong dÃ²ng biáº¿n Ä‘á»•i á»Ÿ trÃªn, Ã¡p dá»¥ng cÃ´ng thá»©c Bayes thÃ¬ ta cÃ³\n$$ q(\\bold{x}_t | \\bold{x}_{t-1}) = \\frac{q(\\bold{x}_{t-1} | \\bold{x}_t) \\cdot q(\\bold{x}_t)}{q(\\bold{x}_{t-1})} $$\nTa cÃ³ nháº­n xÃ©t áº£o ma ráº±ng náº¿u ta cho thÃªm má»™t Ä‘iá»u kiá»‡n lÃ  $\\bold{x}_0$ thÃ¬ cÃ¡c giÃ¡ trá»‹ trÃªn sáº½ dá»… tÃ­nh hÆ¡n ráº¥t nhiá»u (cÃ³ thá»ƒ táº­n dá»¥ng quÃ¡ trÃ¬nh forward). Váº­y thÃ¬ â€œassumeâ€ luÃ´n lÃ \n$$ q(\\bold{x}_t | \\bold{x}_{t-1}) = \\frac{q(\\bold{x}_{t-1} | \\bold{x}_t, \\bold{x}_0) \\cdot q(\\bold{x}_t | \\bold{x}_0)}{q(\\bold{x}_{t-1} | \\bold{x}_0)} $$\nOK, tiáº¿p tá»¥c biáº¿n Ä‘á»•i $L_{VLB}$:\n$$ \\begin{aligned} L_\\text{VLB} \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\color{red}\\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0)} \\color{default} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\color{red}\\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)} \\color{default} + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big]\\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\color{green}-\\log p_\\theta(\\mathbf{x}_T) \\color{default} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{\\color{green}q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{\\color{blue}p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\color{green}\\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_T)} \\color{default} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} - \\color{blue}\\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)\\color{default} \\Big] \\end{aligned} $$\nVÃ  bÆ°á»›c cuá»‘i cÃ¹ng:\n$$ \\begin{aligned}L_{LVB} \u0026amp;= \\mathbb{E}_q [\\color{red}\\underbrace{D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T))}_{L_T} \\color{default} + \\color{blue}\\sum_{t=2}^T \\underbrace{D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t))}_{L_{t-1}} \\color{green} \\underbrace{- \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)}_{L_0} ] \\\\ \u0026amp;= L_T + \\left (L_1 + L_2 + \\cdots + L_{T-1}\\right) + L_0 \\end{aligned} $$\nNhÃ¬n chung thÃ¬ Ä‘Ã¢y toÃ n lÃ  loss liÃªn quan Ä‘áº¿n khoáº£ng cÃ¡ch cÃ¡c phÃ¢n phá»‘i trong tá»«ng timestep. VÃ¬ $\\bold{x}_T \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$ nÃªn $L_T$ lÃ  má»™t háº±ng sá»‘ (khi $T \\to \\infty$ thÃ¬ $L_T \\to 0$) vÃ  ta cÃ³ thá»ƒ bá» qua nÃ³. NgoÃ i ra, vÃ¬ giÃ¡ trá»‹ phÆ°Æ¡ng sai $\\beta_1$ lÃ  ráº¥t nhá» vÃ  thÆ°á»ng thÃ¬ â€œáº£nh\u0026quot; $\\bold{x}_0$ vÃ  $\\bold{x}_1$ trÃ´ng sáº½ ráº¥t giá»‘ng nhau (vÃ¬ lÆ°á»£ng nhiá»…u thÃªm vÃ o lÃ  ráº¥t Ã­t). VÃ¬ lÃ½ do nÃ y mÃ  cÃ¡c tÃ¡c giáº£ cá»§a DDPM tháº­t sá»± Ä‘Ã£ bá» qua nÃ³ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.\nNhÆ° váº­y ta cÃ²n láº¡i má»—i $L_t$ vá»›i $1 \\leq t \\leq T - 1$. Äá»‘i vá»›i cÃ¡i nÃ y thÃ¬ cáº§n cÃ³ má»™t sá»‘ ma thuáº­t.\nTham sá»‘ hÃ³a $L_t$ CÃ¡c giÃ¡ trá»‹ $L_t$ vá»›i $1 \\leq t \\leq T - 1$ liÃªn quan Ä‘áº¿n khoáº£ng cÃ¡ch giá»¯a phÃ¢n phá»‘i Ä‘Æ°á»£c xáº¥p xá»‰ bá»Ÿi model $p_\\theta$ vÃ  má»™t phiÃªn báº£n â€œdá»… tÃ­nh hÆ¡nâ€ cá»§a $q(\\bold{x}_{t-1} | \\bold{x}_t)$ lÃ  $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)$ . Ta sáº½ Ä‘i tÃ­nh mean vÃ  variance cá»§a phÃ¢n phá»‘i nÃ y.\nÄáº§u tiÃªn, Ã¡p dá»¥ng quy táº¯c Bayes thÃ¬\n$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}, \\mathbf{x}_0) \\frac{ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0) }{ q(\\mathbf{x}_t \\vert \\mathbf{x}_0)} $$\nCáº£ 3 phÃ¢n phá»‘i trÃªn Ä‘á»u lÃ  Gaussian distribution trong quÃ¡ trÃ¬nh forward vÃ  ta Ä‘Ã£ biáº¿t Ä‘Æ°á»£c mean, variance cá»§a chÃºng. Nháº¯c láº¡i má»™t chÃºt: CÃ´ng thá»©c hÃ m máº­t Ä‘á»™ xÃ¡c suáº¥t cá»§a Gaussian distribution $\\cal{N}(\\mu,\\sigma)$ lÃ \n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left( -\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2\\right) $$\nDo Ä‘Ã³\n$$ \\begin{aligned} q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \u0026amp;\\propto \\exp \\left(-\\frac{1}{2} \\left(\\frac{(\\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1})^2}{\\beta_t} + \\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\\\ \u0026amp;= \\exp \\left(-\\frac{1}{2} \\left(\\frac{\\mathbf{x}_t^2 - 2\\sqrt{\\alpha_t} \\mathbf{x}_t \\color{blue}{\\mathbf{x}_{t-1}} \\color{default}{+ \\alpha_t} \\color{red}{\\mathbf{x}_{t-1}^2} }{\\beta_t} + \\frac{ \\color{red}{\\mathbf{x}_{t-1}^2} \\color{default}{- 2 \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0} \\color{blue}{\\mathbf{x}_{t-1}} \\color{default}{+ \\bar{\\alpha}_{t-1} \\mathbf{x}_0^2} }{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\\\ \u0026amp;= \\exp \\left( -\\frac{1}{2} \\left( \\left( \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right) \\color{red} \\bold{x}_{t-1}^2 \\color{default} - \\left( \\frac{2\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{2 \\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t-1}} \\bold{x}_0 \\right) \\color{blue}\\bold{x}_{t-1} \\color{default} + C(\\bold{x}_t, \\bold{x}_0)\\right)\\right)\\\\ \\end{aligned} $$\nvá»›i $C(\\bold{x}_t, \\bold{x}_0)$ lÃ  nhá»¯ng Ä‘áº¡i lÆ°á»£ng khÃ´ng liÃªn quan Ä‘áº¿n $\\bold{x}_{t-1}$ vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c bá» qua.\nTa cáº§n biáº¿n Ä‘á»•i cÃ´ng thá»©c trÃªn vá» dáº¡ng cá»§a má»™t Gaussian distribution. Äáº·t\n$$ \\tilde{\\beta}_t = 1/\\left(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\\right) = 1/\\left(\\frac{\\alpha_t - \\bar{\\alpha}_t + \\beta_t}{\\beta_t(1 - \\bar{\\alpha}_{t-1})}\\right) = {\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t} $$\nvÃ \n$$ \\begin{aligned} \\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, \\bold{x}_0) \u0026amp;= \\left(\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0\\right)/\\left(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\\right) \\\\ \u0026amp;= \\left(\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0\\right) {\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t} \\\\ \u0026amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\color{green}\\beta_t}{1 - \\bar{\\alpha}_t} \\color{red}\\mathbf{x}_0 \\end{aligned} $$\nTá»« quÃ¡ trÃ¬nh forward, ta cÃ³ thá»ƒ suy ra ráº±ng $\\bold{x}_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon})$ vá»›i $\\boldsymbol{\\epsilon} \\sim \\cal{N}(\\bold{0}, \\bold{I})$. Do Ä‘Ã³\n$$ \\begin{aligned} \\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t) \u0026amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\color{green}(1 - \\alpha_t)}{1 - \\bar{\\alpha}_t} \\color{red}\\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}) \\\\ \u0026amp;= \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon} \\right) \\end{aligned} $$\nKhi Ä‘Ã³ ta cÃ³\n$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\propto \\exp \\left( -\\frac{1}{2} \\frac{\\left( \\bold{x}_{t-1} - \\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t) \\right)^2}{\\tilde{\\beta}_t} \\right) $$\n, tá»©c lÃ \n$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\cal{N}(\\bold{x}_{t-1};\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t), \\tilde{\\beta}_t\\bold{I}) $$\nMáº·t khÃ¡c, trong pháº§n Ä‘áº·t váº¥n Ä‘á» cá»§a quÃ¡ trÃ¬nh reverse thÃ¬ ta cáº§n huáº¥n luyá»‡n mÃ´ hÃ¬nh $p_\\theta$ Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ trá»‹ mean $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$, sao cho\n$$ p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\beta_t \\bold{I}) $$\nÄiá»u nÃ y nghÄ©a lÃ  thá»© ta cáº§n quan tÃ¢m vá» $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$ lÃ  dá»± Ä‘oÃ¡n Ä‘Æ°á»£c $\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t)$, mÃ  cá»¥ thá»ƒ hÆ¡n lÃ  dá»± Ä‘oÃ¡n giÃ¡ trá»‹ $\\epsilon \\sim \\cal{N}(\\bold{0}, \\bold{I})$ (vÃ¬ toÃ n bá»™ nhá»¯ng giÃ¡ trá»‹ khÃ¡c trong $\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t)$ Ä‘á»u Ä‘Ã£ biáº¿t vÃ¬ chÃºng lÃ  input cá»§a quÃ¡ trÃ¬nh reverse). Tá»« Ä‘Ã³, ta cÃ³ thá»ƒ biá»ƒu diá»…n\n$$ \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = {\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\color{red}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\color{default} \\right)} $$\nDá»± Ä‘oÃ¡n $\\epsilon_t$ cÅ©ng cÃ³ nghÄ©a lÃ  Ä‘i dá»± Ä‘oÃ¡n lÆ°á»£ng nhiá»…u Ä‘Æ°á»£c thÃªm vÃ o $\\bold{x}_{t-1}$ Ä‘á»ƒ táº¡o ra $\\bold{x}_{t}$ trong quÃ¡ trÃ¬nh forward trÆ°á»›c Ä‘Ã³. NhÆ° váº­y, thÃ nh pháº§n loss $L_{t-1}$ sáº½ trá»Ÿ thÃ nh\n$$ \\begin{aligned} L_{t-1} \u0026amp;= D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)) \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\| \\beta_t \\bold{I} \\|^2_2} \\| \\color{blue}{\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{green}{\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)} \\color{default} \\|^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\|\\beta_t \\bold{I} \\|^2_2} \\| \\color{blue}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon} \\Big)} - \\color{green}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\Big)} \\color{default}\\|^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\beta_t \\bold{I} \\|^2_2} \\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\end{aligned} $$\nCÃ¡c tÃ¡c giáº£ cá»§a DDPM cho ráº±ng lÆ°á»£c bá» pháº§n trá»ng sá»‘ sáº½ giÃºp mÃ´ hÃ¬nh há»c tá»‘t hÆ¡n. Khi Ä‘Ã³ ta cÃ³ phiÃªn báº£n Ä‘Æ¡n giáº£n hÆ¡n cá»§a $L_{t-1}$ lÃ \n$$ \\begin{aligned} L_{t-1}^\\text{simple} \u0026amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t)\\|^2 \\Big] \\end{aligned} $$\n, hay diá»…n táº£ dá»… hiá»ƒu hÆ¡n thÃ¬ Ä‘Ã¢y lÃ  Mean Squared Error giá»¯a lÆ°á»£ng nhiá»…u dá»± Ä‘oÃ¡n vÃ  lÆ°á»£ng nhiá»…u tháº­t sá»±.\nVariance scheduler Trong cÃ¡c pháº§n trÆ°á»›c, ta cÃ³ Ä‘á» cáº­p Ä‘áº¿n viá»‡c cÃ¡c giÃ¡ trá»‹ variance $\\beta_t$ lÃ  cá»‘ Ä‘á»‹nh vÃ  chÃºng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c báº±ng cÃ¡ch sá»­ dá»¥ng variance chueduler.\nVÃ¬ sao cáº§n dÃ¹ng Ä‘áº¿n scheduler?\nViá»‡c sá»­ dá»¥ng schedule lÃ m cho quÃ¡ trÃ¬nh forward cá»§a diffusion model lÃ  cá»‘ Ä‘á»‹nh vÃ  cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh trÆ°á»›c. Äiá»u nÃ y cÃ³ thá»ƒ giÃºp cho cÃ¡c tÃ­nh toÃ¡n cá»§a quÃ¡ trÃ¬nh reverse trá»Ÿ nÃªn gá»n nháº¹ hÆ¡n mÃ  khÃ´ng lÃ m áº£nh hÆ°á»Ÿng gÃ¬ Ä‘áº¿n Ä‘á»™ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh. CÃ³ nhiá»u dáº¡ng schedule khÃ¡c nhau nhÆ° linear, cosine. Trong paper DDPM 2020 thÃ¬ cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng linear variance scheduler vá»›i $\\beta_1 = 10^{-4}$ vÃ  $\\beta_T = 0.02$.\nMinh há»a cho linear variance scheduler Trong paper Improved Denoising Diffusion Probabilistic Models - 2021, ngÆ°á»i ta cho ráº±ng sá»­ dá»¥ng cosine variance scheduler sáº½ mang láº¡i káº¿t quáº£ tá»‘t hÆ¡n. Váº¥n Ä‘á» chá»§ yáº¿u náº±m á»Ÿ chá»— vá»›i linear thÃ¬ á»Ÿ nhá»¯ng timesteps phÃ­a sau thÃ¬ háº§u nhÆ° áº£nh Ä‘Ã£ trá»Ÿ thÃ nh â€œisotropic Gaussian distributionâ€ (cÃ³ váº» Ã½ lÃ  linear thÃ¬ thÃªm nhiá»…u hÆ¡i bá»‹ nhanh). á» hÃ¬nh bÃªn dÆ°á»›i thÃ¬ hÃ ng trÃªn lÃ  linear, hÃ ng dÆ°á»›i lÃ  cosine.\nLinear scheduler vÃ  Cosine scheduler QuÃ¡ trÃ¬nh training vÃ  sampling DÃ¹ cÃ¡c ná»n táº£ng lÃ½ thuyáº¿t bÃªn dÆ°á»›i cá»§a diffusion model trÃ´ng ráº¥t rá»‘i nhÆ°ng quÃ¡ trÃ¬nh training vÃ  sampling (inference) thÃ¬ ráº¥t Ä‘Æ¡n giáº£n.\nÄáº§u tiÃªn, ta xÃ©t quÃ¡ trÃ¬nh training:\nTa cáº§n train mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n nhiá»…u. CÃ³ má»™t chi tiáº¿t lÃ  cÃ¡c tÃ¡c giáº£ cá»§a paper DDPM cho ráº±ng viá»‡c training sáº½ hiá»‡u quáº£ hÆ¡n khi ta chá»n ngáº«u nhiÃªn cÃ¡c timestep $t$ vÃ  chá»‰ dá»± Ä‘oÃ¡n nhiá»…u táº¡i timestep Ä‘Ã³, thay vÃ¬ pháº£i dá»± Ä‘oÃ¡n á»Ÿ toÃ n bá»™ timestep nhÆ° nhá»¯ng gÃ¬ Ä‘Æ°á»£c biá»ƒu diá»…n trong hÃ m loss á»Ÿ pháº§n 3.2. MÃ£ giáº£ cho quÃ¡ trÃ¬nh training Trong quÃ¡ trÃ¬nh sampling (inference) thÃ¬ ta sáº½ chá»n trÆ°á»›c sá»‘ lÆ°á»£ng timestep tá»‘i Ä‘a $T$. Báº¯t Ä‘áº§u tá»« $\\bold{x}_T \\sim \\cal{N}(\\bold{0}, \\bold{I})$. Tá»« Ä‘Ã³, dáº§n qua cÃ¡c bÆ°á»›c reverse thÃ¬:\nDá»± Ä‘oÃ¡n nhiá»…u $\\epsilon_\\theta(\\bold{x}_t, t)$. Sample $\\bold{x}_{t-1}$ vá»›i báº±ng re-parameterization trick MÃ£ giáº£ cho quÃ¡ trÃ¬nh sampling LÆ°u Ã½. Trong mÃ£ giáº£ cá»§a quÃ¡ trÃ¬nh sampling, khi sampling $\\bold{x}_0$ thÃ¬ ta khÃ´ng thÃªm nhiá»…u. Äiá»u nÃ y khá»›p vá»›i chi tiáº¿t bá» qua thÃ nh pháº§n loss $L_0$ á»Ÿ trÃªn :D Minh há»a trá»±c quan cho quÃ¡ trÃ¬nh training vÃ  sampling:\nNháº­n xÃ©t Chá»‰ vá»›i nhá»¯ng chi tiáº¿t trÃªn thÃ¬ cÃ³ thá»ƒ nÃ³i ráº±ng diffusion models chÆ°a thá»ƒ vÆ°á»£t máº·t GAN Ä‘Æ°á»£c vá» tá»‘c Ä‘á»™ sampling cÅ©ng nhÆ° cháº¥t lÆ°á»£ng áº£nh. Ta váº«n cÃ²n nhiá»u cáº£i tiáº¿n cho difusion models vÃ  nhá»¯ng cáº£i tiáº¿n nÃ y Ä‘Ã£ cÃ¹ng nhau Ä‘Æ°a diffusion models lÃªn cáº¡nh tranh vá»‹ trÃ­ top 1 trong bÃ i toÃ¡n Image Generation, trong Ä‘Ã³ ná»•i báº­t lÃ  sá»± xuáº¥t hiá»‡n cá»§a Stable Diffusion vá»›i nhá»¯ng ma thuáº­t nhÆ° text-to-image, image-to-image vÃ  hÆ¡n tháº¿ ná»¯a!.\nTa sáº½ Ä‘á» cáº­p Ä‘áº¿n chÃºng trong cÃ¡c bÃ i viáº¿t sau.\nTÃ i liá»‡u tham kháº£o Jascha Sohl-Dickstein et al. â€œDeep Unsupervised Learning using Nonequilibrium Thermodynamics.â€ ICML 2015. Jonathan Ho et al. â€œDenoising diffusion probabilistic models.â€ arxiv Preprint arxiv:2006.11239 (2020). Lilian Wenge, What are diffusion models? The AI Summer, Diffusion models HuggingFace\u0026rsquo;s Blog, Annotated Diffusion Steins, Diffusion Model Clearly Explained! Outlier, Diffusion Models | Paper Explanation | Math Explained Tanishq Abraham, Diffusions Study Group ","date":"2023-04-11T22:52:57+07:00","permalink":"https://htrvu.github.io/post/diffusion-models/","title":"LÃ½ thuyáº¿t vá» diffusion models"},{"content":"BÃ i toÃ¡n Machine Translation Giá»›i thiá»‡u Machine Translation (dá»‹ch mÃ¡y) lÃ  bÃ i toÃ¡n ráº¥t phá»• biáº¿n trong lÄ©nh vá»±c NLP. Sáº£n pháº©m Google Dá»‹ch mÃ  chÃºng ta váº«n dÃ¹ng háº±ng ngÃ y chÃ­nh lÃ  má»™t mÃ´ hÃ¬nh dá»‹ch mÃ¡y khÃ¡ tá»‘t vÃ  nÃ³ Ä‘Æ°á»£c huáº¥n luyá»‡n bá»Ÿi Google ğŸ˜€\nTháº­t ra bÃ i toÃ¡n Machine Translation Ä‘Ã£ ra Ä‘á»i tá»« ráº¥t lÃ¢u. Táº¥t nhiÃªn rá»“i, vÃ¬ nÃ³ Ä‘Ã³ng vai trÃ² ráº¥t quan trá»ng trong giao tiáº¿p. Ká»ƒ tá»« khi Statistical Machine Learning (mÃ¡y há»c thá»‘ng kÃª) rá»“i cho Ä‘áº¿n Deep Learning phÃ¡t triá»ƒn máº¡nh thÃ¬ dáº§n cÃ ng cÃ³ nhiá»u nghiÃªn cá»©u vá» Machine Translation Ä‘Æ°á»£c thá»±c hiá»‡n vÃ  Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ.\nNguá»“n: FreeCodeCamp Rule-based Machine Translation Ban Ä‘áº§u, Machine Translation Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng cÃ¡ch dá»±a vÃ o nhá»¯ng cÃ¡ch nhÆ° dá»‹ch trá»±c tiáº¿p tá»«ng tá»« má»™t dá»±a vÃ o tá»« Ä‘iá»ƒn, sau Ä‘Ã³ lÃ  dá»‹ch tá»«ng cá»¥m, chuyá»ƒn tá»« cÃ¡c tá»« hay cá»¥m tá»« thÃ nh má»™t dáº¡ng biá»ƒu diá»…n trung gian (vÃ­ dá»¥ nhÆ° áº£nh), rá»“i tá»« Ä‘Ã³ tÃ­nh ra tá»« cá»§a ngÃ´n ngá»¯ khÃ¡c. Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y gá»i chung lÃ  Rule-based Machine Translation. Táº¥t nhiÃªn lÃ  dá»‹ch nhÆ° váº­y thÃ¬ Ä‘á»™ chÃ­nh xÃ¡c sáº½ khÃ³ mÃ  cao Ä‘Æ°á»£c rá»“i ğŸ˜€\nMinh há»a cho Rule-based Machine Translation\nNguá»“n: FreeCodeCamp Statistical Machine Translation Vá»›i Statistical Machine Translation (SMT), ngÆ°á»i ta xÃ¢y dá»±ng khÃ¡ nhiá»u cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn cÆ¡ sá»Ÿ lÃ  cÃ¡c mÃ´ hÃ¬nh thá»‘ng kÃª. Ta cÃ³ thá»ƒ ká»ƒ ra má»™t vÃ i phÆ°Æ¡ng phÃ¡p nhÆ° Word-based (bag-of-words, word-alignment), Phase-based, Syntax-based.\nLáº¥y vÃ­ dá»¥ vá»›i bÃ i toÃ¡n dá»‹ch Tiáº¿ng Anh sang Tiáº¿ng Viá»‡t. Vá»›i cÃ¢u input Tiáº¿ng Anh lÃ  $x$, ta sáº½ tÃ¬m cÃ¢u Tiáº¿ng Viá»‡t $y_0$ sao cho xÃ¡c suáº¥t $y_0$ lÃ  cÃ¢u dá»‹ch cá»§a $x$ lÃ  cao nháº¥t:\n$$ y_0 = \\argmax_{y} P(y|x) $$\nSá»­ dá»¥ng quy táº¯c Bayes, ta cÃ³\n$$ P(y|x)= P(x|y) P(y) $$\nDo Ä‘Ã³, trong mÃ´ hÃ¬nh SMT, ta sáº½ cÃ³ sá»± gÃ³p máº·t cá»§a hai thÃ nh pháº§n: Translation Model (cho $P(x|y)$) vÃ  Language Model (cho $P(y)$)\nTranslation Model (TM) liÃªn quan Ä‘áº¿n viá»‡c dá»‹ch cÃ¡c tá»« vÃ  cá»¥m tá»« giá»¯a hai ngÃ´n ngá»¯ (fidelity - sá»± chÃ­nh xÃ¡c trong dá»‹ch thuáº­t). Äá»ƒ huáº¥n luyá»‡n TM thÃ¬ ta cáº§n sá»­ dá»¥ng táº­p dá»¯ liá»‡u â€œsong ngá»¯â€, tá»©c lÃ  táº­p cÃ¡c cáº·p cÃ¢u Anh-Viá»‡t tÆ°Æ¡ng á»©ng. Ká»¹ thuáº­t thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng trong huáº¥n luyá»‡n TM lÃ  word alignment. MÃ¬nh sáº½ khÃ´ng Ä‘á» cáº­p Ä‘áº¿n nÃ³ á»Ÿ trong bÃ i viáº¿t nÃ y ğŸ˜€ Language Model (LM) sáº½ táº­p trung vÃ o sá»± trÃ´i cháº£y cá»§a cÃ¢u Ä‘Æ°á»£c dá»‹ch ra (fluency). Äá»ƒ huáº¥n luyá»‡n LM thÃ¬ ta chá»‰ cáº§n dÃ¹ng táº­p dá»¯ liá»‡u Ä‘Æ¡n ngá»¯. CÃ³ thá»ƒ ká»ƒ Ä‘áº¿n má»™t sá»‘ cÃ¡ch huáº¥n luyá»‡n cÆ¡ báº£n nhÆ° lÃ  MÃ´ hÃ¬nh Markov (thuáº§n tÃºy dá»±a vÃ o xÃ¡c suáº¥t vÃ  thá»‘ng kÃª), hoáº·c lÃ  vá» sau thÃ¬ cÃ³ thÃªm Recurrent Neural Network. Translation Malde vÃ  Language Model\nNguá»“n: VietAI Äá»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh SMT trong thá»±c táº¿, táº¥t nhiÃªn lÃ  ta khÃ´ng thá»ƒ Ä‘i thá»­ toÃ n bá»™ cÃ¢u output $y$ Ä‘á»ƒ tÃ­nh xÃ¡c suáº¥t rá»“i so sÃ¡nh Ä‘Æ°á»£c. Ta sáº½ sá»­ dung má»™t thuáº­t toÃ¡n heuristic search Ä‘á»ƒ tÃ¬m ra cÃ¢u dá»‹ch phÃ¹ há»£p.\nCho Ä‘áº¿n trÆ°á»›c nÄƒm 2016 thÃ¬ Google Dá»‹ch váº«n sá»­ dá»¥ng mÃ´ hÃ¬nh SMT, trÆ°á»›c khi nÃ³ chuyá»ƒn hoÃ n toÃ n sang Neural Machine Translation (pháº§n káº¿ tiáº¿p). Ta cÃ³ thá»ƒ liá»‡t kÃª má»™t sá»‘ háº¡n cháº¿ cá»§a SMT nhÆ° sau:\nHá»‡ thá»‘ng tháº­t sá»± sáº½ ráº¥t phá»©c táº¡p vá»›i nhiá»u thÃ nh pháº§n tÃ¡ch rá»i nhau Cáº§n thá»±c hiá»‡n quÃ¡ trÃ¬nh feature engineering ráº¥t nhiá»u Ä‘á»ƒ cÃ³ thá»ƒ náº¯m báº¯t Ä‘Æ°á»£c Ä‘áº·c trÆ°ng cá»§a tá»«ng ngÃ´n ngá»¯ Chi phÃ­ duy trÃ¬ vÃ  phÃ¡t triá»ƒn ráº¥t tá»‘n kÃ©m. Neural Machine Translation Khi Deep Learning dáº§n phÃ¡t triá»ƒn máº¡nh, ta cÃ³ nhiá»u mÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n Machine Translation. ChÃºng Ä‘Æ°á»£c gá»i chung lÃ  Neural Machine Translation (NMT)\nMinh há»a cho cÃ¡c mÃ´ hÃ¬nh trong nhÃ³m Neural Machine Translation\nNguá»“n: FreeCodeCamp Má»¥c Ä‘Ã­ch cá»§a Machine Translation lÃ  ta Ä‘i dá»‹ch má»™t vÄƒn báº£n tá»« ngÃ´n ngá»¯ X sang ngÃ´n ngá»¯ Y, tá»©c lÃ  input cá»§a bÃ i toÃ¡n nÃ y lÃ  má»™t chuá»—i vÃ  output cÅ©ng lÃ  má»™t chuá»—i. VÃ  vá»›i cÃ¡c bÃ i toÃ¡n cÃ³ dá»¯ liá»‡u dáº¡ng chuá»—i thÃ¬ ta thÆ°á»ng nghÄ© ngay Ä‘áº¿n Recurrent Neural Network!\nLoáº¡i mÃ´ hÃ¬nh RNN thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong bÃ i toÃ¡n nÃ y lÃ  many-to-many. Trong bÃ i viáº¿t vá» RNN, mÃ¬nh cÃ³ Ä‘á» cáº­p Ä‘áº¿n hai dáº¡ng khÃ¡c nhau cá»§a mÃ´ hÃ¬nh many-to-many nhÆ° sau:\nXÃ©t dáº¡ng mÃ´ hÃ¬nh many-to-many phÃ­a bÃªn pháº£i. Ta tháº¥y ráº±ng nÃ³ Ä‘ang hoáº¡t Ä‘á»™ng theo kiá»ƒu nhÆ° dá»‹ch dáº§n tá»«ng chá»¯ má»™t, vÃ  cÃ³ váº» Ä‘Ã¢y khÃ´ng pháº£i lÃ  cÃ¡ch mÃ  con ngÆ°á»i sá»­ dá»¥ng Ä‘á»ƒ dá»‹ch vÄƒn báº£n ğŸ˜€ Äá»‘i vá»›i phÃ­a bÃªn trÃ¡i, mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng theo hÆ°á»›ng lÃ  Ä‘á»c hiá»ƒu toÃ n bá»™ input rá»“i sau Ä‘Ã³ má»›i báº¯t Ä‘áº§u dá»‹ch. Nghe ráº¥t há»£p lÃ½! Dáº¡ng kiáº¿n trÃºc nÃ y thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  Encoder-Decoder, trong Ä‘Ã³: Encoder sáº½ rÃºt trÃ­ch cÃ¡c Ä‘áº·c trÆ°ng á»Ÿ trong cÃ¢u input. Sau khi hoÃ n thÃ nh, nÃ³ sáº½ chuyá»ƒn thÃ´ng tin nÃ y cho decoder. Decoder lÃ  má»™t Language Model sinh ra cÃ¡c tá»« cho cÃ¢u output, dá»±a trÃªn cÃ¡c tá»« Ä‘Ã£ sinh trÆ°á»›c Ä‘Ã³ vÃ  lÆ°á»£ng thÃ´ng tin Ä‘áº¿n tá»« encoder. Náº¿u mÃ´ táº£ ngáº¯n gá»n thÃ¬ ta sáº½ cÃ³ sÆ¡ Ä‘á»“ nhÆ° sau:\nSÆ¡ Ä‘á»“ cá»§a kiáº¿n trÃºc Encoder-Decoder\nNguá»“n: Dive into DL Cá»¥ thá»ƒ hÆ¡n má»™t chÃºt vá»›i dáº¡ng mÃ´ hÃ¬nh RNN Encoder-Decoder, â€œthÃ´ng tinâ€ mÃ  encoder gá»­i cho decoder chÃ­nh lÃ  hidden state cá»§a giai Ä‘oáº¡n cuá»‘i cÃ¹ng trong encoder.\nMinh há»a mÃ´ hÃ¬nh RNN Encoder-Decoder\nNguá»“n: VietAI Ta cÅ©ng cÃ³ thá»ƒ xem NMT nhÆ° lÃ  má»™t SMT vá»›i kháº£ nÄƒng tÃ­nh toÃ¡n trá»±c tiáº¿p xÃ¡c suáº¥t $P(y|x)$:\n$$ P(y|x) = P(y_1|x) \\times P(y_2|y_1, x) \\times \\cdots P(y_T | y_{T -1}, \\cdots, y_1, x) $$\nSo vá»›i SMT, NMT cÃ³ má»™t sá»‘ Ä‘iá»ƒm máº¡nh hÆ¡n nhÆ° sau:\nHiá»‡u nÄƒng tá»‘t hÆ¡n: Dá»‹ch chÃ­nh xÃ¡c, trÃ´i cháº£y hÆ¡n vÃ  cÃ¢u vÄƒn Ä‘a dáº¡ng hÆ¡n Dá»… tá»‘i Æ°u mÃ´ hÃ¬nh hÆ¡n (huáº¥n luyá»‡n end-to-end) Con ngÆ°á»i khÃ´ng cáº§n pháº£i can thiá»‡p quÃ¡ nhiá»u vÃ o thao tÃ¡c feature engineering BÃªn cáº¡nh Ä‘Ã³, NMT cÅ©ng cÃ³ má»™t háº¡n cháº¿ quan trá»ng lÃ  mÃ´ hÃ¬nh nÃ y khÃ³ Ä‘á»ƒ â€œcÃ³ thá»ƒ giáº£i thÃ­ch Ä‘Æ°á»£câ€, cÃ¡c hoáº¡t Ä‘á»™ng bÃªn traong nhÆ° lÃ  má»™t blackbox (xem thÃªm bÃ i viáº¿t vá» XAI táº¡i Ä‘Ã¢y).\nMÃ´ hÃ¬nh Sequence to Sequence Sequence to Sequence (seq2seq) lÃ  mÃ´ hÃ¬nh dá»‹ch mÃ¡y cÃ³ kiáº¿n trÃºc dáº¡ng Encoder-Decoder. NÃ³ Ä‘Æ°á»£c cÃ¡c nhÃ  nghiÃªn cá»©u táº¡i Google nghiÃªn cá»©u vÃ  cÃ´ng bá»‘ vÃ o nÄƒm 2016, cÅ©ng lÃ  nÄƒm mÃ  Google Dá»‹ch chuyá»ƒn tá»« SMT sang NMT ğŸ˜€\nSeq2seq Ä‘Æ¡n giáº£n á» phiÃªn báº£n Ä‘Æ¡n giáº£n nháº¥t cá»§a seq2seq, kiáº¿n trÃºc mÃ´ hÃ¬nh sáº½ giá»‘ng vá»›i hÃ¬nh minh há»a cá»§a Encoder-Decoder á»Ÿ trÃªn. Trong Ä‘Ã³, ta cÃ³ sá»­ dá»¥ng embedding layer vÃ  cÃ¡c cell cÃ³ thá»ƒ lÃ  RNN cell, LSTM cell hoáº·c GRU cell (xem thÃªm bÃ i viáº¿t vá» LSTM vÃ  GRU táº¡i Ä‘Ã¢y). Ta cÃ³ thá»ƒ mÃ´ táº£ kiáº¿n trÃºc nÃ y nhÆ° hÃ¬nh bÃªn dÆ°á»›i:\nKiáº¿n trÃºc Seq2seq Ä‘Æ¡n giáº£n\nNguá»“n: VietAI LÆ°u Ã½. Trong Seq2seq, ta hoÃ n toÃ n cÃ³ thá»ƒ dÃ¹ng Bidirectional RNN. Khi Ä‘Ã³, lÆ°á»£ng â€œthÃ´ng tinâ€, hay lÃ  tráº¡ng thÃ¡i S mÃ  encoder gá»­i cho decoder cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh báº±ng trung bÃ¬nh cá»§a tráº¡ng thÃ¡i cuá»‘i cÃ¹ng cá»§a má»—i hÆ°á»›ng truyá»n. Náº¿u xÃ©t vá» máº·t cÃ´ng thá»©c cá»§a Encoder vÃ  Decoder thÃ¬ nÃ³ sáº½ giá»‘ng vá»›i trong RNN thÃ´ng thÆ°á»ng. Chá»‰ Ä‘áº·c biá»‡t á»Ÿ má»™t pháº§n lÃ  thá»i Ä‘iá»ƒm Ä‘áº§u tiÃªn cá»§a Decoder sáº½ cÃ³ tráº¡ng thÃ¡i áº©n truyá»n vÃ o lÃ  khÃ¡c 0 (nháº­n Ä‘Æ°á»£c tá»« Encoder).\nDeep Seq2seq Tá»« RNN, ta cÃ³ Deep RNN. Váº­y thÃ¬ vá»›i Seq2seq cÅ©ng nhÆ° tháº¿ ğŸ˜€ Deep Seq2seq lÃ  dáº¡ng kiáº¿n trÃºc mÃ  Encoder vÃ  Decoder cÃ³ nhiá»u recurrent layer liÃªn tiáº¿p nhau. Khi Ä‘Ã³, sá»‘ lÆ°á»£ng tráº¡ng thÃ¡i mÃ  Encoder truyá»n qua Decoder cÅ©ng sáº½ nhiá»u lÃªn. VÃ­ dá»¥ nhÆ° sau:\nMinh há»a kiáº¿n trÃºc Deep Seq2seq\nNguá»“n: VietAI Äá»ƒ Ã½ ráº±ng, recurrent layer thá»© $i$ trong Decoder sáº½ nháº­n tráº¡ng thÃ¡i Ä‘áº§u tá»« recurrent layer tÆ°Æ¡ng á»©ng cá»§a Encoder.\nChá»‰ báº±ng cÃ¡ch Ä‘Æ¡n giáº£n lÃ  chá»“ng thÃªm nhiá»u recurrent layer trong kiáº¿n trÃºc mÃ´ hÃ¬nh, Deep Seq2seq Ä‘Ã£ Ä‘áº¡t Ä‘á»™ hiá»‡u quáº£ ráº¥t vÆ°á»£t trá»™i ğŸ˜€\nKá»¹ thuáº­t Teacher Forcing vÃ  Ä‘áº£o ngÆ°á»£c cÃ¢u input Äáº§u tiÃªn, ta tháº¥y ráº±ng Ä‘á»ƒ huáº¥n luyá»‡n Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh Seq2seq thÃ¬ ta cáº§n cÃ³ input cho cáº£ 2 thÃ nh pháº§n lÃ  Encoder vÃ  Decoder. Vá»›i Encoder thÃ¬ cháº¯c cháº¯n input chÃ­nh lÃ  Ä‘oáº¡n vÄƒn báº£n cáº§n dá»‹ch. CÃ²n Decoder thÃ¬ sao?\nInput cá»§a Decoder Ä‘Æ°á»£c táº¡o ra báº±ng má»™t ká»¹ thuáº­t gá»i lÃ  Teacher Forcing. VÃ­ dá»¥, cÃ¢u vÄƒn báº£n input lÃ  â€œHÃ´m nay tÃ´i Ä‘i há»câ€ vÃ  label cá»§a nÃ³ lÃ  â€œToday I go to schoolâ€. Khi Ä‘Ã³, input vÃ  label cá»§a Decoder sáº½ lÃ :\nInput: â€œToday I go toâ€ Label: â€œI go to schoolâ€ NhÃ¬n vÃ o thÃ¬ ta sáº½ tháº¥y ngay Ã½ tÆ°á»Ÿng cá»§a Teacher Forcing ğŸ˜€\nÄá»‘i vá»›i quÃ¡ trÃ¬nh dá»‹ch (hay lÃ  dá»± Ä‘oÃ¡n) thÃ¬ Decoder hoáº¡t Ä‘á»™ng giá»‘ng vá»›i mÃ´ hÃ¬nh RNN thÃ´ng thÆ°á»ng: Sá»­ dá»¥ng output cá»§a thá»i Ä‘iá»ƒm liá»n trÆ°á»›c Ä‘á»ƒ lÃ m input cho thá»i Ä‘iá»ƒm hiá»‡n táº¡i BÃªn cáº¡nh Teacher Forcing, cÃ¡c tÃ¡c giáº£ cá»§a Seq2seq cÃ²n sá»­ dá»¥ng má»™t ká»¹ thuáº­t Ä‘á»ƒ giÃºp Seq2seq Ä‘áº¡t Ä‘Æ°á»£c má»™t hiá»‡u nÄƒng áº¥n tÆ°á»£ng lÃ  Ä‘áº£o ngÆ°á»£c cÃ¢u input (label thÃ¬ giá»¯ nguyÃªn). Nghe ráº¥t áº£o nhÆ°ngâ€¦ it works! VÃ­ dá»¥:\nMinh há»a ká»¹ thuáº­t Ä‘áº£o ngÆ°á»£c cÃ¢u input trong Seq2seq\nNguá»“n: VietAI DÃ¹ khÃ´ng Ä‘Æ°a ra Ä‘Æ°á»£c lá»i giáº£i thÃ­ch cháº·t cháº½ lÃ  vÃ¬ sao ká»¹ thuáº­t nÃ y láº¡i mang Ä‘áº¿n káº¿t quáº£ ráº¥t tá»‘t nhÆ°ng cÃ¡c tÃ¡c giáº£ cá»§a Seq2seq cÅ©ng cÃ³ nÃªu ra má»™t sá»‘ lÃ­ do thiÃªn vá» pháº§n trá»±c giÃ¡c. LÃ­ do chÃ­nh lÃ  vÃ¬ nhá» cÃ¡ch lÃ m nÃ y mÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c Ä‘Æ°á»£c thÃªm cÃ¡c má»‘i quan há»‡ phá»¥ thuá»™c giá»¯a cÃ¡c tá»« trong cÃ¢u input vÃ  label.\nKhi ta â€œdá»‹ch xuÃ´iâ€, vá»›i nhá»¯ng input cÃ³ Ä‘á»™ dÃ i lá»›n thÃ¬ sau khi Encoder tÃ­nh toÃ¡n xong, Ä‘áº¿n vá»›i Decoder thÃ¬ Decoder Ä‘ang Ä‘i dá»‹ch cho má»™t tá»« cÃ¡ch thá»i Ä‘iá»ƒm hiá»‡n táº¡i má»™t khoáº£ng cÃ¡ch ráº¥t xa, vÃ  vá»›i cÃ¡c tá»« sau cÅ©ng váº­y (khoáº£ng cÃ¡ch cá»§a tá»«ng cáº·p lÃ  xáº¥p xá»‰ nhau). Trong khi Ä‘Ã³, náº¿u Ä‘áº£o ngÆ°á»£c input thÃ¬ trung bÃ¬nh khoáº£ng cÃ¡ch nhá»¯ng cáº·p tá»« sáº½ gáº§n nhÆ° khÃ´ng Ä‘á»•i nhÆ°ng sáº½ cÃ³ nhá»¯ng cáº·p á»Ÿ ráº¥t gáº§n nhau. Tá»« Ä‘Ã³ nÃ³ gÃ³p pháº§n lÃ m giáº£m hiá»‡n tÆ°á»£ng vanishing gradient (hay cÃ²n gá»i tÃªn khÃ¡c lÃ  time lag mÃ  cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng trong paper). CÃ¡c á»©ng dá»¥ng khÃ¡c cá»§a Seq2seq DÃ¹ Ä‘Æ°á»£c phÃ¡t triá»ƒn cho bÃ i toÃ¡n Machine Translation Ä‘Æ°á»£c Seq2seq cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o ráº¥t nhiá»u bÃ i toÃ¡n khÃ¡c nhau, vÃ  chÃºng Ä‘á»u lÃ  cÃ¡c bÃ i toÃ¡n ráº¥t thÃº vá»‹ vÃ  liÃªn quan Ä‘áº¿n nhiá»u máº£ng khÃ¡c nhau trong Deep Learning. Trong Ä‘Ã³ cÃ³ hai bÃ i toÃ¡n ná»•i báº­t lÃ  Image Captioning vÃ  Speech Recognition.\nNhÃ¬n vÃ o kiáº¿n trÃºc cá»§a Seq2seq thÃ¬ ta cÃ³ nháº­n xÃ©t ráº±ng náº¿u Encoder Ä‘á»§ tá»‘t Ä‘á»ƒ rÃºt trÃ­ch cÃ¡c Ä‘áº·c trÆ°ng tá»« input vÃ  truyá»n vÃ o cho Decoder thÃ¬ Decoder cÃ³ thá»ƒ lÃ m ráº¥t nhiá»u Ä‘iá»u.\nÄá»‘i vá»›i Image Captioning, Encoder sáº½ rÃºt trÃ­ch Ä‘áº·c trÆ°ng cá»§a áº£nh vÃ  truyá»n vector nÃ y vÃ o Decoder lÃ  ta Ä‘Ã£ cÃ³ kháº£ nÄƒng sinh ra cÃ¢u mÃ´ táº£ cho táº¥m áº£nh Ä‘Ã³ Image Captioning sá»­ dá»¥ng Ã½ tÆ°á»Ÿng Seq2seq\nNguá»“n: Analytics Vidhya Speech Recognition lÃ  bÃ i toÃ¡n sinh ra Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c nÃ³i lÃªn trong file Ã¢m thanh. NhÆ° váº­y, chá»‰ cáº§n má»™t Encoder rÃºt trÃ­ch Ä‘Æ°á»£c Ä‘áº·c trÆ°ng cá»§a Ã¢m thanh rá»“i truyá»n vÃ o Decoder lÃ  ta Ä‘Ã£ cÃ³ thá»ƒ cÃ³ má»™t giáº£i phÃ¡p cho bÃ i toÃ¡n nÃ y. Speech Recognition sá»­ dá»¥ng Ã½ tÆ°á»Ÿng Seq2seq\nNguá»“n: Research Gate Äá»™ Ä‘o BLEU Äá»ƒ biáº¿t Ä‘Æ°á»£c má»™t mÃ´ hÃ¬nh Machine Translation cÃ³ hoáº¡t Ä‘á»™ng Ä‘á»§ tá»‘t hay khÃ´ng thÃ¬ ta cáº§n cÃ³ má»™t Ä‘á»™ Ä‘o. BLEU (Bilingual Evaluation Understudy) chÃ­nh lÃ  má»™t trong nhá»¯ng Ä‘á»™ Ä‘o cÆ¡ báº£n vÃ  phá»• biáº¿n nháº¥t.\nBLEU sáº½ Ä‘Ã¡nh giÃ¡ má»™t cÃ¢u dá»‹ch dá»±a theo cÃ¡c n-grams cá»§a cÃ¢u Ä‘Ã³ vá»›i cÃ¡c cÃ¢u label cÃ³ trong táº­p dá»¯ liá»‡u. VÃ­ dá»¥:\nCÃ¢u input lÃ  $x$ = â€œCon mÃ¨o náº±m á»Ÿ trÃªn bÃ nâ€\nOutput cá»§a mÃ´ hÃ¬nh lÃ  $\\hat{y}$ = â€œThe cat on tableâ€\n1-grams (hay lÃ  unigrams): The, cat, on, table 2-grams (bigrams): The cat, cat on, on table TÆ°Æ¡ng tá»± vá»›i cÃ¡c giÃ¡ trá»‹ n khÃ¡c Giáº£ sá»­ input $x$ cÃ³ hai cÃ¢u label trong táº­p dá»¯ liá»‡u:\n$o_1$ = â€œThe cat is on a tableâ€ $o_2$ = â€œThe cat lies on a deskâ€ Äáº·t $N_0$ lÃ  táº­p cÃ¡c $n_0$-grams cá»§a cÃ¢u output $\\hat{y}$. Khi Ä‘Ã³, giÃ¡ trá»‹ Ä‘iá»ƒm BLEU (hay lÃ  BLEU score) cá»§a $\\hat{y}$ tÃ­nh theo $n_0$-grams lÃ \n$$ s_n = \\frac{\\sum_{n_0\\text{-gram} \\in N_0} \\text{count}_{\\text{clip}}(n_0\\text{-gram})}{\\sum_{n_0\\text{-gram} \\in N_0} \\text{count}(n_0\\text{-gram})} $$\ntrong Ä‘Ã³:\nTá»­ sá»‘ Ä‘Æ°á»£c tÃ­nh theo cÃ¡c cÃ¢u label cá»§a input $x$, vá»›i $\\text{count}_{\\text{clip}} (n_0\\text{-gram})$ lÃ  sá»‘ láº§n xuáº¥t hiá»‡n lá»›n nháº¥t cá»§a gram nÃ y á»Ÿ trong cÃ¡c cÃ¢u label. Vá»›i vÃ­ dá»¥ trÃªn, ta cÃ³ hai cÃ¢u label lÃ  $o_1$ vÃ  $o_2$. Giáº£ sá»­ xÃ©t má»™t 2-gram â€œThe catâ€ thÃ¬ gram nÃ y Ä‘á»u xuáº¥t hiá»‡n 1 láº§n á»Ÿ trong má»—i cÃ¢u label nÃªn giÃ¡ trá»‹ $\\text{count}_\\text{clip}$ cá»§a nÃ³ lÃ  1. Máº«u sá»‘ Ä‘Æ°á»£c tÃ­nh táº¡i chÃ­nh cÃ¢u output $\\hat{y}$, vá»›i $\\text{count}(n_0\\text{-gram})$ lÃ  sá»‘ láº§n xuáº¥t hiá»‡n cá»§a gram nÃ y á»Ÿ trong cÃ¢u output. NhÆ° váº­y, táº¥t nhiÃªn lÃ  $s_n \\leq 1$ vÃ  $s_n$ cÃ ng lá»›n thÃ¬ cÃ¢u output $\\hat{y}$ cÃ ng â€œgáº§nâ€ vá»›i cÃ¡c cÃ¢u label trong táº­p dá»¯ liá»‡u.\nÄá»ƒ tÃ­nh Ä‘Æ°á»£c Ä‘iá»ƒm BLEU tháº­t sá»± cá»§a cÃ¢u $\\hat{y}$, ta sáº½ tÃ­nh $s_n$ vá»›i má»™t sá»‘ giÃ¡ trá»‹ $n$ vÃ  sau Ä‘Ã³ tÃ­nh trung bÃ¬nh theo má»™t cÃ´ng thá»©c khÃ¡ Ä‘áº·c biá»‡t:\n$$ BLEU(\\hat{y}) = BP \\times \\exp \\left ( \\frac{1}{m} \\sum_{n=1}^m s_n \\right ) $$\ntrong Ä‘Ã³ $BP$ lÃ  BLEU penalties, vá»›i Ã½ nghÄ©a lÃ  náº¿u mÃ´ hÃ¬nh cho ra nhá»¯ng cÃ¢u output quÃ¡ ngáº¯n (ngáº¯n hÆ¡n cÃ¡c cÃ¢u trong táº­p dá»¯ liá»‡u) thÃ¬ sáº½ bá»‹ pháº¡t (giáº£m Ä‘iá»ƒm BLEU):\n$$ \\begin{aligned} BP = \\left\\{\\begin{matrix} 1, \u0026amp; \\text{if } len(\\hat{y}) \u0026gt; \\min(len(o_i)) \\\\ \\exp \\left ( 1 - \\frac{\\min(len(o_i))}{len(\\hat{y})} \\right ), \u0026amp; \\text{otherwise} \\end{matrix}\\right. \\end{aligned} $$\nVÃ¬ sao cáº§n pháº£i cÃ³ BLEU penalties?\nÄá»ƒ Ã½ ráº±ng, náº¿u mÃ´ hÃ¬nh chá»‰ cho ra má»™t cÃ¢u output chá»©a Ä‘Ãºng má»™t gram luÃ´n xuáº¥t hiá»‡n trong cÃ¡c cÃ¢u label thÃ¬ ta luÃ´n cÃ³ $s_n = 1$ ğŸ˜€ Náº¿u khÃ´ng pháº¡t thÃ¬ há»ng! TÃ i liá»‡u tham kháº£o FreeCodeCamp, A history of machine translation from the Cold War to deep learning Dive into Deep Learning, Encoder-Decoder Seq2Seq for Machine Translation VietAI, Deep Learning Foundation Course 2019, Lecture 14 - Machine Translation and Sequence to Sequence model DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-03-19T00:25:41+07:00","permalink":"https://htrvu.github.io/post/mt_seq2seq/","title":"BÃ i toÃ¡n Machine Translation, mÃ´ hÃ¬nh Sequence to Sequence vÃ  Ä‘á»™ Ä‘o BLEU"},{"content":"Giá»›i thiá»‡u Qua cÃ¡c bÃ i viáº¿t vá» RNN truyá»n thá»‘ng, LSTM vÃ  GRU thÃ¬ mÃ¬nh Ä‘á»u trÃ¬nh bÃ y vá» cÃ¡c mÃ´ hÃ¬nh vá»›i duy nháº¥t má»™t cell trong kiáº¿n trÃºc (recurrent cell, LSTM cell hoáº·c lÃ  GRU cell). NgoÃ i ra, ta tháº¥y cÃ¡c hidden state cÅ©ng Ä‘Æ°á»£c truyá»n theo má»™t hÆ°á»›ng cá»‘ Ä‘á»‹nh lÃ  tá»« trÃ¡i sang pháº£i (thá»i Ä‘iá»ƒm $t$ Ä‘áº¿n thá»i Ä‘iá»ƒm $t + 1$). Náº¿u bá» qua chi tiáº¿t vá» cÃ¡c \u0026ldquo;thá»i Ä‘iá»ƒm\u0026rdquo; thÃ¬ nhÃ¬n chÃºng sáº½ khÃ´ng khÃ¡c gÃ¬ má»™t mÃ´ hÃ¬nh MLP cÆ¡ báº£n trong Machine Learning.\nCÃ¡c dáº¡ng mÃ´ hÃ¬nh RNN truyá»n thá»‘ng\nNguá»“n: Javatpoint ÄÃ¢y chá»‰ má»›i lÃ  sá»± khá»Ÿi Ä‘áº§u cá»§a RNN. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u nÄƒng tá»‘t nháº¥t cÃ³ thá»ƒ trong cÃ¡c bÃ i toÃ¡n, ta cáº§n pháº£i cÃ³ nhá»¯ng cáº£i tiáº¿n nháº¥t Ä‘á»‹nh. Hai cáº£i tiáº¿n, hay lÃ  hai biáº¿n thá»ƒ, phá»• biáº¿n cá»§a RNN mÃ  mÃ¬nh giá»›i thiá»‡u trong bÃ i viáº¿t nÃ y lÃ  Deep RNN (dÃ¹ng nhiá»u cell trong kiáº¿n trÃºc) vÃ  Bidirectional RNN (truyá»n hidden state theo cáº£ hai hÆ°á»›ng).\nLÆ°u Ã½. Äá»ƒ cho Ä‘Æ¡n giáº£n, cÃ¡c cell Ä‘Æ°á»£c sá»­ dá»¥ng trong kiáº¿n trÃºc mÃ´ hÃ¬nh mÃ  mÃ¬nh trÃ¬nh bÃ y bÃªn dÆ°á»›i Ä‘á»u lÃ  recurrent cell. Ta hoÃ n toÃ n cÃ³ thá»ƒ thay tháº¿ nÃ³ báº±ng LSTM cell, GRU cell.\nDeep RNN Táº¥t nhiÃªn, Deep Learning mÃ , dÃ¹ng nhiá»u cell (hay lÃ  hidden layer) ngay ğŸ˜œ Trong cÃ¡c mÃ´ hÃ¬nh RNN mÃ  mÃ¬nh Ä‘Ã£ trÃ¬nh bÃ y cho Ä‘áº¿n trÆ°á»›c bÃ i viáº¿t nÃ y thÃ¬ chÃºng chá»‰ cÃ³ duy nháº¥t má»™t recurrent cell vÃ  cell nÃ y cá»© nháº­n vÃ o input, tÃ­nh ra hidden state vÃ  output. Náº¿u chÃºng ta dÃ¹ng nhiá»u cell thÃ¬ sao?\nKáº¿t quáº£ sáº½ cÃ³ dáº¡ng nhÆ° hÃ¬nh bÃªn dÆ°á»›i. Trong Ä‘Ã³, input cá»§a recurrent cell thá»© $l$ lÃ  hidden state cá»§a recurrent cell thá»© $l - 1$.\nMinh há»a mÃ´ hÃ¬nh Deep RNN\nNguá»“n: Dive into DL Ta kÃ­ hiá»‡u:\n$L$ lÃ  sá»‘ recurrent cell cá»§a mÃ´ hÃ¬nh Táº¡i recurrent cell thá»© $l$ thÃ¬ $\\bold{H}_t^{(l)} \\in \\mathbb{R}^{h}$ lÃ  hidden state táº¡i thá»i Ä‘iá»ƒm $t$ (quy Æ°á»›c $\\bold{H}_t^{(0)} = \\bold{X}_t$) CÃ¡c ma tráº­n trá»ng sá»‘ láº§n lÆ°á»£t lÃ  $(\\bold{W}_{xh}^{(l)}, \\bold{W}_{hh}^{(l)})$ (vá»›i $l \u0026lt; L$). Activation function dÃ¹ng Ä‘á»ƒ tÃ­nh hidden state lÃ  $\\phi_l$. $\\bold{O}_t \\in \\mathbb{R}^{o}$ lÃ  output táº¡i thá»i Ä‘iá»ƒm $t$ cá»§a mÃ´ hÃ¬nh. Táº¡i recurrent cell thá»© $L$, ma tráº­n trá»ng sá»‘ Ä‘á»ƒ tÃ­nh ra output lÃ  $\\bold{W}_{ho}$ vÃ  activation function lÃ  $\\phi_o$. Khi Ä‘Ã³, quÃ¡ trÃ¬nh feed-forward trong Deep RNN Ä‘Æ°á»£c mÃ´ táº£ nhÆ° sau: Táº¡i thá»i Ä‘iá»ƒm $t$ thÃ¬\nQua tá»«ng recurrent cell thá»© $l = 1, 2, \u0026hellip;, L$, ta cÃ³ $$\\bold{H}_t^{(l)} = \\phi_l(\\bold{W}_{xh}^{(l)} \\bold{H}_t^{(l-1)} + \\bold{W}_{hh}^{(l)}\\bold{H}_{t-1}^{(l)})$$\nOutput táº¡i thá»i Ä‘iá»ƒm $t$ lÃ  $$\\bold{O}_t = \\phi_o (\\bold{W}_{ho} \\bold{H}_t^{(L)})$$\nBidirectional RNN (BiRNN) Ã tÆ°á»Ÿng cá»§a Bidirectional RNN (BiRNN) ráº¥t lÃ  tá»± nhiÃªn vÃ  giá»‘ng vá»›i cÃ¡ch con ngÆ°á»i Ä‘á»c hiá»ƒu ngÃ´n ngá»¯. Äáº§u tiÃªn, ta xÃ©t bÃ i toÃ¡n Name Entity Recognition vá»›i cÃ¢u sau:\nCan you see that? Teddy bears are on sales. He said that Teddy Rooosevelt was a great president. á» cÃ¢u (2) thÃ¬ ta cÃ³ thá»ƒ gÃ¡n cho Teddy thuá»™c lá»›p Name, vÃ  nÃ³ Ä‘Ãºng lÃ  tÃªn cá»§a má»™t ngÆ°á»i tháº­t. Tuy nhiÃªn, trong cÃ¢u (1) mÃ  gÃ¡n nhÆ° tháº¿ lÃ  sai. Äá»ƒ gÃ¡n Ä‘Ãºng vá»›i cÃ¢u (1) thÃ¬ ta cáº§n biáº¿t Ä‘Æ°á»£c tá»« phÃ­a sau Ä‘Ã³ ná»¯a (vÃ  ta pháº£i gÃ¡n nguyÃªn cá»¥m Teddy bears). NhÆ° váº­y, RNN truyá»n thá»‘ng sáº½ tháº¥t báº¡i trong vÃ­ dá»¥ (1), vÃ¬ khi xÃ©t tá»›i Teddy thÃ¬ ta chÆ°a cÃ³ báº¥t kÃ¬ thÃ´ng tin gÃ¬ vá» cÃ¡c tá»« phÃ­a sau nÃ³.\nTheo cÃ¡ch con ngÆ°á»i Ä‘á»c hiá»ƒu ngÃ´n ngá»¯, á»Ÿ cÃ¢u (1) thÃ¬ ta cÅ©ng cáº§n pháº£i Ä‘á»c thÃªm tá»« \u0026ldquo;bears\u0026rdquo; á»Ÿ phÃ­a sau Ä‘á»ƒ biáº¿t Ä‘Æ°á»£c tá»« \u0026ldquo;Teddy\u0026rdquo; á»Ÿ trÆ°á»›c mang Ã½ nghÄ©a gÃ¬. NhÆ° váº­y, trong BiRNN, cÃ¡c tráº¡ng thÃ¡i áº©n sáº½ Ä‘Æ°á»£c truyá»n theo cáº£ hai chiá»u (xuÃ´i vÃ  ngÆ°á»£c). Kiáº¿n trÃºc cá»§a nÃ³ sáº½ cÃ³ dáº¡ng nhÆ° hÃ¬nh bÃªn dÆ°á»›i (Ä‘á»ƒ cho Ä‘Æ¡n giáº£n thÃ¬ ta chá»‰ xÃ©t vá»›i má»™t recurrent cell ğŸ˜œ).\nMinh há»a mÃ´ hÃ¬nh BiRNN\nNguá»“n: Dive into DL Ta kÃ­ hiá»‡u:\nTrong má»—i hÆ°á»›ng truyá»n xuÃ´i vÃ  ngÆ°á»£c thÃ¬: HÆ°á»›ng truyá»n xuÃ´i: Hidden state lÃ  $\\overrightarrow{\\mathbf{H}}_t$ vÃ  hai ma tráº­n trá»ng sá»‘ lÃ  $(\\bold{W}_{xh}^{(f)}, \\bold{W}_{hh}^{(f)})$. HÆ°á»›ng truyá»n ngÆ°á»£c: Hidden state lÃ  $\\overleftarrow{\\mathbf{H}}_t$ vÃ  hai ma tráº­n trá»ng sá»‘ lÃ  $(\\bold{W}_{xh}^{(b)}, \\bold{W}_{hh}^{(b)})$. Ma tráº­n trá»ng sá»‘ Ä‘á»ƒ tÃ­nh ra output lÃ  $\\bold{W}_{ho}$. QuÃ¡ trÃ¬nh feed-forward cá»§a BiRNN sáº½ diá»…n ra nhÆ° sau:\nLáº§n lÆ°á»£t theo cÃ¡c hÆ°á»›ng truyá»n xuÃ´i, ta tÃ­nh Ä‘Æ°á»£c hidden state theo cÃ¡c cÃ´ng thá»©c $$\\overrightarrow{\\bold{H}}_t = \\phi_h(\\bold{W}_{xh}^{(f)}\\bold{X}_t + \\bold{W}_{hh}^{(f)}\\overrightarrow{\\bold{H}}_{t-1})$$\n$$\\overleftarrow{\\bold{H}}_t = \\phi_h(\\bold{W}_{xh}^{(b)}\\bold{X}_t + \\bold{W}_{hh}^{(b)}\\overleftarrow{\\bold{H}}_{t+1})$$\nSau khi Ä‘Ã£ tÃ­nh xong hidden state táº¡i toÃ n bá»™ cÃ¡c thá»i Ä‘iá»ƒm, ta tÃ­nh output: $$\\bold{O}_t = \\phi_o \\left ( \\bold{W}_{ho} \\left [ \\overrightarrow{\\bold{H}}_t , \\overleftarrow{\\bold{H}}_t \\right ] \\right )$$\n, trong Ä‘Ã³ $\\left [ \\overrightarrow{\\bold{H}}_t , \\overleftarrow{\\bold{H}}_t \\right ]$ nghÄ©a lÃ  ná»‘i hai hidden state vá»›i nhau (concatenate).\nNháº­n xÃ©t.\nQua quÃ¡ trÃ¬nh feed-forward cá»§a BiRNN, ta tháº¥y ráº±ng mÃ´ hÃ¬nh pháº£i thá»±c hiá»‡n tÃ­nh toÃ¡n hidden state táº¡i toÃ n bá»™ cÃ¡c thá»i Ä‘iá»ƒm rá»“i má»›i báº¯t Ä‘áº§u Ä‘Æ°a ra output cá»§a má»—i thá»i Ä‘iá»ƒm. Do Ä‘Ã³, Ä‘Ã´i khi BiRNN sáº½ khÃ´ng thá»±c sá»± phÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n real-time nhÆ° speech recognition. TÃ i liá»‡u tham kháº£o Dive into DL, Deep Recurrent Neural Network Dive into DL, Bidirectional Recurrent Neural Network DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-02-24T13:48:53+07:00","permalink":"https://htrvu.github.io/post/deep-rnn_birnn/","title":"Deep RNN vÃ  Bidirectional RNN"},{"content":"Vanishing gradient vÃ  long-term, short-term dependency Trong bÃ i viáº¿t vá» mÃ´ hÃ¬nh RNN truyá»n thá»‘ng, mÃ¬nh Ä‘Ã£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» vanishing gradient cá»§a nÃ³ dá»±a vÃ o cÃ´ng thá»©c cá»§a quÃ¡ trÃ¬nh BPPT (Back-propagation Through Time). Há»‡ quáº£ cá»§a váº¥n Ä‘á» nÃ y lÃ  RNN gáº·p khÃ³ khÄƒn trong viá»‡c ghi nhá»› thÃ´ng tin trong nhá»¯ng cÃ¢u cÃ³ nhiá»u tá»«.\nÄá»ƒ minh há»a rÃµ hÆ¡n vá» há»‡ quáº£ cá»§a vanishing gradient, ta xÃ©t vÃ­ dá»¥ vá»›i mÃ´ hÃ¬nh RNN dÃ¹ng Ä‘á»ƒ sinh ra vÄƒn báº£n. Giáº£ sá»­ Ä‘oáº¡n vÄƒn báº£n Ä‘ang Ä‘Æ°á»£c sinh ra nhÆ° sau:\nTrÆ°a hÃ´m nay, trá»i Ä‘Ã£ mÆ°a ráº¥t to vÃ  tÃ´i thÃ¬ láº¡i Ä‘á»ƒ quÃªn Ã¡o mÆ°a á»Ÿ nhÃ . VÃ¬ sao quÃªn thÃ¬ lÃ  do sÃ¡ng nay ngá»§ dáº­y muá»™n nÃªn tÃ´i chá»‰ táº­p trung nhanh chÃ³ng vá»‡ sinh cÃ¡ nhÃ¢n, soáº¡n sÃ¡ch vá»Ÿ rá»“i Äƒn sÃ¡ng Ä‘á»ƒ Ä‘áº¿n lá»›p thÃ´i. â€¦(vÃ i cÃ¢u gÃ¬ Ä‘Ã³ ná»¯a)â€¦. Káº¿t quáº£ lÃ  lÃºc vá» Ä‘áº¿n nhÃ , cáº£ ngÆ°á»i tÃ´i Ä‘Ã£ bá»‹ _ Tá»« tiáº¿p theo Ä‘Æ°á»£c sinh ra á»Ÿ vá»‹ trÃ­ cá»§a kÃ­ tá»± _ lÃºc nÃ y nÃªn lÃ  â€œÆ°á»›tâ€, nhÆ°ng nhá»¯ng thÃ´ng tin liÃªn quan Ä‘áº¿n váº¥n Ä‘á» bá»‹ Æ°á»›t nÃ y thÃ¬ láº¡i cÃ¡ch vá»‹ trÃ­ hiá»‡n táº¡i ráº¥t xa, á»Ÿ táº­n phÃ­a Ä‘áº§u cá»§a Ä‘oáº¡n vÄƒn báº£n (trá»i mÆ°a, quÃªn Ã¡o mÆ°a). Khi Ä‘Ã³, RNN sáº½ khÃ³ mÃ  nhá»› Ä‘Æ°á»£c nhá»¯ng chi tiáº¿t nÃ y, dáº«n Ä‘áº¿n tá»« sinh ra sáº½ khÃ´ng phÃ¹ há»£p.\nTa cÃ³ thá»ƒ gá»i sá»± phá»¥ thuá»™c giá»¯a tá»« â€œÆ°á»›tâ€ nÃªn Ä‘Æ°á»£c sinh ra vÃ  cÃ¡c chi tiáº¿t á»Ÿ Ä‘áº§u Ä‘oáº¡n vÄƒn lÃ  long-term dependency. MÃ´ hÃ¬nh cáº§n pháº£i nhá»› Ä‘Æ°á»£c nhá»¯ng chi tiáº¿t Ä‘Ã³ thÃ¬ á»Ÿ sau nÃ³ má»›i cÃ³ thá»ƒ sinh ra Ä‘Æ°á»£c tá»« há»£p lÃ½. NhÆ° váº­y, vÃ¬ gáº·p váº¥n Ä‘á» vanishing gradient mÃ  mÃ´ hÃ¬nh RNN truyá»n thá»‘ng gáº·p khÃ³ khÄƒn trong viá»‡c ghi nhá»› cÃ¡c long-term dependency. ÄÃ¢y lÃ  má»™t Ä‘iá»ƒm yáº¿u rÃµ rá»‡t nháº¥t cá»§a RNN.\nNgÆ°á»£c vá»›i long-term thÃ¬ ta cÃ³ short-term dependency. Sá»± phá»¥ thuá»™c nÃ y chá»‰ nhá»¯ng má»‘i tÆ°Æ¡ng quan giá»¯a nhá»¯ng tá»« á»Ÿ gáº§n nhau trong Ä‘oáº¡n vÄƒn báº£n. Vá»›i RNN truyá»n thá»‘ng thÃ¬ nÃ³ hoÃ n toÃ n cÃ³ thá»ƒ nhá»› Ä‘Æ°á»£c cÃ¡c sá»± phá»¥ thuá»™c nÃ y.\nLong Short-Term Memory (LSTM, 1997) vÃ  Gated Recurrent Unit (GRU, 2014) lÃ  cÃ¡c cáº£i tiáº¿n cá»§a mÃ´ hÃ¬nh RNN truyá»n thá»‘ng, nháº±m táº­p trung kháº¯c phá»¥c Ä‘iá»ƒm yáº¿u cá»§a nÃ³ trong váº¥n Ä‘á» ghi nhá»› cÃ¡c long-term dependency.\nLÆ°u Ã½.\nTrÆ°á»›c khi Ä‘i Ä‘áº¿n cÃ¡c pháº§n sau, ta quy Æ°á»›c ráº±ng output cá»§a cÃ¡c cell (LSTM cell, GRU cell) táº¡i thá»i Ä‘iá»ƒm $t$ sáº½ Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  $\\bold{Y}_t$ (trÃ¡nh nháº§m vá»›i $\\bold{O}_t$ trong RNN truyá»n thá»‘ng). NÃ³i chung lÃ  thay O thÃ nh Y ğŸ˜œ Trong bÃ i viáº¿t nÃ y, mÃ¬nh cÅ©ng sáº½ bá» qua cÃ¡c giÃ¡ trá»‹ bias, tÆ°Æ¡ng tá»± nhÆ° trong bÃ i viáº¿t vá» RNN. Long Short-Term Memory (LSTM) Ã tÆ°á»Ÿng vá» internal state TrÆ°á»›c tiÃªn, ta nháº¯c láº¡i quÃ¡ trÃ¬nh feed-forward cá»§a RNN truyá»n thá»‘ng má»™t chÃºt. Táº¡i thá»i Ä‘iá»ƒm $t$, tá»« input $\\bold{X}_t$ vÃ  hidden state $\\bold{H}_{t-1}$ thÃ¬ ta cÃ³\n$$ \\bold{H}_t = \\phi_h (\\bold{W}_{xh} \\bold{X}_t + \\bold{W}_{hh} \\bold{H}_{t-1})$$ $$\\bold{Y}_t = \\phi_y (\\bold{W}_{hy} \\bold{H}_t) $$\nHidden state $\\bold{H}_t$ chÃ­nh lÃ  thÃ nh pháº§n â€œnhá»›â€ cÃ¡c short-term dependency trong RNN. HÆ¡n ná»¯a, cÅ©ng vÃ¬ lÃ½ do ta tÃ­nh $\\bold{H}_t$ dá»±a vÃ o $\\bold{H}_{t-1}$ theo cÃ´ng thá»©c nhÆ° trÃªn nÃªn RNN má»›i gáº·p váº¥n Ä‘á» vanishing gradient descent.\nÃ tÆ°á»Ÿng cá»§a Long Short-Term Memory (LSTM) xuáº¥t phÃ¡t tá»« viá»‡c xÃ¢y dá»±ng thÃªm má»™t thÃ nh pháº§n táº¡i má»—i thá»i Ä‘iá»ƒm Ä‘á»ƒ ghi nhá»› long-term dependency, nÃ³ Ä‘Æ°á»£c gá»i lÃ  internal state. Äá»‘i vá»›i short-term thÃ¬ ta váº«n sáº½ ghi nhá»› chÃºng báº±ng hidden state nhÆ° trong RNN truyá»n thá»‘ng.\nKÃ­ hiá»‡u internal state táº¡i thá»i Ä‘iá»ƒm $t$ lÃ  $\\bold{C}_t$. Ta cÃ³ cÃ¡c thao tÃ¡c liÃªn quan Ä‘áº¿n $\\bold{C}_t$ nhÆ° sau:\nCáº­p nháº­t internal state $\\bold{C}_t$:\nLoáº¡i bá» má»™t sá»‘ thÃ´ng tin khÃ´ng cáº§n thiáº¿t trong long-term dependency nhá»› Ä‘Æ°á»£c tá»« $t-1$ thá»i Ä‘iá»ƒm trÆ°á»›c (hay lÃ  quÃªn bá»›t thÃ´ng tin trong $\\bold{C}_{t-1}$)\nThÃªm vÃ o cÃ¡c thÃ´ng tin cáº§n thiáº¿t tá»« cÃ¡c input cá»§a thá»i Ä‘iá»ƒm hiá»‡n táº¡i vÃ o internal state (hay lÃ  cáº­p nháº­t thÃªm thÃ´ng tin). Ta thÆ°á»ng kÃ­ hiá»‡u lÆ°á»£ng thÃ´ng tin nÃ y lÃ  $\\tilde{\\bold{C}}_t$.\nInput cá»§a thá»i Ä‘iá»ƒm hiá»‡n táº¡i bao gá»“m $\\bold{H}_{t-1}$ (short-term dependency) vÃ  $\\bold{X}_t$ $\\tilde{\\bold{C}}_t$ cÃ²n Ä‘Æ°á»£c gá»i lÃ  candiate internal state. Trong Ä‘Ã³, ta cÃ³ hai giÃ¡ trá»‹ Ä‘iá»u chá»‰nh tá»‰ lá»‡ loáº¡i bá» vÃ  thÃªm vÃ o táº¡i thá»i Ä‘iá»ƒm $t$, nÃ³ sáº½ kiá»ƒu nhÆ°\n$$ \\bold{C}_t = \\alpha_t \\bold{C}_{t-1} + \\beta_t \\tilde{\\bold{C}}_t $$\nTá»« internal state $\\bold{C}_t$, ta cháº¯t lá»c cÃ¡c thÃ´ng tin cÃ³ vai trÃ² nhÆ° lÃ  nhá»¯ng short-term dependency mÃ  mÃ´ hÃ¬nh nÃªn nhá»› á»Ÿ thá»i Ä‘iá»ƒm hiá»‡n táº¡i, tá»©c lÃ  tÃ­nh ra $\\bold{H}_t$.\nNgoÃ i ra, náº¿u cáº§n tÃ­nh ra output $\\bold{Y}_t$ thÃ¬ ta cÅ©ng sáº½ dá»±a vÃ o $\\bold{C}_t$.\nNhÆ° váº­y, input cá»§a LSTM cell táº¡i thá»i Ä‘iá»ƒm $t$ sáº½ cÃ³ tá»•ng cá»™ng 3 pháº§n lÃ  $\\bold{X}_t$, $\\bold{H}_{t-1}$, $\\bold{C}_{t-1}$ vÃ  output sáº½ bao gá»“m $\\bold{H}_t$, $\\bold{C}_t$ (cÃ³ thá»ƒ cÃ³ thÃªm $\\bold{Y}_t$).\nNháº­n xÃ©t.\nTrong LSTM, short-term dependency vÃ  long-term dependency Ä‘Æ°á»£c tÃ¡ch ra vÃ  nÃ³ sá»­ dá»¥ng hai cá»•ng Ä‘á»ƒ ghi nhá»›. Tá»« cÃ´ng thá»©c tÃ­nh $\\bold{C}_t$ á»Ÿ trÃªn, náº¿u $\\alpha_t = 0$ thÃ¬ cÃ³ nghÄ©a lÃ  ta sáº½ quÃªn háº¿t cÃ¡c thÃ´ng tin phÃ­a trÆ°á»›c luÃ´n, chá»‰ táº­p trung hiá»‡n táº¡i thÃ´i, cÃ²n $\\beta_t = 0$ thÃ¬ xem nhÆ° ta khÃ´ng quan tÃ¢m hiá»‡n táº¡i, chá»‰ dÃ¹ng Ä‘Ãºng nhá»¯ng gÃ¬ Ä‘Ã£ biáº¿t trong quÃ¡ khá»©. Tháº­t ra $\\alpha_t$ vÃ  $\\beta_t$ lÃ  cÃ¡c ma tráº­n vÃ  phÃ©p nhÃ¢n Ä‘Æ°á»£c thá»±c hiá»‡n lÃ  element-wise. Nhá» tÃ­nh internal state $\\bold{C}_t$ theo Ã½ tÆ°á»Ÿng cá»§a LSTM, ta Ä‘Ã£ cÃ³ thá»ƒ háº¡n cháº¿ váº¥n Ä‘á» vanishing gradient (háº¡n cháº¿ thÃ´i, váº«n cÃ³ thá»ƒ gáº·p pháº£i nhÆ°ng hiáº¿m hÆ¡n ğŸ˜€) Forget gate, input gate, output gate vÃ  candidate internal state Trong cÃ¡c thao tÃ¡c liÃªn quan Ä‘áº¿n $\\bold{C}_t$ á»Ÿ trÃªn, quan trá»ng nháº¥t lÃ  cÃ¡c chi tiáº¿t vá» loáº¡i bá», thÃªm vÃ o vÃ  cháº¯t lá»c. ChÃºng sáº½ láº§n lÆ°á»£t á»©ng vá»›i ba â€œcá»•ngâ€ lÃ  forget gate $\\bold{F}_t$, input gate $\\bold{I}_t$ vÃ  output gate $\\bold{O}_t$trong LSTM cell.\nLÆ°u Ã½ ráº±ng cÃ¡c giÃ¡ trá»‹ nÃ y lÃ  tá»‰ lá»‡, liÃªn quan Ä‘áº¿n thao tÃ¡c loáº¡i bá», thÃªm vÃ o vÃ  cháº¯t lá»c cÃ¡c thÃ´ng tin truyá»n tá»« bÃªn ngoÃ i vÃ o nÃªn ta sáº½ dÃ¹ng activation function $\\sigma$Â (sigmoid) NgoÃ i ra, thao tÃ¡c tÃ­nh toÃ¡n thÃ´ng tin candidate internal state $\\tilde{\\bold{C}}_t$ dá»±a vÃ o $\\bold{X}_t$ vÃ  $\\bold{H}_{t-1}$ sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n thÃ´ng qua thÃ nh pháº§n input node. Activation function Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tÃ­nh giÃ¡ tá»‹ nÃ y lÃ  $\\tanh$.\nKhi Ä‘Ã³, nhá»¯ng thÃ nh pháº§n nÃ y Ä‘Æ°á»£c biá»ƒu diá»…n trong LSTM cell nhÆ° sau:\nMinh há»a cÃ¡c thÃ nh pháº§n trong LSTM cell\nNguá»“n: Dive into DL NhÆ° váº­y thÃ¬ ta Ä‘Ã£ cÃ³ kha khÃ¡ kÃ½ hiá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biá»ƒu diá»…n cho cÃ¡c giÃ¡ trá»‹. Äiá»u nÃ y cÅ©ng cÃ³ nghÄ©a lÃ  sáº½ cÃ³ ráº¥t nhiá»u ma tráº­n trá»ng sá»‘ ğŸ˜€ Cá»¥ thá»ƒ hÆ¡n, vá»›i má»—i thÃ nh pháº§n $\\bold{F}_t$, $\\bold{I}_t$, $\\bold{O}_t$ vÃ  $\\tilde{\\bold{C}}_t$ thÃ¬ ta sáº½ cÃ³ hai ma tráº­n trá»ng sá»‘, vÃ­ dá»¥ nhÆ° $\\bold{W}_{xf}, \\bold{W}_{hf}$ Ä‘á»‘i vá»›i thÃ nh pháº§n $\\bold{F}_t$.\nQuÃ¡ trÃ¬nh feed-forward Tá»•ng quan quÃ¡ trÃ¬nh feed-forward trong LSTM cell Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh áº£nh bÃªn dÆ°á»›i\nQuÃ¡ trÃ¬nh feed-forward trong LSTM cell\nNguá»“n: Dive into DL Ta sáº½ cÃ³ khÃ¡ nhiá»u phÃ©p tÃ­nh, trÆ°á»›c háº¿t lÃ  tÃ­nh cÃ¡c â€œcá»•ngâ€ $\\bold{F}_t$, $\\bold{I}_t$, $\\bold{O}_t$ dá»±a vÃ o $\\bold{X}_t$ vÃ  $\\bold{H}_{t-1}$:\n$$ \\mathbf{I}_t = \\sigma( \\mathbf{W}_{xi}\\mathbf{X}_t + \\mathbf{W}_{hi}\\mathbf{H}_{t-1} )$$ $$\\mathbf{F}_t = \\sigma( \\mathbf{W}_{xf} \\mathbf{X}_t + \\mathbf{W}_{hf} \\mathbf{H}_{t-1})$$ $$\\mathbf{O}_t = \\sigma( \\mathbf{W}_{xo}\\mathbf{X}_t + \\mathbf{W}_{ho} \\mathbf{H}_{t-1}) $$\nBÃªn cáº¡nh Ä‘Ã³, ta tÃ­nh $\\tilde{\\bold{C}}_t$ báº±ng cÃ´ng thá»©c\n$$ \\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{W}_{xc} \\mathbf{X}_t + \\mathbf{W}_{hc} \\mathbf{H}_{t-1}) $$\nTá»« Ä‘Ã³, internal state $\\bold{C}_t$ vÃ  hidden state $\\bold{H}_t$ sáº½ Ä‘Æ°á»£c tÃ­nh nhÆ° sau:\n$$ \\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$$ $$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t) $$\nNáº¿u á»Ÿ thá»i Ä‘iá»ƒm nÃ y cáº§n tÃ­nh ra output $\\bold{Y}_t$ thÃ¬ sáº½ tÃ­nh theo $\\bold{C}_t$ cÃ¹ng vá»›i ma tráº­n trá»ng sá»‘ $\\bold{W}_{cy}$.\nGated Recurrent Units Ã tÆ°á»Ÿng Ä‘Æ¡n giáº£n hÃ³a LSTM Gated Recurrent Units (GRU - 2014) lÃ  cÃ³ thá»ƒ nÃ³i lÃ  má»™t phiÃªn báº£n tá»‘i Æ°u hÆ¡n cá»§a LSTM vá» máº·t Ä‘á»™ phá»©c táº¡p, trong khi vá» hiá»‡u nÄƒng thÃ¬ cÃ³ thá»ƒ nÃ³i lÃ  hai mÃ´ hÃ¬nh nÃ y ngang ngá»­a nhau. Do Ä‘Ã³, ta thÆ°á»ng tháº¥y GRU Ä‘Æ°á»£c sá»­ dá»¥ng nhiá»u hÆ¡n.\nÃ tÆ°á»Ÿng máº¥u chá»‘t cá»§a GRU lÃ  chá»‰ sá»­ dá»¥ng hidden state $\\bold{H}_t$ Ä‘á»ƒ vá»«a nhá»› cáº£ short-term vÃ  long-term dependency. Trong khi Ä‘Ã³, á»Ÿ LSTM thÃ¬ ta cÃ³ sá»± phÃ¢n tÃ¡ch giá»¯a hai thÃ´ng tin nÃ y.\nBÃªn cáº¡nh Ä‘Ã³, nhÃ¬n vÃ o cÃ´ng thá»©c tÃ­nh internal state $\\bold{C}_t$ dá»±a vÃ o hai giÃ¡ trá»‹ tá»‰ lá»‡ $\\bold{F}_t$ vÃ  $\\bold{I}_t$ trong LSTM cell, ta cÃ³ thá»ƒ cÃ³ â€œcáº£m giÃ¡câ€ lÃ  thÃ´ng thÆ°á»ng thÃ¬ tá»•ng cá»§a chÃºng hay báº±ng 1, nÃªn lÃ  thÃ´i bá» má»™t cÃ¡i Ä‘i ğŸ˜€ GRU lÃ m theo Ä‘Ãºng nhÆ° tháº¿.\nNgoÃ i ra, trong LSTM cÃ³ candidate internal state $\\tilde{\\bold{C}}_t$ dÃ¹ng Ä‘á»ƒ tÃ­nh ra cÃ¡c thÃ´ng tin cáº§n thiáº¿t tá»« cÃ¡c input $\\bold{X}_t$ vÃ  $\\bold{H}_{t-1}$. Vá»›i GRU thÃ¬ ta cÅ©ng cÃ³ thÃ nh pháº§n tÆ°Æ¡ng tá»± lÃ  candidate hidden state $\\tilde{\\bold{H}}_t$ vá»›i cÃ¹ng má»¥c Ä‘Ã­ch nhÆ° tháº¿.\nReset gate, update gate vÃ  candidate hidden state Thay vÃ¬ sá»­ dá»¥ng ba â€œcá»•ngâ€ nhÆ° LSTM thÃ¬ GRU sáº½ dÃ¹ng hai lÃ  reset gate $\\bold{R}_t$ vÃ  update gate $\\bold{Z}_t$. Má»™t thÃ nh pháº§n ná»¯a cÅ©ng ráº¥t quan trá»ng trong GRU lÃ  candidate hidden state $\\tilde{\\bold{H}}_t$. Trong Ä‘Ã³:\n$\\bold{R}_t$ sáº½ Ä‘Ã³ng vai trÃ² loáº¡i bá» má»™t sá»‘ thÃ´ng tin khÃ´ng cáº§n thiáº¿t vá» cÃ¡c short-term dependency trong hidden state cá»§a thá»i Ä‘iá»ƒm trÆ°á»›c lÃ  $\\bold{H}_{t-1}$. Ta sáº½ dÃ¹ng nÃ³ Ä‘á»ƒ tÃ­nh $\\tilde{\\bold{H}}_t$. Do Ä‘Ã³, candidate hidden state $\\tilde{\\bold{H}}_t$ sáº½ chá»©a cÃ¡c thÃ´ng tin cÃ³ Ã­ch vá» short-term dependency. $\\bold{Z}_t$ sáº½ thay tháº¿ cho cáº£ $\\bold{F}_t$ vÃ  $\\bold{I}_t$ trong viá»‡c Ä‘iá»u chá»‰nh tá»‰ lá»‡ loáº¡i bá» thÃ´ng tin khÃ´ng cáº§n thiáº¿t vá» long-term dependency trong $\\bold{H}_{t-1}$ vÃ  thÃªm vÃ o cÃ¡c thÃ´ng tin cáº§n thiáº¿t vá» short-term dependency (chÃ­nh lÃ  $\\tilde{\\bold{H}}_t$). Minh há»a cÃ¡c thÃ nh pháº§n trong GRU cell\nNguá»“n: Dive into DL Ta tháº¥y ráº±ng, sá»‘ ma tráº­n trá»ng sá»‘ cáº§n sá»­ dá»¥ng trong GRU cell sáº½ Ã­t hÆ¡n LSTM cell hai ma tráº­n. Cá»¥ thá»ƒ hÆ¡n thÃ¬ vá»›i má»—i thÃ nh pháº§n $\\bold{R}_t$, $\\bold{Z}_t$ vÃ  $\\tilde{\\bold{H}}_t$ thÃ¬ ta Ä‘á»u cáº§n hai ma tráº­n trá»ng sá»‘.\nQuÃ¡ trÃ¬nh feed-forward Tá»•ng quan quÃ¡ trÃ¬nh feed-forward trong GRU cell Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh áº£nh bÃªn dÆ°á»›i.\nMinh há»a cÃ¡c thÃ nh pháº§n trong GRU cell\nNguá»“n: Dive into DL TrÆ°á»›c háº¿t, ta tÃ­nh cÃ¡c â€œcá»•ngâ€ $\\bold{R}_t$, $\\bold{Z}_t$ dá»±a vÃ o $\\bold{X}_t$ vÃ  $\\bold{H}_{t-1}$:\n$$ \\mathbf{R}_t = \\sigma( \\mathbf{W}_{xr} \\mathbf{X}_t + \\mathbf{W}_{hr} \\mathbf{H}_{t-1})$$ $$\\mathbf{Z}_t = \\sigma(\\mathbf{W}_{xz} \\mathbf{X}_t + \\mathbf{W}_{hz} \\mathbf{H}_{t-1} ) $$\nTá»« $\\bold{R}_t$, ta tÃ­nh Ä‘Æ°á»£c $\\tilde{\\bold{H}}_t$ nhÆ° sau:\n$$ \\tilde{\\mathbf{H}}_t = \\tanh( \\mathbf{W}_{xh} \\mathbf{X}_t + \\mathbf{W}_{hh}\\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) ) $$\nSá»­ dá»¥ng $\\bold{Z}_t$ vÃ  $\\tilde{\\bold{H}}_t$, hidden state táº¡i thá»i Ä‘iá»ƒm $t$ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng\n$$ \\bold{H}_t = \\bold{Z}_t \\odot \\bold{H}_{t-1} + (1 - \\bold{Z}_t) \\odot \\tilde{\\bold{H}}_t $$\nTÃ i liá»‡u tham kháº£o Dive into DL, Long Short-Term Memory Dive into DL, Gated Recurrent Units Rian Dolphin, LSTM Networks | A Detailed Explanation DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-02-24T00:51:43+07:00","permalink":"https://htrvu.github.io/post/lstm-gru/","title":"Long Short-Term Memory (LSTM) vÃ  Gated Recurrent Unit (GRU)"},{"content":"Trong bÃ i viáº¿t vá» Recurrent Neural Network, mÃ¬nh Ä‘Ã£ Ä‘á» cáº­p khÃ¡ ká»¹ vá» mÃ´ hÃ¬nh nÃ y nhÆ°ng Ä‘á»ƒ á»©ng dá»¥ng Ä‘Æ°á»£c nÃ³ vÃ o cÃ¡c bÃ i toÃ¡n thÃ¬ ta cáº§n pháº£i lÃ m thÃªm bÆ°á»›c â€œsá»‘ hÃ³aâ€ dá»¯ liá»‡u tá»« vÄƒn báº£n sao cho mÃ¡y tÃ­nh cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c.\nNáº¿u mÃ¡y tÃ­nh hiá»ƒu Ä‘Æ°á»£c cÃ ng nhiá»u vá» cÃ¡c tá»« thÃ¬ nghÄ©a lÃ  cÃ¡ch sá»‘ hÃ³a cÃ ng cÃ³ hiá»‡u quáº£. Do Ä‘Ã³, ta cáº§n quan tÃ¢m Ä‘áº¿n váº¥n Ä‘á» â€œhiá»ƒuâ€. Hiá»ƒu nhÆ° tháº¿ nÃ o lÃ  Ä‘á»§ tá»‘t? ğŸ˜€ Äá»‘i vá»›i NLP, ta cÃ³ nhá»¯ng phÆ°Æ¡ng phÃ¡p (hay cÃ³ thá»ƒ nÃ³i lÃ  ká»¹ thuáº­t) biá»ƒu diá»…n tá»« phá»• biáº¿n lÃ  One-hot Encoding, TF-IDF vÃ  Word Embedding. Ná»™i dung cá»§a bÃ i viáº¿t nÃ y sáº½ táº­p trung vÃ o One-hot Encoding vÃ  Word Embedding.\nOne-hot Encoding Ã tÆ°á»Ÿng Tá»« Ä‘iá»ƒn (vocabulary) lÃ  má»™t thÃ nh pháº§n khÃ´ng thá»ƒ thiáº¿u cá»§a má»i há»‡ thá»‘ng ngÃ´n ngá»¯. Nhá»¯ng tá»« ta dÃ¹ng thÆ°á»ng ngÃ y háº§u nhÆ° lÃ  sáº½ náº±m á»Ÿ má»™t vá»‹ trÃ­ nÃ o Ä‘Ã³ trong tá»« Ä‘iá»ƒn (cÃ³ thá»ƒ cÃ¡c tá»« Ä‘á»‹a phÆ°Æ¡ng thÃ¬ sáº½ khÃ´ng cÃ³). One-hot Encoding lÃ  phÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n tá»« báº±ng chÃ­nh thÃ´ng tin vá»‹ trÃ­ nÃ y.\nVá»›i nhá»¯ng tá»« khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn thÃ¬ ta thÆ°á»ng sá»­ dá»¥ng má»™t giÃ¡ trá»‹ vá»‹ trÃ­ Ä‘áº·c biá»‡t Ä‘á»ƒ cho biáº¿t tá»« Ä‘Ã³ lÃ  unknown. Giáº£ sá»­ táº­p tá»« Ä‘iá»ƒn cá»§a chÃºng ta cÃ³ $S$ tá»« vÃ  khÃ´ng cÃ³ tá»« trÃªn cÃ¡c vÄƒn báº£n lÃ  khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn. Khi Ä‘Ã³, má»—i tá»« sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t vector nhá»‹ phÃ¢n cÃ³ $S$ chiá»u, vá»›i duy nháº¥t má»™t pháº§n tá»­ báº±ng 1 táº¡i chiá»u á»©ng vá»›i vá»‹ trÃ­ cá»§a tá»« Ä‘Ã³ trong tá»« Ä‘iá»ƒn vÃ  cÃ¡c pháº§n tá»­ cÃ²n láº¡i lÃ  0. VÃ­ dá»¥:\nMinh há»a phÆ°Æ¡ng phÃ¡p One-hot Encoding vá»›i kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn lÃ  9\nNguá»“n: Shane Lynn Khi káº¿t há»£p phÆ°Æ¡ng phÃ¡p One-hot Encoding vÃ o mÃ´ hÃ¬nh RNN Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n thÃ¬ á»Ÿ trong má»—i giai Ä‘oáº¡n ta sáº½ cÃ³:\nInput vÃ  label sáº½ lÃ  cÃ¡c vector nhá»‹ phÃ¢n tÆ°Æ¡ng á»©ng vá»›i cÃ¡c tá»« Output lÃ  má»™t vector thá»ƒ hiá»‡n má»™t phÃ¢n bá»‘ xÃ¡c suáº¥t, vá»›i pháº§n tá»­ thá»© $i$ lÃ  xÃ¡c suáº¥t mÃ  tá»« output lÃ  tá»« á»Ÿ vá»‹ trÃ­ thá»© $i$ trong tá»« Ä‘iá»ƒn (do Ä‘Ã³ activation function thÆ°á»ng dÃ¹ng á»Ÿ Ä‘Ã¢y chÃ­nh lÃ  softmax) VÃ­ dá»¥ Ã¡p dá»¥ng One-hot Encoding vÃ o RNN trong bÃ i toÃ¡n sinh vÄƒn báº£n theo tá»«ng kÃ­ tá»±\nNguá»“n: Stanford - Natural Language Processing VÃ¬ sao ta láº¡i sá»­ dá»¥ng vector nhá»‹ phÃ¢n Ä‘á»ƒ biá»ƒu diá»…n cÃ¡c tá»« mÃ  khÃ´ng dÃ¹ng luÃ´n giÃ¡ trá»‹ sá»‘ thá»±c lÃ  vá»‹ trÃ­ cá»§a tá»« trong tá»« Ä‘iá»ƒn?\nCÃ¢u há»i nÃ y cÅ©ng giá»‘ng nhÆ° há»i ráº±ng trong bÃ i toÃ¡n image classification thÃ¬ vÃ¬ sao ta khÃ´ng cÃ i Ä‘áº·t output lÃ  má»™t sá»‘ thá»±c vÃ  sau Ä‘Ã³ lÃ m trÃ²n Ä‘á»ƒ cÃ³ káº¿t quáº£ mÃ  láº¡i lÃ  má»™t vector phÃ¢n bá»‘ xÃ¡c suáº¥t. Táº¥t nhiÃªn lÃ  náº¿u lÃ m theo cÃ¡ch Ä‘Ã³ thÃ¬ má»i thá»© váº«n CÃ“ THá»‚ á»•n, quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c thÃ nh cÃ´ng. Tuy nhiÃªn, ta cÃ³ nhá»¯ng Ä‘iá»u cáº§n lÆ°u tÃ¢m nhÆ° sau: Vá»›i output lÃ  sá»‘ thá»±c nhÆ° váº­y thÃ¬ cost function háº§u nhÆ° cháº¯c cháº¯n lÃ  MSE (Mean Square Error). Khi Ä‘Ã³, quÃ¡ trÃ¬nh huáº¥n luyá»‡n sáº½ ráº¥t dá»… rÆ¡i vÃ o vá»‹ trÃ­ tá»‘i Æ°u cá»¥c bá»™. Náº¿u mÃ  sá»‘ tá»« trong tá»« Ä‘iá»ƒn lÃ  ráº¥t nhiá»u thÃ¬ káº¿t quáº£ cá»§a cÃ¡c phÃ©p tÃ­nh trong cÃ¡ch biá»ƒu diá»…n dÃ¹ng sá»‘ thá»±c lÃ  ráº¥t lá»›n. Äá»ƒ Ã½ ráº±ng, trong cÃ¡c biá»ƒu diá»…n One-hot Encoding thÃ¬ khoáº£ng cÃ¡ch giá»¯a má»™t tá»« vá»›i cÃ¡c tá»« khÃ¡c nÃ³ sáº½ báº±ng háº±ng sá»‘ lÃ  $\\sqrt{2}$. Trong khi Ä‘Ã³, vá»›i cÃ¡ch biá»ƒu diá»…n dÃ¹ng duy nháº¥t sá»‘ thá»±c thÃ¬ láº¡i khÃ´ng, cÃ³ nhá»¯ng cáº·p tá»« ráº¥t gáº§n nhau vÃ  cÃ³ nhá»¯ng cáº·p tá»« cá»±c kÃ¬ xa nhau, trong khi ta chÆ°a cÃ³ báº¥t cá»© Ä‘iá»u gÃ¬ thá»ƒ hiá»‡n Ä‘Æ°á»£c ráº±ng tá»« nÃ y nÃªn gáº§n vá»›i má»™t tá»« hÆ¡n so vá»›i tá»« kia. Háº¡n cháº¿ One-hot Encoding Trong cÃ¡ch biá»ƒu diá»…n One-hot Encoding, ta tháº¥y ráº±ng mÃ¡y tÃ­nh Ä‘Ã£ cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c tá»« vá»›i nhau, cÃ³ thá»ƒ biáº¿t Ä‘Æ°á»£c tá»« Ä‘Æ°á»£c dÃ¹ng trong cÃ¢u input lÃ  tá»« gÃ¬ vÃ  cÃ³ thá»ƒ cho biáº¿t tá»« mÃ  nÃ³ tÃ­nh ra Ä‘Æ°á»£c á»Ÿ output lÃ  tá»« gÃ¬. NÃ³i chung lÃ  mÃ¡y tÃ­nh Ä‘Ã£ hiá»ƒu Ä‘Æ°á»£c â€œmáº·t trÆ°á»›câ€ cá»§a cÃ¡c tá»«.\nTuy nhiÃªn, ta váº«n chÆ°a thá»ƒ biá»ƒu diá»…n Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau. NhÆ° Ä‘Ã£ Ä‘á» cáº­p á»Ÿ pháº§n trÆ°á»›c, khoáº£ng cÃ¡ch giá»¯a hai cáº·p tá»« phÃ¢n biá»‡t báº¥t ká»³ Ä‘á»u báº±ng $\\sqrt{2}$, trong khi nhá»¯ng tá»« cÃ³ nghÄ©a gáº§n gáº§n nhau nhÆ° â€œgoodâ€ vÃ  â€œniceâ€ thÃ¬ nÃªn cÃ³ khoáº£ng cÃ¡ch gáº§n nhau, cÃ²n nhá»¯ng tá»« trÃ¡i nghÄ©a nhau nhÆ° â€œgoodâ€ vÃ  â€œbadâ€ thÃ¬ cÅ©ng nÃªn cÃ¡ch nhau ráº¥t xa. ChÃ­nh vÃ¬ yáº¿u tá»‘ nÃ y mÃ  thÆ°á»ng thÃ¬ viá»‡c Ã¡p dá»¥ng One-hot Encoding vÃ o RNN khÃ³ cÃ³ thá»ƒ mang láº¡i káº¿t quáº£ nhÆ° mong muá»‘n.\nBÃªn cáº¡nh Ä‘Ã³, cÃ¡ch biá»ƒu diá»…n One-hot Encoding tháº­t sá»± lÃ  ráº¥t tá»‘n kÃ©m vá» máº·t bá»™ nhá»› ğŸ˜€ Náº¿u mÃ  kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn ráº¥t lá»›n thÃ¬ cá»© má»—i tá»« nhÆ° váº­y ta láº¡i cáº§n má»™t vector cÃ³ sá»‘ chiá»u khá»•ng lá»“ Ä‘á»ƒ biá»ƒu diá»…n. Má»™t cÃ¡ch kháº¯c phá»¥c váº¥n Ä‘á» nÃ y lÃ  sá»­ dá»¥ng ma tráº­n thÆ°a (sparse matrix), nhÆ°ng mÃ  viá»‡c cÃ i Ä‘áº·t thÃ¬ cÅ©ng khÃ´ng pháº£i Ä‘Æ¡n giáº£n.\nTá»« cÃ¡c háº¡n cháº¿ cá»§a One-hot Encoding, ta cÃ³ má»™t phÆ°Æ¡ng phÃ¡p tá»‘t hÆ¡n, vá»«a cÃ³ thá»ƒ biá»ƒu diá»…n Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vÃ  vá»«a tiáº¿t kiá»‡m Ä‘Æ°á»£c bá»™ nhá»›, Ä‘Ã³ lÃ  Word Embedding!\nWord Embedding Ã tÆ°á»Ÿng Äáº§u tiÃªn, embedding nÃ³i chung lÃ  phÆ°Æ¡ng phÃ¡p Ä‘Æ°a má»™t vector cÃ³ sá»‘ chiá»u lá»›n (thÆ°á»ng á»Ÿ dáº¡ng thÆ°a, tá»©c lÃ  háº§u háº¿t cÃ¡c pháº§n tá»­ Ä‘á»u báº±ng 0), vá» má»™t vector cÃ³ sá»‘ chiá»u nhá» hÆ¡n (vÃ  khÃ´ng thÆ°a).\nTa tháº¥y ngay ráº±ng one-hot vector Ä‘á»ƒ biá»ƒu diá»…n cÃ¡c tá»« trong má»™t táº­p tá»« Ä‘iá»ƒn lá»›n chÃ­nh lÃ  vector cÃ³ sá»‘ chiá»u lá»›n vÃ  á»Ÿ dáº¡ng thÆ°a ğŸ˜€ Embedding cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng á»Ÿ nhiá»u máº£ng khÃ¡c nhau chá»© khÃ´ng pháº£i má»—i xá»­ lÃ½ ngÃ´n ngá»¯, vÃ­ dá»¥ nhÆ° hÃ¬nh áº£nh cÅ©ng cÃ³. Word Embedding lÃ  má»™t phÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n cÃ¡c tá»« báº±ng má»™t vector Ä‘áº·c trÆ°ng. VÃ­ dá»¥, vá»›i cÃ¡c tá»« {man, woman, king, queen, apple, orange} vÃ  táº­p cÃ¡c Ä‘áº·c trÆ°ng {gender, age, food} thÃ¬ ta cÃ³ thá»ƒ biá»ƒu diá»…n má»—i tá»« báº±ng má»™t vector 3 chiá»u nhÆ° sau:\nman woman king queen apple orange gender -1 1 -0.9 0.97 0.0 0.01 age 0.3 0.25 0.7 0.69 0.02 0.0 food 0.01 0.0 0.005 0.015 0.97 0.96 Trong báº£ng trÃªn, má»—i tá»« trong tá»« Ä‘iá»ƒn ban Ä‘áº§u Ä‘Ã£ Ä‘Æ°á»£c Ã¡nh xáº¡ thÃ nh má»™t vector 3 chiá»u (cÃ²n one-hot vector Ä‘á»ƒ biá»ƒu diá»…n chÃºng thÃ¬ cÃ³ 6 chiá»u). Trong Ä‘Ã³, giÃ¡ trá»‹ vector á»©ng vá»›i má»—i tá»« sáº½ chá»©a nhá»¯ng nÃ©t Ä‘áº·c trÆ°ng vá» máº·t ngá»¯ nghÄ©a cá»§a tá»« Ä‘Ã³. KÃ­ hiá»‡u $e_{word}$ lÃ  embedding vector cá»§a tá»« $word$. Ta cÃ³ má»™t sá»‘ nháº­n xÃ©t sau: $e_{apple}$ vÃ  $e_{orange}$ cÃ³ giÃ¡ trá»‹ táº¡i Ä‘áº·c trÆ°ng food ráº¥t cao vÃ  hai Ä‘áº·c trÆ°ng cÃ²n láº¡i thÃ¬ khÃ´ng. $e_{man}$ cÃ³ Ä‘áº·c trÆ°ng gender lÃ  -1 cÃ²n $e_{woman}$ lÃ  1, hÃ m Ã½ ráº±ng giá»›i tÃ­nh â€œmanâ€ vÃ  â€œwomanâ€ lÃ  trÃ¡i ngÆ°á»£c nhau. $e_{man}$ vá»›i $e_{king}$ cÃ³ giÃ¡ trá»‹ táº¡i Ä‘áº·c trÆ°ng gender ráº¥t giá»‘ng nhau, Ä‘á»‘i vá»›i age thÃ¬ cÃ³ sá»± khÃ¡c biá»‡t, hÃ m Ã½ ráº±ng â€œkingâ€ thÃ¬ thÆ°á»ng lá»›n tuá»•i hÆ¡n â€œmanâ€. Ta cÃ³ nhÃ¢n xÃ©t tÆ°Æ¡ng tá»± vá»›i â€œwomanâ€ vÃ  â€œkingâ€. Náº¿u ta tÃ­nh thá»­ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (similarity) giá»¯a cÃ¡c vector (thÆ°á»ng lÃ  khoáº£ng cÃ¡ch Cosine hoáº·c khoáº£ng cÃ¡ch Euclid), thÃ¬ káº¿t quáº£ sáº½ cÃ³ Ã½ nghÄ©a nhÆ° sau: Hai vector $e_{man}$ vÃ  $e_{king}$ ráº¥t gáº§n nhau. TÆ°Æ¡ng tá»± vá»›i $e_{woman}$ vÃ  $e_{queen}$, $e_{apple}$ vÃ  $e_{orange}$. Äiá»u nÃ y thá»ƒ hiá»‡n ráº±ng cÃ¡c tá»« trong má»—i cáº·p cÃ³ quan há»‡ gáº§n gÅ©i vá»›i nhau vá» ngá»¯ nghÄ©a. Hai vector $e_{man}$ vÃ  $e_{woman}$ cÃ³ hÆ°á»›ng gáº§n nhÆ° lÃ  ngÆ°á»£c nhau, thá»ƒ hiá»‡n ráº±ng hai tá»« nÃ y cÃ³ quan há»‡ trÃ¡i ngÆ°á»£c nhau. ThÃ´ng thÆ°á»ng, ngÆ°á»i ta thÆ°á»ng sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p t-SNE Ä‘á»ƒ giáº£m chiá»u cÃ¡c embedding vector xuá»‘ng 2 chiá»u vÃ  trá»±c quan hÃ³a chÃºng Ä‘á»ƒ cÃ³ gÃ³c nhÃ¬n rÃµ hÆ¡n vá» Word Embedding. VÃ­ dá»¥ nhÆ° hÃ¬nh bÃªn dÆ°á»›i, vá»›i cÃ¡c tá»« cÃ³ nghÄ©a tÆ°Æ¡ng tá»± nhau thÃ¬ ta sáº½ tháº¥y chÃºng cÃ³ xu hÆ°á»›ng cÃ¹ng thuá»™c vá» má»™t cá»¥m:\nSá»­ dá»¥ng t-SNE Ä‘á»ƒ trá»±c quan hÃ³a cÃ¡c embedding vector\nNguá»“n: Neptune AI Äá»‘i vá»›i trá»±c quan hÃ³a trong khÃ´ng gian 3 chiá»u thÃ¬ cÃ¡c báº¡n cÃ³ thá»ƒ truy cáº­p vÃ o trang nÃ y cá»§a Tensorflow. Trong trang web Ä‘Ã³, náº¿u tÃ¬m kiáº¿m tá»« â€œsoccerâ€ thÃ¬ ta sáº½ tháº¥y cÃ¡c vector Ä‘Æ°á»£c highlight lÃªn lÃ  vector á»©ng vá»›i cÃ¡c tá»« cÃ³ nghÄ©a ráº¥t tÆ°Æ¡ng tá»±, vÃ  háº§u háº¿t lÃ  liÃªn quan Ä‘áº¿n thá»ƒ thao.\nNhÆ° váº­y, phÆ°Æ¡ng phÃ¡p Word Embedding Ä‘Ã£ cÃ³ thá»ƒ kháº¯c phá»¥c Ä‘Æ°á»£c háº¡n cháº¿ cá»§a One-hot Encoding trong viá»‡c thá»ƒ hiá»‡n má»‘i quan há»‡ giá»¯a cÃ¡c tá»«.\nTÃ­nh cháº¥t cá»§a Word Embedding Trong kháº£ nÄƒng biá»ƒu diá»…n cÃ¡c tá»« báº±ng vector Ä‘áº·c trÆ°ng vÃ  thá»ƒ hiá»‡n Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a tá»« Ä‘Ã³ vá»›i nhá»¯ng tá»« khÃ¡c, ta cÃ³ má»™t tÃ­nh cháº¥t thÃº vá»‹ liÃªn quan Ä‘áº¿n Analogy Reasoning (suy diá»…n tÆ°Æ¡ng tá»±).\nVÃ­ dá»¥: Cho trÆ°á»›c 3 tá»« â€œmanâ€, â€œwomanâ€ vÃ  â€œkingâ€. Trong Ä‘Ã³, â€œmanâ€ Ä‘Ã£ cÃ³ má»™t quan há»‡ nháº¥t Ä‘á»‹nh vá»›i â€œwomanâ€. Ta cáº§n tÃ¬m má»™t tá»« sao cho quan há»‡ giá»¯a â€œkingâ€ vá»›i tá»« nÃ y cÅ©ng tÆ°Æ¡ng tá»± nhÆ° quan há»‡ giá»¯a â€œmanâ€ vÃ  â€œwomanâ€. Náº¿u má»™t há»‡ thá»‘ng Word Embedding Ä‘á»§ tá»‘t thÃ¬ ta sáº½ cÃ³ tÃ­nh cháº¥t ráº±ng nhá»¯ng cáº·p tá»« $(w_{i1}, w_{i2})$ mÃ  cÃ³ quan há»‡ giá»¯a hai tá»« trong má»™t cáº·p lÃ  ráº¥t tÆ°Æ¡ng tá»± nhau thÃ¬ cÃ¡c vector $x_i = e_{w_{i1}} - e_{w_{i2}}$ sáº½ cÃ³ hÆ°á»›ng cÅ©ng ráº¥t tÆ°Æ¡ng tá»±. VÃ­ dá»¥:\nMinh há»a Analogy Reasoning\nNguá»“n: Polakowo Dá»±a vÃ o tÃ­nh cháº¥t nÃ y, ta cÃ³ thá»ƒ giáº£i quyáº¿t cÃ¢u há»i Ä‘áº·t ra á»Ÿ phÃ­a trÃªn ráº±ng tá»« cáº§n tÃ¬m sáº½ lÃ  â€œqueenâ€. Äá»ƒ kiá»ƒm chá»©ng, hÃ£y xÃ©t láº¡i báº£ng á»Ÿ pháº§n 2.1, ta cÃ³:\n$e_{man} -e_{woman} = \\begin{bmatrix}-2 \u0026amp; 0.05 \u0026amp; 0.01\\end{bmatrix}^\\top$ $e_{king} - e_{queen} = \\begin{bmatrix}-1.87 \u0026amp; 0.01 \u0026amp; -0.01 \\end{bmatrix}^\\top$ Äá»ƒ biá»ƒu diá»…n bÃ i toÃ¡n analogy reasoning nhÆ° vÃ­ dá»¥ bÃªn trÃªn má»™t cÃ¡ch hÃ¬nh thá»©c hÆ¡n, ta cÃ³ thá»ƒ phÃ¡t biá»ƒu nhÆ° sau:\nTÃ¬m tá»« $w$ sao cho\n$$w = \\argmax_w \\left ( \\text{sim} ( e_w, e_{man} - e_{woman} + e_{king} )\\right )$$\nvÃ  káº¿t quáº£ lÃ  $w = queen$.\nSá»­ dá»¥ng Word Embedding trong RNN Trong phÆ°Æ¡ng phÃ¡p One-hot Encoding, ta sá»­ dá»¥ng cÃ¡c one-hot vector á»Ÿ input vÃ  output cá»§a RNN. Äá»‘i vá»›i word-embedding thÃ¬ ta sáº½ thay Ä‘á»•i má»™t chÃºt á»Ÿ input, cÃ²n output thÃ¬ váº«n dÃ¹ng One-hot Encoding Ä‘á»ƒ biáº¿t Ä‘Æ°á»£c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»« nÃ o. ğŸ˜œ\nTáº¡i sao láº¡i nhÆ° tháº¿?\nÄá»‘i vá»›i input, Ä‘Æ°a vÃ o RNN embedding vector thÃ¬ cháº¯c cháº¯n mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c tá»‘t hÆ¡n so vá»›i one-hot vector rá»“i. Trong output, ta tháº¥y ráº±ng viá»‡c mÃ´ hÃ¬nh tÃ­nh ra má»™t vector dáº¡ng phÃ¢n bá»‘ xÃ¡c suáº¥t vÃ  sau Ä‘Ã³ xÃ¡c Ä‘á»‹nh tá»« tÆ°Æ¡ng á»©ng báº±ng cÃ¡ch softmax thÃ¬ sáº½ dá»… hÆ¡n nhiá»u so vá»›i viá»‡c output ra má»™t embedding vector rá»“i tá»« vector nÃ y Ä‘i tÃ¬m tá»« gá»‘c. Tháº­t ra thÃ¬ mÃ¬nh chÆ°a tháº¥y ai thá»±c hiá»‡n tÃ¬m tá»« dá»±a vÃ o embedding vector cáº£. ğŸ˜€ NhÆ° váº­y, Ä‘á»ƒ sá»­ dá»¥ng Word Embedding trong RNN thÃ¬ ta sáº½ dÃ¹ng embedding vector cá»§a cÃ¡c tá»« Ä‘á»ƒ lÃ m input cho RNN.\nVÃ­ dá»¥: mÆ°á»£n táº¡m áº£nh cá»§a cÃ¡c phÃ¡p sÆ° Trung Hoa z =)) Minh há»a sá»­ dá»¥ng Word Embedding trong RNN\nNguá»“n: PhÃ¡p sÆ° nÃ o Ä‘Ã³ LÆ°u Ã½.\nKhi sá»­ dá»¥ng Word Embedding trong RNN thÃ¬ thÆ°á»ng ta sáº½ dÃ¹ng theo hÆ°á»›ng transfer learning hoáº·c fine-tuning. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c embedding vector cá»§a má»—i tá»« cÃ³ thá»ƒ Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p sáºµn, ta chá»‰ viá»‡c Ä‘em vÃ o dÃ¹ng trong mÃ´ hÃ¬nh lÃ  Ä‘á»§ vÃ  náº¿u cáº§n thiáº¿t thÃ¬ cÅ©ng sáº½ tiáº¿p tá»¥c huáº¥n luyá»‡n trÃªn ná»n táº£ng Ä‘Ã£ cÃ³. Embedding matrix á» cÃ¡c pháº§n trÃªn thÃ¬ ta chá»‰ má»›i nÃªu sÆ¡ lÆ°á»£c vá» phÆ°Æ¡ng phÃ¡p Word Embedding chá»© chÆ°a Ä‘á» cáº­p Ä‘áº¿n viá»‡c lÃ m tháº¿ nÃ o Ä‘á»ƒ xÃ¢y dá»±ng Ä‘Æ°á»£c cÃ¡c vector biá»ƒu diá»…n tá»« nhÆ° váº­y. Äáº§u tiÃªn, thá»© chÃºng ta cáº§n xÃ¢y dá»±ng trong Word Embedding Ä‘Æ°á»£c gá»i lÃ  embedding matrix (kÃ­ hiá»‡u lÃ  $E$), vá»›i sá»‘ dÃ²ng lÃ  sá»‘ Ä‘áº·c trÆ°ng Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ mÃ´ táº£ cho má»—i tá»« vÃ  sá»‘ cá»™t báº±ng vá»›i sá»‘ tá»« trong tá»« Ä‘iá»ƒn.\nÄá»ƒ minh há»a, ta sáº½ dÃ¹ng láº¡i vÃ­ dá»¥ á»Ÿ pháº§n 2.1. embedding matrix $E$ sáº½ lÃ \n$$ E = \\begin{bmatrix}-1 \u0026amp; 1 \u0026amp; -0.9Â \u0026amp; 0.97Â \u0026amp; 0.0Â \u0026amp; 0.01 \\\\Â 0.3Â \u0026amp; 0.25Â \u0026amp; 0.7 \u0026amp; 0.69 \u0026amp; 0.02 \u0026amp; 0.0Â \\\\Â 0.01 \u0026amp; 0.0 \u0026amp; 0.005 \u0026amp; 0.015 \u0026amp; 0.97 \u0026amp; 0.96\\end{bmatrix} $$\nVá»›i tá»« $man$, one-hot vector cá»§a tá»« nÃ y lÃ \n$$o_{man} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\end{bmatrix}^\\top$$\nEmbedding vector cá»§a $man$ sáº½ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c\n$$e_{man} = E \\cdot o_{man} = \\begin{bmatrix} -1 \u0026amp; 0.3 \u0026amp; 0.01 \\end{bmatrix}^\\top$$\nÄá»ƒ xÃ¢y dá»±ng cÃ¡c embedding matrix, ta cÃ³ hai hÆ°á»›ng phá»• biáº¿n nhÆ° sau:\nWord2vec: Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh MLP. embedding matrix sáº½ lÃ  má»™t ma tráº­n trá»ng sá»‘ trong mÃ´ hÃ¬nh MLP sau khi Ä‘Ã£ huáº¥n luyá»‡n xong. GloVe (Global Vector for word representations): Máº¡nh hÆ¡n Word2vec, cÃ³ sá»­ dá»¥ng thÃªm nhá»¯ng ká»¹ thuáº­t liÃªn quan Ä‘áº¿n xÃ¡c suáº¥t thá»‘ng kÃª. Trong bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ táº­p trung vÃ o word2vec.\nWord2vec Word2vec lÃ  má»™t mÃ´ hÃ¬nh ráº¥t Ä‘Æ¡n giáº£n vÃ  ná»•i tiáº¿ng trong viá»‡c táº¡o embedding matrix. Ã tÆ°á»Ÿng cá»§a word2vec xuáº¥t phÃ¡t tá»« hai nháº­n xÃ©t sau:\nHai tá»« thÆ°á»ng xuáº¥t hiá»‡n trong cÃ¡c ngá»¯ cáº£nh tÆ°Æ¡ng tá»± nhau thÃ¬ cÃ³ thá»ƒ cÃ³ quan há»‡ gáº§n gÅ©i nhau vá» máº·t ngá»¯ nghÄ©a. Khi cho biáº¿t trÆ°á»›c cÃ¡c tá»« xung quanh tá»« bá»‹ thiáº¿u trong cÃ¢u, ta cÃ³ thá»ƒ dá»± Ä‘oÃ¡n ra Ä‘Æ°á»£c tá»« Ä‘Ã³. VÃ­ dá»¥, vá»›i cÃ¢u â€œHusky lÃ  má»™t â€¦ chÃ³ ráº¥t ngÃ¡oâ€ thÃ¬ tá»« trong dáº¥u ba cháº¥m cÃ³ kháº£ nÄƒng cao lÃ  â€œloÃ iâ€. ÄÃ¢y lÃ  vÃ­ dá»¥ vá»›i cÃ¢u ngáº¯n, cÃ²n náº¿u cÃ¢u dÃ i thÃ¬ Ä‘Ã´i khi ta chá»‰ cáº§n xÃ©t táº§m 10 tá»« xung quanh tá»« cáº§n dá»± Ä‘oÃ¡n lÃ  Ä‘Ã£ Ä‘á»§ Ä‘á»ƒ Ä‘oÃ¡n ra Ä‘Æ°á»£c. Äáº§u tiÃªn, ta Ä‘á» cáº­p Ä‘áº¿n khÃ¡i niá»‡m tá»« má»¥c tiÃªu (target word) vÃ  tá»« ngá»¯ cáº£nh (context word). CÃ³ thá»ƒ nÃ³i tá»« má»¥c tiÃªu lÃ  tá»« ta Ä‘ang xem xÃ©t vÃ  tá»« ngá»¯ cáº£nh lÃ  cÃ¡c tá»« xuáº¥t hiá»‡n xung quanh tá»« má»¥c tiÃªu á»Ÿ trong cÃ¡c Ä‘oáº¡n vÄƒn báº£n cá»§a kho dá»¯ liá»‡u, vá»›i pháº¡m vi lÃ  cÃ¡ch tá»« má»¥c tiÃªu khÃ´ng quÃ¡ $\\dfrac{C}{2}$ tá»«. VÃ¹ng pháº¡m vi nÃ y cÃ²n láº¡i lÃ  cá»­a sá»• trÆ°á»£t (sliding window).\nVá»›i cÃ¢u vÃ­ dá»¥ á»Ÿ trÃªn thÃ¬, tá»« â€œloÃ iâ€ lÃ  target word. Náº¿u xÃ©t cá»­a sá»• trÆ°á»£t cÃ³ kÃ­ch thÆ°á»›c $C = 4$ thÃ¬ cÃ¡c context word sáº½ bao gá»“m â€œlÃ â€, â€œmá»™tâ€, â€œchÃ³â€, â€œráº¥tâ€. Äá»ƒ cÃ³ cÃ¡i nhÃ¬n rÃµ hÆ¡n vá» cÃ¡c khÃ¡i niá»‡m nÃ y, ta xÃ©t vÃ­ dá»¥ bÃªn dÆ°á»›i vá»›i kÃ­ch thÆ°á»›c sliding window lÃ  4. Tá»« mÃ u xanh lÃ  target word, cÃ¡c tá»« trong Ã´ mÃ u tráº¯ng lÃ  context word\nNguá»“n: Machine Learning cho dá»¯ liá»‡u dáº¡ng báº£ng LÆ°u Ã½.\nTá»« phÆ°Æ¡ng phÃ¡p word2vec, ta sáº½ cÃ³ hai embedding vector cho má»—i tá»«, á»©ng vá»›i hai trÆ°á»ng há»£p lÃ  tá»« Ä‘Ã³ Ä‘Ã³ng vai trÃ² target word vÃ  context word. LÃ½ do lÃ  vÃ¬ trong má»—i tÃ¬nh huá»‘ng thÃ¬ ngá»¯ nghÄ©a cá»§a nÃ³ cÃ³ thá»ƒ sáº½ khÃ¡c nhau. Trong word2vec, ta sáº½ Ä‘i xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh MLP (Multi-layer Perceptron, hay nÃ³i cÃ¡ch khÃ¡c lÃ  Neural Network) chá»‰ gá»“m 1 hidden layer, vá»›i má»¥c Ä‘Ã­ch cÃ³ thá»ƒ lÃ :\nDá»±a vÃ o target word Ä‘á»ƒ dá»± Ä‘oÃ¡n context word Dá»±a vÃ o cÃ¡c context word Ä‘á»ƒ dá»± Ä‘oÃ¡n target word TÃ¹y vÃ o má»¥c Ä‘Ã­ch mÃ  ta sáº½ cÃ³ má»™t kiáº¿n trÃºc MLP khÃ¡c nhau. Vá»›i má»¥c Ä‘Ã­ch (1) thÃ¬ ta cÃ³ Skip-gram, má»¥c Ä‘Ã­ch (2) lÃ  CBoW (Continuous Bag of Word)\nQuay láº¡i vá»›i hai nháº­n xÃ©t Ä‘Ã£ má»Ÿ ra Ã½ tÆ°á»Ÿng cho word2vec thÃ¬ nháº­n xÃ©t thá»© nháº¥t Ä‘Æ°á»£c thá»ƒ hiá»‡n rÃµ hÆ¡n á»Ÿ trong Skip-gram vÃ  nháº­n xÃ©t thá»© hai thÃ¬ á»Ÿ trong CBoW ğŸ˜€ Äá»ƒ cÃ³ cÃ¡i nhÃ¬n tá»•ng quan vá» sá»± khÃ¡c biá»‡t giá»¯a Skip-gram vÃ  CBoW, ta cÃ³ hÃ¬nh áº£nh so sÃ¡nh nhÆ° bÃªn dÆ°á»›i:\nSá»± khÃ¡c biá»‡t giá»¯a CBoW vÃ  skip-gram trong má»™t cÃ¢u vá»›i target word lÃ  W(t), context word lÃ  W(t-2), W(t-1), W(t+1), W(t+2)\nNguá»“n: https://i.stack.imgur.com/ShJJX.png Skip-gram Skip-gram lÃ  cÃ¡ch xÃ¢y dá»±ng mÃ´ hÃ¬nh MLP theo hÆ°á»›ng dá»± Ä‘oÃ¡n context word dá»±a vÃ o target word. Vá» máº·t toÃ¡n há»c thÃ¬ ta sáº½ Ä‘i tÃ¬m xÃ¡c suáº¥t xáº£y ra cÃ¡c context word khi biáº¿t trÆ°á»›c target word.\nVÃ­ dá»¥, kho dá»¯ liá»‡u ta cÃ³ hai cÃ¢u lÃ  {â€œem áº¥y há»c toÃ¡n tá»‘tâ€, â€œem áº¥y há»c toÃ¡n giá»iâ€}. Vá»›i tá»« má»¥c tiÃªu â€œhá»câ€ vÃ  kÃ­ch thÆ°á»›c sliding window lÃ  $C=4$, ta sáº½ tÃ¬m xÃ¡c suáº¥t\n$$ P_0 = P(\\text{\u0026ldquo;em\u0026rdquo;}, \\text{\u0026ldquo;áº¥y\u0026rdquo;}, \\text{\u0026ldquo;toÃ¡n\u0026rdquo;}, \\text{\u0026ldquo;tá»‘t\u0026rdquo;}, \\text{\u0026ldquo;giá»i\u0026rdquo;} | \\text{\u0026ldquo;há»c\u0026rdquo;}) $$\nGiáº£ sá»­ cÃ¡c tá»« trÃªn lÃ  Ä‘á»™c láº­p vá»›i nhau, khi Ä‘Ã³\n$$ P_0 = P(\\text{\u0026ldquo;em\u0026rdquo;} | \\text{\u0026ldquo;há»c\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;áº¥y\u0026rdquo;} | \\text{\u0026ldquo;há»c\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;toÃ¡n\u0026rdquo;} | \\text{\u0026ldquo;há»c\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;tá»‘t\u0026rdquo;} | \\text{\u0026ldquo;há»c\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;giá»i\u0026rdquo;} | \\text{\u0026ldquo;há»c\u0026rdquo;}) $$\nTáº­p tá»« Ä‘iá»ƒn sáº½ cÃ³ 6 tá»« nÃªn input cá»§a mÃ´ hÃ¬nh MLP sáº½ lÃ  vector 6 chiá»u. Ta sá»­ dá»¥ng hidden layer vá»›i 300 neuron. XÃ©t hai cáº·p (context, target) láº§n lÆ°á»£t lÃ  (há»c, tá»‘t) vÃ  (há»c, giá»i). Khi Ä‘Ã³, mÃ´ hÃ¬nh skip-gram sáº½ cÃ³ dáº¡ng nhÆ° hÃ¬nh bÃªn dÆ°á»›i, vá»›i $\\bold{U}$ vÃ  $\\bold{V}$ láº§n lÆ°á»£t lÃ  ma tráº­n trá»ng sá»‘ giá»¯a layer input-hidden vÃ  hidden-output.\nTham kháº£o: ProtonX - Word2vec Ta tháº¥y ráº±ng, hai tá»« â€œtá»‘tâ€ vÃ  â€œgiá»iâ€ cÃ¹ng xuáº¥t hiá»‡n trong má»™t ngá»¯ cáº£nh, do Ä‘Ã³ chÃºng nÃªn cÃ³ quan há»‡ nÃ o Ä‘Ã³ vá» máº·t ngá»¯ nghÄ©a ğŸ˜€ LÆ°u Ã½.\nShape cá»§a $\\bold{U}$ cÃ³ thá»ƒ lÃ  (embedding_dim x vocab_size), tá»©c lÃ  $(300 \\times 6)$, hoáº·c lÃ  (vocab_size x embedding_dim), tá»©c lÃ  $(6 \\times 300)$. Vá»›i bÃ i viáº¿t nÃ y thÃ¬ mÃ¬nh sá»­ dá»¥ng (embedding_dim x vocab_size). TÆ°Æ¡ng tá»± nhÆ° vá»›i $\\bold{V}$. Khi láº§n Ä‘áº§u tÃ¬m hiá»ƒu vá» Skip-gram, mÃ¬nh cÃ³ má»™t tháº¯c máº¯c mÃ  mÃ¬nh nghÄ© lÃ  cÅ©ng ráº¥t nhiá»u ngÆ°á»i cÃ³ cÃ¹ng tháº¯c máº¯c nhÆ° tháº¿ ğŸ˜œ Trong hÃ¬nh trÃªn, ta tháº¥y ráº±ng mÃ´ hÃ¬nh cÃ¹ng nháº­n vÃ o má»™t input lÃ  one-hot vector cá»§a tá»« há»c, sá»­ dá»¥ng cÃ¹ng hai ma tráº­n trá»ng sá»‘ $\\bold{U}$ vÃ  $\\bold{V}$, vÃ¬ sao label láº¡i cÃ³ nhá»¯ng giÃ¡ trá»‹ khÃ¡c nhau?\nThá»±c cháº¥t, trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n skip-gram, ta sáº½ sá»­ dá»¥ng optimizer SGD (stochastic gradient descent), táº¡i má»—i thá»i Ä‘iá»ƒm thÃ¬ ta sáº½ chá»n ngáº«u nhiÃªn má»™t cáº·p (target, context) rá»“i tiáº¿n hÃ nh cáº­p nháº­t cÃ¡c ma tráº­n trá»ng sá»‘ má»™t chÃºt dá»±a theo cáº·p Ä‘Æ°á»£c chá»n. Trong Ä‘Ã³, phÃ©p chá»n nÃ y khÃ´ng nÃªn tuÃ¢n theo phÃ¢n phá»‘i Ä‘á»u mÃ  nÃªn cÃ³ heuristic má»™t chÃºt, vÃ­ dá»¥ nhÆ° cáº·p nÃ o xuáº¥t hiá»‡n cÃ ng nhiá»u thÃ¬ cÃ³ xÃ¡c suáº¥t Ä‘Æ°á»£c chá»n cÃ ng cao. Do Ä‘Ã³, cáº·p (target, context) nÃ o xuáº¥t hiá»‡n cÃ ng nhiá»u thÃ¬ mÃ´ hÃ¬nh cÃ ng â€œhá»câ€ Ä‘Æ°á»£c nhiá»u thá»© vá» nÃ³. Káº¿t quáº£ lÃ  sau quÃ¡ trÃ¬nh huáº¥n luyá»‡n, tá»« dá»± Ä‘oÃ¡n cá»§a má»™t context word sáº½ lÃ  má»™t phÃ¢n bá»‘ xÃ¡c suáº¥t â€œÄ‘á»§ gáº§nâ€ vá»›i táº¥t cáº£ cÃ¡c target word cá»§a nÃ³ trong kho dá»¯ liá»‡u. NgoÃ i ra, trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n skip-gram thÃ¬ chÃºng ta thÆ°á»ng sá»­ dá»¥ng loss function lÃ  cross-entropy (cÃ³ dÃ¹ng Ä‘áº¿n $\\text{softmax}$ activation function).\nCBoW (Continuous Bag of Words) CÃ³ thá»ƒ nÃ³i CBoW lÃ  má»™t phiÃªn báº£n ngÆ°á»£c láº¡i cá»§a Skip-gram. Trong CBoW, ta sáº½ sá»­ dá»¥ng cÃ¡c context word Ä‘á»ƒ dá»± Ä‘oÃ¡n target word. Vá» máº·t toÃ¡n há»c thÃ¬ ta sáº½ Ä‘i tÃ¬m xÃ¡c suáº¥t xáº£y ra target word khi biáº¿t trÆ°á»›c cÃ¡c context word.\nVá»›i cÃ¹ng vÃ­ dá»¥ nhÆ° pháº§n vá» Skip-gram, ta sáº½ tÃ¬m xÃ¡c suáº¥t $$P_0 = P(\\text{\u0026ldquo;há»c\u0026rdquo;} | \\text{\u0026ldquo;em\u0026rdquo;}, \\text{\u0026ldquo;áº¥y\u0026rdquo;}, \\text{\u0026ldquo;toÃ¡n\u0026rdquo;}, \\text{\u0026ldquo;tá»‘t\u0026rdquo;}, \\text{\u0026ldquo;giá»i\u0026rdquo;})$$\nLÃºc nÃ y, ta thÆ°á»ng tÃ­nh má»™t â€œtá»« trung bÃ¬nhâ€ cá»§a cÃ¡c context word (embedding vector trung bÃ¬nh), sau Ä‘Ã³ thay vÃ o biá»ƒu thá»©c trÃªn $$P_0 = P(\\text{\u0026ldquo;há»c\u0026rdquo;} | \\overline{word})$$\nÄá»ƒ dá»… minh há»a, ta xÃ©t hai context words â€œemâ€, â€œtoÃ¡nâ€ cá»§a target word â€œhá»câ€. Khi Ä‘Ã³, mÃ´ hÃ¬nh CBoW sáº½ cÃ³ dáº¡ng nhÆ° hÃ¬nh bÃªn dÆ°á»›i, vá»›i $\\bold{V}$ vÃ  $\\bold{U}$ lÃ  ma tráº­n trá»ng sá»‘ giá»¯a layer input-hidden vÃ  hidden-output. LÆ°u Ã½ ráº±ng, sau khi tÃ­nh ra output táº¡i hidden layer cá»§a cÃ¡c context word thÃ¬ ta sáº½ thá»±c hiá»‡n thao tÃ¡c tÃ­nh trung bÃ¬nh Ä‘á»ƒ cÃ³ má»™t vector trung bÃ¬nh, sau Ä‘Ã³ má»›i tÃ­nh ra predicted word. Tham kháº£o: ProtonX - Word2vec Trong huáº¥n luyá»‡n mÃ´ hÃ¬nh, ta cÅ©ng loss function lÃ  cross-entropy vÃ  optimizer SGD giá»‘ng nhÆ° Skip-gram. Trong Ä‘Ã³, á»Ÿ má»—i bÆ°á»›c cá»§a SGD thÃ¬ thá»© ta chá»n ngáº«u nhiÃªn lÃ  má»™t cÃ¢u ngáº¯n trong kho dá»¯ liá»‡u.\nTrÃ­ch xuáº¥t embedding matrix Sau khi huáº¥n luyá»‡n xong cÃ¡c mÃ´ hÃ¬nh nhÆ° Skip-gram vÃ  CBoW thÃ¬ ta thu Ä‘Æ°á»£c cÃ¡c ma tráº­n trá»ng sá»‘ $\\bold{U}$ vÃ  $\\bold{V}$. Náº¿u cÃ¡c báº¡n Ä‘á»ƒ Ã½ thÃ¬ trong má»—i mÃ´ hÃ¬nh, thá»© tá»± mÃ¬nh sá»­ dá»¥ng kÃ­ hiá»‡u $\\bold{U}$ vÃ  $\\bold{V}$ lÃ  khÃ¡c nhau.\nTrong Skip-gram, $\\bold{U}$ lÃ  ma tráº­n trá»ng sá»‘ ná»‘i giá»¯a input-hidden, liÃªn quan Ä‘áº¿n target word vÃ  $\\bold{V}$ thÃ¬ ná»‘i giá»¯a hidden-output vÃ  nÃ³ liÃªn quan Ä‘áº¿n context word. CÅ©ng vÃ¬ sá»± â€œliÃªn quanâ€ giá»¯a ma tráº­n trá»ng sá»‘ lÃ  cÃ¡c tá»«, trong CBoW thÃ¬ $\\bold{V}$ Ä‘Æ°á»£c Ä‘Æ°a lÃªn thÃ nh ma tráº­n trá»ng sá»‘ giá»¯a input-hidden, tÆ°Æ¡ng tá»± cho $\\bold{U}$. Ta biáº¿t ráº±ng ma tráº­n trá»ng sá»‘ ná»‘i giá»¯a layer input-hidden cÃ³ nhiá»‡m vá»¥ chÃ­nh lÃ  â€œhá»câ€ cÃ¡c Ä‘áº·c trÆ°ng cá»§a tá»« input, cÃ²n ma tráº­n trá»ng sá»‘ ná»‘i giá»¯a hidden-output thÃ¬ cÃ³ nhiá»‡m vá»¥ chÃ­nh lÃ  dá»± Ä‘oÃ¡n tá»«. NhÆ° váº­y, rÃµ rÃ ng lÃ  ta nÃªn dÃ¹ng ma tráº­n trá»ng sá»‘ Ä‘áº§u tiÃªn Ä‘á»ƒ lÃ m embedding matrix. Tuy nhiÃªn, cÃ³ sá»± khÃ¡c biá»‡t nÃ o giá»¯a cÃ¡c ma tráº­n thu Ä‘Æ°á»£c tá»« Skip-gram vÃ  CBoW?\nÄá»‘i vá»›i Skip-gram, $\\bold{U}$ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n target word. Khi Ä‘Ã³, embedding vector cá»§a má»—i tá»« tÃ­nh Ä‘Æ°á»£c dá»±a vÃ o $\\bold{U}$ sáº½ mang nhiá»u thÃ´ng tin vá» máº·t ngá»¯ nghÄ©a hÆ¡n. NgÆ°á»£c láº¡i, trong CBoW, $\\bold{V}$ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¡c context word nÃªn embedding vector cá»§a má»—i tá»« tÃ­nh Ä‘Æ°á»£c sáº½ nghiÃªng vá» phÃ­a ngá»¯ phÃ¡p. VÃ­ dá»¥, vá»›i tá»« â€œcatâ€, ta tÃ­nh embedding vector cá»§a nÃ³ theo cáº£ hai ma tráº­n $\\bold{U}$ trong Skip-gram vÃ  $\\bold{V}$ trong CBoW. Tiáº¿p Ä‘áº¿n thÃ¬ ta sáº½ tÃ¬m tá»« tÆ°Æ¡ng Ä‘á»“ng vá»›i â€œcatâ€ nháº¥t . Khi Ä‘Ã³, sá»­ dá»¥ng $\\bold{U}$ thÃ¬ káº¿t quáº£ cÃ³ thá»ƒ lÃ  â€œdogâ€, cÃ²n dÃ¹ng $\\bold{V}$ thÃ¬ ráº¥t cÃ³ thá»ƒ sáº½ lÃ  â€œcatsâ€ ğŸ˜€.\nNháº­n xÃ©t Hai hÆ°á»›ng tiáº¿p cáº­n Skip-gram vÃ  CBoW Ä‘á»u cÃ³ nhá»¯ng Ä‘iá»ƒm máº¡nh vÃ  yáº¿u cá»§a riÃªng nÃ³ (vÃ­ dá»¥ nhÆ° vá» máº·t ngá»¯ nghÄ©a vÃ  ngá»¯ phÃ¡p) nhÆ°ng nhÃ¬n chung thÃ¬ chÃºng Ä‘á»u cho ta nhá»¯ng embedding vector Ä‘á»§ tá»‘t Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c bÃ i toÃ¡n khÃ¡c.\nTuy nhiÃªn, ta cÃ³ má»™t Ä‘iá»ƒm yáº¿u khÃ¡ quan trá»ng trong viá»‡c huáº¥n luyá»‡n Skip-gram vÃ  CBoW. Vá» hÃ m loss function thÃ¬ mÃ¬nh Ä‘Ã£ Ä‘á» cáº­p lÃ  chÃºng Ä‘á»u sá»­ dá»¥ng cross-entropy vÃ  cáº§n Ä‘áº¿n $\\text{softmax}$ activation function. Trong trÆ°á»ng há»£p tá»« Ä‘iá»ƒn cÃ³ ráº¥t nhiá»u tá»« thÃ¬ thao tÃ¡c tÃ­nh $\\text{softmax}$ nÃ y sáº½ ráº¥t ráº¥t lÃ¢u ğŸ˜€\nLÄá»ƒ kháº¯c phá»¥c, ta cÃ³ má»™t sá»‘ cÃ¡ch nhÆ° lÃ  sá»­ dá»¥ng Hierarchy Softmax hoáº·c lÃ  Negative Sampling. Trong bÃ i viáº¿t nÃ y thÃ¬ mÃ¬nh sáº½ khÃ´ng Ä‘á» cáº­p Ä‘áº¿n chÃºng ğŸ˜œ\nVáº¥n Ä‘á» thiÃªn vá»‹ trong Word Embdding Nghe ráº¥t lÃ  áº£o, nhÆ°ng mÃ  nÃ³ cÃ³ tá»“n táº¡i ğŸ˜… Äiá»u nÃ y xáº£y ra pháº§n lá»›n lÃ  do kho dá»¯ liá»‡u vÄƒn báº£n mÃ  chÃºng ta sá»­ dá»¥ng Ä‘á»ƒ xÃ¢y dá»±ng embedding matrix.\nÄá»ƒ láº¥y vÃ­ dá»¥, mÃ¬nh sáº½ xÃ©t trÆ°á»ng há»£p liÃªn quan Ä‘áº¿n giá»›i tÃ­nh. CÃ¹ng quay láº¡i bÃ i toÃ¡n Analogy Reasoning trong pháº§n 2.2:\nVá»›i 3 tá»« â€œmanâ€, â€œwomanâ€ vÃ  â€œkingâ€ thÃ¬ ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c tá»« â€œqueenâ€ sao cho quan há»‡ giá»¯a â€œkingâ€ vÃ  â€œqueenâ€ sáº½ tÆ°Æ¡ng tá»± nhÆ° giá»¯a â€œman\u0026quot; vÃ  â€œwomanâ€. BÃ¢y giá» giáº£ sá»­ ta cÃ³ â€œmanâ€, â€œdoctorâ€, â€œwomanâ€ vÃ  cáº§n tÃ¬m tá»« X sao cho quan há»‡ giá»¯a â€œwomanâ€ vÃ  X tÆ°Æ¡ng tá»± nhÆ° giá»¯a â€œman\u0026quot; vÃ  â€œdoctorâ€. Náº¿u kho dá»¯ liá»‡u liÃªn quan pháº§n lá»›n Ä‘áº¿n viá»‡c ngÆ°á»i Ä‘Ã n Ã´ng lÃ  trá»¥ cá»™t trong gia Ä‘Ã¬nh (xÃ£ há»™i thá»i xa xÆ°a) thÃ¬ káº¿t quáº£ X ráº¥t cÃ³ thá»ƒ lÃ  â€œbabysitterâ€ (ngÆ°á»i trÃ´ng tráº»). NhÆ° váº­y, Ä‘Ã£ cÃ³ váº¥n Ä‘á» thiÃªn vá»‹ cho nam giá»›i. Äá»ƒ háº¡n cháº¿ váº¥n Ä‘á» nÃ y, ta cÃ³ má»™t sá»‘ phÆ°Æ¡ng phÃ¡p nhÆ° Hard Debiasing, Soft Debiasing. MÃ¬nh sáº½ khÃ´ng Ä‘á» cáº­p Ä‘áº¿n nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y á»Ÿ Ä‘Ã¢y, cÃ¡c báº¡n cÃ³ thá»ƒ tá»± tÃ¬m Ä‘á»c nhÃ© ğŸ˜€ NÃ³ pháº§n lá»›n lÃ  liÃªn quan Ä‘áº¿n cÃ¡c phÃ©p biáº¿n Ä‘á»•i toÃ¡n há»c.\nDÆ°á»›i Ä‘Ã¢y lÃ  minh há»a cho váº¥n Ä‘á» thiÃªn vá»‹ mÃ  mÃ¬nh Ä‘Ã£ láº¥y vÃ­ dá»¥ á»Ÿ trÃªn Ä‘á»ƒ cho cÃ¡c báº¡n dá»… hÃ¬nh dung:\nÄáº§u tiÃªn, hÆ°á»›ng thay Ä‘á»•i giá»›i tÃ­nh lÃ  tá»« trÃ¡i qua pháº£i. VÃ¬ váº¥n Ä‘á» thiÃªn vá»‹ Ä‘ang xáº£y ra liÃªn quan Ä‘áº¿n giá»›i tÃ­nh nÃªn ta gá»i Ä‘Ã¢y lÃ  bias direction. TrÆ°á»›c khi Ä‘iá»u chá»‰nh, ta tháº¥y embedding vector cá»§a doctor nghiÃªng vá» phÃ­a bÃªn nam giá»›i hÆ¡n, tÆ°Æ¡ng tá»± nhÆ° babysitter. Sau khi háº¡n cháº¿ váº¥n Ä‘á» thiÃªn vá»‹ Ä‘iá»u chá»‰nh, cÃ¡c embedding vector cá»§a doctor vÃ  baby sitter nÃªn nghiÃªng vá» phÃ­a â€œcÃ´ng báº±ngâ€ hÆ¡n Ä‘á»‘i vá»›i hai giá»›i tÃ­nh, tá»©c lÃ  hÆ°á»›ng trá»±c giao vá»›i hÆ°á»›ng bias direction táº¡i vá»‹ trÃ­ trung bÃ¬nh. TrÆ°á»›c khi Ä‘iá»u chá»‰nh (embedding vector gá»‘c)\nSau khi thá»±c hiá»‡n debiasing\nNguá»“n: Vagdevik\nTÃ i liá»‡u tham kháº£o VÅ© Há»¯u Tiá»‡p, Machine Learning cho dá»¯ liá»‡u dáº¡ng báº£ng - Word2vec Kavita Gane, Word2Vec: A Comparison Between CBOW, SkipGram \u0026amp; SkipGramSI Dive into Deep Learning, Word2vec ","date":"2023-02-19T10:51:57+07:00","permalink":"https://htrvu.github.io/post/word-embedding/","title":"Word Embedding"},{"content":"ÄÃ¢y lÃ  bÃ i viáº¿t Ä‘áº§u tiÃªn cá»§a mÃ¬nh trong lÄ©nh vá»±c Natural Language Processing (NLP - Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn). NÃ³ sáº½ khÃ¡ dÃ i má»™t chÃºt, mong cÃ¡c báº¡n Ä‘á»c háº¿t nhÃ©! ğŸ˜€\nSÆ¡ lÆ°á»£c vá» Natural Language Processing BÃªn cáº¡nh Computer Vision (CV - Thá»‹ giÃ¡c mÃ¡y tÃ­nh) thÃ¬ Natural Language Processing (NLP - Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn) cÅ©ng lÃ  má»™t máº£ng ráº¥t quan trá»ng vÃ  Ä‘Æ°á»£c nghiÃªn cá»©u rá»™ng rÃ£i trong Deep Learning. CÃ¡c sáº£n pháº©m ná»•i tiáº¿ng trÃªn tháº¿ giá»›i liÃªn quan Ä‘áº¿n chá»¯ viáº¿t, giá»ng nÃ³i, Ã¢m thanh nhÆ° Google Dá»‹ch, Google Assistant, Siri, Alexa, ChatGPTâ€¦ Ä‘á»u lÃ  cÃ¡c thÃ nh quáº£ cá»§a viá»‡c Ã¡p dá»¥ng NLP vÃ o thá»±c táº¿.\nNguá»“n: Data Science Dojo Tuy NLP vÃ  CV lÃ  hai lÄ©nh vá»±c nghiÃªn cá»©u khÃ¡c nhau nhÆ°ng chÃºng cÃ³ nhá»¯ng má»‘i quan há»‡ ráº¥t Ä‘áº·c biá»‡t vÃ  thÃº vá»‹. Ta cÃ³ thá»ƒ Ä‘em Ã½ tÆ°á»Ÿng cá»§a CV qua NLP, vÃ­ dá»¥ nhÆ° sá»­ dá»¥ng phÃ©p toÃ¡n convolution trong xá»­ lÃ½ tÃ­nh toÃ¡n vá»›i vÄƒn báº£n, vÃ  ngÆ°á»£c láº¡i lÃ  Ä‘em Ã½ tÆ°á»Ÿng cá»§a NLP qua CV, mÃ  Ä‘áº·c biá»‡t ná»•i tiáº¿ng gáº§n Ä‘Ã¢y lÃ  sá»­ dá»¥ng Attention, Transformer trong CV. Viá»‡c káº¿t há»£p NLP vÃ  CV Ä‘Ã£ táº¡o ra nhiá»u káº¿t quáº£ ráº¥t ná»•i báº­t trong cÃ¡c á»©ng dá»¥ng nhÆ° Image Captioning, Text-to-Image.\nNáº¿u mÃ  ká»ƒ tÃªn ra thÃ¬ ta cÃ³ thá»ƒ nháº¯c Ä‘áº¿n ngay mÃ´ hÃ¬nh ráº¥t áº£o diá»‡u lÃ  Stable Diffusion ğŸ˜€ Ná»n mÃ³ng cá»§a NLP báº¯t nguá»“n tá»« nhá»¯ng mÃ´ hÃ¬nh toÃ¡n há»c vÃ  xÃ¡c suáº¥t, Ä‘áº·c biá»‡t lÃ  mÃ´ hÃ¬nh Markov. Khi Deep Learning báº¯t Ä‘áº§u phÃ¡t triá»ƒn máº¡nh máº½, ta Ä‘Ã£ cÃ³ thÃªm nhá»¯ng mÃ´ hÃ¬nh khÃ¡c vá»›i Ä‘á»™ hiá»‡u quáº£ ráº¥t tuyá»‡t vá»i nhÆ° Recurrent Neural Network, Sequence to Sequence, Attention Mechanism vÃ  Transformer. Trong Ä‘Ã³, Recurrent Neural Network, hay lÃ  RNN, lÃ  sá»± khá»Ÿi Ä‘áº§u thÃº vá»‹ cá»§a Deep Learning trong NLP. MÃ´ hÃ¬nh RNN cÅ©ng lÃ  sáº½ chá»§ Ä‘á» cá»§a bÃ i viáº¿t nÃ y.\nSequence data, sequence models Sequence data Ta cÃ³ thá»ƒ hiá»ƒu sequence data (dá»¯ liá»‡u dáº¡ng chuá»—i) lÃ  dáº¡ng dá»¯ liá»‡u mÃ  cÃ¡c giÃ¡ trá»‹ trong Ä‘Ã³ Ä‘Æ°á»£c sáº¯p xáº¿p theo má»™t trÃ¬nh tá»± khÃ´ng gian/thá»i gian nÃ o Ä‘Ã³ vÃ  chÃºng cÃ³ nhá»¯ng má»‘i liÃªn há»‡ vá»›i nhau. Ta khÃ³ cÃ³ thá»ƒ chá»‰ dá»±a vÃ o má»™t giÃ¡ trá»‹ cá»¥ thá»ƒ Ä‘á»ƒ biáº¿t Ä‘Æ°á»£c dá»¯ liá»‡u cÃ³ Ã½ nghÄ©a lÃ  gÃ¬ mÃ  pháº£i sá»­ dá»¥ng quan há»‡ giá»¯a cÃ¡c giÃ¡ trá»‹ vá»›i nhau. VÃ­ dá»¥:\nVÄƒn báº£n: ÄÃ¢y lÃ  dáº¡ng sequence data ráº¥t phá»• biáº¿n. CÃ¡c tá»« trong cÃ¢u táº¥t nhiÃªn lÃ  Ä‘Æ°á»£c sáº¯p xáº¿p theo má»™t trÃ¬nh tá»± nháº¥t Ä‘á»‹nh Ä‘á»ƒ táº¡o ra Ä‘Æ°á»£c má»™t cÃ¢u cÃ³ nghÄ©a Ã‚m thanh: Má»™t Ä‘oáº¡n ghi Ã¢m giá»ng nÃ³i, má»™t báº£n nháº¡c Video: CÃ¡c frame (áº£nh) cá»§a video theo cÃ¡c thá»i Ä‘iá»ƒm liÃªn tiáº¿p nhau Sinh há»c: TrÃ¬nh tá»± cá»§a má»™t Ä‘oáº¡n gen, dÃ£y protein,â€¦ Time series: CÃ¡c dá»¯ liá»‡u thu tháº­p theo thá»i gian nhÆ° thá»‹ trÆ°á»ng chá»©ng khoÃ¡n VÄƒn báº£n\nÃ‚m thanh\nVideo\nTime series\nLÆ°u Ã½:\nTÃ¹y theo dáº¡ng dá»¯ liá»‡u mÃ  ta sáº½ cÃ³ cÃ¡ch â€œsá»‘ hÃ³a\u0026quot; chÃºng sao cho cÃ¡c mÃ´ hÃ¬nh cÃ³ thá»ƒ tiáº¿n hÃ nh â€œhá»câ€ Ä‘Æ°á»£c. VÃ¬ sao cáº§n pháº£i lÃ m váº­y? VÃ¬ mÃ¡y tÃ­nh chá»‰ hiá»ƒu Ä‘Æ°á»£c nhá»¯ng con sá»‘, mÃ  cá»¥ thá»ƒ hÆ¡n lÃ  chá»‰ hiá»ƒu 0 vÃ  1 ğŸ˜€\nTrong bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ táº¡m thá»i chÆ°a Ä‘á» cáº­p Ä‘áº¿n Ä‘iá»u nÃ y. Äá»‘i vá»›i dá»¯ liá»‡u dáº¡ng vÄƒn báº£n, cÃ¡c báº¡n cÃ³ thá»ƒ xem bÃ i viáº¿t tiáº¿p theo vá» Word Embeddings nhÃ©.\nSequence models KhÃ¡c vá»›i cÃ¡c dá»¯ liá»‡u dáº¡ng hÃ¬nh áº£nh mÃ  ta thÆ°á»ng tháº¥y trong Computer Vision, Natural Language Processing sáº½ táº­p trung vÃ o viá»‡c xá»­ lÃ½ cÃ¡c dá»¯ liá»‡u dáº¡ng chuá»—i. Do Ä‘Ã³, cÃ¡c mÃ´ hÃ¬nh trong NLP thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  sequence model.\nÄá»ƒ cÃ³ sá»± phÃ¢n biá»‡t rÃµ hÆ¡n giá»¯a sequence model vÃ  cÃ¡c model trong CV, ta thÆ°á»ng xÃ©t Ä‘áº¿n input vÃ  cÃ¡c thá»©c tÃ­nh toÃ¡n cá»§a chÃºng.\nVá»›i model trong CV, input cá»§a ta sáº½ lÃ  má»™t áº£nh xÃ¡m hoáº·c lÃ  RGB (ma tráº­n nhiá»u chiá»u). Trong quÃ¡ trÃ¬nh tÃ­nh toÃ¡n, ta cÃ³ thá»ƒ thá»±c hiá»‡n tÃ­nh toÃ¡n song song trÃªn cÃ¡c giÃ¡ trá»‹ Ä‘áº§u vÃ o. Trong khi Ä‘Ã³, sequence model sáº½ nháº­n vÃ o input lÃ  dá»¯ liá»‡u á»Ÿ cÃ¡c giai Ä‘oáº¡n khÃ¡c nhau, má»—i giai Ä‘oáº¡n thÃ¬ ta sáº½ cÃ³ má»™t vector hay má»™t ma tráº­n nhiá»u chiá»u. Khi tÃ­nh toÃ¡n, ta sáº½ tÃ­nh toÃ¡n tuáº§n tá»± tá»«ng giai Ä‘oáº¡n má»™t. VÃ­ dá»¥, vá»›i input lÃ  má»™t cÃ¢u vÄƒn báº£n thÃ¬ tá»«ng giai Ä‘oáº¡n sáº½ á»©ng vá»›i tá»«ng tá»«, má»—i tá»« cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi má»™t sá»‘ hoáº·c má»™t vector. Minh há»a sequence model Nguá»“n: Jeddy92 Sequence models Ä‘Æ°á»£c sá»­ dá»¥ng cho nhiá»u bÃ i toÃ¡n phá»• biáº¿n trong thá»±c táº¿ nhÆ° sau:\nNguá»“n: deeplearning.ai Recurrent Neural Network (RNN) Äá»ƒ cho dá»… diá»…n Ä‘áº¡t, ta xÃ©t má»™t bÃ i toÃ¡n trong NLP vá»›i input lÃ  má»™t cÃ¢u cÃ³ Ä‘á»™ dÃ i $T_x$, tá»« á»©ng vá»›i vá»‹ trÃ­ thá»© $i$ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng vector $x^{\u0026lt; i \u0026gt;}$ cÃ³ $D$ chiá»u, output lÃ  má»™t cÃ¢u cÃ³ Ä‘á»™ dÃ i $T_y$ vÃ  nhá»¯ng tá»« tÆ°Æ¡ng á»©ng Ä‘Æ°á»£c biá»ƒu diá»…n lÃ  $y^{\u0026lt; i \u0026gt;}$ (tÆ°Æ¡ng tá»± nhÆ° $x^{\u0026lt; i \u0026gt;}$).\nThÃ´ng thÆ°á»ng thÃ¬ ta sáº½ giáº£ sá»­ luÃ´n $T_x = T_y$ Ä‘á»ƒ bÃ i toÃ¡n Ä‘Æ¡n giáº£n hÆ¡n má»™t chÃºt. Äá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c thÃ¬ chá»‰ Ä‘Æ¡n giáº£n lÃ  padding/truncate Ä‘á»ƒ chÃºng báº±ng nhau thÃ´i ğŸ˜œ Náº¿u báº¡n Ä‘ang tháº¯c máº¯c lÃ  cÃ³ bÃ i toÃ¡n nÃ o dáº¡ng nhÆ° nÃ y thÃ¬ hÃ£y nghÄ© Ä‘áº¿n Name Entity Recognition (phÃ¢n loáº¡i cÃ¡c thÃ nh pháº§n trong cÃ¢u thÃ nh cÃ¡c nhÃ³m nhÆ° tÃªn ngÆ°á»i, Ä‘á»‹a Ä‘iá»ƒm, thá»i gian, sá»‘ lÆ°á»£ng,\u0026hellip;) Háº¡n cháº¿ cá»§a mÃ´ hÃ¬nh Multi-layers Percentron Má»™t cÃ¡ch tá»± nhiÃªn, ta hoÃ n toÃ n cÃ³ thá»ƒ xÃ¢y dá»±ng má»™t mÃ´ hÃ¬nh MLP (Multi-layers Perceptron) vá»›i input layer cÃ³ $T_x \\times D$ neurons, output layer cÅ©ng cÃ³ $T_y \\times D$ neurons vÃ  á»Ÿ giá»¯a lÃ  cÃ¡c hidden layer.\nNháº±m má»¥c Ä‘Ã­ch minh há»a, mÃ¬nh sáº½ chá»‰ biá»ƒu diá»…n má»—i vector $x^{\u0026lt; i \u0026gt;}$ vÃ  $y^{\u0026lt; i \u0026gt;}$ lÃ  má»™t neuron. Khi Ä‘Ã³, kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh MLP sáº½ cÃ³ dáº¡ng nhÆ° sau:\nMinh há»a sá»­ dá»¥ng MLP cho bÃ i toÃ¡n Ä‘áº·t ra\nNguá»“n: deeplearning.ai Vá»›i MLP thÃ¬ ta Ä‘Ã£ thá»±c hiá»‡n tÃ­nh toÃ¡n song song, tá»©c lÃ  tÃ­nh luÃ´n trÃªn toÃ n bá»™ input vÃ  cho ra output. Tuy nhiÃªn, cÃ¡ch tiáº¿p nÃ y cÃ³ nhá»¯ng háº¡n cháº¿ khÃ¡ nghiÃªm trá»ng:\nKhÃ´ng pháº£i cÃ¢u input nÃ o cÅ©ng cÃ³ Ä‘á»™ dÃ i $T_x$ nhÆ° nhau, output cÅ©ng váº­y. Náº¿u cÃ¡c cÃ¢u nÃ y cÃ³ nhiá»u tá»« vÃ  má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi vector cÃ³ sá»‘ chiá»u lá»›n thÃ¬ mÃ´ hÃ¬nh sáº½ cÃ³ ráº¥t ráº¥t nhiá»u trá»ng sá»‘. MÃ´ hÃ¬nh khÃ´ng há»c Ä‘Æ°á»£c sá»± â€œchia sáº» Ä‘áº·c trÆ°ngâ€ giá»¯a cÃ¡c vá»‹ trÃ­ khÃ¡c nhau trong cÃ¢u. VÃ­ dá»¥, cá»¥m tá»« â€œtÃ´i Ä‘i há»câ€ xuáº¥t hiá»‡n trong cÃ¢u input thÃ¬ cho dÃ¹ nÃ³ báº¯t Ä‘áº§u á»Ÿ vá»‹ trÃ­ nÃ o Ä‘i ná»¯a, ta váº«n nÃªn há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng ráº¥t tÆ°Æ¡ng tá»± nhau. LÆ°u Ã½. ÄÃ¢y cÅ©ng lÃ  má»™t trong nhá»¯ng váº¥n Ä‘á» dáº«n Ä‘áº¿n mÃ´ hÃ¬nh CNN Ä‘Æ°á»£c Ã¡p dá»¥ng nhiá»u hÆ¡n trong lÄ©nh vá»±c Computer Vision chá»© khÃ´ng pháº£i lÃ  MLP. Ã tÆ°á»Ÿng cá»§a RNN VÃ¬ sequence data cÃ³ má»™t Ä‘áº·c Ä‘iá»ƒm lÃ  thá»© tá»± cá»§a cÃ¡c giÃ¡ trá»‹ trong input lÃ  ráº¥t quan trá»ng nÃªn ta thÆ°á»ng thiÃªn vá» hÆ°á»›ng láº§n lÆ°á»£t xá»­ lÃ½ trÃªn tá»«ng vá»‹ trÃ­ má»™t. Äá»“ng thá»i, khi Ä‘i Ä‘áº¿n cÃ¡c vá»‹ trÃ­ sau thÃ¬ ta cÅ©ng nÃªn cÃ³ thÃ´ng tin Ä‘Ã£ trÃ­ch xuáº¥t Ä‘Æ°á»£c tá»« cÃ¡c vá»‹ trÃ­ trÆ°á»›c. Ã tÆ°á»Ÿng cá»§a RNN chÃ­nh lÃ  nhÆ° váº­y.\nSÆ¡ lÆ°á»£c vá» cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a RNN Ä‘Æ°á»£c mÃ´ táº£ nhÆ° sau:\nVá»›i Ä‘iá»u kiá»‡n giáº£ sá»­ $T_x=T_y$, ta sáº½ thá»±c hiá»‡n tÃ­nh toÃ¡n $T_x$ láº§n, táº¡i thá»i Ä‘iá»ƒm (hay lÃ  vá»‹ trÃ­) thá»© $i$ thÃ¬ tá»« $x^{\u0026lt; i \u0026gt;}$ (input) ta tÃ­nh ra $y^{\u0026lt; i \u0026gt;}$ (output). HÆ¡n ná»¯a, Ä‘á»ƒ thá»±c hiá»‡n thao tÃ¡c lÆ°u giá»¯ cÃ¡c thÃ´ng tin cho Ä‘áº¿n thá»i Ä‘iá»ƒm hiá»‡n táº¡i vÃ  Ä‘Æ°a nÃ³ qua cÃ¡c thá»i Ä‘iá»ƒm sau, ta cÅ©ng cáº§n tÃ­nh thÃªm má»™t giÃ¡ trá»‹ lÃ  $h^{\u0026lt; i \u0026gt;}$ (giáº£ sá»­ luÃ´n $h^{\u0026lt;0\u0026gt;} = 0$). LÃºc nÃ y, ta gá»i $h^{\u0026lt; i \u0026gt;}$ lÃ  tráº¡ng thÃ¡i áº©n (hidden state). Vá»›i vai trÃ² cá»§a hidden state $h^{\u0026lt; i \u0026gt;}$ thÃ¬ ta tháº¥y ráº±ng output $y^{\u0026lt; i \u0026gt;}$ cháº¯c cháº¯n lÃ  nÃªn Ä‘Æ°á»£c tÃ­nh dá»±a trÃªn $h^{\u0026lt; i \u0026gt;}$. ğŸ˜ƒ NhÆ° váº­y, táº¡i má»—i thá»i Ä‘iá»ƒm,ta cáº§n tÃ­nh ra $h^{\u0026lt; i \u0026gt;}$ vÃ  $y^{\u0026lt; i \u0026gt;}$, dá»±a vÃ o 2 input lÃ  $x^{\u0026lt; i \u0026gt;}$ vÃ  thÃ´ng tin tá»« nhá»¯ng thá»i Ä‘iá»ƒm trÆ°á»›c Ä‘Æ°á»£c tá»•ng há»£p táº¡i $h^{\u0026lt;i - 1\u0026gt;}$. Minh há»a ban Ä‘áº§u cho RNN\nNguá»“n: deeplearning.ai NhÃ¬n vÃ o hÃ¬nh trÃªn, ta cÃ³ thá»ƒ suy ra ráº±ng nhá»¯ng trá»ng sá»‘ mÃ  RNN cáº§n há»c lÃ  ma tráº­n trá»ng sá»‘ cá»§a hidden layer á»Ÿ cÃ¡c thá»i Ä‘iá»ƒm.\nÄá»ƒ thuáº­n tiá»‡n cho cÃ¡c pháº§n sau, ta sáº½ quy Æ°á»›c kÃ­ hiá»‡u nhÆ° sau:\nVector input, output vÃ  hidden state táº¡i thá»i Ä‘iá»ƒm $t$ láº§n lÆ°á»£t lÃ  $\\bold{X}_t \\in \\mathbb{R}^d$, $\\bold{O}_t \\in \\mathbb{R}^o$ vÃ  $\\bold{H}_t \\in \\mathbb{R}^h$. Ma tráº­n trá»ng sá»‘ cho phÃ©p tÃ­nh liÃªn quan giá»¯a $\\bold{X}_t$ vÃ  $\\bold{H}_t$ lÃ  $\\bold{W}^{t}_{ xh } \\in \\mathbb{R}^{ h \\times d }$. TÆ°Æ¡ng tá»± nhÆ° trÃªn, ta cÃ³ $\\bold{W}^{t}_{hh} \\in \\mathbb{R}^{h \\times h}$ vÃ  $\\bold{W}^{t}_{ho} \\in \\mathbb{R}^{o \\times h}$. Khi Ä‘Ã³, ta cÃ³ thá»ƒ biá»ƒu diá»…n RNN nhÆ° sau:\nNguá»“n: Dive into DL ThÃ´ng thÆ°á»ng, ta gá»i pháº§n xá»­ lÃ½ tÃ­nh toÃ¡n tá»« $\\bold{X}_t$, $\\bold{H}_{t-1}$ ra $\\bold{H}_t$ vÃ  $\\bold{O}_t$ á»Ÿ cÃ¡c thá»i Ä‘iá»ƒm (Ã´ vuÃ´ng Ä‘Æ°á»£c váº½ á»Ÿ hÃ¬nh trÃªn) lÃ  recurrent cell.\nSá»± chia sáº» trá»ng sá»‘ giá»¯a cÃ¡c thá»i Ä‘iá»ƒm Qua mÃ´ táº£ vá» cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a RNN á»Ÿ pháº§n trÆ°á»›c, ta tháº¥y ráº±ng náº¿u á»Ÿ má»—i thá»i Ä‘iá»ƒm mÃ  ta cáº§n dÃ¹ng má»™t bá»™ trá»ng sá»‘ khÃ¡c nhau thÃ¬ lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh RNN sáº½ lá»›n khÃ´ng kÃ©m gÃ¬ MLP á»Ÿ pháº§n 3.1 ğŸ˜€\nTrong RNN, giá»¯a cÃ¡c thá»i Ä‘iá»ƒm sáº½ cÃ³ sá»± chia sáº» trá»ng sá»‘, tá»©c lÃ  má»i thá»i Ä‘iá»ƒm Ä‘á»u dÃ¹ng cÃ¹ng má»™t bá»™ trá»ng sá»‘ $(\\bold{W}_{xh}, \\bold{W}_{hh}, \\bold{W}_{ho})$ Ä‘á»ƒ tÃ­nh toÃ¡n $\\bold{O}_t$ vÃ  $\\bold{H}_t$. Lá»£i Ã­ch cá»§a viá»‡c chia sáº» trá»ng sá»‘ bao gá»“m:\nSá»‘ lÆ°á»£ng trá»ng sá»‘ trong RNN sáº½ giáº£m Ä‘i ráº¥t nhiá»u láº§n so vá»›i MLP Ta cÅ©ng cÃ³ thá»ƒ kháº¯c phá»¥c Ä‘Æ°á»£c nhÆ°á»£c Ä‘iá»ƒm cá»§a MLP trong viá»‡c â€œchia sáº» Ä‘áº·c trÆ°ngâ€ giá»¯a cÃ¡c vá»‹ trÃ­ khÃ¡c nhau trong cÃ¢u. Khi Ä‘Ã³, tá»« hÃ¬nh á»Ÿ pháº§n 3.2, sau khi chÃº thÃ­ch vá»‹ trÃ­ cÃ¡c ma tráº­n trá»ng sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng thÃ¬ ta cÃ³ hÃ¬nh sau:\nThay vÃ¬ pháº£i biá»ƒu diá»…n Ä‘á»§ cÃ¡c thá»i Ä‘iá»ƒm, ta cÃ³ thá»ƒ viáº¿t gá»n láº¡i RNN nhÆ° hÃ¬nh bÃªn dÆ°á»›i:\nBiá»ƒu diá»…n gá»n hÆ¡n cá»§a Recurrent Neural Network QuÃ¡ trÃ¬nh feed-forward Äáº§u tiÃªn, ta sáº½ xem hidden state ban Ä‘áº§u lÃ  $\\bold{H}_0 = \\bold{0}$. Äá»ƒ Ä‘Æ¡n giáº£n, ta sáº½ bá» qua cÃ¡c giÃ¡ trá»‹ bias á»©ng vá»›i $\\bold{H}_t$ vÃ  $\\bold{O}_t$. Khi Ä‘Ã³, quÃ¡ trÃ¬nh feed-forward táº¡i thá»i Ä‘iá»ƒm $t \u0026gt; 0$ diá»…n ra nhÆ° sau:\n$$\\begin{equation} \\bold{H}_t = \\phi_h (\\bold{W}_{xh} \\bold{X}_t + \\bold{W}_{hh} \\bold{H}_{t-1}) \\end{equation}$$ $$\\begin{equation} \\bold{O}_t = \\phi_o (\\bold{W}_{ho} \\bold{H}_t) \\end{equation}$$\n, vá»›i $\\phi_h$ vÃ  $\\phi_o$ lÃ  cÃ¡c activation function. ThÃ´ng thÆ°á»ng, $\\phi_h$ lÃ  $ReLU$ hoáº·c $\\tanh$ vÃ  $\\phi_o$ thÆ°á»ng lÃ  $\\text{softmax}$.\nNhÆ° váº­y, tá»« $\\bold{X}_{t}$ vÃ  $\\bold{H}_{t-1}$ ta sáº½ tÃ­nh Ä‘Æ°á»£c $\\bold{H}_{t}$, vÃ  tá»« $\\bold{H}_{t}$ thÃ¬ ta sáº½ tÃ­nh Ä‘Æ°á»£c $\\bold{O}_{t}$.\nNháº­n xÃ©t.\nTa cÃ³ thá»ƒ nháº­n tháº¥y ráº¥t rÃµ sá»± khÃ¡c biá»‡t giá»¯a feed-forward trong RNN so vá»›i MLP. Vá»›i MLP thÃ¬ nÃ³ chá»‰ cáº§n hai phÃ©p toÃ¡n (xem nhÆ° pháº§n hidden layers chá»‰ cÃ³ 1 layer) lÃ  cÃ³ luÃ´n káº¿t quáº£ cuá»‘i cÃ¹ng:\n$$\\begin{equation*} \\bold{H} = \\phi_h (\\bold{X} \\bold{W}_{xh}^\\top ) \\end{equation*}$$ $$\\begin{equation*} \\bold{O} = \\phi_o (\\bold{H} \\bold{W}^\\top _{ho} ) \\end{equation*}$$\n, vá»›i $\\bold{X} \\in \\mathbb{R}^{n \\times d}$ vÃ  $\\bold{H} \\in \\mathbb{R}^{n \\times h}$ lÃ  cÃ¡c ma tráº­n á»©ng vá»›i toÃ n bá»™ giÃ¡ trá»‹ input vÃ  hidden state (trong RNN thÃ¬ $\\bold{X}_t$ vÃ  $\\bold{H}_t$ lÃ  cÃ¡c vector).\nBack-propagation Through Time (BPTT) Khi mÃ  feed-forward trong RNN diá»…n ra khÃ¡c vá»›i MLP thÃ¬ táº¥t nhiÃªn lÃ  back-propagation cÅ©ng khÃ¡c ğŸ˜€ Thuáº­t toÃ¡n back-propagation trong sequence model Ä‘Æ°á»£c gá»i lÃ  Back-propagation Through Time (BPTT).\nÄáº§u tiÃªn, ta sáº½ Ä‘á» cáº­p Ä‘áº¿n cost function. Giáº£ sá»­ output cá»§a RNN táº¡i thá»i Ä‘iá»ƒm $t$ lÃ  $\\bold{O}_t$ vÃ  label lÃ  $\\bold{Y}_t$. KÃ­ hiá»‡u $l(\\bold{O}_t, \\bold{Y}_t)$ lÃ  loss táº¡i thá»i Ä‘iá»ƒm $t$. Khi Ä‘Ã³ cost function cá»§a ta lÃ \n$$\\begin{equation} L= \\sum_{i=1}^{T} l(\\bold{O}_t, \\bold{Y}_t) \\end{equation}$$\nThá»±c ra lÃ  ta cÃ³ chia $L$ cho $T$ ná»¯a nhÆ°ng Ä‘á»ƒ cho gá»n thÃ¬ thÃ´i bá» qua ğŸ˜œ Nhá»¯ng gÃ¬ ta cáº§n thá»±c hiá»‡n trong quÃ¡ trÃ¬nh BPTT lÃ  tÃ­nh Ä‘áº¡o hÃ m cá»§a $L$ theo cÃ¡c ma tráº­n trá»ng sá»‘ $\\bold{W}_{xh}$, $\\bold{W}_{hh}$ vÃ  $\\bold{W}_{ho}$. Äiá»u Ä‘áº·c biá»‡t á»Ÿ Ä‘Ã¢y lÃ  trong quÃ¡ trÃ¬nh feed-forward thÃ¬ $\\bold{H}_{t-1}$ sáº½ láº¡i Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tÃ­nh $\\bold{H}_t$ chá»© nÃ³ khÃ´ng Ä‘i má»™t â€œmáº¡ch\u0026quot; tá»« $\\bold{X}$ Ä‘áº¿n $\\bold{H}$ rá»“i tá»« $\\bold{H}$ Ä‘áº¿n $\\bold{O}$ nhÆ° trong feed-forward cá»§a MLP thÃ´ng thÆ°á»ng.\nÄá»ƒ cÃ´ng thá»©c Ä‘Æ°á»£c gá»n nháº¹ hÆ¡n, ta sáº½ giáº£ sá»­ luÃ´n cÃ¡c activation function $\\phi_h$ vÃ  $\\phi_o$ lÃ  hÃ m Ä‘á»“ng nháº¥t, tá»©c lÃ \n$$\\phi_h(\\bold{x}) = \\phi_o(\\bold{x}) = \\bold{x}$$\nQuÃ¡ trÃ¬nh BPPT diá»…n ra nhÆ° sau:\nÄáº§u tiÃªn, dá»… nháº¥t lÃ  tÃ­nh Ä‘áº¡o hÃ m $L$ theo $\\bold{W}_{ho}$ ğŸ˜€ Tá»« biá»ƒu thá»©c $(3)$ thÃ¬ ta cÃ³ ngay\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{O}_{t}} = \\frac{\\partial l(\\bold{O}_t, \\bold{Y}_t)}{\\partial \\bold{O}_{t}} \\end{equation}$$\nDo Ä‘Ã³, káº¿t há»£p $(2)$ vÃ  $(4)$ thÃ¬\n$$ \\frac{\\partial L}{\\partial \\bold{W}_{ho}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{O}_{t}} \\frac{\\partial \\bold{O}_t}{\\partial \\bold{W}_{ho}} \\right ) = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{O}_{t}}\\bold{H}_t^\\top \\right ) $$\nLÆ°u Ã½. CÃ¡ch tÃ­nh giÃ¡ trá»‹ cá»§a $\\dfrac{\\partial l(\\bold{O}_t, \\bold{Y}_t)}{\\partial \\bold{O}_{t}}$ trong biá»ƒu thá»©c $(4)$ sáº½ phá»¥ thuá»™c vÃ o hÃ m $l$ vÃ  thÆ°á»ng thÃ¬ nÃ³ ráº¥t dá»… tÃ­nh ğŸ˜œ\nTiáº¿p theo, ta cÃ³ nháº­n xÃ©t sau:\n$\\bold{H}_T$ chá»‰ tham gia vÃ o má»™t biá»ƒu thá»©c trong quÃ¡ trÃ¬nh feed-forward (Ä‘á»ƒ tÃ­nh ra $\\bold{O}_T$) $\\bold{H}_{t}$ vá»›i $t \u0026lt; T$ thÃ¬ tham gia vÃ o hai biá»ƒu thá»©c (tÃ­nh $\\bold{H}_{t +1}$ vÃ  $\\bold{O}_t$) Do Ä‘Ã³, cÃ¡ch tÃ­nh $\\dfrac{\\partial L}{\\partial \\bold{H}_{t}}$ sáº½ cÃ³ sá»± khÃ¡c biá»‡t tÃ¹y theo giÃ¡ trá»‹ $t$.\nVá»›i $t = T$: Tá»« $(2)$ vÃ  $(4)$ ta cÃ³\n$$ \\frac{\\partial L}{\\partial \\bold{H}_{T}} = \\frac{\\partial L}{\\partial \\bold{O}_{T}} \\frac{\\partial \\bold{O}_T}{\\partial \\bold{H}_{T}} = \\bold{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\bold{O}_{T}} $$\nVá»›i $t \u0026lt; T$: Tá»« $(1), (2)$ vÃ  $(4)$ thÃ¬\n$$ \\frac{\\partial L}{\\partial \\bold{H}_{t}} = \\frac{\\partial L}{\\partial \\bold{O}_{t}} \\frac{\\partial \\bold{O}_t}{\\partial \\bold{H}_{t}} + \\frac{\\partial L}{\\partial \\bold{H}_{t+1}} \\frac{\\partial \\bold{H}_{t+1}}{\\partial \\bold{H}_{t}} = \\bold{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\bold{O}_{t}} + \\bold{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\bold{H}_{t+1}} $$\nCá»© tiáº¿p tá»¥c biáº¿n Ä‘á»•i tiáº¿p vá»›i $\\dfrac{\\partial L}{\\partial \\bold{H}_{t+1}}$ vÃ  cá»© nhÆ° tháº¿ cho Ä‘áº¿n $T$, ta sáº½ cÃ³\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\mathbf{H}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_{hh}^\\top\\right)}^{T-i} \\mathbf{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\mathbf{O}_{T+t-i}} \\end{equation}$$\nÄá»ƒ Ã½ ráº±ng biá»ƒu thá»©c $(5)$ cÅ©ng Ä‘Ãºng vá»›i $t = T$.\nVáº­y tá»« $(1)$ vÃ  $(5)$ thÃ¬\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{W}_{xh}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\frac{\\partial \\bold{H}_t}{\\partial \\bold{W}_{xh}} \\right ) = = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\bold{X}_t^\\top \\right ) \\end{equation}$$\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{W}_{hh}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\frac{\\partial \\bold{H}_t}{\\partial \\bold{W}_{hh}} \\right ) = = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\bold{H}_{t-1}^\\top \\right ) \\end{equation}$$\nVáº¥n Ä‘á» vanishing vÃ  exploding gradients trong RNN Qua cÃ¡c biá»ƒu thá»©c $(6)$ vÃ  $(7)$, ta tháº¥y ráº±ng náº¿u giÃ¡ trá»‹ $T$ lá»›n (tá»©c lÃ  cÃ¢u input gá»“m ráº¥t nhiá»u tá»«) thÃ¬ sáº½ cÃ³ hai trÆ°á»ng há»£p xáº£y ra Ä‘á»‘i vá»›i cÃ¡c giÃ¡ trá»‹ gradient $\\dfrac{\\partial L}{\\partial \\bold{W}_{xh}}$ vÃ  $\\dfrac{\\partial L}{\\partial \\bold{W}_{ho}}$:\nVanishing: Trong quÃ¡ trÃ¬nh tÃ­nh $\\left ( \\bold{W}_{hh}^\\top \\right ) ^ {T-i}$ cÃ³ nhiá»u giÃ¡ trá»‹ nhá» hÆ¡n 1 Ä‘Æ°á»£c nhÃ¢n vá»›i nhau. Exploding: NgÆ°á»£c láº¡i (lá»›n hÆ¡n 1). Ta cÃ³ má»™t giáº£i phÃ¡p Ä‘á»ƒ háº¡n cháº¿ hiá»‡n tÆ°á»£ng nÃ y lÃ  Truncated BPTT, tá»©c lÃ  ta chá»‰ lan truyá»n gradients Ä‘áº¿n má»™t tráº¡ng thÃ¡i cÃ¡ch tráº¡ng thÃ¡i hiá»‡n táº¡i má»™t khoáº£ng nÃ o Ä‘Ã³ thÃ´i chá»© khÃ´ng lan truyá»n toÃ n bá»™. Má»™t phÆ°Æ¡ng phÃ¡p cÅ©ng khÃ¡ phá»• biáº¿n trong viá»‡c háº¡n cháº¿ exploding gradient lÃ  gradient clipping cÅ©ng co thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng.\nÄá»‘i vá»›i hiá»‡n tÆ°á»£ng vanishing gradient, ta cÃ³ thá»ƒ diá»…n Ä‘áº¡t nÃ³ má»™t cÃ¡ch vÄƒn vá»Ÿ hÆ¡n lÃ  mÃ´ hÃ¬nh Ä‘Ã£ bá»‹ â€œquÃªnâ€ nhá»¯ng thÃ´ng tin tÃ­ch lÅ©y tá»« phÃ­a trÆ°á»›c.\nVÃ­ dá»¥, ta cÃ³ hai cÃ¢u sau:\nThe cat, which always â€¦ (very long descriptions) â€¦, was very cute The cats, which always â€¦ (very long descriptions) â€¦, were very cute ThÃ´ng thÆ°á»ng, Ä‘á»™ng tá»« to-be á»Ÿ trÆ°á»›c â€œvery cuteâ€ sáº½ phá»¥ thuá»™c vÃ o danh tá»« á»Ÿ Ä‘áº§u cÃ¢u (â€catâ€ hay â€œcatsâ€). Tuy nhiÃªn, náº¿u RNN bá»‹ vanishing gradient thÃ¬ nÃ³ sáº½ khÃ´ng thá»ƒ nhá»› Ä‘Æ°á»£c trÆ°á»›c Ä‘Ã³ lÃ  má»™t con mÃ¨o hay nhiá»u con mÃ¨o Ä‘á»ƒ mÃ  chá»n Ä‘á»™ng tá»« to-be cho Ä‘Ãºng. ğŸ˜€\nYáº¿u tá»‘ nÃ y Ä‘Ã£ má»Ÿ ra má»™t hÆ°á»›ng phÃ¡t triá»ƒn cho RNN lÃ  ta sáº½ cá»‘ gáº¯ng tÃ­nh toÃ¡n thÃªm nhá»¯ng phÃ©p toÃ¡n khÃ¡c Ä‘á»ƒ duy trÃ¬ Ä‘Æ°á»£c cÃ¡c thÃ´ng tin tá»« trÆ°á»›c, tá»« phÃ­a xa mÃ  RNN hiá»‡n táº¡i khÃ´ng thá»ƒ ghi nhá»› Ä‘Æ°á»£c. Tá»« Ä‘Ã³, ta cÃ³ sá»± ra Ä‘á»i cá»§a Long Short-Term Memory (LSTM) vÃ  Gated Recurrent Unit (GRU).\nQuÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh RNN trong thá»±c táº¿ Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh ML hay CNN thÃ´ng thÆ°á»ng, nhá»¯ng thao tÃ¡c diá»…n ra trong bÆ°á»›c huáº¥n luyá»‡n (training) vÃ  sá»­ dá»¥ng trong thá»±c táº¿ (testing) lÃ  ráº¥t giá»‘ng nhau: Tá»« má»™t intput cho ra má»™t output vÃ  chá»‰ nhÆ° váº­y lÃ  xong (ta táº¡m bá» qua back-propagation).\nNguá»“n: Towards Data Science Tuy nhiÃªn, trong RNN thÃ¬ training vÃ  testing sáº½ cÃ³ sá»± khÃ¡c biá»‡t khÃ¡ rÃµ rá»‡t. Äá»‘i vá»›i training, á»Ÿ má»—i thá»i Ä‘iá»ƒm thÃ¬ ta sáº½ luÃ´n cÃ³ input vÃ  label á»©ng vá»›i thá»i Ä‘iá»ƒm Ä‘Ã³. Tuy nhiÃªn, trong testing thÃ¬ ta chá»‰ cÃ³ duy nháº¥t input cho thá»i Ä‘iá»ƒm Ä‘áº§u tiÃªn vÃ  input cá»§a nhá»¯ng thá»i Ä‘iá»ƒm sau chÃ­nh lÃ  output cá»§a thá»i Ä‘iá»ƒm trÆ°á»›c Ä‘Ã³.\nTa xÃ©t vÃ­ dá»¥ vá»›i bÃ i toÃ¡n xÃ¢y dá»±ng mÃ´ hÃ¬nh RNN sinh ra Ä‘oáº¡n vÄƒn báº£n nhÆ° sau:\nNguá»“n: ML Lectures Trong training, tá»« má»™t cÃ¢u cÃ³ $T_x$ tá»« lÃ  $x_1, x_2,\u0026hellip;, x_{T_x}$ thÃ¬ ta sáº½ táº¡o ra Ä‘Æ°á»£c má»™t training sample vá»›i $(T_x + 1)$ thá»i Ä‘iá»ƒm, á»Ÿ thá»i Ä‘iá»ƒm $t$ thÃ¬ input vÃ  label láº§n lÆ°á»£t lÃ  $x_t$ vÃ  $x_{t+1}$ (giáº£ sá»­ $x_0$ vÃ  $x_{T_x + 1}$ láº§n lÆ°á»£t lÃ  cÃ¡c tá»« Ä‘áº·c biá»‡t nháº±m bÃ¡o hiá»‡u báº¯t Ä‘áº§u vÃ  káº¿t thÃºc Ä‘oáº¡n).\nTrong testing, tá»« má»™t tá»« ban Ä‘áº§u lÃ  $x_1$ (thÆ°á»ng lÃ  tá»« báº¯t Ä‘áº§u Ä‘oáº¡n vÄƒn báº£n), ta sáº½ cÃ³ testing sample vá»›i 1 thá»i Ä‘iá»ƒm. Sau khi qua RNN, ta cÃ³ thÃªm tá»« má»›i lÃ  $y_1$. Sau Ä‘Ã³, $y_1$ Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° lÃ  input cá»§a thá»i Ä‘iá»ƒm thá»© hai, Ä‘Æ°a qua RNN vÃ  cÃ³ tiáº¿p tá»« $y_2$. QuÃ¡ trÃ¬nh cá»© láº·p láº¡i cho Ä‘áº¿n khi Ä‘Ã£ sinh ra Ä‘á»§ sá»‘ tá»« chÃºng ta cáº§n hoáº·c lÃ  tá»« Ä‘Æ°á»£c sinh ra chÃ­nh lÃ  tá»« káº¿t thÃºc Ä‘oáº¡n.\nCÃ¡c dáº¡ng mÃ´ hÃ¬nh RNN ChÃºng ta Ä‘á»ƒ Ã½ ráº±ng dáº¡ng mÃ´ hÃ¬nh RNN mÃ¬nh Ä‘Ã£ trÃ¬nh bÃ y á»Ÿ pháº§n trÆ°á»›c Ä‘ang nháº­n input lÃ  má»™t cÃ¢u vÃ  output cá»§a nÃ³ cÅ©ng lÃ  má»™t cÃ¢u. Dáº¡ng mÃ´ hÃ¬nh nÃ y cÃ²n gá»i lÃ  many-to-many. TÃ¹y vÃ o bÃ i toÃ¡n cáº§n giáº£i quyáº¿t mÃ  ta cÃ³ cÃ¡c dáº¡ng nhÆ° sau:\nCÃ¡c dáº¡ng cá»§a mÃ´ hÃ¬nh RNN\nNguá»“n: Javatpoint One-to-one: CÃ¡i nÃ y thÃ¬ ráº¥t thÆ°á»ng tháº¥y, vÃ­ dá»¥ nhÆ° Image Classification. One-to-many: CÃ³ thá»ƒ láº¥y vÃ­ dá»¥ nhÆ° bÃ i toÃ¡n sinh ra vÄƒn báº£n hoáº·c Ã¢m nháº¡c. Ta cung cáº¥p input lÃ  má»™t tá»« báº¥t kÃ¬ vÃ  mÃ´ hÃ¬nh sáº½ táº¡o ra tá»« káº¿ tiáº¿p, ta láº¡i Ä‘em tá»« nÃ y vÃ o lÃ m input Ä‘á»ƒ cÃ³ tá»« tiáº¿p theo. Many-to-one: CÃ¡c bÃ i toÃ¡n nhÆ° Sentiment Analysis, Mail fitlering,â€¦ Trong dáº¡ng mÃ´ hÃ¬nh nÃ y, chá»‰ cÃ³ thá»i Ä‘iá»ƒm cuá»‘i cÃ¹ng lÃ  cÃ³ tÃ­nh ra output, cÃ¡c thá»i Ä‘iá»ƒm trÆ°á»›c thÃ¬ chá»‰ cÃ³ tÃ­nh hidden state. Many-to-many: Trong hÃ¬nh trÃªn, ta tháº¥y ráº±ng cÃ³ hai loáº¡i \u0026ldquo;many-to-many\u0026rdquo;: Äáº§u tiÃªn lÃ  hÃ¬nh thá»© 4, khi mÃ  tá»« thá»i Ä‘iá»ƒm cuá»‘i cÃ¹ng cá»§a input thÃ¬ ta báº¯t Ä‘áº§u sinh ra cÃ¡c tá»« cá»§a output, theo nguyÃªn táº¯c giá»‘ng nhÆ° one-to-many. ÄÃ¢y thá»±c ra lÃ  dáº¡ng cá»§a mÃ´ hÃ¬nh Ä‘áº§u tiÃªn Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c khÃ¡ áº¥n tÆ°á»£ng trong bÃ i toÃ¡n Machine Translation, mÃ´ hÃ¬nh nÃ y cÃ³ tÃªn lÃ  Sequence to Sequence (seq2seq). Tiáº¿p theo lÃ  hÃ¬nh thá»© 5, vá»›i kiáº¿n trÃºc giá»‘ng nhÆ° mÃ´ hÃ¬nh RNN mÃ  mÃ¬nh Ä‘Ã£ láº¥y vÃ­ dá»¥ á»Ÿ cÃ¡c pháº§n trÃªn. BÃ i toÃ¡n Ä‘iá»ƒn hÃ¬nh cho dáº¡ng nÃ y lÃ  Name Entity Recognition. TÃ i liá»‡u tham kháº£o Robin M. Schmidtm, Recurrent Neural Networks (RNNs): A gentle Introduction and Overview Dive into DL, Recurrent Neural Network DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-02-15T21:42:44+07:00","permalink":"https://htrvu.github.io/post/rnn/","title":"Recurrent Neural Network (RNN)"},{"content":"Giá»›i thiá»‡u Ta biáº¿t ráº±ng, háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh CNN thÆ°á»ng Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« má»™t phiÃªn báº£n ban Ä‘áº§u (cÃ³ thá»ƒ lÃ  dá»±a theo má»™t nguá»“n tÃ i nguyÃªn nÃ o Ä‘Ã³), sau Ä‘Ã³ chÃºng Ä‘Æ°á»£c scale dáº§n lÃªn Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n, vÃ  táº¥t nhiÃªn lÃ  Ä‘á»™ phá»©c táº¡p cÅ©ng tÄƒng theo\nVÃ­ dá»¥: Vá»›i ResNet thÃ¬ ta cÃ³ ResNet18 cho Ä‘áº¿n ResNet152, DenseNet thÃ¬ DenseNet121 cho Ä‘áº¿n 201, MobileNet thÃ¬ ta cÃ³ siÃªu tham sá»‘ width multiplier Ä‘á»ƒ Ä‘iá»u chá»‰nh sá»‘ channel trong tá»«ng layer vÃ  resolutiom multiplier Ä‘á»ƒ Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c táº¡i cÃ¡c layer,â€¦ Nhá»¯ng cÃ¡ch lÃ m Ä‘Ã³ gá»i lÃ  model scaling. Tuy nhiÃªn, ta nháº­n tháº¥y ráº±ng nhá»¯ng thao tÃ¡c model scaling trÆ°á»›c Ä‘Ã³ chá»‰ táº­p trung vÃ o má»™t trong 3 yáº¿u tá»‘: depth - $d$ (sá»‘ layer), width - $w$ (sá»‘ channel) vÃ  resolution - $r$. HÆ¡n ná»¯a, viá»‡c Ä‘iá»u chá»‰nh cÅ©ng khÃ´ng theo má»™t nguyÃªn táº¯c nÃ o mÃ  cÃ²n mang Ä‘áº­m tÃ­nh cháº¥t ngáº«u nhiÃªn, â€œhÃªn xuiâ€, cáº§n pháº£i thá»­ nghiá»‡m ráº¥t nhiá»u láº§n má»›i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»™t Ä‘á»™ chÃ­nh xÃ¡c mong muá»‘n. Khi Ä‘Ã³ thÃ¬ sá»‘ lÆ°á»£ng tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh cÅ©ng tÄƒng chÃ³ng máº·t!\nCÃ¡c tÃ¡c giáº£ cá»§a paper Ä‘Ã£ cho tháº¥y káº¿t quáº£ thá»±c nghiá»‡m ráº±ng viá»‡c Ä‘iá»u chá»‰nh má»™t trong 3 yáº¿u tá»‘ cÃ³ thá»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c nhÆ°ng chá»‰ tÄƒng Ä‘áº¿n má»™t má»©c nÃ o Ä‘Ã³ thÃ´i, sau Ä‘Ã³ nÃ³ sáº½ bá»‹ bÃ£o hÃ²a. VÃ­ dá»¥ nhÆ° á»Ÿ hÃ¬nh bÃªn dÆ°á»›i:\nTa cÃ³ thá»ƒ Ä‘Æ°a ra cÃ¡c nháº­n xÃ©t nhÆ° sau:\nNáº¿u mÃ´ hÃ¬nh cÃ³ width lá»›n (má»—i layer cÃ³ nhiá»u channel) thÃ¬ nÃ³ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c nhiá»u loáº¡i Ä‘áº·c trÆ°ng khÃ¡c nhau. NhÆ°ng náº¿u mÃ´ hÃ¬nh khÃ´ng Ä‘á»§ sÃ¢u thÃ¬ cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã³ cÅ©ng chÆ°a pháº£i Ä‘áº·c trÆ°ng á»Ÿ má»©c high-level (ná»•i báº­t cho Ä‘á»‘i tÆ°á»£ng) Náº¿u mÃ´ hÃ¬nh cÃ³ depth lá»›n thÃ¬ nÃ³ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng high-level nhÆ°ng náº¿u khÃ´ng cÃ³ width lá»›n thÃ¬ cÅ©ng khÃ´ng há»c Ä‘Æ°á»£c nhiá»u loáº¡i Ä‘áº·c trÆ°ng* Vá» máº·t trá»±c giÃ¡c, náº¿u ta Ä‘Æ°a vÃ o mÃ´ hÃ¬nh má»™t bá»©c áº£nh cÃ³ resolution cao thÃ¬ mÃ´ hÃ¬nh nÃªn cÃ³ depth lá»›n Ä‘á»ƒ cÃ³ thá»ƒ dáº§n há»c cÃ¡c Ä‘áº·c trÆ°ng tá»« cÃ¡c feature maps cÃ³ resolution lá»›n, Ä‘á»“ng thá»i cÅ©ng vÃ¬ sáº½ cÃ³ nhiá»u Ä‘áº·c trÆ°ng hÆ¡n nÃªn ta cáº§n width lá»›n. Do Ä‘Ã³, model scaling nÃªn táº­p trung vÃ o viá»‡c Ä‘iá»u chá»‰nh Ä‘á»“ng thá»i cáº£ 3 yáº¿u tá»‘ $d$, $w$, $r$. Paper cÃ´ng bá»‘ EfficientNet cÃ³ tÃªn lÃ  â€œEfficientNet: Rethinking Model Scaling for Convolutional Neural Networksâ€. CÃ¡c tÃ¡c giáº£ táº­p trung vÃ o viá»‡c Ä‘i tÃ¬m má»™t phÆ°Æ¡ng phÃ¡p model scaling hiá»‡u quáº£, cÃ³ nguyÃªn táº¯c, Ä‘iá»u chá»‰nh Ä‘á»“ng thá»i cáº£ 3 yáº¿u tá»‘ nhÆ° Ä‘Ã£ Ä‘á» cáº­p. PhÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c giá»›i thiá»‡u cÃ³ tÃªn lÃ  compound scaling.\n\u0026ldquo;NguyÃªn táº¯c\u0026rdquo; trong phÆ°Æ¡ng phÃ¡p nÃ y ráº¥t Ä‘Æ¡n giáº£n, ta sáº½ cÃ¹ng Ä‘iá»u chá»‰nh $d$, $w$, $r$ cá»§a toÃ n bá»™ network theo cÃ¹ng má»™t há»‡ sá»‘ gá»i lÃ  compound coefficient (kÃ­ hiá»‡u lÃ  $\\phi$).\nMinh há»a cho cÃ¡c phÆ°Æ¡ng phÃ¡p model scaling. (a) lÃ  mÃ´ hÃ¬nh ban Ä‘áº§u. (b)-(d) thá»±c hiá»‡n Ä‘iá»u chá»‰nh má»™t trong ba yáº¿u tá»‘. (e) lÃ  phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t, nÃ³ tiáº¿n hÃ nh Ä‘iá»u chá»‰nh cáº£ ba. Táº¥t nhiÃªn lÃ  model scaling chá»‰ phÃ¡t huy tÃ¡c dá»¥ng khi mÃ  mÃ´ hÃ¬nh ban Ä‘áº§u lÃ  Ä‘á»§ tá»‘t. Tá»« phÆ°Æ¡ng phÃ¡p compound scaling, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ Ã¡p dá»¥ng nÃ³ cho ResNet, MobileNet Ä‘á»ƒ chá»©ng tá» Ä‘á»™ hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p. Sau Ä‘Ã³, má»™t há» mÃ´ hÃ¬nh má»›i Ä‘Æ°á»£c Ä‘á» xuáº¥t lÃ  EfficientNet, vá»› 8 phiÃªn báº£n tá»« B0 Ä‘áº¿n B7 vá»›i Ä‘á»™ phá»©c táº¡p vÃ  Ä‘á»™ chÃ­nh xÃ¡c tÄƒng dáº§n trÃªn táº­p ImageNet. EfficientNet-B7 Ä‘Ã£ trá»Ÿ thÃ nh SOTA (state-of-the-art) vá»›i Ä‘á»™ phá»©c táº¡p nhá» hÆ¡n ráº¥t nhiá»u láº§n so vá»›i mÃ´ hÃ¬nh SOTA trÆ°á»›c Ä‘Ã³.\nBÃ i toÃ¡n model scaling Giáº£ sá»­ conv layer thá»© $i$ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  hÃ m sá»‘ $Y_i = F_i(X_i)$, vá»›i input $X_i$ cÃ³ shape lÃ  $\\left (H_i, W_i, C_i \\right)$. Khi Ä‘Ã³, má»™t CNN $N$ cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n lÃ \n$$ N = F_k \\bigodot F_{k-1} \\bigodot \\cdots F_1(X_1) = \\bigodot_{j=1,\u0026hellip;,k} F_j(X_1) $$\nThÃ´ng thÆ°á»ng, cÃ¡c máº¡ng CNN thÆ°á»ng Ä‘Æ°á»£c xÃ¢y dá»±ng theo kiá»ƒu gá»“m nhiá»u giai Ä‘oáº¡n, má»—i giai Ä‘oáº¡n lÃ  sá»± láº·p láº¡i cÃ¡c block cÃ³ cÃ¹ng dáº¡ng cáº¥u trÃºc, chá»‰ khÃ¡c nhau má»™t sá»‘ chi tiáº¿t nhÆ° sá»‘ layer trong block, kÃ­ch thÆ°á»›c cá»§a filter,â€¦ VÃ­ dá»¥, ResNet Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cÃ¡c residual block, MobileNet thÃ¬ lÃ  cÃ¡c depthwise separable block,â€¦ Do Ä‘Ã³, ta cÃ³ thá»ƒ viáº¿t láº¡i $N$ thÃ nh\n$$ N = \\bigodot_{i=1,\u0026hellip;,s} F_i ^ {L_i}(X_{(H_i, W_i, C_i}) $$\n, vá»›i $F_i$ lÃ  layer Ä‘Æ°á»£c láº·p láº¡i $L_i$ láº§n trong giai Ä‘oáº¡n thá»© $i$, vá»›i input lÃ  $X$ cÃ³ shape $(H_i, W_i, C_i)$.\nBÃ i toÃ¡n model scaling sáº½ cá»‘ Ä‘á»‹nh layer $F_i$ vÃ  Ä‘i Ä‘iá»u chá»‰nh cÃ¡c giÃ¡ trá»‹ $L_i, H_i, W_i, C_i$, sao cho mÃ´ hÃ¬nh thá»a mÃ£n cÃ¡c rÃ ng buá»™c vá» tÃ i nguyÃªn vÃ  Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t cÃ³ thá»ƒ.\nÄiá»u chá»‰nh $L_i$ $\\Leftrightarrow$ Äiá»u chá»‰nh depth Äiá»u chá»‰nh $C_i$ $\\Leftrightarrow$ Äiá»u chá»‰nh width Äá»u chá»‰nh $H_i, W_i$ $\\Leftrightarrow$ Äiá»u chá»‰nh resolution Äá»ƒ giáº£m khÃ´ng gian tÃ¬m kiáº¿m, ta sáº½ Ä‘iá»u chá»‰nh cÃ¡c giÃ¡ trá»‹ trÃªn cá»§a toÃ n bá»™ layer trong mÃ´ hÃ¬nh theo cÃ¹ng má»™t tá»‰ lá»‡. Khi Ä‘Ã³, bÃ i toÃ¡n cá»§a ta lÃ  bÃ i toÃ¡n tá»‘i Æ°u nhÆ° sau:\n, vá»›i $d, w, r$ lÃ  há»‡ sá»‘ Ä‘á»ƒ Ä‘iá»u chá»‰nh depth, width, resolution; $\\hat{F_i}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i}$ lÃ  cÃ¡c giÃ¡ trá»‹ ban Ä‘áº§u cá»§a mÃ´ hÃ¬nh baseline.\nPhÆ°Æ¡ng phÃ¡p compound scaling PhÆ°Æ¡ng phÃ¡p nÃ y sá»­ dá»¥ng compound coefficient $\\phi$ Ä‘á»ƒ Ä‘iá»u chá»‰nh depth, width, resolution theo nguyÃªn táº¯c nhÆ° sau:\nVá»›i mÃ´ hÃ¬nh baseline ban Ä‘áº§u, ta thá»±c hiá»‡n grid search Ä‘á»ƒ tÃ¬m ra bá»™ 3 giÃ¡ trá»‹ tá»‰ lá»‡ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tá»‘t nháº¥t cÃ³ thá»ƒ, sao cho\n$$ \\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\text{ vÃ  } \\alpha \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 $$\nSau Ä‘Ã³, ta sáº½ scale mÃ´ hÃ¬nh lÃªn theo há»‡ sá»‘ $\\phi$ vá»›i\n$$ d = \\alpha ^ \\phi, ; w = \\beta^\\phi, ; r = \\gamma^\\phi $$\nVá» máº·t trá»±c giÃ¡c, ta cÃ³ thá»ƒ xem $\\phi$ nhÆ° lÃ  cÃ¡ch mÃ  chÃºng ta cho biáº¿t lÆ°á»£ng tÃ i nguyÃªn dÃ nh cho model scaling lÃ  bao nhiÃªu, cÃ²n cÃ¡c giÃ¡ trá»‹ $\\alpha, \\beta, \\gamma$ lÃ  cÃ¡ch chÃºng ta phÃ¢n phá»‘i tÃ i nguyÃªn Ä‘Ã³ cho depth, width vÃ  resolution. Giáº£i thÃ­ch cho cÃ¡c rÃ ng buá»™c cho $\\alpha, \\beta, \\gamma$ Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° sau:\nTáº¥t nhiÃªn lÃ  Ä‘á»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c viá»‡c scale mÃ´ hÃ¬nh lÃªn thÃ¬ giÃ¡ trá»‹ cá»§a chÃºng pháº£i khÃ´ng nhá» hÆ¡n 1 NgoÃ i ra, má»™t phÃ©p toÃ¡n convolution sáº½ cÃ³ Ä‘á»™ phá»©c táº¡p tá»‰ lá»‡ thuáº­n vá»›i $d, w^2, r^2$. Do Ä‘Ã³, náº¿u ta scale model lÃªn theo há»‡ sá»‘ $\\phi$ thÃ¬ Ä‘á»™ phá»©c táº¡p sáº½ tÄƒng lÃªn má»™t lÆ°á»£ng báº±ng $(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2)^\\phi$. CÃ¡c tÃ¡c giáº£ mong muá»‘n Ä‘á»™ phá»©c táº¡p tÄƒng khoáº£ng $2^\\phi$, do Ä‘Ã³ ta cÃ³ rÃ ng buá»™c $\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2$. Há» mÃ´ hÃ¬nh EfficientNet Neural architecture search EfficientNet lÃ  má»™t há» cÃ¡c mÃ´ hÃ¬nh ráº¥t Ä‘áº·c biá»‡t:\nThá»© nháº¥t, chÃºng Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng â€œmÃ¡yâ€ ğŸ˜€ VÃ o nÄƒm 2017, má»™t ma thuáº­t Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘ trong paper â€œNeural architecture search with reinforcement learningâ€ cá»§a chÃ­nh tÃ¡c giáº£ Quoc V. Le, nÃ³ giÃºp chÃºng ta xÃ¢y má»™t kiáº¿n trÃºc phÃ¹ há»£p nháº¥t cÃ³ thá»ƒ dá»±a theo Ä‘á»™ chÃ­nh xÃ¡c, Ä‘á»™ phá»©c táº¡p mÃ  chÃºng ta yÃªu cáº§u.\nVá»›i há» EfficientNet, cÃ¡c tÃ¡c giáº£ táº­p trung vÃ o viá»‡c giá»›i háº¡n Ä‘á»™ phá»©c táº¡p (cá»¥ thá»ƒ lÃ  FLOPS). Má»¥c tiÃªu tá»‘i Æ°u cá»§a reinforcement learning lÃ \n$$ ACC(m) \\times \\left ( \\frac{FLOPS(m)}{T} \\right )^w $$\n, vá»›i $m$ lÃ  mÃ´ hÃ¬nh, $ACC$ vÃ  $FLOPS$ lÃ  Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ phá»©c táº¡p, $T$ lÃ  FLOPS mong muá»‘n vÃ  nÃ³ báº±ng $400 \\times 10^6$, $w=-0.07$ lÃ  háº±ng sá»‘ Ä‘iá»u chá»‰nh trade-off giá»¯a $ACC$ vÃ  $FLOPS$\nThá»© hai, tá»« má»™t mÃ´ hÃ¬nh ban Ä‘áº§u lÃ  EfficientNet-B0, ta tiáº¿n hÃ nh scale theo 2 bÆ°á»›c:\nBÆ°á»›c 1: Cá»‘ Ä‘á»‹nh $\\phi = 1$, giáº£ sá»­ lÆ°á»£ng tÃ i nguyÃªn mÃ  ta cÃ³ thá»ƒ sá»­ dá»¥ng lÃ  nhiá»u gáº¥p Ä‘Ã´i hiá»‡n táº¡i. Khi Ä‘Ã³, thá»±c hiá»‡n grid search Ä‘á»ƒ tÃ¬m cÃ¡c giÃ¡ trá»‹ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tá»‘t nháº¥t BÆ°á»›c 2: Tá»« cÃ¡c giÃ¡ trá»‹ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tÃ¬m Ä‘Æ°á»£c, tiáº¿n hÃ nh scale theo cÃ¡c giÃ¡ trá»‹ $\\phi$ lá»›n hÆ¡n Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c phiÃªn báº£n B1-B7. á» Ä‘Ã¢y, ta hoÃ n toÃ n cÃ³ thá»ƒ tÄƒng $\\phi$ lÃªn rá»“i láº¡i grid search nhÆ°ng lÃºc nÃ y chi phÃ­ thá»±c hiá»‡n lÃ  ráº¥t lá»›n. Do Ä‘Ã³, cÃ¡c tÃ¡c giáº£ chá»‰ grid search má»™t láº§n rá»“i sau Ä‘Ã³ chá»‰ cáº§n tÄƒng $\\phi$.\nKiáº¿n trÃºc mÃ´ hÃ¬nh Äáº§u tiÃªn, tá»•ng quan kiáº¿n trÃºc cá»§a EfficientNet-B0 nhÆ° sau:\nTrong Ä‘Ã³: MBConv chÃ­nh lÃ  inverted residual block trong MobileNetV2, cÃ¹ng vá»›i má»™t sá»‘ cáº£i tiáº¿n nhÆ° trong paper â€œSqueeze-and-excitation networksâ€ CÃ¡c mÃ´ hÃ¬nh EfficientNet-B1 cho Ä‘áº¿n B7 chÃ­nh lÃ  káº¿t quáº£ cá»§a viá»‡c Ã¡p dá»¥ng compound scaling lÃªn EfficientNet-B0.\nTÃ i liá»‡u tham kháº£o Paper EfficientNet: https://arxiv.org/abs/1905.11946 ","date":"2023-02-15T11:17:46+07:00","permalink":"https://htrvu.github.io/post/efficientnet/","title":"EfficientNet (2020)"},{"content":"Giá»›i thiá»‡u Tá»« sá»± thÃ nh cÃ´ng cá»§a MobileNet (2017) trong viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh Deep Learning trÃªn cÃ¡c thiáº¿t bá»‹ biÃªn (smartphone, embedded,â€¦) nhá» vÃ o viá»‡c sá»­ dá»¥ng hiá»‡u quáº£ phÃ©p toÃ¡n depthwise separable convolution, nhiá»u nghiÃªn cá»©u dá»±a trÃªn hÆ°á»›ng phÃ¡t triá»ƒn nÃ y Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh.\nDá»±a theo cÃ¡c â€œkinh nghiá»‡mâ€ cÃ³ Ä‘Æ°á»£c cá»§a báº£n thÃ¢n, nhÃ¬n vÃ o MobileNet thÃ¬ ta sáº½ tháº¥y ngay ráº±ng, nÃ³ chÆ°a cÃ³ cÃ¡i skip connection nÃ o cáº£ ğŸ˜€ ÄÃºng z, skip connection Ä‘Ã£ cho tháº¥y Ä‘Æ°á»£c sá»± hiá»‡u quáº£ cá»§a mÃ¬nh trong cÃ¡c mÃ´ hÃ¬nh nhÆ° ResNet, Inception-ResNet, DenseNet,â€¦ táº¡i sao ta khÃ´ng thá»­ thÃªm vÃ o MobileNet? Boom, thÃªm ngay!\nMobileNetV2 Ä‘Æ°á»£c cÃ´ng bá»‘ vá»›i sá»± káº¿ thá»«a tá»« MobileNet vÃ  bá»• sung thÃªm skip connection. Táº¥t nhiÃªn lÃ  khÃ´ng chá»‰ dá»«ng á»Ÿ Ä‘Ã³ ğŸ™‚ CÃ¡c tÃ¡c giáº£ xÃ¢y dá»±ng MobileNetV2 dá»±a trÃªn cÃ¡c inverted residual block, nÆ¡i mÃ  cÃ¡c skip connection dÃ¹ng Ä‘á»ƒ káº¿t ná»‘i cÃ¡c bottleneck layer vá»›i nhau! HÆ¡n ná»¯a, ta cÃ²n cÃ³ má»™t Ä‘iá»ƒm ráº¥t thÃº vá»‹ lÃ  cÃ¡c bottleneck layer nÃ y sá»­ dá»¥ng activation function lÃ  linear!\nMobileNetV2 Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n MobileNet trÃªn táº­p ImageNet, vá»›i sá»‘ tham sá»‘ Ã­t hÆ¡n, lÆ°á»£ng bá»™ nhá»› cáº§n dÃ¹ng táº¡i má»—i layer lÃ  Ã­t hÆ¡n. Tá»« sá»± ra Ä‘á»i cá»§a mÃ´ hÃ¬nh nÃ y, ngÆ°á»i ta cÅ©ng Ä‘Ã£ phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh hiá»‡u quáº£ trong bÃ i toÃ¡n Object Detection nhÆ° SSDLite, hay Semantic Segmentation nhÆ° Mobile DeepLabv3\nBÃ n vá» ReLU Ta biáº¿t ráº±ng tá»« khi paper AlexNet giá»›i thiá»‡u activation function ReLU thÃ¬ nÃ³ Ä‘Ã£ trá»Ÿ thÃ nh má»™t activation function ráº¥t phá»• biáº¿n vÃ  Ä‘Æ°á»£c dÃ¹ng thÆ°á»ng xuyÃªn trong cÃ¡c mÃ´ hÃ¬nh Deep Learning vá»›i Ä‘iá»ƒm máº¡nh quan trá»ng lÃ  Ä‘áº¡o hÃ m cá»§a nÃ³ ráº¥t Ä‘Æ¡n giáº£n. CÃ´ng thá»©c cá»§a ReLU lÃ \n$$ReLU(x) = \\max(x, 0)$$\n, tá»©c lÃ  nÃ³ sáº½ â€œvá»©tâ€ nhá»¯ng giÃ¡ trá»‹ bÃ© hÆ¡n 0 trong input. Äiá»u nÃ y nghÄ©a lÃ  ta sáº½ bá»‹ máº¥t thÃ´ng tin!. Náº¿u input truyá»n vÃ o lÃ  má»™t channel thÃ¬ ta sáº½ bá»‹ máº¥t má»™t lÆ°á»£ng thÃ´ng tin nhá» (hoáº·c cÃ³ thá»ƒ lÃ  lá»›n) trÃªn channel Ä‘Ã³.\nActivation function ReLU Nguá»“n: Research Gate Váº­y táº¡i sao trÆ°á»›c giá» ReLU váº«n luÃ´n Ä‘Æ°á»£c sá»­ dá»¥ng?\nÄiá»u quan trá»ng lÃ  chÃºng ta cÃ³ ráº¥t nhiá»u channel vÃ  giá»¯a cÃ¡c channel nÃ y cÃ³ nhá»¯ng má»‘i liÃªn há»‡ nháº¥t Ä‘á»‹nh. Do Ä‘Ã³, viá»‡c máº¥t thÃ´ng tin á»Ÿ má»™t channel nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c channel khÃ¡c bÃ¹ Ä‘áº¯p. NhÆ° váº­y lÃ  ok. Äá»ƒ minh há»a cho yáº¿u tá»‘ lÃ m máº¥t thÃ´ng tin, cÃ¡c tÃ¡c giáº£ Ä‘Æ°a ra vÃ­ dá»¥ sau:\nBan Ä‘áº§u, input cá»§a ta á»Ÿ khÃ´ng gian 2 chiá»u. Qua phÃ©p biáº¿n Ä‘á»•i báº±ng má»™t ma tráº­n $T$ báº¥t kÃ¬ vÃ  Ã¡p dá»¥ng ReLU, ta cÃ³ cÃ¡c output á»Ÿ cÃ¡c khÃ´ng gian cÃ³ sá»‘ chiá»u khÃ¡c nhau lÃ  2, 3, 5, 15, 30. Äá»ƒ xÃ¡c Ä‘á»‹nh xem thÃ´ng tin cÃ³ bá»‹ máº¥t hay khÃ´ng, ta chiáº¿u cÃ¡c output nÃ y vá» láº¡i khÃ´ng gian 2 chiá»u báº±ng cÃ¡ch dÃ¹ng ma tráº­n nghá»‹ch Ä‘áº£o $T^{-1}$. Khi Ä‘Ã³, káº¿t quáº£ thu Ä‘Æ°á»£c lÃ  cÃ¡c hÃ¬nh tÆ°Æ¡ng á»©ng á»Ÿ trÃªn. RÃµ rÃ ng lÃ  tÃ­nh cháº¥t ban Ä‘áº§u cá»§a input Ä‘Ã£ bá»‹ máº¥t. Gá»‰a sá»­ tá»« má»™t input $D_F \\times D_F \\times M$, qua má»™t sá»‘ layer thÃ¬ ta cÃ³ output $D_F \\times D_F \\times N$ vÃ  ta chuáº©n bá»‹ Ã¡p dá»¥ng ReLU cho output. CÃ¡c tÃ¡c giáº£ chá»©ng minh Ä‘Æ°á»£c ráº±ng ReLU sáº½ khÃ´ng lÃ m máº¥t thÃ´ng tin ban Ä‘áº§u cá»§a input náº¿u nhÆ° $N \u0026lt; M$. Äiá»u nÃ y cÃ³ thá»ƒ phÃ¡t biá»ƒu báº±ng lá»i lÃ  náº¿u má»™t input cÃ³ thá»ƒ Ä‘Æ°á»£c embedded (hay lÃ  nÃ©n) vÃ o má»™t khÃ´ng gian Ã­t chiá»u hÆ¡n (sá»‘ channel Ã­t hÆ¡n) thÃ¬ viá»‡c Ã¡p dá»¥ng ReLU lÃªn káº¿t quáº£ nÃ©n Ä‘Ã³ sáº½ khÃ´ng lÃ m máº¥t thÃ´ng tin.\ná» vÃ­ dá»¥ phÃ­a trÃªn thÃ¬ ta Ä‘Ã£ cÃ³ $N \\geq M$ vÃ  thÃ´ng tin tháº­t sá»± lÃ  Ä‘Ã£ bá»‹ máº¥t. Tá»« nháº­n xÃ©t trÃªn, ta tháº¥y ráº±ng khÃ´ng pháº£i lÃºc nÃ o xÃ i ReLU cÅ©ng tá»‘t. Náº¿u ngáº«m láº¡i, trong cÃ¡c kiáº¿n trÃºc nhÆ° VGG, ResNet, Inception, MobileNet thÃ¬ sá»‘ channel cá»§a chÃºng háº§u nhÆ° luÃ´n tÄƒng qua tá»«ng block (chÃ­nh lÃ  cá»¥m â€œmá»™t sá»‘ layerâ€) nhÆ°ng activation function Ä‘Æ°á»£c sá»­ dá»¥ng luÃ´n lÃ  ReLU. Äiá»u nÃ y lÃ  vÃ¬ chÃºng cÃ³ ráº¥t nhiá»u channel (tÄƒng theo bá»™i 2) nÃªn má»i thá»© váº«n á»•n ğŸ˜€\nNáº¿u báº¡n tháº¯c máº¯c lÃ  vÃ¬ sao sá»‘ channel thÆ°á»ng tÄƒng nhÆ° váº­y thÃ¬ trong CNN, nhá»¯ng conv layer Ä‘áº§u thÆ°á»ng sáº½ há»c nhá»¯ng Ä‘áº·c trÆ°ng Ä‘Æ¡n giáº£n nhÆ° cáº¡nh ngang, dá»c, chÃ©o, vá»‹ trÃ­ cá»§a Ä‘á»‘i tÆ°á»£ng trong áº£nh,â€¦ cÃ ng vá» sau thÃ¬ sáº½ cÃ³ cÃ¡c Ä‘áº·c trÆ°ng cá»¥ thá»ƒ, ná»•i báº­t lÃªn cá»§a Ä‘á»‘i tÆ°á»£ng (vÃ­ dá»¥ nhÆ° tai mÃ¨o, máº¯t mÃ¨o, mÅ©i mÃ¨o,â€¦). Do Ä‘Ã³, cÃ ng vá» sau thÃ¬ ta nÃªn cÃ³ cÃ ng nhiá»u channel Ä‘á»… há»c Ä‘Æ°á»£c nhiá»u Ä‘áº·c trÆ°ng. Linear bottleneck Äáº§u tiÃªn, ta sáº½ nháº¯c láº¡i vá» bottleneck layer. ÄÃ¢y lÃ  dáº¡ng layer thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng vá»›i má»¥c Ä‘Ã­ch lÃ  â€œcÃ´ Ä‘á»ng kiáº¿n thá»©câ€ cá»§a mÃ´ hÃ¬nh, hay nÃ³i rÃµ hÆ¡n lÃ  nÃ©n lÆ°á»£ng thÃ´ng tin láº¡i sao cho vá»«a giá»¯ Ä‘Æ°á»£c thÃ´ng tin vÃ  vá»«a tiáº¿t kiá»‡m tÃ i nguyÃªn (bá»™ nhá»›, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n).\nNhá»¯ng phÃ¡t hiá»‡n vá» ReLU nhÆ° Ä‘Ã£ Ä‘á» cáº­p lÃ  nguá»“n gá»‘c cá»§a cÃ¡c linear bottleneck Ä‘Æ°á»£c sá»­ dá»¥ng trong MobileNetV2.\nTáº¡i sao láº¡i lÃ  linear mÃ  khÃ´ng tiáº¿p tá»¥c dÃ¹ng ReLU rá»“i tÄƒng sá»‘ channel nhÆ° nhá»¯ng block trÆ°á»›c?\nViá»‡c giáº£m sá»‘ channel trong cÃ¡c layer sáº½ giáº£m lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh, tá»« Ä‘Ã³ giáº£m Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n. Do Ä‘Ã³, náº¿u xÃ¢y dá»±ng Ä‘Æ°á»£c má»™t kiáº¿n trÃºc mÃ  sá»‘ lÆ°á»£ng channel trong má»—i layer lÃ  nhá» thÃ¬ nÃ³ sáº½ ráº¥t phÃ¹ há»£p cho cÃ¡c thiáº¿t bá»‹ biÃªn. CÃ¡c tÃ¡c giáº£ hÆ°á»›ng Ä‘áº¿n viá»‡c giá»¯ cho sá»‘ channel cá»§a input vÃ  output cá»§a cÃ¡c block lÃ  nhá», tá»©c lÃ  sá»‘ channel chÆ°a cháº¯c Ä‘á»§ nhiá»u Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng ReLU khÃ´ng lÃ m máº¥t thÃ´ng tin ğŸ˜œ.\nNáº¿u mÃ  toÃ n cÃ¡c output cÃ³ channel nhá» nhÆ° váº­y thÃ¬ lÃ m sao mÃ  mÃ´ hÃ¬nh Ä‘áº¡t hiá»‡u quáº£ Ä‘Æ°á»£c? LÃ­ do lÃ  vÃ¬ nÃ³ lÃ  output cá»§a cÃ¡c bottleneck nÃªn váº«n ok ğŸ˜€ Sá»‘ channel trong cÃ¡c layer cá»§a MobileNetV2\nNguá»“n: Machine Think Inverted residual block vÃ  expansion factor Inverted residual block lÃ  thÃ nh pháº§n chÃ­nh xÃ¢y dá»±ng nÃªn MobileNetV2. Trong block nÃ y, ta sáº½ Ã¡p dá»¥ng cáº£ dethwise separable convolution, linear bottleneck vÃ  skip connection.\nTuy nhiÃªn, lÆ°u Ã½ ráº±ng sá»‘ channel cá»§a input vÃ  output cá»§a cÃ¡c block nÃ y lÃ  ráº¥t nhá». Qua hÃ¬nh á»Ÿ trÃªn thÃ¬ ta tháº¥y chÃºng chá»‰ quanh quáº©n 16, 24, 32. Do Ä‘Ã³, trÆ°á»›c khi Ã¡p dá»¥ng depthwise separable convolution lÃªn input thÃ¬ cÃ¡c tÃ¡c giáº£ thá»±c hiá»‡n giáº£i nÃ©n (expansion) lÆ°á»£ng kiáº¿n thá»©c trong input (input nÃ y lÃ  output cá»§a má»™t block trÆ°á»›c Ä‘Ã³, nÆ¡i mÃ  kiáº¿n thá»©c Ä‘Ã£ Ä‘Æ°á»£c nÃ©n láº¡i bá»Ÿi linear bottleneck).\nVá» máº·t trá»±c giÃ¡c, lÃ­ do cá»§a thao tÃ¡c nÃ y cÃ³ thá»ƒ hiá»ƒu lÃ  ta sáº½ thá»±c hiá»‡n convolution trÃªn thÃ´ng tin Ä‘áº§y Ä‘á»§ hÆ¡n Ä‘á»ƒ cÃ³ thá»ƒ phÃ¡t hiá»‡n Ä‘Æ°á»£c cÃ ng nhiá»u Ä‘áº·c trÆ°ng cÃ ng tá»‘t).\nViá»‡c giáº£i nÃ©n thá»±c cháº¥t lÃ  ta sá»­ dá»¥ng má»™t conv layer $1 \\times 1$, vá»›i sá»‘ lÆ°á»£ng filter sáº½ báº±ng vá»›i sá»‘ lÆ°á»£ng channel cá»§a input nhÃ¢n vá»›i má»™t siÃªu tham sá»‘. SiÃªu tham sá»‘ nÃ y gá»i lÃ  expansion factor vÃ  Ä‘Æ°á»£c kÃ­ hiá»‡u lÃ  $t$.\nTiáº¿p Ä‘áº¿n, theo sau phÃ©p toÃ¡n depthwise separable convolution thÃ¬ ta sáº½ sá»­ dá»¥ng linear bottleneck Ä‘á»ƒ tÃ­nh ra output cá»§a block.\nNhÆ° váº­y, Ä‘iá»ƒm qua cÃ¡c layer sáº½ cÃ³ trong inverted residual block sáº½ bao gá»“m:\nLÆ°u Ã½. Layer â€œlinear 1x1 conv2dâ€ chÃ­nh lÃ  linear bottleneck. TÃ¹y theo giÃ¡ trá»‹ sá»‘ channel $k$ vÃ  $k\u0026rsquo;$ cÃ³ báº±ng nhau hay khÃ´ng mÃ  ta sáº½ Ã¡p dá»¥ng thÃªm skip connection. HÆ¡n ná»¯a, cÃ¡c skip connection trong inverted residual block Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ ná»‘i cÃ¡c bottleneck layer vá»›i nhau!\nVÃŒ sao láº¡i lÃ  ná»‘i bottleneck chá»© khÃ´ng pháº£i ná»‘i cÃ¡c layer khÃ¡c? Output cá»§a bottleneck lÃ  cÃ¡c â€œkiáº¿n thá»©câ€ Ä‘Ã£ Ä‘Æ°á»£c cÃ´ Ä‘á»ng, Ä‘Ã¢y lÃ  nhá»¯ng gÃ¬ mÃ  mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c vÃ  Ä‘Æ°á»£c biá»ƒu diá»…n trong má»™t khÃ´ng gian Ã­t chiá»u hÆ¡n. Do Ä‘Ã³, ta vá»«a tiáº¿t kiá»‡m Ä‘Æ°á»£c tÃ i nguyÃªn vÃ  vá»«a liÃªn káº¿t Ä‘Æ°á»£c cÃ¡c kiáº¿n thá»©c quan trá»ng vá»›i nhau. Trong cÃ i Ä‘áº·t, tÃ¹y theo giÃ¡ trá»‹ stride cá»§a depthwise conv layer mÃ  ta sáº½ Ã¡p dá»¥ng skip connection hoáº·c lÃ  khÃ´ng. Cá»¥ thá»ƒ nhÆ° sau:\nInverted Residual Block vá»›i stride=1 (cÃ³ skip connection) vÃ  stride=2 (khÃ´ng cÃ³) NgoÃ i ra, ta cÃ³ thá»ƒ tháº¥y trong inverted residual block thÃ¬ activation Ä‘Æ°á»£c dÃ¹ng cho 2 layer Ä‘áº§u tiÃªn lÃ  ReLU6. ÄÃ¢y lÃ  má»™t biáº¿n thá»ƒ cá»§a ReLU, nÃ³ giá»›i háº¡n giÃ¡ trá»‹ output náº±m trong Ä‘oáº¡n $[0, 6]$ nháº±m Ä‘áº£m báº£o sá»± á»•n Ä‘á»‹nh trong tÃ­nh toÃ¡n vá»›i sá»‘ cháº­m Ä‘á»™ng.\nActivation function RELU6\nNguá»“n: Mmuratarat Äá»ƒ dá»… hÃ¬nh dung hÆ¡n vá» inverted residual block, ta cÃ¹ng xem má»™t vÃ­ dá»¥ cho quÃ¡ trÃ¬nh tÃ­nh toÃ¡n vá»›i expansion factor lÃ  6:\nNguá»“n: Machine Think LÆ°u Ã½.\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p thÃªm Ä‘áº¿n luá»“ng truyá»n thÃ´ng tin cá»§a MobileNetV2, yáº¿u tá»‘ má»Ÿ ra nhá»¯ng hÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p theo trong tÆ°Æ¡ng lai. Ta tháº¥y ráº±ng inverted residual block Ä‘Ã£ táº¡o ra Ä‘Æ°á»£c sá»± Ä‘á»™c láº­p giá»¯a sá»‘ channel cá»§a intput/output cá»§a block vÃ  cá»§a cÃ¡c layer náº±m bÃªn trong block:\nPháº§n bÃªn trong Ä‘Æ°á»£c gá»i lÃ  layer transformation vá»›i nhá»¯ng phÃ©p biáº¿n Ä‘á»•i phi tuyáº¿n. Ta hoÃ n toÃ n cÃ³ thá»ƒ nghiÃªn cá»©u thÃªm nhá»¯ng cÃ¡ch xÃ¢y dá»±ng bá»™ pháº­n nÃ y Ä‘á»ƒ tÄƒng Ä‘á»™ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh. Náº¿u expansion factor cá»§a ta \u0026lt; 1 thÃ¬ block nÃ y sáº½ ráº¥t giá»‘ng vá»›i block trong ResNet:\nKiáº¿n trÃºc MobileNetV2 MobileNetV2 Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn viá»‡c sá»­ dá»¥ng nhiá»u inverted residual block. Kiáº¿n trÃºc tá»•ng quan cá»§a nÃ³ nhÆ° sau:\nTrong Ä‘Ã³:\n$t$ lÃ  expansion factor. $c$ lÃ  sá»‘ output channel cá»§a pháº§n bottleneck trong inverted residual block. $n$ lÃ  sá»‘ láº§n sá»­ dá»¥ng block. $s$ lÃ  stride cá»§a block Ä‘áº§u tiÃªn trong dÃ£y $t$ block liÃªn tiáº¿p nhau, cÃ¡c block cÃ²n láº¡i trong dÃ£y cÃ³ stride 1. ToÃ n bá»™ filter Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»u lÃ  $3 \\times 3$. CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t MobileNetV2 báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nTÃ i liá»‡u tham kháº£o Paper MobileNetV2: https://arxiv.org/abs/1801.04381 MachineThink, MobileNet version 2 ","date":"2023-02-13T11:32:55+07:00","permalink":"https://htrvu.github.io/post/mobilenet_v2/","title":"MobileNet V2 (2019)"},{"content":"Skip connection vÃ  concatenate TrÆ°á»›c Ä‘Ã³, kiáº¿n trÃºc ResNet Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ  nÃ³ Ä‘Ã£ cho tháº¥y Ä‘Æ°á»£c sá»©c máº¡nh cá»§a cÃ¡c skip connection khi chÃºng Ä‘Æ°á»£c thÃªm vÃ o cÃ¡c mÃ´ hÃ¬nh tá»« sÃ¢u cho Ä‘áº¿n ráº¥t sÃ¢u (vÃ­ dá»¥ nhÆ° ResNet152). Ta tháº¥y ráº±ng nhá»¯ng kiáº¿n trÃºc Ã¡p dá»¥ng skip connection trÆ°á»›c Ä‘Ã¢y Ä‘á»u cÃ³ má»™t Ä‘iá»ƒm chung lÃ  trong má»™t block thÃ¬ ta sáº½ cÃ³ nhá»¯ng Ä‘iá»ƒm ná»‘i 1 feature map vÃ o lÃ m input cá»§a má»™t layer sau Ä‘Ã³, vÃ  chÃºng Ä‘á»u sá»­ dá»¥ng phÃ©p toÃ¡n cá»™ng.\nResidual block trong ResNet sá»­ dá»¥ng skip connection vá»›i phÃ©p toÃ¡n cá»™ng Nguá»“n: Idiot Developer CÃ´ng thá»©c vá» skip connection trong block trÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ° sau:\n$$ x_l = H_l(x_{l - 1}) + x_{l-1} $$\n, vá»›i $H_l$ lÃ  phÃ©p biáº¿n Ä‘á»•i phi tuyáº¿n á»Ÿ layer thá»© $l$, $x_l$ lÃ  output cá»§a layer thá»© $l$.\nPaper DenseNet giá»›i thiá»‡u má»™t kiáº¿n trÃºc vá»›i Ã½ tÆ°á»Ÿng lÃ  feature-map táº¡i layer $l$ sáº½ sá»­ dá»¥ng toÃ n bá»™ feature-maps á»Ÿ phÃ­a trÆ°á»›c (layer $l - 1, l - 2,\u0026hellip;$) Ä‘á»ƒ lÃ m input, vÃ  chÃºng sá»­ dá»¥ng concatenate (thay vÃ¬ phÃ©p toÃ¡n cá»™ng nhÆ° ResNet). Vá»›i tÆ° tÆ°á»Ÿng nhÆ° váº­y, cÃ¡c feature-maps ta cÃ³ Ä‘Æ°á»£c cÃ³ thá»ƒ xem lÃ  má»™t tráº¡ng thÃ¡i cÃ³ pháº¡m vi toÃ n cá»¥c vÃ  báº¥t kÃ¬ layer nÃ o cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng tráº¡ng thÃ¡i nÃ y trong viá»‡c tÃ­nh toÃ¡n ra feature-maps cá»§a nÃ³. Náº¿u viáº¿t theo kiá»ƒu cÃ´ng thá»©c thÃ¬ ta sáº½ cÃ³\n$$ x_l = H_l([x_0, x_1,\u0026hellip;, x_{l-1}]) $$\nLÆ°u Ã½. Äá»ƒ thá»±c hiá»‡n phÃ©p toÃ¡n concatenate thÃ¬ cÃ¡c feature-maps pháº£i cÃ³ cÃ¹ng size, hay lÃ  width vÃ  height.\nMá»™t vÃ­ dá»¥ cho kiáº¿n trÃºc DenseNet nhÆ° sau:\nTrong hÃ¬nh trÃªn, vá»›i $L$ layer, ta cÃ³ $\\dfrac{L(L+1)}{2}$ káº¿t ná»‘i trá»±c tiáº¿p giá»¯a cÃ¡c layer. CÃ¡c káº¿t ná»‘i Ä‘Æ°á»£c táº¡o ra lÃ  ráº¥t dÃ y Ä‘áº·c (dense). Tá»« Ä‘Ã³, tÃªn cá»§a kiáº¿n trÃºc Ä‘Æ°á»£c Ä‘áº·t lÃ  Dense Convolutional Network (DenseNet). BÃ n vá» cÃ¡ch tá»• chá»©c cÃ¡c liÃªn káº¿t nhÆ° váº­y má»™t chÃºt:\nNhÃ³m tÃ¡c giáº£ cho ráº±ng kiáº¿n trÃºc nhÆ° DenseNet sáº½ Ä‘áº£m báº£o lÆ°á»£ng thÃ´ng tin cÅ©ng nhÆ° gradient truyá»n qua cÃ¡c layer lÃ  nhiá»u nháº¥t cÃ³ thá»ƒ , tá»« Ä‘Ã³ mÃ´ hÃ¬nh sáº½ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c tá»« nhiá»u thÃ´ng tin hÆ¡n, vÃ  táº¥t nhiÃªn lÃ  nÃ³ sáº½ táº¡o ra hiá»‡u á»©ng lÃ m dá»‹u bá»›t hiá»‡n tÆ°á»£ng vanishing gradient.\nÄá»“ng thá»i, viá»‡c sá»­ phÃ©p toÃ¡n concatenate cÃ³ mang Ä‘áº¿n cho ta trá»±c giÃ¡c lÃ  cÃ³ sá»± phÃ¢n biá»‡t rÃµ hÆ¡n giá»¯a input trá»±c tiáº¿p tá»« layer á»Ÿ ngay phÃ­a trÆ°á»›c nÃ³ vá»›i cÃ¡c thÃ´ng tin Ä‘Æ°á»£c â€œlÆ°u trá»¯â€ vÃ  truyá»n Ä‘áº¿n tá»« cÃ¡c layer á»Ÿ phÃ­a trÆ°á»›c ná»¯a. Náº¿u sá»­ dá»¥ng phÃ©p toÃ¡n cá»™ng, nhá»¯ng yáº¿u tá»‘ nÃ y Ä‘Ã£ bá»‹ pha láº«n vÃ o nhau.\nCÃ³ má»™t chi tiáº¿t mÃ  ta thÆ°á»ng nghÄ© Ä‘áº¿n á»Ÿ cÃ¡c mÃ´ hÃ¬nh cÃ³ kiáº¿n trÃºc ráº¥t sÃ¢u (nhiá»u layer) lÃ  sá»‘ lÆ°á»£ng tham sá»‘ cá»§a nÃ³ sáº½ ráº¥t lá»›n. Tuy nhiÃªn, vá»›i DenseNet thÃ¬ Ä‘iá»u nÃ y khÃ´ng pháº£i lÃ  váº¥n Ä‘á». Sá»‘ feature-maps cá»§a cÃ¡c layer trong DenseNet sáº½ ráº¥t nhá» (chá»‰ táº§m khÃ´ng quÃ¡ 60), vá»›i lÃ½ do lÃ  Ä‘á»ƒ tÃ­nh toÃ¡n cho layer káº¿ tiáº¿p thÃ¬ ta Ä‘Ã£ dÃ¹ng toÃ n bá»™ feature-maps á»Ÿ cÃ¡c phÃ­a trÆ°á»›c rá»“i chá»© khÃ´ng pháº£i chá»‰ má»—i layer liá»n trÆ°á»›c nÃ³ nhÆ° háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh khÃ¡c, nÃªn táº¡i má»—i layer ta chá»‰ cáº§n táº§m Ä‘Ã³ lÃ  Ä‘á»§ rá»“i ğŸ˜€\nKáº¿t quáº£ so sÃ¡nh giá»¯a DenseNet vÃ  ResNet trÃªn dataste ImageNet Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ cÃ´ng bá»‘ nhÆ° hÃ¬nh bÃªn dÆ°á»›i. Ta tháº¥y ráº±ng DenseNet cÃ³ sá»‘ lÆ°á»£ng tham sá»‘ vÃ  sá»‘ phÃ©p toÃ¡n Ã­t hÆ¡n ResNet, cÃ¹ng vá»›i Ä‘á»™ hiá»‡u quáº£ cao hÆ¡n.\nDense block, transition layer vÃ  growth rate Dense block vÃ  transition layer Ta tháº¥y ráº±ng náº¿u Ã¡p dá»¥ng Ã½ tÆ°á»Ÿng káº¿t ná»‘i dÃ y Ä‘áº·t cá»§a DenseNet cho toÃ n bá»™ layer trong mÃ´ hÃ¬nh thÃ¬ toÃ n bá»™ feature-maps trong táº¥t cáº£ layer nÃ y Ä‘á»u pháº£i cÃ³ cÃ¹ng size (do phÃ©p toÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  concatnerate).\nTuy nhiÃªn, náº¿u toÃ n bá»™ cÃ¡c layer trong kiáº¿n trÃºc Ä‘á»u cÃ³ cÃ¹ng size nhÆ° váº­y thÃ¬ ta khÃ³ mÃ  down-sampling feature-maps vá» cÃ¡c size nhá» hÆ¡n vÃ  rá»“i sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c layer nhÆ° Average Pooling, Dense Ä‘á»ƒ cho ra output nhÆ° cÃ¡c kiáº¿n trÃºc khÃ¡c Ä‘Æ°á»£c. VÃ  viá»‡c â€œcÃ´ Ä‘á»ngâ€ kiáº¿n thá»©c cá»§a mÃ´ hÃ¬nh cÅ©ng sáº½ gáº·p khÃ³ khÄƒn.\nDo Ä‘Ã³, Ã½ tÆ°á»Ÿng káº¿t ná»‘i dÃ y Ä‘áº·t Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ Ã¡p dá»¥ng trong tá»«ng khá»‘i (gá»i lÃ  Dense block), viá»‡c down-sampling sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n trong cÃ¡c khá»›p ná»‘i cÃ¡c Dense block vá»›i nhau (gá»i lÃ  Transition layer).\nCÃ³ tá»•ng cá»™ng 4 dáº¡ng Dense block nhÆ° sau:\nDense block cÆ¡ báº£n:\nHÃ m $H_l$ trong block nÃ y lÃ  sá»± káº¿t há»£p theo thá»© tá»± 3 phÃ©p toÃ¡n: $$ BN \\to ReLU \\to Conv (3 \\times 3) $$\nNgoÃ i ra, ta cÃ³ thá»ƒ thÃªm dropout vÃ o sau Conv Ä‘á»ƒ giáº£m overfitting.\nDense-B block (bottleneck):\nÄá»ƒ tÄƒng hiá»‡u suáº¥t vá» máº·t tÃ­nh toÃ¡n, ta cÃ³ thá»ƒ thÃªm má»™t phÃ©p toÃ¡n Conv $1 \\times 1$ vÃ o $H_l$ Ä‘á»ƒ giáº£m bá»›t sá»‘ lÆ°á»£ng feature-maps input. LÃºc nÃ y, thá»© tá»± cÃ¡c phÃ©p toÃ¡n sáº½ lÃ  $$ BN \\to ReLU \\to Conv (1 \\times 1) \\to BN \\to ReLU \\to Conv (3 \\times 3) $$\nDense-C block (compression):\nTa sáº½ giáº£m sá»‘ lÆ°á»£ng output feature-maps cá»§a cÃ¡c Dense block theo tham sá»‘ $0 \u0026lt; \\theta \\leq 1$: tá»« $m$ feature-maps thÃ nh $\\lfloor \\theta m \\rfloor$ ThÃ´ng thÆ°á»ng, pháº§n cÃ i Ä‘áº·t cá»§a thao tÃ¡c compression Ä‘Æ°á»£c ghÃ©p vÃ o transition layer. Dense-BC block:\nKáº¿t há»£p bottleneck vÃ  compression vÃ o Dense block. Trong kiáº¿n trÃºc tá»•ng thá»ƒ, náº¿u trÆ°á»›c Ä‘Ã³ ta dÃ¹ng Dense-C hoáº·c Dense-BC block thÃ¬ theo sau nÃ³ sáº½ cÃ³ thÃªm layer bottleneck (Conv $1 \\times 1) vÃ  thÃ nh pháº§n nÃ y gá»i lÃ  transition layer. BÃªn cáº¡nh conv layer, thÃ nh pháº§n khÃ´ng thá»ƒ thiáº¿u trong transition layer lÃ  má»™t lá»›p Pooling (cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng Average Pooling) Ä‘á»ƒ thá»±c hiá»‡n down-sampling cÃ¡c feature-maps. Thá»© tá»± cÃ¡c phÃ©p toÃ¡n trong transition layer sáº½ lÃ \n$$ BN \\to ReLU \\to Conv (1 \\times 1) \\to AvgPool (2 \\times 2) $$\nGrowth rate NhÆ° Ä‘Ã£ Ä‘á» cáº­p á»Ÿ pháº§n Ã½ tÆ°á»Ÿng, lÆ°á»£ng tham sá»‘ trong DenseNet Ä‘Æ°á»£c tá»‘i thiá»ƒu hÃ³a lÃ  nhá» vÃ o chi tiáº¿t sá»‘ lÆ°á»£ng feature-maps táº¡i cÃ¡c layer trong DenseNet lÃ  nhá». CÃ¡c tÃ¡c giáº£ xem sá»‘ lÆ°á»£ng feature-maps $k$ táº¡i cÃ¡c layer lÃ  má»™t siÃªu tham sá»‘ cá»§a DenseNet, vÃ  nÃ³ Ä‘Æ°á»£c gá»i lÃ  growth rate.\nThá»±c nghiá»‡m cho tháº¥y ráº±ng cÃ¡c giÃ¡ trá»‹ $k$ mang láº¡i káº¿t quáº£ tá»‘t trÃªn cÃ¡c dataset thÆ°á»ng khÃ´ng quÃ¡ lá»›n. Vá» máº·t trá»±c giÃ¡c, ta cÃ³ thá»ƒ hiá»ƒu $k$ Ä‘iá»u chá»‰nh lÆ°á»£ng thÃ´ng tin má»›i mÃ  má»™t layer cÃ³ thá»ƒ Ä‘Ã³ng gÃ³p vÃ o tráº¡ng thÃ¡i toÃ n cá»¥c (Ä‘Ã³ng gÃ³p má»™t lÆ°á»£ng vá»«a Ä‘á»§ thÃ¬ sáº½ tá»‘t hÆ¡n lÃ  quÃ¡ nhiá»u hay quÃ¡ Ã­t).\nMinh há»a Dense Block vá»›i growth rate lÃ  4 Nguá»“n: https://reliablecho-programming.tistory.com/3 Kiáº¿n trÃºc DenseNet TÃ¹y vÃ o loáº¡i Dense block Ä‘Æ°á»£c sá»­ dá»¥ng, ta cÅ©ng cÃ³ cÃ¡c tÃªn gá»i khÃ¡c nhau cho DenseNet (DenseNet, DenseNet-B, Denset-C, DenseNet-BC). Kiáº¿n trÃºc DenseNet-C (hoáº·c DenseNet-BC) vá»›i 3 Dense block Ä‘Æ°á»£c mÃ´ táº£ trong hÃ¬nh bÃªn dÆ°á»›i: TrÆ°á»›c khi Ä‘áº¿n vá»›i quÃ¡ trÃ¬nh tÃ­nh toÃ¡n qua cÃ¡c Dense block vÃ  Transition layer, ta cÃ³ má»™t layer Conv (vÃ  cÃ³ thá»ƒ cÃ³ thÃªm Pooling, BN) nhÆ° Ä‘a sá»‘ cÃ¡c kiáº¿n trÃºc CNN khÃ¡c. CÃ¡c layer cá»§a cá»§a mÃ´ hÃ¬nh cÅ©ng sá»­ dá»¥ng Global Pooling vÃ  Dense cÃ¹ng activation softmax Ä‘á»ƒ táº¡o ra vector output. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c nhÃ³m tÃ¡c giáº£ thá»­ nghiá»‡m vá»›i dataset ImageNet Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:\nQuan sÃ¡t báº£ng trÃªn, ta cÃ³ nháº­n xÃ©t lÃ  cÃ¡c mÃ´ hÃ¬nh trÃªn Ä‘á»u thuá»™c loáº¡i DenseNet-BC. NgoÃ i ra, cÃ¡c tÃ¡c giáº£ cho biáº¿t giÃ¡ trá»‹ growth rate Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  $k=32$.\nCÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t DenseNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nKhi cÃ i Ä‘áº·t DenseNet, ta thÆ°á»ng sáº½ hÆ¡i phÃ¢n vÃ¢n vá» cÃ¡ch cÃ i Ä‘áº·t cÃ¡c Dense block. LÃ m sao Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡c káº¿t ná»‘i dÃ y Ä‘áº·c nhÆ° váº­y?\nThá»±c ra cÃ¡ch cÃ i Ä‘áº·t lÃ  ráº¥t Ä‘Æ¡n giáº£n. Ta gá»i khá»‘i gá»“m (Conv $1 \\times 1$, Conv $3 \\times 3$) nhÆ° báº£ng trÃªn lÃ  bottleneck block (bb). Khi Ä‘Ã³ 1 dense block vá»›i 4 bottleneck block sáº½ cÃ³ dáº¡ng nhÆ° sau:\n$$x \\to bb_1 \\to x_1 \\to bb_2 \\to x_2 \\to bb_3 \\to x_3 \\to bb_4 \\to x_4 (output) $$\nTrong Ä‘Ã³:\nbb_1.input = [ x ] bb_2.input = [x1, x] bb_3.input = [x2, x1, x] bb_4.input = [x3, x2, x1, x] MÃ£ giáº£ cho cÃ¡ch cÃ i Ä‘áº·t dense block nÃ y nhÆ° sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def bottleneck_block(input, k): x = BN(input) x = ReLU(x)\tx = conv_1x1(x) x = BN(x) x = ReLU(x) x = conv_3x3(x) return x def dense_block(input, k): c = input x1 = bottleneck_block(c, k) c = concatenate(x1, c) # c = [x1, input] x2 = bottleneck_block(c, k) c = concatenate(x2, c) # c = [x2, x1, input] x3 = bottleneck_block(c, k) c = concatenate(x3, c) # c = [x3, x2, x1, input] x4 = bottleneck_block(c, k) c = concatenate(x4, c) # c = [x4, x3, x2, x1, input] return c # done! NhÆ° váº­y, ta hoÃ n toÃ n cÃ³ thá»ƒ dÃ¹ng má»™t vÃ²ng láº·p Ä‘á»ƒ cÃ i Ä‘áº·t dense block:\n1 2 3 4 5 6 def dense_block(input, k): c = input for i in range(4): x = bottleneck_block(c, k) c = concatenate(x, c) return c TÃ i liá»‡u tham kháº£o Paper DenseNet: https://arxiv.org/abs/1608.06993 ","date":"2023-02-11T18:09:08+07:00","permalink":"https://htrvu.github.io/post/densenet/","title":"DenseNet (2018)"},{"content":"Giá»›i thiá»‡u Class Activation Map (CAM) lÃ  phÆ°Æ¡ng phÃ¡p phá»• biáº¿n trong viá»‡c giáº£i thÃ­ch sá»± hoáº¡t Ä‘á»™ng cá»§a CNN. NÃ³ cho ta biáº¿t ráº±ng CNN sáº½ táº­p trung vÃ o nhá»¯ng pháº§n nÃ o cá»§a áº£nh input Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t áº£nh Ä‘Ã³ thá»¥Ã´c vá» má»™t class nÃ o Ä‘Ã³. ThÃ´ng thÆ°á»ng, CAM cÃ²n Ä‘Æ°á»£c gá»i lÃ  Attention Map.\nÄá»ƒ dá»… hÃ¬nh dung hÆ¡n vá» CAM, ta cÃ³ 2 vÃ­ dá»¥ nhÆ° sau:\nCNN táº­p trung vÃ o pháº§n Ä‘áº§u cá»§a con chÃ³ Ä‘á»ƒ Ä‘Æ°a ra xÃ¡c suáº¥t mÃ  bá»©c áº£nh thuá»™c class â€œdogâ€ CNN táº­p trung vÃ o pháº§n Ä‘áº§u cá»§a con chÃ³ Ä‘á»ƒ Ä‘Æ°a ra xÃ¡c suáº¥t mÃ  bá»©c áº£nh thuá»™c class â€œdogâ€\nCNN táº­p trung vÃ o con mÃ¨o khi Ä‘Æ°a ra xÃ¡c suáº¥t mÃ  bá»©c áº£nh thuá»™c class â€œcatâ€\nNguá»“n: GlassBoxMedicine CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c xáº¿p vÃ o nhÃ³m post-hoc, tá»©c lÃ  ta chá»‰ tiáº¿n hÃ nh sinh ra CAM Ä‘á»ƒ giáº£i thÃ­ch sá»± hoáº¡t Ä‘á»™ng cá»§a CNN sau khi mÃ´ hÃ¬nh nÃ y Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  cÃ³ má»™t bá»™ trá»ng sá»‘ cá»‘ Ä‘á»‹nh.\nViá»‡c giáº£i thÃ­ch CNN báº±ng CAM lÃ  ráº¥t há»£p lÃ½, vÃ¬:\nTa cÃ³ thá»ƒ biáº¿t Ä‘Æ°á»£c mÃ´ hÃ¬nh cá»§a mÃ¬nh cÃ³ Ä‘ang tháº­t sá»± hoáº¡t Ä‘á»™ng tá»‘t hay khÃ´ng (táº­p trung vÃ o Ä‘Ãºng pháº§n quan trá»ng trong áº£nh), tá»©c lÃ  cÃ³ chá»©ng cá»© rÃµ rÃ ng cho cÃ¡c dá»± Ä‘oÃ¡n NÃ³ giÃºp ta ká»‹p thá»i phÃ¡t hiá»‡n nhá»¯ng Ä‘áº·c trÆ°ng mÃ  mÃ´ hÃ¬nh â€œhiá»ƒu láº§mâ€ khi há»c vá» má»™t Ä‘á»‘i tÆ°á»£ng nÃ o Ä‘Ã³. VÃ­ dá»¥, nhá» CAM thÃ¬ ta tháº¥y ráº±ng mÃ´ hÃ¬nh há»c cÃ¡ch nháº­n dáº¡ng tÃ u há»a dá»±a vÃ o cÃ¡c Ä‘Æ°á»ng ray trong áº£nh thÃ¬ rÃµ rÃ ng lÃ  nÃ³ Ä‘Ã£ há»c sai Ä‘áº·c trÆ°ng. TrÆ°á»ng há»£p nÃ y hoÃ n toÃ n cÃ³ thá»ƒ xáº£y ra vÃ¬ pháº§n lá»›n bá»©c áº£nh cÃ³ tÃ u há»a thÃ¬ cÅ©ng cÃ³ Ä‘Æ°á»ng ray, nhÆ°ng ngÆ°á»£c láº¡i thÃ¬ khÃ´ng. Ta cÃ³ thá»ƒ cÃ³ má»™t chiáº¿c xe Ã´ tÃ´ cháº¡y ngang Ä‘Æ°á»ng ray ğŸ˜€ Trong cÃ¡c phÆ°Æ¡ng phÃ¡p sinh ra CAM cho má»™t CNN (theo tá»«ng input) thÃ¬ ta cÃ³ cÃ¡c phÆ°Æ¡ng phÃ¡p ná»•i báº­t nhÆ° Default CAM, Grad-CAM, Score-CAM. Ta sáº½ láº§n lÆ°á»£t Ä‘á» cáº­p Ä‘áº¿n cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã³.\nCÃ¡c phÆ°Æ¡ng phÃ¡p sinh CAM Default CAM (2016) PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c Ä‘á» xuáº¥t ngay tá»« khi cÃ¡c Ã½ tÆ°á»Ÿng vá» CAM Ä‘Æ°á»£c cÃ´ng bá»‘. Ta sáº½ dá»±a vÃ o output cá»§a conv layer cuá»‘i cÃ¹ng trong kiáº¿n trÃºc, ngay trÆ°á»›c fully connected layer sinh ra output cá»§a mÃ´ hÃ¬nh vÃ  cÃ¡c trá»ng sá»‘ trong output layer.\nÄáº§u tiÃªn, Ä‘á»ƒ mÃ´ táº£ vá» Ã½ nghÄ©a cá»§a cÃ¡c feature maps trong output cá»§a conv layer cuá»‘i cÃ¹ng thÃ¬ ta xÃ©t vÃ­ dá»¥ sau: Trong hÃ¬nh áº£nh bÃªn dÆ°á»›i, conv layer cuá»‘i cÃ¹ng cá»§a ta lÃ  â€œConv Layer nâ€. Output cá»§a nÃ³ cÃ³ $k$ channel, hay lÃ  $k$ feature maps, má»—i feature map sáº½ liÃªn quan Ä‘áº¿n má»™t Ä‘áº·c trÆ°ng nÃ o Ä‘Ã³ trong áº£nh input. Giáº£ sá»­ nhÆ° feature map $1$ sáº½ phÃ¡t hiá»‡n máº·t ngÆ°á»i trong áº£nh, feature map $2$ sáº½ phÃ¡t phiá»‡n lÃ´ng cá»§a con chÃ³,â€¦, feature map $k$ sáº½ phÃ¡t hiá»‡n tai cá»§a con chÃ³. Minh há»a Ã½ nghÄ©a cÃ¡c feature maps Nguá»“n: Johfischer CÃ¡c trá»ng sá»‘ trong output layer sáº½ mang Ã½ nghÄ©a lÃ  táº§m quan trá»ng cá»§a feature map tÆ°Æ¡ng á»©ng vÆ¡i trá»ng sá»‘ Ä‘Ã³ trong viá»‡c Ä‘Æ°a ra xÃ¡c suáº¥t dá»± Ä‘oÃ¡n áº£nh input thuá»™c má»™t class nÃ o Ä‘Ã³. VÃ­ dá»¥, ta xÃ©t class 2 nhÆ° hÃ¬nh áº£nh bÃªn dÆ°á»›i Khi Ä‘Ã³, vá»›i $k$ feature maps $F_i$, ta cÃ³ $k$ trá»ng sá»‘ $w_i$ vÃ  báº±ng cÃ¡ch tÃ­nh tá»• há»£p tuyáº¿n tÃ­nh cá»§a $F_i$ vÃ  $w_i$ thÃ¬ ta sáº½ cÃ³ CAM cá»§a áº£nh input á»©ng vá»›i class 2. PhÃ©p tá»• há»£p tuyáº¿n tÃ­nh giá»¯a cÃ¡c feature maps Nguá»“n: Johfischer $$CAM_2 = w_1 * F_1 + w_2 * F_2 + \u0026hellip; + w_k * F_k$$\nLÆ°u Ã½: KÃ­ch thÆ°á»›c width vÃ  height cá»§a CAM Ä‘ang báº±ng vá»›i cÃ¡c feature maps vÃ  nÃ³ thÆ°á»ng nhá» hÆ¡n nhiá»u so vá»›i áº£nh input. Äá»ƒ cÃ³ thá»ƒ visualize Ä‘Æ°á»£c nhÆ° trÃªn, ta chá»‰ cáº§n upsample CAM lÃªn báº±ng kÃ­ch thÆ°á»›c cá»§a áº£nh input. Vá»›i má»—i class khÃ¡c nhau thÃ¬ CAM tÃ­nh Ä‘Æ°á»£c lÃ  khÃ¡c nhau Sau khi trÃ¬nh bÃ y Ã½ tÆ°á»Ÿng cá»§a CAM thÃ¬ ta tháº¥y ngay má»™t Ä‘iá»u kiá»‡n mÃ  CNN cáº§n thá»a mÃ£n Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y lÃ  sau cÃ¡c conv layer thÃ¬ nÃ³ chá»‰ cÃ³ duy nháº¥t má»™t fully connected layer Ä‘á»ƒ sinh ra output, vÃ  Ä‘iá»ƒm ná»‘i giá»¯a hai pháº§n nÃ y lÃ  má»™t global average pooling layer (GAP). VÃ­ dá»¥ nhÆ° hÃ¬nh bÃªn dÆ°á»›i:\nNguá»“n: GlassBoxMedicine Trong kiáº¿n trÃºc trÃªn, ta cÃ³ má»™t CNN vá»›i output cá»§a conv layer cuá»‘i cÃ¹ng lÃ  3 feature map lÃ  A1, A2, A3. Qua GAP thÃ¬ ta thu Ä‘Æ°á»£c má»™t layer vá»›i 3 giÃ¡ trá»‹ sá»‘ thá»±c. Theo sau Ä‘Ã³ lÃ  má»™t fully connected layer sinh ra output cá»§a mÃ´ hÃ¬nh. VÃ¬ sao ta cáº§n cÃ³ duy nháº¥t má»™t fully connected layer? Ta Ä‘Ã£ Ä‘á» cáº­p ráº±ng cÃ¡c trá»ng sá»‘ trong output layer sáº½ mang Ã½ nghÄ©a lÃ  táº§m quan trá»ng cá»§a feature map tÆ°Æ¡ng á»©ng. Do Ä‘Ã³, ná»‘i ngay á»Ÿ Ä‘Ã¢y thÃ¬ nÃ³ má»›i â€œÄ‘Ãºng Ã½â€ (vá»›i má»—i neuron trong output layer, ta cÃ³ Ä‘Ãºng $k$ trá»ng sá»‘ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n $k$ feature maps) VÃ¬ sao ta cáº§n GAP? NÃ³ sáº½ táº¡o ra cáº§u ná»‘i giá»¯a cÃ¡c feature map vÃ  output layer vÃ  Ä‘áº£m báº£o ráº±ng sá»‘ channel trong input cá»§a output layer Ä‘Ãºng báº±ng sá»‘ feature maps cá»§a conv layer cuá»‘i cÃ¹ng. Viáº¿t má»™t cÃ¡ch tá»•ng quÃ¡t vÃ  â€œformalâ€ hÆ¡n, ta sáº½ cÃ³ nhÆ° sau:\nXÃ©t má»™t CNN thá»a Ä‘iá»u kiá»‡n Ã¡p dá»¥ng CAM vá»›i conv layer cuá»‘i cÃ¹ng cÃ³ $k$ feature maps $F_i$, output layer cÃ³ $C$ neurons á»©ng vá»›i $C$ class. Ma tráº­n trá»ng sá»‘ táº¡i output layer lÃ  $W_{C \\times k}$. Khi Ä‘Ã³, vá»›i input $X$, CAM Ä‘Æ°á»£c sinh ra cho class $c$ lÃ \n$$CAM_c = W_{c,1} * F_1 + W_{c,2} * F_2 + \u0026hellip; + W_{c,k} * F_k$$\nGrad-CAM (2016) Gradient-weighted Class Activation Mapping (Grad-CAM) lÃ  má»™t phiÃªn báº£n cáº£i tiáº¿n cá»§a Default CAM vá»›i hai yáº¿u tá»‘ sau:\nKhÃ´ng rÃ ng buá»™c Ä‘iá»u kiá»‡n Ä‘á»‘i vá»›i kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh. Pháº§n fully connected layers Ä‘Æ°á»£c phÃ©p á»Ÿ báº¥t kÃ¬ cÃ¡ch tá»• chá»©c nÃ o. LÆ°u Ã½ lÃ  ta váº«n dÃ¹ng GAP (global average pooling). Thay vÃ¬ pháº£i Ã¡p dá»¥ng cho conv layer cuá»‘i cÃ¹ng thÃ¬ ta Ã¡p dá»¥ng cho layer nÃ o cÅ©ng Ä‘Æ°á»£c, nhÆ°ng thÆ°á»ng thÃ¬ ngÆ°á»i ta váº«n hay cÃ¹ng conv layer cuá»‘i hÆ¡n ğŸ˜œ. â€œTáº§m quan trá»ngâ€ cá»§a cÃ¡c feature maps sáº½ Ä‘Æ°á»£c tÃ­nh theo cÃ¡ch khÃ¡c, vÃ  cÃ¡ch tÃ­nh nÃ y sáº½ dá»±a vÃ o gradient! Grad-CAM thÆ°á»ng cho ra káº¿t quáº£ CAM tá»‘t hÆ¡n, táº­p trung vÃ o Ä‘Ãºng vÃ¹ng quan trá»ng hÆ¡n trong áº£nh input khi so sÃ¡nh vá»›i Default CAM. Vi dá»¥:\nSo sÃ¡nh Default CAM vÃ  Grad-CAM Nguá»“n: MDPI XÃ©t má»™t CNN vá»›i conv layer cuá»‘i cÃ¹ng cÃ³ $k$ feature maps $F_i$, output layer cÃ³ $C$ neurons á»©ng vá»›i $C$ class. Gá»i giÃ¡ trá»‹ output cho class $c$ lÃ  $y_c$. Vá»›i áº£nh input $X$, ta Ä‘áº·t:\nTáº§m quan trá»ng cá»§a feature map $F_i$ trong viá»‡c Ä‘Æ°a ra xÃ¡c suáº¥t $X$ thuá»™c class $c$ Ä‘Æ°á»£c tÃ­nh dá»±a vÃ o gradient cá»§a output $Y_c$ theo $F_i$, tá»©c lÃ \n$$ \\alpha_{c,i} = GAP(\\frac{\\partial Y_c}{\\partial F_i}) $$\n, vá»›i GAP lÃ  global average pooling.\nÄá»ƒ Ã½ ráº±ng, $\\dfrac{\\partial Y_c}{\\partial F_i}$ cÃ³ cÃ¹ng shape vá»›i $F_i$, ta tiáº¿n hÃ nh tÃ­nh GAP Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c giÃ¡ trá»‹ sá»‘ thá»±c $\\alpha_{c,i}$\nTá»« Ä‘Ã³, CAM cho class $c$ sáº½ Ä‘Æ°á»£c tÃ­nh báº±ng tá»• há»£p tuyáº¿n tÃ­nh giá»¯a $F_i$ vÃ  $\\alpha_{c,i}$:\n$$CAM_{c} = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nGrad-CAM Ä‘Ã£ hoáº¡t Ä‘á»™ng ráº¥t tá»‘t vÃ  Ä‘Æ°á»£c dÃ¹ng nhiá»u trong viá»‡c giáº£i thÃ­ch cho CNN. Tuy nhiÃªn, Ä‘áº¿n nÄƒm 2021 thÃ¬ cÃ³ 2 tÃ¡c giáº£ Ä‘Ã£ chá»‰ ra ráº±ng cÃ³ nhá»¯ng trÆ°á»ng há»£p Grad-CAM cho ra káº¿t quáº£ khÃ´ng tháº­t sá»± Ä‘Ãºng, khi mÃ  CAM Ä‘Æ°á»£c sinh ra táº­p trung khÃ´ng Ä‘Ãºng vÃ o cÃ¡c pháº§n quan trá»ng. Vi dá»¥ trong y há»c mÃ  cÃ¡c tÃ¡c giáº£ Ä‘Æ°a ra nhÆ° sau:\nLÃ½ do chÃ­nh dáº«n Ä‘áº¿n yáº¿u tá»‘ nÃ y lÃ  á»Ÿ phÃ©p toÃ¡n GAP trong viá»‡c tÃ­nh $\\alpha_{c,i}$. CÃ³ nhá»¯ng tÃ¬nh huá»‘ng mÃ  GAP sáº½ lÃ m máº¥t Ä‘i má»™t sá»‘ Ä‘iá»ƒm ná»•i báº­t á»Ÿ trong má»™t feature map. PhÆ°Æ¡ng phÃ¡p má»›i Ä‘Æ°á»£c giá»›i thiá»‡u lÃ  HiresCam (2021), báº±ng cÃ¡ch thay tháº¿ GAP thÃ nh phÃ©p toÃ¡n tÃ­ch element-wise. CÃ¡c báº¡n cÃ³ thá»ƒ tá»± tÃ¬m hiá»ƒu vá» cÃ¡i nÃ y nhÃ©. Score-CAM (2019) Score-CAM lÃ  má»™t phÆ°Æ¡ng phÃ¡p dá»±a chá»‰ dá»±a vÃ o score cá»§a cÃ¡c class (vector output cá»§a output layer) Ä‘á»ƒ tÃ­nh táº§m quan trá»ng cá»§a cÃ¡c feature maps, tá»« Ä‘Ã³ sinh ra CAM. Káº¿t quáº£ ta cÃ³ Ä‘Æ°á»£c lÃ  cÃ¡c CAM tá»‘t hÆ¡n phÆ°Æ¡ng phÃ¡p Grad-CAM, khi mÃ  vÃ¹ng Ä‘Æ°á»£c chÃº Ã½ trong áº£nh input lÃ  Ä‘Ãºng hÆ¡n, â€œgá»nâ€ hÆ¡n (khÃ´ng lan rá»™ng ra nhá»¯ng thá»© khÃ´ng liÃªn quan láº¯m á»Ÿ cÃ¡c phÃ­a xung quanh). VÃ­ dá»¥:\nSo sÃ¡nh Score-CAM vÃ  Grad-CAM Nguá»“n: Paper Score-CAM - Figure 1 LÆ°u Ã½.\nTa xÃ©t score cá»§a class khi chÆ°a Ã¡p dá»¥ng softmax Ä‘á»ƒ Ä‘Æ°a vá» xÃ¡c suáº¥t. Score-CAM cÅ©ng khÃ´ng cÃ³ rÃ ng buá»™c gÃ¬ vá» kiáº¿n trÃºc cá»§a CNN vÃ  ta cÃ³ thá»ƒ sinh CAM cho cÃ¡c feature map táº¡i báº¥t ká»³ layer nÃ o nhÆ° Grad-CAM. Äá»ƒ trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p nÃ y thÃ¬ ta pháº£i dÃ¹ng cÃ¡c kÃ­ hiá»‡u toÃ¡n há»c hÆ¡i nhiá»u má»™t chÃºt ğŸ˜€\nIncrease of Confidence: Giáº£ sá»­ ta cÃ³ hÃ m sá»‘ $y = f(X)$ vá»›i input $X$ lÃ  vector $[x_1, x_2,\u0026hellip;, x_n]^T$ vÃ  $y$ lÃ  sá»‘ thá»±c. Vá»›i má»™t input cÆ¡ sá»Ÿ $X_b$ Ä‘Ã£ biáº¿t nÃ o Ä‘Ã³, táº§m quan trá»ng cá»§a thÃ nh pháº§n $x_i$ Ä‘á»‘i vá»›i viá»‡c tÃ­nh ra giÃ¡ trá»‹ $y$ sáº½ báº±ng vá»›i Ä‘á»™ chÃªnh lá»‡ch cá»§a output khi ta thay Ä‘á»•i pháº§n thá»© $i$ trong $X_b$ báº±ng cÃ¡ch nhÃ¢n thÃªm $x_i$, tá»©c lÃ \n$$ c_i = f(X_b \\circ H_i) - f(X_b) $$\n, vá»›i $\\circ$Â lÃ  element-wise product, $H_i = [1, \u0026hellip;, 1, x_i, 1, \u0026hellip; 1]^T$ (thÃ nh pháº§n thá»© $i$ lÃ  $x_i$)\nBÃ¢y giá», Ã¡p dá»¥ng Increase of Confidence trÃªn vá»›i viá»‡c ta tÃ­nh táº§m quan trá»ng cá»§a cÃ¡c feature maps: Giáº£ sá»­ ta cÃ³ má»™t CNN $Y = f(X)$ vá»›i input $X$ lÃ  áº£nh, $Y$ lÃ  vector cÃ³ $C$ pháº§n tá»­, á»©ng vá»›i score cá»§a $C$ class (chÆ°a Ã¡p dá»¥ng softmax). Ta xÃ©t conv layer thá»© $k$, feature map thá»© $i$ táº¡i layer nÃ y Ä‘Æ°á»£c kÃ­ hiá»‡u lÃ  $F_{k, i}$. Vá»›i má»™t input cÆ¡ sá»Ÿ $X_b$, táº§m quan trá»ng cá»§a $F_{k, i}$ Ä‘á»‘i vá»›i score cá»§a class $c$ $( Y_c = f_c(X))$ lÃ \n$$ C(F_{k, i}) = f_c(X_b \\circ H_{k, i}) - f_c(X_b) $$\n, vá»›i $H_{k, i} = s(Up(F_{k, i})$ lÃ  ta upsample $F_{k, i}$ lÃªn cÃ¹ng shape vá»›i áº£nh input $X$, sau Ä‘Ã³ normalize nÃ³ theo cÃ´ng thá»©c $Z \\leftarrow \\dfrac{Z - \\min_Z}{\\max_Z - \\min_Z}$\nSau khi tÃ­nh cÃ¡c giÃ¡ trá»‹ $C(F_{k, i})$, ta Ä‘Æ°a chÃºng qua softmax Ä‘á»ƒ cÃ³ cÃ¡c giÃ¡ trá»‹ táº§m quan trá»ng vá»›i tá»•ng báº±ng 1:\n$$ \\alpha_{c,i} = \\frac{\\exp(C(F_{k,i}))}{\\sum \\exp(C(F_{k,i})) } $$\nKhi Ä‘Ã³, ta cÃ³ thá»ƒ sinh ra CAM cho class $c$ nhÆ° sau:\n$$CAM_c = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nNhÆ° váº­y, ta Ä‘Ã£ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c táº§m quan trá»ng cá»§a cÃ¡c feature map theo cÃ¡ch lÃ  chá»‰ dá»±a vÃ o output cá»§a CNN (score cá»§a cÃ¡c class).\nNháº­n xÃ©t Qua cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã£ trÃ¬nh bÃ y lÃ  Default CAM, Grad-CAM, Score-CAM, ta cÃ³ thá»ƒ tháº¥y ráº±ng Ä‘iá»ƒm khÃ¡c biá»‡t lá»›n nháº¥t giá»¯a chÃºng lÃ  cÃ¡ch tÃ­nh táº§m quan trá»ng cá»§a cÃ¡c feature maps trong viá»‡c sinh ra output cá»§a CNN.\nCÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o notebook sau: Google Colab\nNá»™i dung cá»§a notebook trÃªn lÃ  sinh ra CAM cho cÃ¡c mÃ´ hÃ¬nh CNN nhÆ° ResNet, DenseNet, EfficientNet vÃ  so sÃ¡nh output cá»§a CAM, Grad-CAM vÃ  Score-CAM. CÃ¡c káº¿t quáº£ cháº¡y Ä‘Æ°á»£c trong notebook trÃªn nhÆ° sau:\nCAM cá»§a cÃ¡c mÃ´ hÃ¬nh CNN khÃ¡c nhau: So sÃ¡nh CAM, Grad-CAM vÃ  Score-CAM cá»§a mÃ´ hÃ¬nh ResNet50: TÃ i liá»‡u tham kháº£o GlassBox, CNN Heat Maps: Class Activation MappingÂ (CAM) GlassBox, Grad-CAM: Visual Explanations from DeepÂ Networks Paper Grad-CAM: https://arxiv.org/abs/1610.02391 Paper Score-CAM: https://arxiv.org/abs/1910.01279 ","date":"2023-02-09T18:30:41+07:00","permalink":"https://htrvu.github.io/post/cam/","title":"CAM, Grad-CAM vÃ  Score-CAM trong CNN"},{"content":"XAI lÃ  gÃ¬? Háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh AI nÃ³i chung hay Deep Learning nÃ³i riÃªng luÃ´n Ä‘Æ°á»£c ngÆ°á»i ta vÃ­ nhÆ° lÃ  má»™t chiáº¿c há»™p Ä‘en (black-box). ChÃºng ta xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh vá»›i ráº¥t nhiá»u layer, tá»« convolution cho Ä‘áº¿n fully connected, sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c optimizer nhÆ° Adam, RMSprop,â€¦ (hoáº·c nÃ³i chung chung lÃ  gradient descent) Ä‘á»ƒ tá»‘i Æ°u mÃ´ hÃ¬nh, tá»©c lÃ  tÃ¬m ra bá»™ trá»ng sá»‘ sao cho hÃ m máº¥t mÃ¡t cÃ³ giÃ¡ trá»‹ nhá» nháº¥t cÃ³ thá»ƒ. Tuy nhiÃªn, náº¿u ta nhÃ¬n láº¡i mÃ´ hÃ¬nh vÃ  tÃ¬m cÃ¡ch giáº£i thÃ­ch lÃ  vÃ¬ sao cÃ¡c mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c tá»‘t nhÆ° váº­y thÃ¬ Ä‘Ã¢y luÃ´n lÃ  má»™t cÃ¢u há»i khÃ³, Viá»‡c Ä‘Æ°a nhá»¯ng chá»©ng minh cháº·t cháº½, rÃµ rÃ ng lÃ  khÃ´ng Ä‘á» hÆ¡n giáº£n. Tá»« Ä‘Ã³, ta cÃ³ má»™t hÆ°á»›ng nghiÃªn cá»©u vá» cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i thÃ­ch sá»± hoáº¡t Ä‘á»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh AI vÃ  lÄ©nh vá»±c nÃ y Ä‘Æ°á»£c gá»i lÃ  Explainable AI (XAI).\nNguá»“n: https://impact.nuigalway.ie/wp-content/uploads/2022/01/blackboxpng.png VÃ¬ sao chÃºng ta cáº§n pháº£i tÃ¬m cÃ¡ch giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh AI?\nTa láº¥y má»™t vÃ­ dá»¥ vá» mÃ´ hÃ¬nh cháº©n Ä‘oÃ¡n ung thÆ° dáº¡ dÃ y dá»±a vÃ o hÃ¬nh áº£nh chá»¥p ná»™i soi Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ cÃ¡c bá»‡nh viá»‡n. LÃºc nÃ y, tÃ­nh chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh sáº½ trá»Ÿ nÃªn Ä‘áº·c biá»‡t nghiÃªm trá»ng, nÃ³ cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»©c khá»e vÃ  cáº£ tÃ­nh máº¡ng cá»§a bá»‡nh nhÃ¢n. Náº¿u mÃ´ hÃ¬nh cháº©n Ä‘oÃ¡n lÃ  ung thÆ° thÃ¬ ta cÅ©ng cáº§n nÃ³ Ä‘Æ°a ra nhá»¯ng â€œchá»©ng cá»©â€ cho cháº©n Ä‘oÃ¡n Ä‘Ã³, táº¥t nhiÃªn lÃ  chá»©ng cá»© pháº£i Ä‘Ãºng, mang tÃ­nh thuyáº¿t phá»¥c cao thÃ¬ má»›i cháº¥p nhÃ¢n Ä‘Æ°á»£c. NgoÃ i lÄ©nh vá»±c y táº¿ thÃ¬ ta cÃ²n cÃ³ cÃ¡c vÃ­ dá»¥ khÃ¡c nhÆ° trong há»‡ thá»‘ng báº£o máº­t cá»§a ngÃ¢n hÃ ng,â€¦\nNguá»“n: Webflow\nNguá»“n: MicroAI\nKhi AI cÃ ng Ä‘Æ°á»£c á»©ng dá»¥ng nhiá»u vÃ o cuá»™c sá»‘ng thÃ¬ nhu cáº§u giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh AI cÅ©ng sáº½ dáº§n nhiá»u lÃªn. Äiá»u Ä‘Ã³ dáº«n Ä‘áº¿n sá»± phÃ¡t triá»ƒn máº¡nh cá»§a XAI trong thá»i gian gáº§n Ä‘Ã¢y.\nDiá»…n giáº£i má»™t mÃ´ hÃ¬nh AI Kháº£ nÄƒng diá»…n giáº£i mÃ´ hÃ¬nh (interpretability) lÃ  má»©c Ä‘á»™ hiá»ƒu biáº¿t cá»§a chÃºng ta vá» cÃ¡ch mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng, mÃ  cá»¥ thá»ƒ hÆ¡n lÃ  vá» quÃ¡ trÃ¬nh Ä‘Æ°a ra dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh. Ta cÃ³ hai hÆ°á»›ng tiáº¿p cáº­n chÃ­nh Ä‘á»‘i vá»›i viá»‡c diá»…n giáº£i mÃ´ hÃ¬nh lÃ  intrinsic vÃ  post-hoc.\nNguá»“n: Kemal Erdem Intrinsic (dá»±a vÃ o báº£n cháº¥t cá»§a mÃ´ hÃ¬nh): CÃ¡ch tiáº¿p cáº­n nÃ y thÆ°á»ng dÃ¹ng cho nhá»¯ng mÃ´ hÃ¬nh thuá»™c nhÃ³m white-box, Ä‘áº·c biá»‡t lÃ  nhá»¯ng mÃ´ hÃ¬nh Machine Learning nhÆ° Linear Regresion, Decision Tree, SVM,â€¦ Äáº±ng sau nhá»¯ng mÃ´ hÃ¬nh Ä‘Ã³ lÃ  cÃ¡c lÃ½ thuyáº¿t toÃ¡n cháº·t cháº½, ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c ngay cÃ´ng thá»©c tÃ­nh ra trá»ng sá»‘ tá»‘i Æ°u cá»§a bÃ i. NÃ³i cÃ¡ch khÃ¡c, khi chÆ°a cáº§n huáº¥n luyá»‡n thÃ¬ ta cÅ©ng cÃ³ thá»ƒ giáº£i thÃ­ch ráº±ng mÃ´ hÃ¬nh sáº½ hoáº¡t Ä‘á»™ng theo cÃ¡ch nhÆ° tháº¿ nÃ y, nhÆ° tháº¿ kia. Decision Tree Nguá»“n: javatpoint\nSVM Nguá»“n: Wikipedia\nPost-hoc: ÄÃ¢y lÃ  cÃ¡ch tiáº¿p cáº­n chÃºng ta thÆ°á»ng dÃ¹ng khi diá»…n giáº£i cÃ¡c mÃ´ hÃ¬nh Deep Learning, vÃ  Ä‘áº·c biá»‡t lÃ  nÃ³ Ä‘Æ°á»£c tiáº¿n hÃ nh sau khi mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i má»™t bá»™ trá»ng sá»‘ Ä‘á»§ tá»‘t. VÃ¬ viá»‡c giáº£i thÃ­ch, chá»©ng minh cháº·t cháº½, chÃ­nh xÃ¡c vá» quÃ¡ trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh Deep Learning lÃ  ráº¥t khÃ³ khÄƒn nÃªn post-hoc lÃ  hÆ°á»›ng tiáº¿p cáº­n Ä‘Æ°á»£c Æ°u tiÃªn hÆ¡n. Trong post-hoc, ta cÃ³ 2 cÃ¡ch diá»…n giáº£i lÃ  model-agnostic vÃ  model-specific.\nModel-agnostic: CÃ¡ch nÃ y nghÄ©a lÃ  chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¹ng má»™t phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ diá»…n giáº£i cho toÃ n bá»™ cÃ¡c mÃ´ hÃ¬nh mÃ  khÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n kiáº¿n trÃºc cá»§a chÃºng. NhÆ° váº­y, ta chá»‰ dá»±a vÃ o input vÃ  output cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘Æ°a ra cÃ¡ch diá»…n giáº£i. Model-specific: Vá»›i cÃ¡ch nÃ y thÃ¬ tÃ¹y theo nhá»¯ng mÃ´ hÃ¬nh, hay lÃ  há» cÃ¡c mÃ´ hÃ¬nh, mÃ  ta sáº½ Ä‘Æ°a ra cÃ¡ch diá»…n giáº£i tÆ°Æ¡ng á»©ng. Ta cÃ³ thá»ƒ tháº¥y ráº±ng model-specific cÃ³ thá»ƒ dá»… tiáº¿n hÃ nh hÆ¡n model-agnostic ráº¥t nhiá»u.\nNhá»¯ng phÆ°Æ¡ng phÃ¡p trong XAI mÃ  mÃ¬nh trÃ¬nh bÃ y trong tÆ°Æ¡ng lai sáº½ chá»§ yáº¿u thuá»™c vá» hÆ°á»›ng post-hoc.\nVÃ¬ sao chÃºng ta bÃ n nhiá»u vá» kháº£ nÄƒng diá»…n giáº£i mÃ´ hÃ¬nh (interpretability) nhÆ°ng lÄ©nh vá»±c nÃ y láº¡i gá»i lÃ  Explainable AI (thiÃªn vá» kháº£ nÄƒng giáº£i thÃ­ch mÃ´ hÃ¬nh)?\n2 thuáº­t ngá»¯ diá»…n giáº£i vÃ  giáº£i thÃ­ch cÃ³ thá»ƒ xem lÃ  mang Ã½ nghÄ©a tÆ°Æ¡ng tá»± vÃ  cÃ³ thá»ƒ dÃ¹ng thay tháº¿ cho nhau. Tuy nhiÃªn, cÃ³ má»™t vÃ i quan Ä‘iá»ƒm cho ráº±ng kháº£ nÄƒng diá»…n giáº£i lÃ  nÃ³i Ä‘áº¿n má»™t tÃ­nh cháº¥t bá»‹ Ä‘á»™ng cá»§a mÃ´ hÃ¬nh vÃ  nÃ³ cáº§n con ngÆ°á»i chÃºng ta can thiá»‡p vÃ o, cÃ²n kháº£ nÄƒng giáº£i thÃ­ch lÃ  thiÃªn vá» chá»§ Ä‘á»™ng, tá»©c lÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ tá»± giáº£i thÃ­ch cho chÃ­nh nÃ³. á» Ä‘Ã¢y, con ngÆ°á»i chÃºng ta Ä‘ang tÃ¬m cÃ¡ch giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh, do Ä‘Ã³ ta Æ°u tiÃªn gá»i lÃ  diá»…n giáº£i. ÄÃ¡nh giÃ¡ phÆ°Æ¡ng phÃ¡p XAI Má»™t váº¥n Ä‘á» khÃ¡c mÃ  ngÆ°á»i ta thÆ°á»ng quan tÃ¢m Ä‘áº¿n lÃ  cÃ¡ch Ä‘Ã¡nh giÃ¡ má»™t phÆ°Æ¡ng phÃ¡p XAI, tá»©c lÃ  xÃ©t xem cÃ¡ch diá»…n giáº£i mÃ´ hÃ¬nh A Ä‘Ã£ thuyáº¿t phá»¥c, Ä‘Ã£ Ä‘Ãºng hay chÆ°a. Hiá»‡n táº¡i, ta chÆ°a cÃ³ má»™t Ä‘á»™ Ä‘o nÃ o Ä‘á»ƒ cÃ³ thá»ƒ so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p vá»›i nhau. Pháº§n lá»›n thÃ¬ nÃ³ náº±m á»Ÿ cÃ¡c nháº­n xÃ©t cá»§a con ngÆ°á»i thÃ´ng qua viá»‡c quan sÃ¡t ğŸ˜€\nTÃ i liá»‡u tham kháº£o Mobiquity, An introduction to Explainable Artificial Intelligence (XAI) Erdem, XAI Methods - The Introduction ","date":"2023-02-09T15:30:31+07:00","permalink":"https://htrvu.github.io/post/intro-xai/","title":"Giá»›i thiá»‡u vá» XAI"},{"content":"Giá»›i thiá»‡u Qua cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c giá»›i thiá»‡u nhÆ° VGG, GoogLeNet hay ResNet thÃ¬ ta tháº¥y ráº±ng chÃºng Ä‘á»u Ä‘Æ°á»£c phÃ¡t triá»ƒn theo hÆ°á»›ng tÄƒng dáº§n Ä‘á»™ sÃ¢u vÃ  Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n, ká»ƒ tá»« khi AlexNet Ä‘Æ°á»£c cÃ´ng bá»‘. Sá»‘ lÆ°á»£ng tham sá»‘ cá»§a chÃºng lÃ  ráº¥t lá»›n.\nTuy nhiÃªn, cÃ¡c á»©ng dá»¥ng AI trong thá»±c táº¿ nhÆ° robotics, xe tá»± hÃ nh thÃ¬ cÃ¡c phÃ©p tÃ­nh toÃ¡n cá»§a mÃ´ hÃ¬nh cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n trong má»™t khoáº£ng thá»i gian giá»›i háº¡n, cÃ¹ng vá»›i tÃ i nguyÃªn pháº§n cá»©ng háº¡n cháº¿. Do Ä‘Ã³, ta pháº£i Ä‘á»‘i máº·t vá»›i má»™t trade-off giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»…, kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh.\nVÃ o thá»i Ä‘iá»ƒm nÃ y, cÃ³ 2 hÆ°á»›ng giáº£i phÃ¡p chÃ­nh Ä‘á»ƒ cÃ³ thá»ƒ Ä‘Æ°a cÃ¡c mÃ´ hÃ¬nh vÃ o á»©ng dá»¥ng thá»±c táº¿ nhÆ° sau:\nNÃ©n cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p láº¡i thÃ´ng qua cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° lÆ°á»£ng tá»­ hÃ³a (quantization), hashing, cáº¯t tá»‰a mÃ´ hÃ¬nh XÃ¢y dá»±ng vÃ  huáº¥n luyÃªn cÃ¡c mÃ´ hÃ¬nh nhá», Ä‘á»™ phá»©c táº¡p tháº¥p ngay tá»« Ä‘áº§u. MobileNet Ä‘Æ°á»£c phÃ¡t triá»ƒn theo hÆ°á»›ng thá»© 2, trong Ä‘Ã³, nÃ³ táº­p trung vÃ o cÃ¡c yáº¿u tá»‘:\nVá»«a Ä‘áº£m báº£o kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh Ä‘á»§ nhá», tá»‘c Ä‘á»™ suy diá»…n Ä‘á»§ nhanh (Ä‘á»™ trá»… tháº¥p) vÃ  vá»›i Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»§ cao. Cung cáº¥p hai siÃªu tham sá»‘ cho phÃ©p ta Ä‘iá»u chá»‰nh trade-off giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»…, kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh: width multiplier (liÃªn quan Ä‘áº¿n sá»‘ channel trong tá»«ng layer) vÃ  resolution multiplier (width vÃ  height trong tá»«ng layer) Depthwise separable convolutions MobileNet Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« cÃ¡c layer convolution khÃ¡ Ä‘áº·c biá»‡t, chÃºng Ä‘Æ°á»£c gá»i lÃ  depthwise separable convolutions. Depthwise separable convolution Ä‘Æ°á»£c táº¡o ra tá»« hai phÃ©p toÃ¡n:\nDepthwise convolution: Ãp dá»¥ng tá»«ng filter cho tá»«ng channel cá»§a input. Náº¿u input cÃ³ bao nhiÃªu channel thÃ¬ ta sáº½ cÃ³ báº¥y nhiÃªu filter. Pointwise convolution: ÄÃ¢y thá»±c cháº¥t lÃ  convolution layer thÃ´ng thÆ°á»ng vá»›i filter 1 x 1. NÃ³ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tá»•ng há»£p cÃ¡c káº¿t quáº£ tá»« phÃ©p toÃ¡n depthwise convolution vÃ  tÃ­nh ra output, thÃ´ng qua cÃ¡c phÃ©p toÃ¡n tá»• há»£p tuyáº¿n tÃ­nh. Nguá»“n: Research Gate Ta cÃ³ thá»ƒ tháº¥y ngay sá»± khÃ¡c biá»‡t giá»¯a depthwise separable convolution vÃ  convolution thÃ´ng thÆ°á»ng nhÆ° sau:\nConvolution thÃ´ng thÆ°á»ng: Má»—i filter sáº½ tÆ°Æ¡ng tÃ¡c vá»›i toÃ n bá»™ channel cá»§a input. Giáº£ sá»­ input cá»§a ta lÃ  $D_F \\times D_F \\times M$, má»™t filter $3 \\times 3$ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ¬ filter nÃ y sáº½ trá»Ÿ thÃ nh má»™t tensor vá»›i shape $3 \\times 3 \\times M$, ta thá»±c hiá»‡n convolution trÃªn tá»«ng channel vÃ  sau Ä‘Ã³ cá»™ng $M$ ma tráº­n láº¡i vá»›i nhau, thu Ä‘Æ°á»£c káº¿t quáº£ $D_F \\times D_F$. Náº¿u sá»­ dá»¥ng $N$ filter Ä‘á»ƒ tÃ­nh thÃ¬ ta sáº½ cÃ³ káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  $D_F \\times D_F \\times N$. Depthwise separable convolution: Ban Ä‘áº§u, cÃ¡c channel Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘á»™c láº­p vá»›i tá»«ng filter riÃªng, sau Ä‘Ã³ má»›i káº¿t há»£p láº¡i sau nhá» vÃ o pointwise convolution. Vá»›i input $D_F \\times D_F \\times M$ thÃ¬ khi Ä‘Æ°a qua depthwise convotution, ta sáº½ cÃ³ káº¿t quáº£ lÃ  $D_F \\times D_F \\times M$. Náº¿u pointwise convolution sá»­ dá»¥ng $N$ filter $1 \\times 1$ thÃ¬ ta cÃ³ káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  $D_F \\times D_F \\times N$ Váº¥n Ä‘á» Ä‘áº·t ra lÃ  táº¡i sao sá»­ dá»¥ng depthwise separable convolution láº¡i cÃ³ thá»ƒ giÃºp cho MobileNet gá»n nháº¹ hÆ¡n, tÃ­nh toÃ¡n nhanh hÆ¡n vÃ  cÃ³ Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»§ tá»‘t, khÃ´ng há» kÃ©m cáº¡nh cÃ¡c mÃ´ hÃ¬nh to lá»›n khÃ¡c. Ta sáº½ Ä‘áº·t tÃ­nh má»™t chÃºt:\nGiáº£ sá»­ input cá»§a ta lÃ  feature maps $\\bold{F}: D_F \\times D_F \\times M$, output cuá»‘i cÃ¹ng lÃ  $\\bold{G}: D_F \\times D_F \\times N$.\nVá»›i convolution thÃ´ng thÆ°á»ng: Giáº£ sá»­ ta dÃ¹ng $N$ filter $\\bold{K}: D_K \\times D_K$, stride lÃ  1, padding sao cho kÃ­ch thÆ°á»›c width vÃ  height khÃ´ng Ä‘á»•i. Khi Ä‘Ã³, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n sáº½ lÃ \n$$ O_1 = D_K \\times D_K \\times D_F \\times D_F \\times M \\times N $$\n, vÃ¬ vá»›i má»—i filter thÃ¬: má»—i láº§n tÃ­nh toÃ¡n ta pháº£i thá»±c hiá»‡n $D_K \\times D_K$ phÃ©p toÃ¡n nhÃ¢n, sau Ä‘Ã³ cá»™ng chÃºng láº¡i; ta tÃ­nh táº¡i $D_F \\times D_F$ vá»‹ trÃ­ trÃªn $M$ channel cá»§a input, vÃ  ta sá»­ dá»¥ng $N$ filter.\nVá»›i depthwise separable convolution: á» bÆ°á»›c depthwise convolution thÃ¬ ta dÃ¹ng $M$ filter $\\bold{K}: D_K \\times D_K$, stride lÃ  1, padding phÃ¹ há»£p. Khi Ä‘Ã³, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n sáº½ lÃ \n$$ D_K \\times D_K \\times D_F \\times D_F \\times M $$\n, vÃ¬ ta chá»‰ Ä‘Æ¡n giáº£n lÃ  Ã¡p dá»¥ng Ä‘Æ¡n láº» tá»«ng filter cho tá»«ng channels\nVá»›i pointwise convolution thÃ¬ ta dÃ¹ng $N$ filter $\\bold{K}: 1 \\times 1$, stride lÃ  1, padding 0. Khi Ä‘Ã³, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n sáº½ lÃ \n$$ D_F \\times D_F \\times M \\times N $$\nDo Ä‘Ã³, ta cÃ³ Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n lÃ \n$$ O_2 = D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N $$\nLÃºc nÃ y, Ä‘em chia cho nhau thÃ¬ ta cÃ³ tá»‰ lá»‡\n$$ \\frac{O_2}{O_1}=\\frac{D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N}{D_K \\times D_K \\times D_F \\times D_F \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_K^2} $$\nNhÆ° váº­y, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n khi sá»­ dá»¥ng depthwise separable convolution Ä‘Ã£ giáº£m khoáº£ng $D_K^2$ láº§n so vá»›i convolution thÃ´ng thÆ°á»ng. MobileNet sá»­ dá»¥ng cÃ¡c filter $3 \\times 3$, tá»« Ä‘Ã³ giáº£m Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n Ä‘i khoáº£ng 8 Ä‘áº¿n 9 láº§n, trong khi Ä‘á»™ chÃ­nh xÃ¡c chá»‰ giáº£m Ä‘i má»™t pháº§n nhá».\nSiÃªu tham sá»‘ Ä‘iá»u chá»‰nh trade-off Äá»ƒ cÃ³ thá»ƒ há»— trá»£ tá»‘t hÆ¡n viá»‡c Ã¡p dá»¥ng MobileNet vÃ o cÃ¡c thiáº¿t bá»‹ biÃªn trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿, cÃ¡c tÃ¡c giáº£ cÃ²n cung cáº¥p thÃªm cho ta hai siÃªu tham sá»‘ Ä‘á»ƒ Ä‘iá»u chá»‰nh trade-off giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»…, kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh\nWidth multiplier Tham sá»‘ width multiplier (kÃ­ hiá»‡u lÃ  $\\alpha$) sáº½ tÃ¡c Ä‘á»™ng lÃªn giÃ¡ trá»‹ sá»‘ channel cá»§a cÃ¡c layer. Vá»›i nhá»¯ng cÃ´ng thá»©c á»Ÿ trÃªn thÃ¬ sá»‘ channel chÃ­nh lÃ  $M$ vÃ  $N$. GiÃ¡ trá»‹ $\\alpha \\in (0, 1]$ vÃ  ta thÆ°á»ng Ä‘áº·t lÃ  $1, 0.75, 0.5, 0.25.$ Khi Ä‘Ã³, thá»© tháº­t sá»± Ä‘Æ°á»£c thay Ä‘á»•i chÃ­nh lÃ  sá»‘ lÆ°á»£ng filter mÃ  ta dÃ¹ng trong cÃ¡c phÃ©p toÃ¡n pointwise convolution.\nÄá»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a depthwise separable convolution khi ta cÃ³ sá»­ dá»¥ng width multiplier $\\alpha$ lÃ \n$$ D_K \\times D_K \\times D_F \\times D_F \\times \\alpha M + D_F \\times D_F \\times \\alpha M \\times \\alpha N $$\nResolution multiplier Tham sá»‘ resolution muiltiplier (kÃ­ hiá»‡u lÃ  $\\rho$ ) liÃªn quan Ä‘áº¿n kÃ­ch thÆ°á»›c width vÃ  height (chÃ­nh lÃ  $D_F$ trong cÃ¡c cÃ´ng thá»©c trÃªn). Miá»n giÃ¡ trá»‹ cá»§a nÃ³ cÅ©ng sáº½ tÆ°Æ¡ng tá»± nhÆ° $\\alpha$. Thá»±c cháº¥t thÃ¬ ta sáº½ chá»‰ Ã¡p dá»¥ng nÃ³ vÃ o input ban Ä‘áº§u cá»§a mÃ´ hÃ¬nh (áº£nh). CÃ¡c kÃ­ch thÆ°á»›c input mÃ  ta thÆ°á»ng sá»­ dá»¥ng vá»›i mÃ´ hÃ¬nh MobileNet lÃ  224, 192, 160 hoáº·c 128.\nÄá»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a depthwise separable convolution khi ta cÃ³ sá»­ dá»¥ng thÃªm resolution multiplier $\\rho$ lÃ \n$$ D_K \\times D_K \\times \\rho D_F \\times \\rho D_F \\times \\alpha M + \\rho D_F \\times \\rho D_F \\times \\alpha M \\times \\alpha N $$\nKiáº¿n trÃºc mÃ´ hÃ¬nh Depthwise Separable block MÃ´ hÃ¬nh MobileNet V1 Ä‘Æ°á»£c táº¡o thÃ nh bá»Ÿi cÃ¡c thÃ nh pháº§n chÃ­nh lÃ  depthwise separable block. ChÃºng bao gá»“m hai phÃ©p toÃ¡n nhÆ° ta Ä‘Ã£ Ä‘á» cáº­p lÃ  depthwise convolution vÃ  pointwise convolution. Äi kÃ¨m vá»›i cÃ¡c layer Ä‘Ã³ lÃ  batch norm vÃ  activation ReLU.\nNguá»“n: Research Gate Kiáº¿n trÃºc MobileNet MobileNet sá»­ dá»¥ng táº¥t cáº£ gá»“m 13 depthwise separable block. Tá»•ng thá»ƒ kiáº¿n trÃºc cá»§a MobileNet Ä‘Æ°á»£c thá»ƒ hiá»‡n á»Ÿ báº£ng sau:\nTrong Ä‘Ã³:\nConv dw lÃ  depthwise convolution. Ta cÃ³ thá»ƒ tháº¥y cÃ¡c filter shape cá»§a chÃºng luÃ´n cÃ³ cÃ¹ng sá»‘ channel trong input size. CÃ¡c conv layer vá»›i filter shape $1 \\times 1$ chÃ­nh lÃ  pointwise convolution. s1 tá»©c lÃ  stride = 1, tÆ°Æ¡ng tá»± vá»›i s2. ToÃ n bá»™ cÃ¡c conv layer trong mÃ´ hÃ¬nh Ä‘á»u cÃ³ padding sao cho kÃ­ch thÆ°á»›c width vÃ  height cá»§a input vÃ  output cá»§a layer Ä‘Ã³ lÃ  nhÆ° nhau. Note:\nTrong báº£ng trÃªn cÃ³ váº» cÃ³ má»™t chá»— gÃµ nháº§m. Äá»ƒ Ã½ Ä‘áº¿n layer â€œConv dw / s2â€ Ä‘áº§u tiÃªn tá»« phÃ­a dÆ°á»›i lÃªn, náº¿u Ä‘Ã¢y lÃ  s2 thÃ¬ input size cá»§a layer â€œConv / s1â€ tiáº¿p theo pháº£i bá»‹ giáº£m size chá»© khÃ´ng pháº£i $7 \\times 7$. Do Ä‘Ã³, trong cÃ i Ä‘áº·t mÃ´ hÃ¬nh á»Ÿ bÃªn dÆ°á»›i thÃ¬ mÃ¬nh Ä‘Ã£ Ä‘á»•i nÃ³ thÃ nh â€œConv dw / s1â€. CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t MobileNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nTÃ i liá»‡u tham kháº£o Paper MobileNet: https://arxiv.org/abs/1704.04861 ","date":"2023-02-08T18:13:00+07:00","permalink":"https://htrvu.github.io/post/mobilenet/","title":"MobileNet (2017)"},{"content":"Giá»›i thiá»‡u CÃ¡c mÃ´ hÃ¬nh thuá»™c há» Inception-ResNet Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn Ã½ tÆ°á»Ÿng lÃ  káº¿t há»£p skip connection vÃ o cÃ¡c Inception block (cÃ¡c Ã½ tÆ°á»Ÿng tá»« ResNet vÃ  GoogLeNet). VÃ¬ paper nÃ y chá»‰ mang tÃ­nh thá»±c nghiá»‡m lÃ  chÃ­nh nÃªn mÃ¬nh sáº½ khÃ´ng trÃ¬nh bÃ y chi tiáº¿t ğŸ‘€.\nKiáº¿n trÃºc mÃ´ hÃ¬nh Inception-ResNet V1 Vá» máº·t tá»•ng quan, Inception-ResNet V1 cÃ³ kiáº¿n trÃºc nhÆ° sau:\nKiáº¿n trÃºc Inception-ResNet V1\nStem cá»§a Inception-ResNet V1\nTa sáº½ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c loáº¡i Inception-ResNet block vÃ  Reduction:\nInception-ResNet block: Ta tháº¥y ráº±ng pháº§n â€œInceptionâ€ trong cÃ¡c block nÃ y lÃ  Ä‘Æ¡n giáº£n hÆ¡n khÃ¡ nhiá»u so vá»›i cÃ¡c Inception block nguyÃªn máº«u. Inception-ResNet-A V1\nInception-ResNet-B V1\nInception-ResNet-C V1\nReduction: ChÃºng thá»±c hiá»‡n nhiá»‡m vá»¥ giáº£m kÃ­ch thÆ°á»›c (width, height) cá»§a cÃ¡c tensor Ä‘i má»™t ná»­a. Kiáº¿n trÃºc cá»§a chÃºng ráº¥t giá»‘ng vá»›i cÃ¡c Inception block Reduction-A\nReduction-B\nInception-ResNet V2 PhiÃªn báº£n thá»© hai cá»§a Inception-ResNet cÃ³ kiáº¿n trÃºc tá»•ng thá»ƒ giá»‘ng há»‡t vá»›i phiÃªn báº£n Ä‘áº§u tiÃªn, ta chá»‰ cÃ³ má»™t sá»‘ thay Ä‘á»•i á»Ÿ cÃ¡c block Inception-ResNet vÃ  Reduction\nTÃ i liá»‡u tham kháº£o Paper Inception-ResNet: https://arxiv.org/abs/1602.07261 ","date":"2023-02-08T18:03:46+07:00","permalink":"https://htrvu.github.io/post/inception-resnet/","title":"Inception-Reset (2016)"},{"content":"KhÃ³ khÄƒn trong huáº¥n luyá»‡n mÃ´ hÃ¬nh lá»›n Ta biáº¿t ráº±ng, viá»‡c táº¡o ra cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n (nhiá»u layer) chÆ°a cháº¯c Ä‘Ã£ mang láº¡i hiá»‡u quáº£ tá»‘t hÆ¡n nhá»¯ng mÃ´ hÃ¬nh â€œcáº¡nâ€ hÆ¡n. VÃ­ dá»¥, vá»›i táº­p CIFAR10 thÃ¬ ta cÃ³ má»™t káº¿t quáº£ thá»­ nghiá»‡m cho tháº¥y ráº±ng mÃ´ hÃ¬nh sÃ¢u hÆ¡n láº¡i cÃ³ Ä‘á»™ hiá»‡u quáº£ kÃ©m hÆ¡n:\nÄá»‘i vá»›i viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n thÃ¬ ta cÃ³ thá»ƒ sáº½ bá»‹ gáº·p pháº£i cÃ¡c váº¥n Ä‘á» sau:\nOverfitting: MÃ´ hÃ¬nh cÃ ng sÃ¢u thÆ°á»ng sáº½ cÃ ng phá»©c táº¡p nÃªn nÃ³ ráº¥t dá»… bá»‹ overfitting. Vanishing/exploding gradient: Váº¥n Ä‘á» nÃ y thÃ¬ ta Ä‘Ã£ cÃ³ má»™t sá»‘ cÃ¡ch giáº£i quyáº¿t phá»• biáº¿n nhÆ° thay Ä‘á»•i activation function, cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o trá»ng sá»‘ nhÆ° He Initialization. Trong He Initialization, táº¡i má»—i layer thÃ¬ ta khá»Ÿi táº¡o cÃ¡c giÃ¡ trá»‹ bias vá»›i gÃ¡i trá»‹ 0, cÃ¡c trá»ng sá»‘ sáº½ tuÃ¢n theo phÃ¢n phá»‘i chuáº©n vá»›i ká»³ vá»ng 0, phÆ°Æ¡ng sai $\\dfrac{2}{D_h}$, trong Ä‘Ã³ $D_h$ lÃ  sá»‘ units cá»§a layer liá»n trÆ°á»›c layer hiá»‡n táº¡i. NgoÃ i 2 váº¥n Ä‘á» trÃªn, ta cÃ³ má»™t váº¥n Ä‘á» Ä‘áº·c biá»‡t hÆ¡n lÃ  degradation: Accuracy tÄƒng dáº§n cho Ä‘áº¿n má»™t Ä‘á»™ sÃ¢u nháº¥t Ä‘á»‹nh thÃ¬ ngá»«ng tÄƒng (bÃ£o hÃ²a) rá»“i sau Ä‘Ã³ sáº½ giáº£m dáº§n.\nLÆ°u Ã½ ráº±ng, nhiá»u trÆ°á»ng há»£p degradation khÃ´ng pháº£i do overfitting gÃ¢y ra. Khi mÃ´ hÃ¬nh cÃ³ Ä‘á»™ phá»©c táº¡p Ä‘á»§ lá»›n, nhá»¯ng sá»± thay Ä‘á»•i dÃ¹ lÃ  ráº¥t nhá» trong trá»ng sá»‘ cÅ©ng sáº½ gÃ¢y ra sá»± biáº¿n thiÃªn lá»›n trong giÃ¡ trá»‹ cá»§a gradient, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n cÃ¡c bÆ°á»›c cáº­p nháº­t trá»ng sá»‘ qua nhá»¯ng láº§n cháº¡y gradient descent sáº½ khÃ´ng mang láº¡i lá»£i Ã­ch gÃ¬ nhiá»u mÃ  cÃ²n cÃ³ thá»ƒ khiáº¿n quÃ¡ trÃ¬nh huáº¥n luyá»‡n \u0026ldquo;Ä‘i láº¡c\u0026rdquo;.\nSá»± biáº¿n thiÃªn cá»§a gradient khi trá»ng sá»‘ thay Ä‘á»•i trong cÃ¡c mÃ´ hÃ¬nh\n(a) MÃ´ hÃ¬nh chá»‰ cÃ³ 1 hidden layer; (b) MÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n vá»›i 24 hidden layers\nNguá»“n: Understanding Deep Learning - Simon J.D. Prince Degradation cho chÃºng ta tháº¥y ráº±ng viá»‡c tá»‘i Æ°u cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n lÃ  khÃ´ng há» dá»… dÃ ng. Trong quÃ¡ trÃ¬nh xÃ¢y dá»±ng mÃ´ hÃ¬nh, tá»« má»™t mÃ´ hÃ¬nh ban Ä‘áº§u, sau khi thÃªm má»™t sá»‘ layer vÃ o thÃ¬ táº¥t nhiÃªn lÃ  ta mong ráº±ng mÃ´ hÃ¬nh má»›i pháº£i cÃ³ Ä‘á»™ hiá»‡u quáº£ Ã­t nháº¥t lÃ  ngang mÃ´ hÃ¬nh gá»‘c. Tuy nhiÃªn, khi xáº£y ra degradation thÃ¬ mong muá»‘n Ä‘Ã³ Ä‘Ã£ khÃ´ng thá»ƒ thÃ nh sá»± tháº­t Ä‘Æ°á»£c. ğŸ˜€\nResNet Ä‘Æ°á»£c cÃ´ng bá»‘ nháº±m giáº£i quyáº¿t váº¥n Ä‘á» degradation Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n. Khi nháº¯c Ä‘áº¿n ResNet thÃ¬ ta sáº½ láº­p tá»©c nghÄ© Ä‘áº¿n nhá»¯ng mÃ´ hÃ¬nh vá»›i Ä‘á»™ sÃ¢u ráº¥t khá»§ng, tháº­m chá»‰ lÃ  lÃªn Ä‘áº¿n 100, 200 layer.\nHÃ m pháº§n dÆ° vÃ  skip connection Ã tÆ°á»Ÿng vá» hÃ m pháº§n dÆ° Nháº¯c láº¡i vá» cÃ¡i mong muá»‘n á»Ÿ pháº§n trÆ°á»›c, ráº±ng mÃ´ hÃ¬nh má»›i pháº£i cÃ³ Ä‘á»™ hiá»‡u quáº£ Ã­t nháº¥t lÃ  ngang mÃ´ hÃ¬nh gá»‘c, ta cÃ³ thá»ƒ nghÄ© ngay Ä‘áº¿n má»™t phÆ°Æ¡ng phÃ¡p cá»±c kÃ¬ Ä‘Æ¡n giáº£n: cÃ¡c layer phÃ­a sau sáº½ lÃ  identity mapping, tá»©c lÃ  input vÃ  output cá»§a nÃ³ sáº½ giá»‘ng nhau. Vá»›i cÃ¡ch lÃ m nÃ y thÃ¬ hiá»ƒn nhiÃªn lÃ  ta Ä‘áº¡t Ä‘Æ°á»£c mong muá»‘n rá»“i, vÃ¬ Ä‘á»™ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh má»›i vÃ  mÃ´ hÃ¬nh gá»‘c rÃµ rÃ ng sáº½ y há»‡t nhau.\nTuy nhiÃªn, náº¿u chá»‰ dá»«ng láº¡i á»Ÿ Ä‘Ã³ thÃ´i thÃ¬ thÃªm layer vÃ o lÃ m gÃ¬ :v Ta muá»‘n Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n! CÃ¡c tÃ¡c giáº£ cá»§a paper ResNet giá»›i thiá»‡u má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  deep residual learning (há»c pháº§n dÆ°).\nGiáº£ sá»­ ta cÃ³ má»™t block $B$ cÃ¡c layer, input cá»§a nÃ³ lÃ  $\\bold{x}$. Vá»›i má»™t mÃ´ hÃ¬nh thÃ´ng thÆ°á»ng, ta sáº½ â€œhá»câ€ má»™t hÃ m sá»‘ Ä‘áº§u ra mong muá»‘n lÃ  $f(\\bold{x})$. LÃºc nÃ y, ban Ä‘áº§u thÃ¬ ta hoÃ n toÃ n chÆ°a cÃ³ má»™t thÃ´ng tin gÃ¬ vá» $f(\\bold{x})$ cáº£, viá»‡c â€œhá»câ€ sáº½ xuáº¥t phÃ¡t tá»« má»™t Ä‘áº¡i lÆ°á»£ng ngáº«u nhiÃªn.\nÄá»‘i vá»›i phÆ°Æ¡ng phÃ¡p deep residual learning, output cá»§a block $B$ sáº½ cÃ³ dáº¡ng\n$$h(\\bold{x}) = \\bold{x} + f(\\bold{x})$$\nvÃ  ta sáº½ Ä‘i há»c $f(\\bold{x})$. LÃºc nÃ y, $f(\\bold{x})$ Ä‘Æ°á»£c gá»i lÃ  residual function (hÃ m pháº§n dÆ°)\nTa cÃ³ cÃ¡c nháº­n xÃ©t sau:\nTrong deep residual learning, ta Ä‘Ã£ cÃ³ sá»± â€œgá»£i Ã½â€ cho hÃ m mong muá»‘n thÃ´ng qua giÃ¡ trá»‹ input $\\bold{x}$. Äá»‘i vá»›i thÃ´ng thÆ°á»ng thÃ¬ khÃ´ng cÃ³ sá»± gá»£i Ã½ nÃ o Ä‘Æ°á»£c Ä‘Æ°a ra cáº£. Viá»‡c há»c $f(\\bold{x})$ nhÆ° lÃ  má»™t hÃ m pháº§n dÆ° lÃ  dá»… hÆ¡n so vá»›i viá»‡c há»c hÃ m mong muá»‘n. Náº¿u identity mapping lÃ  káº¿t quáº£ tá»‘i Æ°u thÃ¬ ta cÃ³ luÃ´n $f(\\bold{x})=0$ Vá» máº·t báº£n cháº¥t, vá»›i phÆ°Æ¡ng phÃ¡p deep residual learning, ta Ä‘Ã£ tÃ¡c Ä‘á»™ng vÃ o lá»›p hÃ m chá»©a hÃ m mong muá»‘n, sao cho lá»›p má»›i lÃ  lá»›n hÆ¡n (bao gá»“m) lá»›p cÅ©. Nguá»“n: Dive into DL Äá»ƒ minh há»a cho yáº¿u tá»‘ \u0026ldquo;giÃºp viá»‡c há»c trá»Ÿ nÃªn dá»… hÆ¡n\u0026rdquo;, ta cÃ³ vÃ­ dá»¥ nhÆ° sau:\nÄá»“ thá»‹ cá»§a loss function cÅ©ng \"trÆ¡n\" hÆ¡n ráº¥t nhiá»u khi cÃ³ sá»­ dá»¥ng skip connection, tá»« Ä‘Ã³ ta sáº½ dá»… huáº¥n luyá»‡n mÃ´ hÃ¬nh hÆ¡n Nguá»“n: Jeremy Jordan Skip connection NhÆ° váº­y, Ä‘iá»ƒm nháº¥n cá»§a ResNet lÃ  ta Ä‘i há»c cÃ¡c hÃ m pháº§n dÆ°, thÃ´ng qua viá»‡c â€œgá»£i Ã½â€ cho hÃ m sá»‘ mong muá»‘n má»™t giÃ¡ trá»‹ báº±ng vá»›i chÃ­nh giÃ¡ trá»‹ input ban Ä‘áº§u. Trong cÃ i Ä‘áº·t, thao tÃ¡c nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua má»™t káº¿t ná»‘i gá»i lÃ  skip connection. Ta sáº½ cá»™ng input $\\bold{x}$ vÃ o output cá»§a block thÃ´ng thÆ°á»ng.\nÄá»‘i vá»›i viá»‡c cá»™ng nhÆ° váº­y thÃ¬ thá»±c cháº¥t lÃ  ta Ä‘ang Ä‘i cá»™ng hai ma tráº­n. Khi Ä‘Ã³, má»™t váº¥n Ä‘á» cÃ³ thá»ƒ náº£y sinh lÃ  vá» shape cá»§a chÃºng, mÃ  thÆ°á»ng lÃ  sá»‘ channel. Náº¿u sá»‘ channel khÃ´ng khá»›p thÃ¬ ta cáº§n pháº£i tiáº¿n hÃ nh â€œÄ‘iá»u chá»‰nhâ€. VÃ¬ cÃ¡c block Ä‘Æ°á»£c sá»­ dá»¥ng thÆ°á»ng gá»“m cÃ¡c conv layer vá»›i stride vÃ  padding phÃ¹ há»£p Ä‘á»ƒ giá»¯ nguyÃªn width vÃ  height nÃªn ta sáº½ chá»‰ xÃ©t Ä‘áº¿n sá»‘ channel. Náº¿u sá»‘ channel cá»§a $\\bold{x}$ vÃ  output ban Ä‘áº§u lÃ  nhÆ° nhau thÃ¬ ta gá»i skip connection nÃ y lÃ  identity skip connection. NgÆ°á»£c láº¡i, ta sáº½ dÃ¹ng conv layer $1 \\times 1$ Ä‘á»ƒ Ä‘iá»u chá»‰nh sá»‘ channel cá»§a $\\bold{x}$. LÃºc nÃ y, skip connection Ä‘Æ°á»£c gá»i lÃ  projection skip connection. Sá»­ dá»¥ng skip connection (identity) Nguá»“n: Dive into DL NgoÃ i ra, ta cÃ²n cÃ³ má»™t Ä‘iá»ƒm máº¡nh quan trá»ng cá»§a skip connection lÃ  nÃ³ giÃºp cho gradient Ä‘Æ°á»£c lan truyá»n tá»‘t hÆ¡n trong quÃ¡ trÃ¬nh backpropagation, tá»« Ä‘Ã³ gÃ³p pháº§n lÃ m giáº£m hiá»‡n tÆ°á»£ng vanishing gradient.\nQuan sÃ¡t hÃ¬nh phÃ­a trÃªn, ta tháº¥y ráº±ng khi backpropagation thÃ¬ layer á»Ÿ ngay trÆ°á»›c block nháº­n Ä‘Æ°á»£c gradient tá»« hai layer phÃ­a sau nÃ³ (má»™t layer liá»n trÆ°á»›c nÃ³ vÃ  má»™t layer Ä‘Æ°á»£c káº¿t ná»‘i thÃ´ng qua skip connection). Váº¥n Ä‘á» exploding gradient NhÆ° mÃ¬nh Ä‘Ã£ Ä‘á» cáº­p á»Ÿ Ä‘áº§u bÃ i viáº¿t, vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n thÃ¬ hai váº¥n Ä‘á» Ä‘á»‘i vá»›i gradient mÃ  ta thÆ°á»ng gáº·p lÃ  vanishing vÃ  exploding. Äáº§u tiÃªn, ta cÃ³ nhá»¯ng cÃ¡ch khá»Ÿi táº¡o trá»ng sá»‘ Ä‘á»ƒ háº¡n cháº¿ cÃ¡c váº¥n Ä‘á» nÃ y nhÆ° He Initialization. NgoÃ i ra, vá»›i vanishing thÃ¬ cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c pháº§n nÃ o háº¡n cháº¿ Ä‘i báº±ng skip connection. Tuy nhiÃªn, náº¿u dÃ¹ng He Initialization cÃ¹ng vá»›i skip connection thÃ¬ skip connection láº¡i cÃ³ kháº£ nÄƒng gÃ¢y ra exploding gradient ğŸ˜ƒ\nLÃ½ do cá»§a Ä‘iá»u nÃ y náº±m á»Ÿ thao tÃ¡c cá»™ng giÃ¡ trá»‹ $\\bold{x}$ vá»›i giÃ¡ trá»‹ tÃ­nh Ä‘Æ°á»£c cá»§a hÃ m $f(\\bold{x})$. Ta hoÃ n toÃ n cÃ³ thá»ƒ xem $\\bold{x}$ vÃ  $f(\\bold{x})$ lÃ  hai biáº¿n ngáº«u nhiÃªn Ä‘á»™c láº­p, khi Ä‘Ã³\n$$\\text{var}(\\bold{x} + f(\\bold{x})) = \\text{var} (\\bold{x}) + \\text{var} (f(\\bold{x}))$$\nHÆ¡n ná»¯a, khi ta dÃ¹ng ReLU vÃ  He Initialization thÃ¬ $\\text{var}(f(\\bold{x})) = \\text{var}(\\bold{x})$. Do Ä‘Ã³, cÃ³ thá»ƒ nÃ³i lÃ  ta Ä‘Ã£ lÃ m cho phÆ°Æ¡ng sai cá»§a input $\\bold{x}$ tÄƒng gáº¥p Ä‘Ã´i sau khi Ä‘i qua má»™t block cÃ³ dÃ¹ng skip connection. NhÆ° váº­y, khi dáº§n qua nhiá»u block thÃ¬ phÆ°Æ¡ng sai sáº½ cá»© tÄƒng lÃªn theo sá»‘ mÅ© 2 vÃ  cÃ³ thá»ƒ dáº«n Ä‘áº¿n hiá»‡n tÆ°á»£ng exploding gradient.\nÄá»ƒ háº¡n cháº¿ váº¥n Ä‘á» exploding nhÆ° trÃªn, ta cÃ³ hai cÃ¡ch phá»• biáº¿n:\nSau khi tÃ­nh xong $h(\\bold{x}) = \\bold{x} + f(\\bold{x})$ thÃ¬ ta sáº½ scale $h(\\bold{x})$ báº±ng cÃ¡ch nhÃ¢n nÃ³ vá»›i $\\dfrac{1}{\\sqrt{2}}$. Sá»­ dá»¥ng Batch Normalization vá»›i hai trá»ng sá»‘ $(\\gamma, \\delta) = (1, 0)$ Ä‘á»ƒ chuáº©n hÃ³a $\\bold{x}$ trÆ°á»›c khi tÃ­nh $f(\\bold{x})$ (khÃ´ng thay Ä‘á»•i giÃ¡ trá»‹ $\\bold{x}$ gá»‘c vÃ¬ giÃ¡ trá»‹ gá»‘c sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng cho phÃ©p cá»™ng á»Ÿ sau. Báº±ng cÃ¡ch nÃ y, káº¿t há»£p vá»›i He Initialization thÃ¬ $f(\\bold{x})$ cÅ©ng sáº½ cÃ³ phÆ°Æ¡ng sai lÃ  1. Tá»« Ä‘Ã³, phÆ°Æ¡ng sai cá»§a tá»•ng $h(\\bold{x})$ sáº½ chá»‰ tÄƒng tuyáº¿n tÃ­nh theo sá»‘ lÆ°á»£ng block Ä‘Æ°á»£c sá»­ dá»¥ng. Minh há»a hai cÃ¡ch háº¡n cháº¿ exploding trong skip connection\nNguá»“n: Understanding Deep Learning - Simon J.D. Prince CÃ¡ch giáº£i quyáº¿t sá»‘ 2 Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng trong kiáº¿n trÃºc cá»§a ResNet. ÄÃ¢y lÃ  lÃ½ do ta tháº¥y cÃ¡c layer Batch Normalization Ä‘Æ°á»£c sá»­ dá»¥ng trong residual block á»Ÿ pháº§n tiáº¿p theo.\nResidual block CÃ¡c loáº¡i residual block Residual block (khá»‘i pháº§n dÆ°) Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch thÃªm má»™t skip connection vÃ o má»™t block thÃ´ng thÆ°á»ng trong cÃ¡c mÃ´ hÃ¬nh CNN nhÆ° VGG block. Ta cÃ³ hai loáº¡i residual block nhÆ° sau:\nBasic: CÃ¡c conv layer trong block nÃ y cÃ³ filter $3 \\times 3$. Block dáº¡ng nÃ y thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng cho cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u vá»«a pháº£i. Bottleneck: Nháº±m giáº£m bá»›t sá»‘ lÆ°á»£ng tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n, cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng dáº¡ng block nÃ y vá»›i hai conv layer $1 \\times 1$ vá»›i vai trÃ² lÃ  giáº£m/tÄƒng sá»‘ channel, táº¡o ra má»™t hÃ¬nh dÃ¡ng giá»‘ng nhÆ° nÃºt tháº¯t cá»• chai. Hai layer conv $1 \\times 1$ nhÆ° váº­y Ä‘Æ°á»£c gá»i lÃ  bottleneck layer. Äá»ƒ cho dá»… hÃ¬nh dÃ¹ng thÃ¬ ta cÃ³ thá»ƒ xem Ä‘Ã¢y nhÆ° lÃ  thao tÃ¡c â€œcÃ´ Ä‘á»ng kiáº¿n thá»©câ€ cá»§a mÃ´ hÃ¬nh, hay nÃ³i rÃµ hÆ¡n lÃ  nÃ©n lÆ°á»£ng thÃ´ng tin láº¡i sao cho vá»«a giá»¯ Ä‘Æ°á»£c thÃ´ng tin vÃ  vá»«a tiáº¿t kiá»‡m tÃ i nguyÃªn (bá»™ nhá»›, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n). Basic Residual Block vÃ  Bottleneck Residual Block NgoÃ i ra, ta cÃ²n cÃ³ 2 loáº¡i skip connection lÃ  identity vÃ  projection. LÃºc nÃ y, tÃ¹y vÃ o sá»‘ channels cá»§a input vÃ  output ban Ä‘áº§u cÃ³ khá»›p hay khÃ´ng mÃ  block tÆ°Æ¡ng á»©ng sáº½ chá»©a loáº¡i skip connection phÃ¹ há»£p.\nBasic residual block vá»›i identity vÃ  projection skip connection Nguá»“n: Dive into DL Thá»© tá»± thá»±c hiá»‡n cÃ¡c phÃ©p toÃ¡n Quan sÃ¡t ká»¹ hÃ¬nh áº£nh mÃ´ táº£ residual block á»Ÿ phÃ­a trÃªn, ta sáº½ tháº¥y ráº±ng thao tÃ¡c cá»™ng input $\\bold{x}$ vÃ  output cá»§a layer Batch Norm thá»© hai (chÃ­nh lÃ  $f(\\bold{x})$) Ä‘Æ°á»£c thá»±c hiá»‡n trÆ°á»›c khi Ä‘i qua ReLU. VÃ¬ sao láº¡i nhÆ° tháº¿?\nLÃ½ do ráº¥t Ä‘Æ¡n giáº£n. Náº¿u ta Ä‘Æ°a $f(\\bold{x})$ qua ReLU trÆ°á»›c rá»“i má»›i cá»™ng vÃ o $\\bold{x}$ thÃ¬ $\\bold{x}$ Ä‘ang Ä‘Æ°á»£c cá»™ng vá»›i má»™t lÆ°á»£ng khÃ´ng Ã¢m. Cá»© nhiá»u láº§n nhÆ° tháº¿ thÃ¬ giÃ¡ trá»‹ model tÃ­ch lÅ©y Ä‘Æ°á»£c sáº½ chá»‰ cÃ³ tÄƒng dáº§n lÃªn chá»© khÃ´ng cÃ³ giáº£m ğŸ˜ƒ Äiá»u nÃ y cháº¯c cháº¯n lÃ  khÃ´ng á»•n vÃ  cÃ³ thá»ƒ khiáº¿n mÃ´ hÃ¬nh khÃ´ng thá»ƒ há»c Ä‘Æ°á»£c Ä‘áº·c trÆ°ng gÃ¬ cÃ³ Ã­ch cáº£.\nKiáº¿n trÃºc ResNet ResNet Ä‘Æ°á»£c táº¡o nÃªn báº±ng cÃ¡ch sá»­ dá»¥ng nhiá»u residual block liÃªn tiáº¿p nhau, tÆ°Æ¡ng tá»± nhÆ° nhá»¯ng gÃ¬ mÃ  GoogLeNet hay VGG Ä‘Ã£ thá»±c hiá»‡n. CÃ¡c tÃ¡c giáº£ cá»§a paper ResNet táº¡o ra nhiá»u phiÃªn báº£n ResNet khÃ¡c nhau vá»›i Ä‘á»™ sÃ¢u tÄƒng dáº§n.\nDá»±a vÃ o sá»‘ lÆ°á»£ng layer cÃ³ trá»ng sá»‘ thÃ¬ ta sáº½ cÃ³ tÃªn cÃ¡c mÃ´ hÃ¬nh nhÆ° ResNet18 (18 layer cÃ³ trá»ng sá»‘), ResNet34,â€¦ CÃ¡c phiÃªn báº£n ResNet Ta cÃ³ má»™t sá»‘ nháº­n xÃ©t nhÆ° sau:\nCÃ¡c phiÃªn báº£n cÃ³ Ä‘á»™ sÃ¢u lá»›n nhÆ° 50, 101 vÃ  152 sá»­ dá»¥ng Bottleneck Residual Block. Vá»›i 18 vÃ  34 thÃ¬ chÃºng dÃ¹ng Basic block Trong cÃ¡c kiáº¿n trÃºc trÃªn thÃ¬ ta sá»­ dá»¥ng cáº£ 2 loáº¡i skip connection: identity vÃ  projection VÃ­ dá»¥, vá»›i ResNet18 thÃ¬ trong cá»¥m Basic block Ä‘áº§u tiÃªn cá»§a cá»¥m conv3_x sáº½ cÃ³ sá»‘ channel lÃ  64, cÃ²n output ban Ä‘áº§u cá»§a block nÃ y thÃ¬ lÃ  128 nÃªn ta pháº£i Ã¡p dá»¥ng projection vÃ o input Äá»‘i vá»›i height vÃ  width thÃ¬ cÃ¡c tÃ¡c giáº£ cho biáº¿t viá»‡c downsampling input sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n táº¡i conv layer Ä‘áº§u tiÃªn cá»§a cÃ¡c cá»¥m conv3_x, conv4_x, conv5_x (vá»›i stride lÃ  2). CÃ¡c conv layer khÃ¡c thÃ¬ Ä‘á»u cÃ³ stride 1. Ta sáº½ cáº§n chÃº Ã½ Ä‘áº¿n chi tiáº¿t nÃ y khi tiáº¿n hÃ nh cÃ i Ä‘áº·t ResNet. CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t ResNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nTÃ i liá»‡u tham kháº£o Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into DL, ResNet Simon J.D. Prince, Understanding Deep Learning ","date":"2023-02-08T17:41:09+07:00","permalink":"https://htrvu.github.io/post/resnet/","title":"Resnet (2015)"},{"content":" CÃ¡ nhÃ¢n mÃ¬nh tháº¥y GoogLeNet lÃ  má»™t paper khÃ³ Ä‘á»c. Khi viáº¿t ra bÃ i nÃ y thÃ¬ mÃ¬nh váº«n Ä‘ang cáº£m tháº¥y hÆ¡i lÃº vá» ná»™i dung cá»§a nÃ³ ğŸ˜€\nGiá»›i thiá»‡u Tá»« khi AlexNet Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o nÄƒm 2012 vÃ  Ä‘áº·t ná»n táº£ng cho cÃ¡c máº¡ng Deep CNN, GoogLeNet, hay Inception V1 (2014), lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc cÃ³ cÃ¡ch thiáº¿t káº¿ ráº¥t thÃº vá»‹ khi nÃ³ táº­n dá»¥ng hiá»‡u quáº£ cÃ¡c conv layer, Ä‘áº·t ná»n mÃ³ng cho nhiá»u mÃ´ hÃ¬nh sau nÃ y.\nâ€œInceptionâ€ cÃ³ thá»ƒ dá»‹ch lÃ  â€œsá»± khá»Ÿi Ä‘áº§uâ€, nghe cÃ³ váº» ráº¥t há»£p lÃ½ ğŸ˜€. NgoÃ i ra, á»Ÿ trong bÃ i viáº¿t vá» VGG, mÃ¬nh cÃ³ Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» thiáº¿t káº¿ kiáº¿n trÃºc mÃ´ hÃ¬nh theo hÆ°á»›ng cÃ³ sá»± láº·p láº¡i cÃ¡c khuÃ´n máº«u. GoogLeNet cÅ©ng sáº½ Ä‘Æ°á»£c thiáº¿t káº¿ nhÆ° váº­y.\nLÆ°u Ã½. TÃªn cá»§a mÃ´ hÃ¬nh nÃ y lÃ  GoogLeNet, khÃ´ng pháº£i GoogleNet =)) TÃ¡c giáº£ cho biáº¿t Ã½ nghÄ©a cá»§a cÃ¡i tÃªn nÃ y lÃ  â€œThis name is a homage to Yann LeCuns pioneering LeNet 5 network.â€\nGoogLeNet Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« nhá»¯ng má»¥c tiÃªu cá»§a nghiÃªn cá»©u nhÆ° sau:\nNÃ¢ng cao kháº£ nÄƒng táº­n dá»¥ng tÃ i nguyÃªn tÃ­nh toÃ¡n Cho phÃ©p tÄƒng chiá»u rá»™ng vÃ  chiá»u sÃ¢u cá»§a kiáº¿n trÃºc mÃ´ hÃ¬nh mÃ  váº«n Ä‘áº£m báº£o Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n lÃ  á»Ÿ má»©c cháº¥p nháº­n Ä‘Æ°á»£c. GoogLeNet tháº­t sá»± Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng Ä‘iá»u Ä‘Ã³, vÃ  nÃ³ Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn nguyÃªn lÃ½ Hebbian: â€œneurons that fire together, wire togetherâ€. Paper GoogLeNet Ä‘Ã£ Ä‘Æ°a ra cÃ¡c ná»n táº£ng lÃ½ thuyáº¿t ráº¥t â€œcÄƒng tháº³ngâ€ Ä‘á»ƒ cho tháº¥y ráº±ng mÃ´ hÃ¬nh CNN cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng â€œÄ‘á»§ tá»‘tâ€, Ä‘iá»u mÃ  ta chÆ°a Ä‘Æ°á»£c biáº¿t Ä‘áº¿n á»Ÿ trong cÃ¡c paper trÆ°á»›c Ä‘Ã³ ğŸ˜€\nNgoÃ i ra, cÃ³ má»™t quan Ä‘iá»ƒm khÃ¡ thÃº vá»‹ khi mÃ´ táº£ vá» kiáº¿n trÃºc cá»§a GoogLeNet nhÆ° sau: Khi xÃ¢y dá»±ng kiáº¿n trÃºc CNN, thay vÃ¬ pháº£i suy nghÄ© xem trong cÃ¡c máº¡ng CNN ta nÃªn Ã¡p dá»¥ng filter vá»›i kÃ­ch thÆ°á»›c bao nhiÃªu, hÃ£y Ã¡p dá»¥ng luÃ´n nhiá»u filter vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau vÃ  tá»•ng há»£p káº¿t quáº£ láº¡i ğŸ˜œ\nMÃ¬nh cÅ©ng cÃ³ Ä‘á»“ng tÃ¬nh vá»›i quan Ä‘iá»ƒm nÃ y. Tuy nhiÃªn, nguá»“n gá»‘c Ä‘áº±ng sau nÃ³ cÃ³ váº» khÃ´ng chá»‰ Ä‘Æ¡n giáº£n lÃ  nhÆ° váº­y. Trong paper, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ tiáº¿n hÃ nh phÃ¢n tÃ­ch vÃ  thá»­ nghiá»‡m nhiá»u láº¯m cho ra Ä‘Æ°á»£c cÃ¡i kiáº¿n trÃºc cá»§a GoogLeNet. Káº¿t ná»‘i thÆ°a vÃ  CNN Äáº§u tiÃªn, táº¡i thá»i Ä‘iá»ƒm trÆ°á»›c khi Inception Ä‘Æ°á»£c cÃ´ng bá»‘, chÃºng ta cÃ³ thá»ƒ cáº£i thiá»‡n má»™t mÃ´ hÃ¬nh DNN báº±ng nhá»¯ng cÃ¡ch nhÆ° sau:\nTÄƒng chiá»u sÃ¢u cá»§a mÃ´ hÃ¬nh (tá»©c lÃ  sá»‘ layer) TÄƒng chiá»u rá»™ng (sá»‘ channel trong má»—i layer) Tuy nhiÃªn, vá»›i hai cÃ¡ch trÃªn thÃ¬ sáº½ cÃ³ nhá»¯ng váº¥n Ä‘á» mÃ  ta cáº§n lÆ°u tÃ¢m lÃ  hiá»‡n tÆ°á»£ng overfitting vÃ  sá»± gia tÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh.\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n má»™t hÆ°á»›ng Ä‘i cÃ³ thá»ƒ giáº£m bá»›t hai váº¥n Ä‘á» trÃªn lÃ  sá»­ dá»¥ng kiáº¿n trÃºc káº¿t ná»‘i thÆ°a (sparsely connected architectures). Ta cÃ³ thá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° sau:\nVá»›i hai fully connected layer liÃªn tiáº¿p nhau, má»—i neuron trong layer sau sáº½ káº¿t ná»‘i vá»›i táº¥t cáº£ cÃ¡c unit trong layer trÆ°á»›c. Náº¿u nhÆ° ta thay Ä‘á»•i Ä‘i má»™t chÃºt, má»—i neuron trong layer sau sáº½ chá»‰ káº¿t ná»‘i Ä‘áº¿n má»™t vÃ i unit trong layer trÆ°á»›c, kiáº¿n trÃºc cÃ³ Ä‘Æ°á»£c sáº½ trá»Ÿ nÃªn â€œthÆ°aâ€ hÆ¡n. Káº¿t ná»‘i thÆ°a trong fully connected layer Äá»‘i vá»›i conv layer thÃ¬ ta Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c tÃ­nh cháº¥t thÆ°a nhÆ° váº­y. Ta biáº¿t ráº±ng, vá»›i má»—i pháº§n tá»­ trÃªn feature map cá»§a layer hiá»‡n táº¡i thÃ¬ ta tÃ­nh nÃ³ dá»±a vÃ o má»™t vÃ¹ng nhá» trÃªn feature map output cá»§a layer trÆ°á»›c Ä‘Ã³. Giáº£ sá»­ hai layer nÃ y Ä‘á»u chá»‰ cÃ³ 1 channel thÃ¬ ta cÃ³ thá»ƒ biá»ƒu diá»…n nÃ³ nhÆ° hÃ¬nh bÃªn dÆ°á»›i: Káº¿t ná»‘i thÆ°a trong conv layer Kiáº¿n trÃºc thÆ°a Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ mÃ´ táº£ lÃ  mÃ´ phá»ng láº¡i há»‡ thá»‘ng sinh há»c (vÃ­ dá»¥ nhÆ° khi ta nhÃ¬n vÃ o má»™t Ä‘á»‘i tÆ°á»£ng thÃ¬ ta thÆ°á»ng chá»‰ chÃº Ã½ má»™t sá»‘ Ä‘iá»ƒm trÃªn Ä‘á»‘i tÆ°á»£ng Ä‘Ã³ thÃ´i, khÃ³ mÃ  chÃº Ã½ táº¥t cáº£ Ä‘Æ°á»£c).\nNgoÃ i ra, cÃ³ má»™t ná»n tÃ ng lÃ½ thuyáº¿t ráº¥t trÃ¢u bÃ² vá» kiáº¿n trÃºc thÆ°a cá»§a Arora nhÆ° sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Tháº­t sá»± mÃ¬nh cÅ©ng chÆ°a hiá»ƒu háº¿t Ã½ cá»§a cÃ¢u trÃªnâ€¦ NhÆ°ng Ä‘áº¡i Ã½ cá»§a nÃ³ cÃ³ váº» lÃ  náº¿u ta biá»ƒu diá»…n Ä‘Æ°á»£c phÃ¢n phá»‘i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cá»§a má»™t dataset báº±ng má»™t kiáº¿n trÃºc DNN lá»›n vÃ  thÆ°a thÃ¬ tá»« Ä‘Ã³ ta cÃ³ thá»ƒ xÃ¢y nÃªn Ä‘Æ°á»£c má»™t kiáº¿n trÃºc tá»‘i Æ°u (vá» cáº£ Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ phá»©c táº¡p), báº±ng cÃ¡ch xÃ¢y tá»«ng layer má»™t ğŸ˜€\nTuy nhiÃªn, vá»›i cÃ¡c tÃ i nguyÃªn pháº§n cá»©ng trong thá»i gian nÃ y thÃ¬ ráº¥t khÃ³ Ä‘á»ƒ ta cÃ³ thá»ƒ tÃ­nh toÃ¡n trÃªn cÃ¡c máº¡ng DNN thÆ°a. Do Ä‘Ã³, cÃ¡c tÃ¡c giáº£ Ä‘i theo hÆ°á»›ng tÃ¬m má»™t mÃ´ hÃ¬nh lÃ  cÃ³ táº­n dá»¥ng má»™t sá»‘ thÃ´ng tin vá» tÃ­nh cháº¥t thÆ°a nhÆ°ng váº«n thá»±c hiá»‡n tÃ­nh toÃ¡n trÃªn cÃ¡c ma tráº­n Ä‘áº§y Ä‘á»§. Äáº¥y chÃ­nh lÃ  hÆ°á»›ng sá»­ dá»¥ng cÃ¡c phÃ©p toÃ¡n convolution!.\nCÅ©ng vÃ¬ lÃ½ do nÃ y, á»Ÿ pháº§n kiáº¿n trÃºc cá»§a GoogLeNet thÃ¬ ta sáº½ tháº¥y nÃ³ chá»‰ cÃ³ duy nháº¥t má»™t fully connected layer Ä‘á»ƒ sinh ra output, cÃ²n láº¡i chá»‰ toÃ n conv layer thÃ´i ğŸ˜— NÃ³i Ä‘áº¿n viá»‡c Ã¡p dá»¥ng cÃ¡c filter trong conv layer, cÃ¡c tÃ¡c giáº£ cho ráº±ng:\nMá»—i pháº§n tá»­ trong feature map cá»§a má»™t layer sáº½ cÃ³ má»‘i tÆ°Æ¡ng quan vá»›i má»™t vÃ¹ng nÃ o Ä‘Ã³ trÃªn áº£nh input (receptive field). Ta sáº½ gom cÃ¡c pháº§n tá»­ cÃ¹ng tÆ°Æ¡ng quan vá»›i má»™t vÃ¹ng vÃ o cÃ¹ng má»™t cá»¥m. Äá»ƒ Ã½ ráº±ng, trong nhá»¯ng layer á»Ÿ gáº§n áº£nh input thÃ¬ ta sáº½ cÃ³ nhiá»u cá»¥m vÃ  kÃ­ch thÆ°á»›c má»—i cá»¥m lÃ  nhá». CÃ ng Ä‘i qua cÃ¡c layer CNN thÃ¬ sá»‘ lÆ°á»£ng cá»¥m sáº½ Ã­t láº¡i vÃ  kÃ­ch thÆ°á»›c cá»¥m sáº½ lan rá»™ng ra. Táº¥t nhiÃªn lÃ  váº«n cÃ³ thá»ƒ cÃ³ nhá»¯ng cá»¥m cÃ³ kÃ­ch thÆ°á»›c nhá» táº¡i nhá»¯ng layer Ä‘Ã³. Do váº­y, ta nÃªn cÃ³ cÃ¡c filter vá»›i kÃ­ch thÆ°á»›c lá»›n hÆ¡n Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng táº¡i cÃ¡c cá»¥m lá»›n, Ä‘á»“ng thá»i cÅ©ng cáº§n cÃ³ filter kÃ­ch thÆ°á»›c nhá» Ä‘á»‘i vá»›i cÃ¡c cá»¥m nhá» hÆ¡n. Qua má»™t loáº¡t cÃ¡c thá»­ nghiá»‡m, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ chá»n 3 filter lÃ  $1 \\times 1$, $3 \\times 3$ vÃ  $5 \\times 5$. Má»™t khá»‘i chá»©a 3 filter trÃªn Ä‘Æ°á»£c Ä‘áº·t tÃªn lÃ  Inception module.\nBÃ¢y giá» quay láº¡i vá»›i lÃ½ thuyáº¿t cá»§a Arora, á»Ÿ Ã½ xÃ¢y dá»±ng máº¡ng tá»‘i Æ°u qua tá»«ng layer má»™t. GoogLeNet Ä‘Æ°á»£c táº¡o nÃªn báº±ng Ä‘Ãºng Ã½ tÆ°á»Ÿng nhÆ° váº­y, ta ná»‘i Inception module - by - Inception module ğŸ˜€\nP/s: CÃ¡c báº¡n cÃ³ thá»ƒ tÃ¬m Ä‘á»c pháº§n Motivation trong paper gá»‘c vÃ  tá»± cáº£m nháº­n nÃ³ nhÃ©.\nInception module Inception module Qua cÃ¡c mÃ´ táº£ á»Ÿ pháº§n trÃªn, ta cÃ³ thá»ƒ liÃªn tÆ°á»Ÿng Ä‘áº¿n kiáº¿n trÃºc cá»§a Inception module lÃ  má»™t thá»© gÃ¬ Ä‘Ã³ giá»‘ng vá»›i cÃ¡i á»Ÿ hÃ¬nh (a), vá»›i Ä‘á»§ 3 loáº¡i filter lÃ  $1 \\times 1$, $3 \\times 3$ vÃ  $5 \\times 5$. CÃ¡c tÃ¡c giáº£ dÃ¹ng thÃªm cáº£ má»™t layer max pooling trong Ä‘Ã³ ná»¯a, vá»›i lÃ­ do ráº¥t Ä‘Æ¡n giáº£n lÃ  vÃ¬ á»Ÿ thá»i Ä‘iá»ƒm Ä‘Ã³ thÃ¬ max pooling thÆ°á»ng mang láº¡i hiá»‡u quáº£ tá»‘t trong cÃ¡c kiáº¿n trÃºc máº¡ng CNN =))\nTruy nhiÃªn, cÃ¡ch cÃ i Ä‘áº·t nhÆ° hÃ¬nh (a) sáº½ dáº«n Ä‘áº¿n sá»‘ lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh lÃ  ráº¥t lá»›n. Thay vÃ o Ä‘Ã³, ta cÃ³ thá»ƒ táº¡o ra kiáº¿n trÃºc dáº¡ng â€œbottleneckâ€ báº±ng cÃ¡ch sá»­ dá»¥ng thÃªm cÃ¡c layer convolution $1 \\times 1$ nhÆ° hÃ¬nh (b), nháº±m má»¥c Ä‘Ã­ch chÃ­nh lÃ  giáº£m sá»‘ channel. Sá»‘ lÆ°á»£ng phÃ©p tÃ­nh lÃºc nÃ y sáº½ Ä‘Æ°á»£c giáº£m má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ.\nLÆ°u Ã½ ráº±ng, á»Ÿ cuá»‘i module ta cÃ³ phÃ©p toÃ¡n concatenate, tá»©c lÃ  cÃ¡c feature-maps cá»§a toÃ n bá»™ layer convolution Ä‘á»u pháº£i cÃ³ cÃ¹ng size (tá»©c lÃ  width vÃ  height)\nKiáº¿n trÃºc GoogLeNet GoogLeNet sá»­ dá»¥ng tá»•ng cá»™ng 9 Inception module, gá»“m cÃ³ 22 layer cÃ³ trá»ng sá»‘ (tÃ­nh cáº£ pooling lÃ  27), Ä‘Æ°á»£c tÃ³m táº¯t qua báº£ng sau:\nMá»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t khi train GoogLeNet lÃ  cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng cÃ¡c auxiliary classifiers (xem hÃ¬nh bÃªn dÆ°á»›i). CÃ¡c thÃ nh pháº§n nÃ y sáº½ khÃ¡ giá»‘ng nhÆ° giá»‘ng há»‡t vá»›i pháº§n cuá»‘i cá»§a mÃ´ hÃ¬nh (bá»™ classifier). NhÆ° váº­y, ta cÃ³ thá»ƒ xem GoogLeNet cÃ³ 3 bá»™ classifier. Auxiliary classifiers Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i cÃ¡c Ã½ nghÄ©a nhÆ° sau:\nHáº¡n cháº¿ vanishing gradient TÄƒng regularization LÆ°u Ã½:\nLoss khi huáº¥n luyá»‡n sáº½ cá»™ng loss cá»§a cáº£ ba láº¡i vá»›i nhau. Khi test thÃ¬ ta thÆ°á»ng chá»‰ quan tÃ¢m Ä‘áº¿n bá»™ classifer cuá»‘i cÃ¹ng. Má»™t cÃ¡ch lÃ m khÃ¡c lÃ  ta xÃ i cáº£ 3, sau Ä‘Ã³ láº¥y káº¿t quáº£ trung bÃ¬nh. Nguá»“n: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t GoogLeNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau. Trong cÃ¡ch cÃ i Ä‘áº·t nÃ y, mÃ¬nh sáº½ bá» qua auxiliary classifiers.\nTÃ i liá»‡u tham kháº£o Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"https://htrvu.github.io/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giá»›i thiá»‡u Dá»±a trÃªn sá»± thÃ nh cÃ´ng cá»§a AlexNet vÃ o nÄƒm 2012, nhiá»u nghiÃªn cá»©u Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh nháº±m tÃ¬m ra cÃ¡c phÆ°Æ¡ng phÃ¡p hay kiáº¿n trÃºc má»›i Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n, vÃ­ dá»¥ nhÆ°:\nThay Ä‘á»•i (tÄƒng, giáº£m) kÃ­ch thÆ°á»›c cá»§a conv filter Thay Ä‘á»•i stride, padding cá»§a conv layer Train vÃ  test trÃªn cÃ¡c input vá»›i nhiá»u Ä‘á»™ phÃ¢n giáº£i (resolution) áº£nh khÃ¡c nhau Trong nÄƒm 2014, VGG lÃ  má»™t trong nhá»¯ng káº¿t quáº£ nghiÃªn cá»©u ná»•i báº­t nháº¥t, vÃ  nÃ³ táº­p trung vÃ o má»™t váº¥n Ä‘á» khÃ¡c vá»›i cÃ¡c hÆ°á»›ng trÃªn lÃ  Ä‘á»™ sÃ¢u (depth, hay lÃ  sá»‘ layer) cá»§a mÃ´ hÃ¬nh. VGG Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c káº¿t quáº£ tá»‘t nháº¥t vÃ o thá»i Ä‘iá»ƒm nÃ³ ra má»›i trÃªn dataset ImageNet vÃ  cÃ¡c dataset khÃ¡c, trong cÃ¡c task nhÆ° classification, localization,â€¦\nNgoÃ i ra, ta cÃ³ thá»ƒ Ä‘Æ°a ra nháº­n xÃ©t nhÆ° sau vá» AlexNet:\nDÃ¹ AlexNet Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c CNN cÃ³ thá»ƒ Ä‘áº¡t Ä‘á»™ hiá»‡u quáº£ tá»‘t, nÃ³ láº¡i khÃ´ng cung cáº¥p má»™t khuÃ´n máº«u nÃ o cho viá»‡c nghiÃªn cá»©u, thiáº¿t káº¿ cÃ¡c máº¡ng má»›i. Theo thá»i gian, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ thay Ä‘á»•i suy nghÄ© tá»« quy mÃ´ nhá»¯ng neuron riÃªng láº» sang cÃ¡c táº§ng, rá»“i sau Ä‘Ã³ lÃ  cÃ¡c khá»‘i (block) gá»“m cÃ¡c táº§ng láº·p láº¡i theo khuÃ´n máº«u. Kiáº¿n trÃºc cá»§a VGG lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc phá»• biáº¿n Ä‘áº§u tiÃªn Ä‘Æ°á»£c xÃ¢y dá»±ng theo Ã½ tÆ°á»Ÿng nhÆ° váº­y.\nVGG block Äiá»ƒm ná»•i báº­t cá»§a VGG lÃ  ta chá»‰ dÃ¹ng duy nháº¥t má»™t kÃ­ch thÆ°á»›c filter trong má»i conv layer lÃ  $3 \\times 3$, vÃ  ta dáº§n tÄƒng Ä‘á»™ sÃ¢u cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡c conv layer. HÆ¡n ná»¯a, ta cÃ²n Ã¡p dá»¥ng nhiá»u conv layer liá»n nhau rá»“i má»›i dÃ¹ng Ä‘áº¿n max pooling. Ta cÃ³ thá»ƒ gá»i má»™t block gá»“m nhá»¯ng layer nhÆ° tháº¿ lÃ  VGG block.\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n má»™t váº¥n Ä‘á» cho cÃ¡ch Ã¡p dá»¥ng nÃ y nhÆ° sau: Viá»‡c dÃ¹ng nhiá»u conv layer 3 x 3 liá»n nhau nhÆ° váº­y so vá»›i dÃ¹ng má»™t conv layer vá»›i filter lá»›n hÆ¡n (vÃ­ dá»¥ 7 x 7) nhÆ° háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o thá»i Ä‘iá»ƒm trÆ°á»›c Ä‘Ã³ thÃ¬ cÃ³ gÃ¬ â€œtá»‘tâ€ hÆ¡n, khi mÃ  features map sau cÃ¹ng ta thu Ä‘Æ°á»£c cÃ³ thá»ƒ cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c? Äá»ƒ tráº£ lá»i, ta cÃ³ 2 Ã½ chÃ­nh nhÆ° sau: Giáº£m sá»‘ lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh (Ä‘áº·t tÃ­nh lÃ  biáº¿t nhaa ğŸ˜œ) DÃ¹ng nhiá»u conv layer thÃ¬ ta cÃ³ kháº£ nÄƒng sáº½ phÃ¡t hiá»‡n Ä‘Æ°á»£c nhiá»u feature cÃ³ Ã­ch hÆ¡n (hai conv layer sáº½ táº¡o thÃ nh má»™t \u0026ldquo;hÃ m há»£p\u0026rdquo;), tá»« Ä‘Ã³ decision function sáº½ ok hÆ¡n. NgoÃ i ra, VGG block sá»­ dá»¥ng padding 1 (giá»¯ nguyÃªn kÃ­ch thÆ°á»›c input), theo sau Ä‘Ã³ lÃ  max pooling vá»›i pool size $2 \\times 2$ vÃ  stride 2 (giáº£m kÃ­ch thÆ°á»›c input Ä‘i má»™t ná»­a). Kiáº¿n trÃºc cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ nhÆ° hÃ¬nh bÃªn dÆ°á»›i:\nVGG block Nguá»“n: Dive intro AI Trong cÃ¡c bÃ i toÃ¡n Ã¡p dá»¥ng VGG, Ä‘Ã´i khi ta cÃ³ thá»ƒ gáº·p VGG block vá»›i má»™t conv layer $1 \\times 1$ á»Ÿ trÆ°á»›c max pooling. Block dáº¡ng nÃ y thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i má»¥c Ä‘Ã­ch chÃ­nh lÃ  bá»• sung thÃªm má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh ná»¯a. Tuy nhiÃªn, trong thá»±c nghiá»‡m thÃ¬ cÃ¡c tÃ¡c giáº£ Ä‘Ã£ cho tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng block dáº¡ng nÃ y khÃ´ng hiá»‡u quáº£ hÆ¡n so vá»›i toÃ n cÃ¡c conv layer vá»›i filter $3 \\times 3$ (cÃ¹ng sá»‘ lÆ°á»£ng layer).\nKiáº¿n trÃºc VGG Báº±ng cÃ¡ch káº¿t há»£p nhiá»u VGG block vá»›i nhau, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ táº¡o ra nhiá»u phiÃªn báº£n VGG khÃ¡c nhau, vá»›i sá»‘ layer cÃ³ trá»ng sá»‘ lÃ  trong Ä‘oáº¡n 11 - 19. Trong paper VGG, ta cÃ³ 6 kiáº¿n trÃºc vá»›i Ä‘á»™ sÃ¢u tÄƒng dáº§n vÃ  tiáº¿n hÃ nh so sÃ¡nh vá»›i nhau. Äiá»ƒm chung cá»§a cÃ¡c kiáº¿n trÃºc nÃ y lÃ  pháº§n fully connected Ä‘á»u cÃ³ 3 layer, vÃ  toÃ n bá»™ layer Ä‘á»u sá»­ dá»¥ng activation ReLU.\nCÃ¡c phiÃªn báº£n VGG Ta cÃ³ má»™t sá»‘ nháº­n xÃ©t nhÆ° sau:\nSá»‘ lÆ°á»£ng conv layer trong cÃ¡c VGG block cá»§a cÃ¡c phiÃªn báº£n lÃ  tÄƒng dáº§n. Äá»ƒ Ã½ Ä‘áº¿n B, C vÃ  D thÃ¬: C = B + conv layer $1 \\times 1$ trong má»—i VGG block D = C + Ä‘á»•i conv layer $1 \\times 1$ thÃ nh $3 \\times 3$ Khi thá»±c nghiá»‡m, ta cÃ³ B \u0026lt; C \u0026lt; D. Do Ä‘Ã³, viá»‡c thÃªm phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh báº±ng conv layer $1 \\times 1$ giÃºp mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n, nhÆ°ng nÃ³ váº«n khÃ´ng báº±ng vá»›i viá»‡c lÃ  ta thÃªm luÃ´n conv layer $3 \\times 3$ ğŸ¤”. Äá»™ rá»™ng (sá»‘ channel) trong tá»«ng block Ä‘Æ°á»£c tÄƒng theo bá»™i 2. Ã tÆ°á»Ÿng nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t rá»™ng rÃ£i trong thá»i Ä‘iá»ƒm nÃ y vÃ  cáº£ vá» sau Äá»ƒ háº¡n cháº¿ overfitting, ta cÃ³ thá»ƒ sá»­ dá»¥ng thÃªm dropout cho hai táº§ng fully connected Ä‘áº§u tiÃªn. Hai kiáº¿n trÃºc phá»• biáº¿n nháº¥t trong viá»‡c Ã¡p dá»¥ng VGG vÃ o cÃ¡c bÃ i toÃ¡n khÃ¡c lÃ  D (VGG16) vÃ  E (VGG19). Äá»ƒ trá»±c quan hÆ¡n, ta cÃ³ thá»ƒ thá»ƒ biá»ƒu diá»…n VGG16 nhÆ° sau:\nNguá»“n: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t VGG báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nTÃ i liá»‡u tham kháº£o Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"https://htrvu.github.io/post/vgg/","title":"VGG (2014)"}]
[{"content":"Note. Nội dung bài viết này phần lớn được trích từ báo cáo đồ án môn học Học thống kê của nhóm mình và bạn Trần Hữu Thiên tại HCMUS, với chủ đề là English-Vietnamese Machine Translation. Các bạn có thể xem mã nguồn của đồ án này tại đây 🚀\nTrong bài viết trước, mình đã giới thiệu về bài toán Machine Translation cùng một mô hình khá quen thuộc trong bài toán đó là Seq2seq. Trong giai đoạn trước năm 2017, các mô hình Deep Learning được xây dựng cho bài toán Machine Translation thường được tổ chức theo kiến trúc Encoder-Decoder như Seq2seq với phần lõi là các biến thể cải tiến hơn của Recurrent Neural Network (RNN). Tuy nhiên, những mô hình NTM đó đều có một số hạn chế nhất định khi thực hiện dịch những câu có câu trúc phức tạp hoặc có độ dài lớn, mà thường thấy nhất là hạn chế về độ phức tạp tính toán và việc ghi nhớ quan hệ phụ thuộc giữa các từ trong câu về mặt ngữ nghĩa. Lý do của những hạn chế này phần lớn đến từ thao tác tính toán, xử lý tuần tự các từ (hay là token) của RNN.\nTransformer, một mô hình được công bố vào năm 2017 đã tạo ra sự đột phá lớn trong lĩnh vực NLP với độ chính xác cũng như tốc độ xử lý đều vượt xa những mô hình Deep Learning trước đó. Dù vẫn tổ chức mô hình theo kiến trúc Encoder-Decoder thông dụng nhưng Transformer đã khắc phục được những nhược điểm của các mô hình trước nhờ vào việc tận dụng tối đa sức mạnh của cơ chế Attention với hai phép toán Self-Attention và Cross-Attention. Kể từ đó, mô hình Transformer đã trở thành một tiêu chuẩn mới trong bài toán Machine Translation nói riêng và trong nhiều bài toán thuộc lĩnh vực NLP nói chung. Không dừng lại ở đó, ý tưởng từ kiến trúc của Transformer cũng đã được áp dụng một cách rất thành công ở các bài toán thuộc lĩnh vực khác như Computer Vision.\nTrong bài viết này, mình sẽ trình bày về Transformer cùng cơ chế Attention (tổng quan và sau đó đi vào cụ thể với Self-Attention và Cross-Attention). Nội dung của bài này sẽ là rất dài.\nTổng quan về kiến trúc Transformer Mô hình Transformer có kiến trúc được tổ chức dạng Encoder-Decoder với phần lõi là cơ chế Attention. Tổng quan kiến trúc của mô hình này được thể hiện trong hình bên dưới, trong đó:\nEncoder có vai trò chính là học mối tương quan giữa các từ với nhau trong câu thuộc ngôn ngữ nguồn (cụ thể hơn là giữa từng từ với các từ còn lại trong câu). Ta có thể hiểu đây như là việc ta đọc một câu Tiếng Anh và cố gắng hiểu ý nghĩa của câu đó. Decoder sẽ lần lượt sinh ra các từ cho câu thuộc ngôn ngữ đích. Trong quá trình sinh từ này thì Decoder sẽ vừa để ý đến các từ nó đã sinh ra trước đó và vừa để ý đến một số từ liên quan trong câu ngôn ngữ nguồn để dịch cho đúng. Đây cũng có thể xem là sau khi đã hiểu ý nghĩa của câu Tiếng Anh rồi thì ta từng bước dịch câu đó ra Tiếng Việt sao cho trôi chảy, mạch lạc. Tổng quan kiến trúc của mô hình Transformer\nTa sẽ lần lượt đề cập đến những thành phần quan trọng trong kiến trúc của Transformer, mà đặc biệt là Scale Dot-Product Attention và Multi-Head Attention, thành phần đóng vai trò rất lớn trong việc tạo nên sức mạnh của mô hình này.\nWord Embedding và Positional Encoding Phần đầu tiên trong kiến trúc của Transformer là embedding, với hai thành phần là Word Embedding và Positional Encoding.\nWord Embedding là một phương pháp biểu diễn các từ trong câu thành các vector đặc trưng một cách hợp lý, sao cho các vector này thể hiện được mối quan hệ giữa các từ với nhau. Mình đã có đề cập đến nó ở trong bài viết này.\nĐối với bài toán Machine Translation, ta cần lưu ý rằng tập từ điển của ngôn ngữ nguồn và đích có thể có số từ khác nhau nên vector one-hot biểu diễn các từ trong câu thuộc ngôn ngữ nguồn và đích cũng có thể có số chiều khác nhau. Tuy nhiên, ta sẽ đều đưa chúng về các word embedding vectors với cùng số chiều và giá trị này được ký hiệu là là $d_{model}$. Kết quả khi sử dụng thuật toán t-SNE để trực quan hóa các word embedding vector trên không gian 2 chiều của tập từ vựng Tiếng Anh\nVì mô hình Transformer không tính toán tuần tự theo thứ tự của các từ trong câu như những mô hình dựa trên phần lõi là RNN (RNN-based) nên để có thể cung cấp thông tin về vị trí của các từ trong câu cho mô hình, các tác giả của Transformer đã đề xuất một phương pháp gọi là Positional Encoding. Phần thông tin có được từ Positional Encoding sẽ được cộng vào word embedding vectors của các từ trong câu ngôn ngữ nguồn và ngôn ngữ đích. Thao tác cộng này được thể hiện ở phần ngay sau \u0026ldquo;Input Embedding\u0026rdquo; và \u0026ldquo;Output Embedding\u0026rdquo; của hình mô tả kiến trúc ở phần 1.\nCông thức của Positional Encoding được xác định thông qua hàm Sinusoid. Tất nhiên là để cộng được positional encoding vector (PE) với word embedding vector (WE) thì hai vector này phải có cùng số chiều là $d_{model}$. Với từ ở vị trí thứ $pos$ trong câu, giá trị của phần tử tại ví trí $2i$ và $(2i+1)$ ($0 \\leq 2i, 2i+1 \\leq d_{model} - 1)$ trong PE được xác định bởi: $$ \\begin{align*} PE(pos, 2i) \u0026amp;= \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right) \\\\ PE(pos, 2i+1) \u0026amp;= \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right) \\end{align*} $$\nMinh họa cho kết quả của PE với các từ trong câu được thể hiện trong hình bên dưới với một câu gồm 20 từ, mỗi từ được biểu diễn bằng một WE 128 chiều. Ta thấy rằng PE của các từ trong câu đều đôi một khác nhau, tức là nó đã giúp ta mã hóa được thông tin vị trí của các từ.\nMinh họa kết quả của các positional encoding vectors\nNgoài ra, các tác giả của Transformer còn cho biết rằng việc sử dụng hàm Sinusoid để tính PE có thể giúp mô hình học được vị trí tương đối của các từ, khi mà $PE(pos)$ và $PE(pos + k)$ có thể được biến đổi qua lại thông qua một phép biến đổi tuyến tính, tức là tồn tại ma trận $M$ sao cho $$ \\begin{equation*} M \\cdot \\begin{bmatrix} \\sin(pos \\cdot \\omega_i) \\\\ \\cos(pos \\cdot \\omega_i) \\end{bmatrix} = \\begin{bmatrix} \\sin((pos + k) \\cdot \\omega_i) \\\\ \\cos((pos + k) \\cdot \\omega_i) \\end{bmatrix} \\end{equation*} $$ , với $\\omega_i = \\left (10000^{\\frac{2i}{d_{model}}} \\right)^{-1}$.\nNhư vậy, với $x_{pos}$ là vector one-hot của từ thứ $pos$ trong câu, ta có thể biểu diễn kết quả sau việc kết hợp word embedding vector và positional encoding vector như sau: $$ \\begin{equation*} z_{pos} = WE(x_{pos}) * \\sqrt{d_{model}} + PE(pos) \\end{equation*} $$ , trong đó $*$ là phép nhân element-wise.\nLưu ý 1. Một cách giải thích cho việc nhân WE với $\\sqrt{d_{model}}$ trước khi cộng với PE là ta muốn lượng thông tin nhận được từ PE sẽ không tác động quá nhiều đến những thông tin mà WE đã cung cấp về các từ.\nLưu ý 2. Ta còn có thể hình dung ý nghĩa positional encoding vector thông qua ví dụ về cách biểu diễn nhị phân của các số nguyên. Xét hình minh họa cho 16 số nguyên đầu tiên biên dưới thì:\nMỗi số được biểu diễn bằng duy nhất một chuỗi nhị phân và các chuỗi này đôi một phân biệt. Đi từ bit thấp đến bit cap thì \u0026ldquo;tần suất thay đổi\u0026rdquo; của các bit tăng dần. Ví dụ, từ 0 tới 7 thì bit 0 giữ nguyên, bit 1 thay đổi 1 lần, bit 2 thay đổi 3 lần, v.v. Nếu ta quan sát lại hình minh họa kết quả các positional encoding vectors ở phía trên thì cũng thấy tính chất tương tự. Liên hệ giữa Positional Encoding và cách biểu diễn nhị phân của các số nguyên\nCơ chế Attention và sự truy xuất thông tin Attention là một ý tưởng rất thú vị trong Deep Learning khi mà nó đã mô phỏng lại cách bộ não của con người hoạt động khi chúng ta phân tích, nhìn nhận một đối tượng. Ví dụ, mắt chúng ta có tầm nhìn rất rộng nhưng tại mỗi thời điểm thì ta chỉ tập trung vào một vùng nhất định trong tầm nhìn để lấy thông tin. Attention đã được áp dụng thành công vào nhiều lĩnh vực khác nhau, nhiều bài toán khác nhau trong Deep Learning.\nĐể mô tả sơ lược về quá trình tính toán của cơ chế Attention, ta xét cách Attention được áp dụng trong các mô hình trong bài toán Machine Translation, ở giai đoạn trước khi Transformer được công bố (hình bên dưới). Giả sử input của Encoder là các vector $x_i$ và output vector tương ứng là $h_i$, ta đang tính toán cho từ đầu tiên trong output của Decoder với input của Decoder là vector $y_1$. Lúc này, ta thực hiện Attention từ $y_1$ đến các vector $x_i$, với ý nghĩa là khi ta dịch từ đầu tiên này thì ta nên chú ý vào các từ nào ở trong câu nguồn.\nMô tả quá trình tính toán của cơ chế Attention\nTrong Attention, ta có các khái niệm về context vector $c_i$ và attention weight $\\alpha_{ij}$. Các giá trị attention weight $\\alpha_{ij}$ sẽ nằm trong đoạn [0, 1] và cho biết mức độ chú ý của vector $y_i$ vào vector $x_j$ và context vector $c_i$ là kết quả thu được khi thực hiện Attention từ vector $y_i$.\nTa tính context vector $c_i$ theo công thức $$ \\begin{equation*} c_i = \\sum_{j=1}^N \\left( \\alpha_{ij} \\cdot h_j \\right ) \\end{equation*} $$ , trong đó $N$ là độ dài của câu nguồn và attention weight $\\alpha_{ij}$ được xác định như sau: $$ \\begin{equation} \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{t=1}^N \\exp(e_{it})} % \\label{eq:attn_weights_1} \\end{equation} $$ , với $e_{ij} = f(y_i, h_j)$ và $f$ là một mô hình học, nó có thể đơn giản chỉ là Neural Network với một layer, input vector sẽ là vector được nối giữa $\\alpha_{ij}$ và $h_j$, output là vector 1 chiều.\nTừ công thức $(1)$, ta có thể viết gọn bằng cách biểu diễn bằng các vector $N$ chiều $\\alpha_i$ và $e_i$ như sau: $$ \\begin{equation*} \\alpha_i = \\text{softmax}(e_i) \\end{equation*} $$\nNhư vậy, $\\alpha_{ij}$ càng lớn thì $y_i$ càng chú ý vào $x_j$ và rõ ràng ta có $\\sum_{j=1}^N \\alpha_{ij} = 1$. Về mặt trực quan, người ta thường biểu diễn attention weights bằng một ma trận như hình bên dưới, trong đó ô có màu càng sáng thì sẽ ứng với attention weights càng lớn.\nMinh họa trực quan về attention weights\nTrong Transformer, Attention có thể được xem như là một cơ chế truy xuất thông tin, khi mà attention weights $\\alpha_{ij}$ càng lớn thì càng thể hiện sự \u0026ldquo;liên quan đến nhau\u0026rdquo; giữa của $y_i$ và $x_i$. Khi nói đến sự truy xuất thông tin, ta có 3 khái niệm được sử dụng là query, key và value. Một cách mô tả rất dễ hiểu về 3 khái niệm này với ví dụ Google Search trong bài viết của anh Quốc như sau:\nQuery: Vector dùng để chứa thông tin của từ được tìm kiếm, so sánh (hoặc là ma trận với mỗi dòng là vector ứng với một từ). Ví dụ, từ khóa mà ta nhập vào ô tìm kiếm của Google Search là query. Key: Ma trận dùng để biểu diễn thông tin chính của các từ được so sánh với từ cần tìm kiếm ở trên, mỗi dòng là vector ứng với một từ. Ví dụ, tiêu đề các trang web mà Google sẽ so sánh với từ khóa ta đã nhập là các key. Value: Ma trận biểu diễn đầy đủ nội dung, ý nghĩa của các từ có trong key, mỗi dòng là vector ứng với một từ. Nó như là nội dung các trang web được hiển thị cho người dùng sau khi tìm kiếm. \\end{itemize} Trong ví dụ về attention ở hình trên, query sẽ là vector $y_1$, keys và values sẽ đều là ma trận $H$ tạo bởi các vector $h_i$. Quá trình tính toán giữa query, keys và values trong ví dụ đó được mô tả lại trong hình bên dưới, với:\nAttention weights được tính từ query và keys. Context vector được tính từ attention weights và values. Attention với query, key và value. Trong đó, vector key và value của các từ trong hầu hết các trường hợp là giống nhau.\nLưu ý. Với ý nghĩa của từng ma trận, trong một số tình huống thì keys và values có thể có giá trị khác nhau chứ không nhất thiết là luôn giống nhau.\nScale Dot-Product Attention Trong mô hình Transformer, Attention được tính toán đơn giản và nhanh hơn so với hầu hết các mô hình sử dụng Attention trong Machine Translation trước đó. Lý do là vì mô hình học $f$ dùng để tính attention weights $\\alpha_{ij}$ sẽ chỉ đơn giản là phép toán tích vô hướng (Dot-Product). Ngoài ra, ta còn có thêm thao tác scale các giá trị trong ma trận kết quả với một giá trị hằng số. Do đó, quá trình tính toán Attention trong Transformer được gọi là Scale Dot-Product Attention.\nTại sao lại chỉ đơn giản là dùng phép toán tích vô hướng?\nĐể ý rằng, kết quả tích vô hướng của hai vector càng lớn thì hai vector đó càng \u0026ldquo;liên quan đến nhau\u0026rdquo;. Nếu xét hai vector có chuẩn bằng 1 thì điều này đang cho biết rằng góc giữa hai vector đó đang rất nhỏ. Như vậy, phép toán tích vô hướng có thể làm được nhiệm vụ của mô hình học $f$ một cách rất nhanh gọn. Trước hết, ký hiệu các ma trận queries, keys và values lần lượt là $Q$, $K$ và $V$, với $Q \\in \\mathbb{R}^{length_q \\times d_q}$, $K \\in \\mathbb{R}^{length_k \\times d_k}$ và $V \\in \\mathbb{R}^{length_v \\times d_v}$. Gọi các vector query trong ma trận $Q$ là $q_i$ (ứng với từng dòng của ma trận), tương tự với $k_i$ trong $K$ và $v_i$ trong $V$. Ta có một số lưu ý sau:\nĐể dễ hình dung, với ví dụ về Attention ở hình này thì $length_k = length_v = 5$ và $length_q$ sẽ bằng với số từ ở phía Decoder (và trong hình đó thì là 1). Ta xét ma trận $Q$ vì từ phần về Attention thì ta thấy rằng với mỗi vector $y_i$, quá trình tính toán Attention từ nó đến các vector $x_j$ là hoàn toàn độc lập với các vector $y_t$ khác, tức là ta có thể thực hiện tính toán Attention song song với tất cả các vector query. Vì $f$ là phép toán tích vô hướng nên số chiều của vector query và vector key phải giống nhau, tức là $d_q = d_k$. Ngoài ra, ta thường có $length_q = length_k = length_v$. Quá trình tính toán của Scale Dot-Product Attention được thể hiện trong hình bên dưới. Về mặt công thức, ma trận output sẽ thuộc $\\mathbb{R}^{length_q \\times d_v}$ và nó được xác định bằng $$ \\begin{equation*} \\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V \\end{equation*} $$ , trong đó phép toán $\\text{softmax}$ được thực hiện trên từng dòng của ma trận $QK^\\top$.\nCác bước tính toán trong Scale Dot-Product Attention.\nLưu ý. Để giải thích cho sự cần thiết của việc scale các phần tử trong ma trận $QK^\\top$ bằng cách chia cho $\\sqrt{d_k}$ trước khi tính softmax thì ta cần quan tâm đến phương sai của các giá trị này. Nếu ban đầu các vector $q_i$ và $k_j$ có phương sai là 1 thì các phần tử trong $QK^\\top$ sẽ có phương sai là $d_k$ (hoặc là $d_q$ vì ta có $d_q = d_k$). Đó là một giá trị rất lớn và nó sẽ ảnh hưởng đến quá trình tính attention weights (phép toán softmax): giảm độ chính xác và thời gian tính tăng lên khá nhiều.\nSelf-Attention và Cross-Attention Khác với các mô hình RNN-based, Transformer đã thay thế toàn bộ quá trình tính toán hồi quy bằng các phép toán Attention và một số fully connected layer. Nói cách khác, Transformer đã sử dụng Attention để học mối quan hệ giữa các từ trong câu nguồn và trong câu đích.\nNgoài ra, quá trính tính toán này hoàn toàn có thể diễn ra song song chứ không cần phải tuần tự từng vị trí như RNN-based. Để ngắn gọn hơn, từ phần này trở đi, khi nói đến Attention trong Transformer thì ta hiểu đó là Scale Dot-Product Attention. Trong Transformer, Attention được sử dụng theo hai dạng là Self-Attention và Cross-Attention. Sự khác nhau giữa Self-Attention và Cross-Attention nằm ở các cách xác định giá trị ma trận $Q$, $K$ và $V$ để tính Scale Dot-Product Attention, trong đó:\nSelf-Attention: Ba ma trận $Q$, $K$ và $V$ đều được tính toán từ input của Encoder hoặc của Decoder, tức là câu thuộc ngôn ngữ nguồn hoặc ngôn ngữ đích. Như vậy, Self-Attention sẽ diễn ra trong nội bộ câu nguồn và nội bộ câu đích. Cross-Attention: Ma trận $Q$ được tính toán từ input của Decoder, tức là câu ngôn ngữ đích. Trong khi đó, $K$ và $V$ được tính toán từ câu nguồn ở phía Encoder. Điều này nghĩa là Cross-Attention thực hiện Attention từ câu đích vào câu nguồn (đây thật là ý tưởng sử dụng Attention trong những mô hình trước đó). Self-Attention Trước tiên, ta xét đến Self-Attention. Self-Attention sẽ thực hiện nhiệm vụ học mối quan hệ giữa các từ với nhau trong cùng một câu (thay thế quá trình tính toán hồi quy trong các mô hình RNN-based). Do đó mà các ma trận $Q$, $K$ và $V$ được tính từ các từ trong cùng một câu. Hơn nữa, ta cũng có $length_q = length_k = length_v = n$ với $n$ là số từ của câu đó.\nGiả sử rằng, câu input sau khi qua các phần Embedding (Word Embedding và Positional Encoding) thì ta thu được ma trận $X \\in \\mathbb{R}^{n \\times d_{model}}$. Khi đó:\nTa sử dụng 3 fully connected layer để biến đổi $X$ thành các ma trận $Q$, $K$ và $V$. Gọi 3 ma trận trọng số ứng với các layer đó là $W_Q$, $W_K$ và $W_V$. Đây chính là các ma trận tham số mà Transformer cần học cho quá trình Self-Attention. Từ $Q$, $K$ và $V$, qua phép toán Scale Dot-Product Attention, ta thu được ma trận kết quả $Z \\in \\mathbb{R}^{n \\times d_v}$. Quá trình tính toán này của Self-Attention được mô tả trong hình bên dưới:\nQuá trình tính toán của Self-Attention với d = d_model\nMinh họa cho kết quả của phép toán Self-Attention trong một câu Tiếng Anh được thể hiện trong hình bên dưới. Trong đó, màu càng đậm thể hiện cho attention weights từ từ \u0026ldquo;it\u0026rdquo; vào từ tương ứng là càng lớn.\nMinh họa kết quả của Self-Attention tại từ \"it\" trong câu đầu vào\nNhư vậy, Self-Attention đã có thể học được mối quan hệ giữa các từ trong cùng một câu về mặt ngữ nghĩa một cách đơn giản và hiệu quả hơn so với quá trình tính toán hồi quy trong các mô hình RNN-based.\nCross-Attention Cross-Attention thực ra chính là cách sử dụng Attention trong nhiều mô hình trước đó với bài toán Machine Translation. Nhiệm vụ của nó trong bài toán Machine Translation là thực hiện Attention từ câu đích vào câu nguồn. Điều này có thể xem như là việc ta sử dụng những gì đã hiểu được ở câu thuộc ngôn ngữ nguồn để dịch dần câu đó ra câu ngôn ngữ đích.\nCụ thể hơn, ta có:\nGiá trị của ma trận $K$ và $V$ sẽ được tính toán từ câu nguồn theo một cách nào đó (các phần về Encoder và Decoder sẽ đề cập kĩ hơn về chi tiết này). Đối với ma trận $Q$ thì nó cũng được xác định từ các từ hiện có trong câu ngôn ngữ đích tính đến vị trí đang xét. Ta vẫn sẽ đảm bảo rằng $d_q = d_k$ và quá trình tính toán của Cross-Attention hoàn toàn tương tự như Self-Attention. Mô hình Transformer cũng cần học ba ma trận tham số tương ứng để tính ra $Q$, $K$ và $V$ trước khi thực hiện Scale Dot-Product Attention). Để dễ hình dung hơn về sự khác nhau giữa $Q$, $K$ và $V$ trong Self-Attention và Cross-Attention, ta có minh họa trong hình bên dưới, với các hàm $f$, $g$ và $h$ đại diện cho quá trình tính toán ra ba ma trận đó.\nSự khác nhau về cách tính toán $Q$, $K$ và $V$ trong Self-Attention và Cross-Attention\nLưu ý. Cross-Attention là một thành phần rất thú vị. Nhờ có Cross-Attention mà kiến trúc Transformer có thể được áp dụng hiệu quả vào nhiều bài toán khác nhau. Ví dụ, với bài toán Image Captioning, ta có thể sử dụng Encoder để \u0026ldquo;hiểu\u0026rdquo; ý nghĩa của ảnh đầu vào, sau đó, trong quá trình sinh câu mô tả cho ảnh, ta có thể sử dụng Cross-Attention đến các đặc trưng rút ra từ ảnh đó để thu được các câu mô tả tốt hơn.\nMulti-Head Attention và Masked Multi-Head Attention Mutli-Head Attention Để ý rằng, Self-Attention đang thực hiện Attention từ một từ trong câu đến toàn bộ các từ còn lại trong câu (kể cả chính từ đó). Tương tự như với Cross-Attention. Điều này có nghĩa là ta đang thực hiện Global Attention. Tuy nhiên, trong ngôn ngữ, đôi khi từ X có thể có quan hệ \u0026ldquo;mạnh\u0026rdquo; với một từ Y theo một phương diện nào đó, và nó cũng có thể có quan hệ mạnh với từ Z theo một phương diện khác nữa. Do đó, nếu thực hiện Global Attention thì các quan hệ này có thể bị trung hòa lẫn nhau và khiến ta mất đi những đặc trưng có ích.\nTa xét một ví dụ được đề cập trong bài giảng của ProtonX. Với câu \u0026ldquo;Tôi đi học ở Hà Nội\u0026rdquo; và ta đang xét từ \u0026ldquo;Tôi\u0026rdquo; như là query vector. Nếu xét theo 3 phương diện, hay là 3 câu hỏi, sau đây: \u0026ldquo;Ai?\u0026rdquo;, \u0026ldquo;Làm gì?\u0026rdquo;, \u0026ldquo;Ở đâu?\u0026rdquo;, thì với mỗi phương diện, mối quan hệ giữa từ \u0026ldquo;Tôi\u0026rdquo; với các từ con lại được thể hiện mạnh nhất ở lần lượt các từ \u0026ldquo;Tôi\u0026rdquo;, \u0026ldquo;đi\u0026rdquo; và \u0026ldquo;học\u0026rdquo;, \u0026ldquo;ở\u0026rdquo; và \u0026ldquo;Hà\u0026rdquo; và \u0026ldquo;Nội\u0026rdquo; (minh họa ở hình bên dưới).\nXét từ \"Tôi\" trong ba phương diện khác nhau khi thực hiện Self-Attention\nChi tiết này là xuất phát cho ý tưởng của Multi-Head Attention. Nó sẽ hạn chế ảnh hưởng của Global Attention trong việc trung hòa mối quan hệ giữa các từ với nhau trong nhiều phương diện.\nĐể đạt được điều này, Multi-Head Attention sẽ chia nhỏ từng ma trận trong các ma trận $Q$, $K$ và $V$ thành $h$ phần. Ví dụ, với $Q \\in \\mathbb{R}^{length_q \\times d_q}$ thì ta sẽ có $h$ ma trận $Q_i \\in \\mathbb{R}^{length_q \\times (d_k / h)}$ (hay là chia thành $h$ heads). Sau đó, Scale Dot-Product Attention sẽ được thực hiện trên $h$ bộ ba ma trận $(Q_i, K_i, V_i)$ và các kết quả sẽ được tổng hợp lại. Ta có thể hình dung rằng mỗi phần nhỏ của các ma trận $Q$, $K$ và $V$ sẽ cố gắng biểu diễn đặc trưng của các từ ở một phương diện nào đó.\nQuá trình tính toán của Multi-Head Attention được thể hiện trong hình bên dưới. Có một số lưu ý như sau:\nVới Multi-Head Attention trong Transformer, ta sẽ xét $length_q = length_k = length_v = n$ với $n$ là độ dài của câu đầu vào $d_q = d_k = d_v = d_{model}$ (đây là số chiều của embedding vector của các từ, sau khi bổ sung thông tin về Positional Encoding trong phần \\ref{sec:pos-enc}). Quá trình tính toán trong Multi-Head Attention\nĐặt $d_x = d_{model} / h$. Về mặt công thức, ma trận kết quả của Multi-Head Attention với $h$ head có thể xác định bằng $$ \\begin{equation*} \\begin{aligned} \\text{MultiHead}(Q, K, V) \u0026amp;= \\text{Concat}(\\text{head}_1; \\dots; \\text{head}_h)W^O \\\\ \\text{với head}_i \u0026amp;= \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i) \\end{aligned} \\end{equation*} $$ , trong đó $Q, K, V \\in \\mathbb{R}^{n \\times d_{model}}$, các ma trận trọng số $W^Q_i, W^V_i, W^K_i \\in \\mathbb{R}^{d_{model} \\times d_x}$, kết quả Scale Dot-Product Attention $\\text{head}_i \\in \\mathbb{R}^{n \\times d_x}$ và $W^O \\in \\mathbb{R}^{d_{model} \\times d_{model}}$. Như vậy, ma trận kết quả của Multi-Head Attention thuộc $\\mathbb{R}^{n \\times d_{model}}$.\nMasked-Multi Head Attention Masked-Multi Head Attention là một biến thể của Multi-Head Attention và nó được sử dụng trong Decoder của Transformer. Mục đích của Masked-Multi Head Attention là ngăn chặn quá trình Attention tại một số vị trí trong câu. Ta có thể gọi phép tính Attention ở trong Masked-Multi Head Attention là Masked Scale Dot-Product Attention, và Self-Attention tại đây có thể được gọi là Masked Self-Attention.\nLý do ta cần đến thành phần này ở Decoder là để tránh việc mô hình \u0026ldquo;nhìn thấy\u0026rdquo; được các từ ở phía sau từ hiện tại khi nó đang đưa ra dự đoán khi ta thực hiện Self-Attention tại Decoder. Minh họa cho ý tưởng này được thể hiện trong hình bên dưới.\nMinh họa ý tưởng của Masked Self-Attention\nLưu ý. Bản chất của phép toán là ta sẽ làm cho giá trị attention weights đến các từ nên được bỏ qua thành một giá trị rất nhỏ để nó hầu như không thể tác động gì đến kết quả chung của quá trình Attention.\nĐể thực hiện được điều đó, ta sẽ dùng một mặt nạ (mask) để đánh dấu những từ bị bỏ qua khi đang xét đến một từ nào đó trong câu. Ví dụ, với câu \u0026ldquo;Tôi đi học ở Hà Nội\u0026rdquo;, ta xét từ \u0026ldquo;đi\u0026rdquo; thì giá trị của mask sẽ là [0, 0, 1, 1, 1, 1], với phần tử bằng 1 có nghĩa là giá trị tại thành phần đó bị bỏ qua. Lúc đó, trước khi tính attention weights, ta kiểm tra mask xem giá trị nào bằng 1 thì gán cho phần tử tương ứng ở đó là $-\\infty$. Khi đó, sau phép toán softmax, giá trị attention weight tại đó sẽ xấp xỉ 0. Như vậy, trong Masked Multi-Head Attention, quá trình tính toán của Scale Dot-Product Attention sẽ bổ sung thêm một phần kiểm tra mask và gán lại giá trị trước khi tính softmax. Quá trình này được thể hiện trong hình bên dưới.\nQuá trình tính toán trong Scale Dot-Product Attention khi thực hiện thêm thao tác kiểm tra mask\nLayer Normalization Trong các mô hình Deep Learning, ta thường thấy sự xuất hiện của các normalization layer nhằm cải thiện khả năng hội tụ của mô hình. Hai loại normalization layer thường thấy nhất là Batch Normalization (chuẩn hóa theo batch) và Layer Normalization (chuẩn hóa theo từng mẫu). Đối với các mô hình thuộc bài toán NLP nói chung và Transformer nói riêng thì loại layer được sử dụng thường là Layer Normalization. Sự khác nhau giữa hai loại layer này khi áp dụng vào bài toán ngôn ngữ được thể hiện trong hình bên dưới, trong đó:\nLayer Normalization: Ta chuẩn hóa vector đặc trưng của từng vị trí trên từng mẫu một (với số vị trí bằng với độ dài của câu). Mọi vị trí của cùng một mẫu sẽ sử dụng chung một bộ tham số gain và bias. Như vậy, việc chuẩn hóa của từng mẫu sẽ độc lập với nhau. Batch Normalization: Đối với loại layer này, ta sẽ chuẩn hóa từng phần tử của vector đặc trưng tại từng vị trí trong câu đầu vào và việc tính toán được dựa theo toàn bộ các mẫu trong cùng batch. Layer Normalization và Batch Normalization khi áp dụng vào bài toán ngôn ngữ\nQua sự khác biệt đó, ta có thể thấy rằng Batch Normalization không nên được áp dụng vào các bài toán NLP vì vấn đề sự khác nhau về độ dài thật sự của các câu trong cùng một batch sẽ ảnh hưởng đến kết quả chuẩn hóa (dù ta đã sử dụng kĩ thuật padding để đưa các câu đó về cùng một độ dài nhưng các vị trí được padding lại có vector đặc trưng với giá trị khá vô nghĩa).\nCụ thể hơn, nếu trong cùng một batch, có một câu có độ dài lớn và nhiều câu có độ dài nhỏ thì đặc trưng của các từ ở vị trí phía sau của câu dài hơn đó sẽ có khả năng cao bị mất đi khi ta áp dụng Batch Normalization. Feed Forward Network và skip connection Bên cạnh các quá trình tính toán Attention, các tác giả của Transformer sử dụng thêm một số Feed Forward Network (gồm các fully connected layer) và kỹ thuật Skip connection để tăng thêm khả năng học các đặc trưng của mô hình (các ô màu xanh dương trong hình ở phần này).\nĐầu tiên, Feed Forward Network (FFN) trong Transformer sử dụng 2 fully connected layers với số unit lần lượt là $d_{ff}$ và $d_{model}$ cùng activation function ReLU trong layer đầu tiên. Input của FFN là một ma trận thuộc $\\mathbb{R}^{n \\times d_{model}}$. Như vậy, ta có thể biểu diễn kết quả đầu ra của FFN theo công thức $$ \\begin{equation*} FFN(X) = \\max(0, XW_1 + b_1) W_2 + b_2 \\end{equation*} $$ , với hai ma trận trọng số $W_1 \\in \\mathbb{R}^{d_{model \\times d_{ff}}}$ và $W_2 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$ và $b_1$, $b_2$ là các bias vector. Do đó $FFN(X) \\in \\mathbb{R}^{n \\times d_{model}}$.\nNgoài ra, skip connection là một ý tưởng rất hay và phổ biến trong Deep Learning kể từ khi nó được áp dụng thành công trong mô hình ResNet. Skip connection có thể giúp cho gradient được lan truyền tốt hơn trong quá trình huấn luyện mô hình, từ đó góp phần làm giảm hiện tượng vanishing gradient. Mình đã đề cập đến ý tưởng này trong bài viết về ResNet.\nTrong kiến trúc Transformer, các tác giả đã áp dụng skip connection cùng với Layer Normalization ở rất nhiều vị trí (các khối \u0026ldquo;Add \u0026amp; Norm\u0026rdquo; trong hình ở phần này). Ta có thể biểu diễn output của các khối đó ở dạng $$\\text{LayerNorm}(X + \\text{Sublayer}(X)))$$ , với $\\text{Sublayer}$ được sử dụng để đại diện cho những thành phần ở phía trước các khối đó.\nLưu ý. Một hướng giải thích cho chi tiết Feed Forward Network chỉ áp dụng activation function ReLU cho layer đầu tiên là vì ngay sau đó ta đã sử dụng skip connection. Đây là một kĩ thuật thường được sử dụng khi làm việc với skip connection.\nNếu ta sử dụng activation function ReLU rồi sau đó áp dụng skip connection thì có thể nói là giá trị các phần tử trong ma trận đầu vào sẽ luôn không giảm, và hiệu ứng này có thể sẽ có ảnh hưởng không tốt đến mô hình. Encoder Sau khi đã trình bày về các thành phần quan trọng trong kiến trúc của Transformer, ta sẽ đi vào các nhánh chính của kiến trúc và nhánh đầu tiên là Encoder. Kiến trúc của Encoder được thể hiện trong hình bên dưới:\nLayer Kiến trúc của Transformer Encoder\nThành phần Encoder trong Transformer là sự kết hợp của Word Embedding, Positional Encoding và một dãy gồm $N$ Encoder Layer liên tiếp nhau. Trong đó, Encoder Layer bao gồm nhiều thành phần như Multi-Head Attention, Feed Forward, skip connection và Layer Normalization.\nCụ thể hơn, trong bài toán Machine Translation, input của Encoder sẽ là câu văn thuộc ngôn ngữ nguồn gồm $L$ từ. Trong đó, các \u0026ldquo;từ\u0026rdquo; trong câu lúc này là các con số ứng với vị trí của từ đó trong từ điển của ngôn ngữ nguồn. Khi đó:\nSau khi qua Word Embedding và Positional Encoding, ta được một ma trận $X \\in \\mathbb{R}^{L \\times d_{model}}$. Tại mỗi Encoder Layer, từ ma trận input $X\u0026rsquo; \\in \\mathbb{R}^{N \\times d_{model}}$, ta sẽ sử dụng ba ma trận trọng số $W_Q, W_K, W_V$ để tính ra $Q$ (được mô tả ở phần , $K$, $V$ từ $X\u0026rsquo;$ và sau đó bắt đầu đi qua các thành phần trong đó. Ma trận output của Encoder Layer cũng sẽ là một ma trận thuộc $\\mathbb{R}^{L \\times d_{model}}$. Lưu ý. Encoder output sẽ là một ma trận thuộc $\\mathbb{R}^{L \\times d_{model}}$, một ma trận có shape giống với ma trận đầu vào $X$ của Encoder. Như vậy, ta có thể hình dung rằng Encoder đang làm nhiệm vụ là bổ sung thêm những đặc trưng quan trọng vào embedding vector ban đầu của các từ trong câu.\nĐối với Encoder output, ta sẽ sử dụng ma trận này để tham gia vào phép tính Cross-Attention trong Decoder.\nDecoder Tổng quan về kiến trúc Kiến trúc của Decoder được thể hiện trong hình bên dưới. Decoder cũng có cách tổ chức khá tương tự Encoder khi ta bắt đầu bằng Word Embedding và Positional Encoding, sau đó là dãy gồm $N$ Decoder Layer liên tiếp nhau. Phần cuối của Decoder là một fully connected layer kèm theo hàm softmax để ta chọn ra từ phù hợp nhất làm output của mô hình đối với vị trí hiện tại trong câu thuộc ngôn ngữ đích.\nKiến trúc của Transformer Decoder\nTa cần để ý rằng, trong Decoder Layer, ta sẽ thực hiện cả hai phép tính Self-Attention và Cross-Attention. Trong đó:\nSelf-Attention được thực hiện trước để tiến hành Attention đến các vị trí ở phía trước vị trí hiện tại trong câu, tức là ta đang sử dụng Masked Multi-Head Attention. Sau đó, Cross-Attention được thực hiện với hai thành phần $K$ và $V$ được tính toán từ Encoder output. Lúc này thì ta đang thực hiện Attention đến toàn bộ các vị trí trong Encoder output nên Multi-Head Attention được sử dụng. Ngoài ra, ta sẽ đặt số lượng từ trong câu nguồn và câu đích của mô hình là bằng nhau và bằng $L$ (trong trường hợp câu nào ngắn hơn thì ta sẽ sử dụng kỹ thuật padding để thêm các từ vào). Do đó:\nTrước khi đến với fully connected layer cuối cùng của Decoder để tiến hành phân lớp, ma trận output ta nhận được cũng sẽ thuộc $\\mathbb{R}^{L \\times d_{model}}$. Số chiều của output vector của fully connected layer cuối cùng sẽ bằng với kích thước tập từ điển của ngôn ngữ đích. Decoder trong quá trình huấn luyện Trong quá trình huấn luyện mô hình, ta sẽ tạo ra các cặp (input, ground truth) của Decoder theo cách khá dặc biệt. Đầu tiên, ta sẽ thêm các từ đánh dấu cho việc \u0026ldquo;bắt đầu\u0026rdquo; và \u0026ldquo;kết thúc\u0026rdquo; của quá trình dịch. Ta gọi đây là các từ \u0026ldquo;\u0026rdquo; và \u0026ldquo;\u0026rdquo;. Các cặp (input, ground truth) dùng để huấn luyện Decoder sẽ được tạo ra bằng cách \u0026ldquo;shifted right\u0026rdquo; một câu thuộc ngôn ngữ đích. Ví dụ, với câu Tiếng Việt là \u0026ldquo;tôi đi học\u0026rdquo;, ta sẽ có các cặp (input, ground truth) tương ứng như sau:\nInput: \u0026ldquo;\u0026rdquo;, \u0026ldquo;tôi\u0026rdquo;, \u0026ldquo;đi\u0026rdquo;, \u0026ldquo;học\u0026rdquo;. Ground truth: \u0026ldquo;tôi\u0026rdquo;, \u0026ldquo;đi\u0026rdquo;, \u0026ldquo;học\u0026rdquo;, \u0026ldquo;\u0026rdquo; Ngoài ra, cho dù Decoder có dự đoán ra từ nào ở vị trí $t$ của câu đích đi nữa thì input của Decoder ở vị trí $t+1$ cũng luôn là một từ đúng (tức là ground truth của vị trí $t$). Kỹ thuật này gọi là Teacher Forcing và nó được sử dụng rất nhiều trong các bài toán NLP. Quá trình này được minh họa trong hình bên dưới.\nKỹ thuật Teacher Forcing với Decoder trong huấn luyện Transformer\nDecoder trong quá trình dự đoán Đối với quá trình dự đoán (hay là dịch) của Transformer, Decoder sẽ bắt đầu với một từ là \u0026ldquo;\u0026lt;start\u0026gt;\u0026rdquo; và quá trình dịch sẽ tiếp tục cho đến khi mô hình dự đoán ra từ \u0026ldquo;\u0026lt;end\u0026gt;\u0026rdquo;. Hơn nữa, từ được mô hình dự đoán ra ở vị trí $t$ sẽ được dùng làm input của Decoder cho vị trí $t+1$. Quá trình này được thể hiện trong hình bên dưới.\nDecoder trong quá trình dự đoán\nKết luận Như vậy, sau một bài viết rất dài thì mình đã trình bày về Transformer với khá nhiều phân tích vào ý tưởng và bản chất của các thành phần. Từ sự thành công của Transformer trong Machine Translation, một kỷ nguyên mới thật sự đã mở ra đối với NLP nói riêng và Deep Learning nói chung:\nCác mô hình ngôn ngữ lớn như ChatGPT, Bard,\u0026hellip; đều dựa trên nền tảng kiến trúc của Transformer. Rõ hơn một chút thì chúng sẽ sử dụng Transformer Decoder và sẽ không dùng đến Cross Attention 😉 Đối với các bài toán thuộc lĩnh vực CV, hay là multi-model như text-to-image (Stable Diffusion,\u0026hellip;) thì cũng đều đang tận dụng sức mạnh của Transformer và các thành phần của chúng, đặc biệt là Cross Attention. Tài liệu tham khảo Vaswani, Ashish, et al. \u0026ldquo;Attention is all you need.\u0026rdquo; Advances in neural information processing systems 30 (2017). Weng, Lilian, \u0026ldquo;Attention? Attention!\u0026rdquo; Phạm Bá Cường Quốc, \u0026ldquo;Tìm hiểu mô hình Transformer - Ngươi Không Phải Là Anh Hùng, Ngươi Là Quái Vật Nhiều Đầu\u0026rdquo; Shen, Sheng, et al. \u0026ldquo;Powernorm: Rethinking batch normalization in transformers.\u0026rdquo; International Conference on Machine Learning. PMLR, 2020. ProtonX, Transformer - Encoder ProtonX, Transformer - Decoder Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \u0026ldquo;Effective approaches to attention-based neural machine translation.\u0026rdquo;, arXiv preprint arXiv:1508.04025 (2015). Notes On AI, \u0026ldquo;Attention Machenism\u0026rdquo; Sebastian Raschka, \u0026ldquo;Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch\u0026rdquo; Jay Alammar, \u0026ldquo;The Illustrated Transformer\u0026rdquo; Dive Into Deep Learning, \u0026ldquo;Beam Search\u0026rdquo; ","date":"2023-07-17T21:58:29+07:00","permalink":"https://htrvu.github.io/post/transformer_01/","title":"Cơ chế Attention và mô hình Transformer"},{"content":"Note. Vì gần đây mình bận khá nhiều việc nên blog đang bị “đóng băng” 🥲 Bài viết này chỉ mang tính chất chữa cháy sau hơn 1 tháng không có bài mới (đây là bài đã được viết sẵn từ trước) =))\nTuy nhiên, MLP Mixer cũng là một mô hình này cũng rất thú vị. Chỉ hơi tiếc một chút là mình lại post nó trước khi viết những bài về Transformer và Vision Transformer…\nGiới thiệu Vào thời điểm năm 2021, có thể nói rằng nhắc tới các mô hình trong Computer Vision thì ai ai cũng nghĩ đến một là CNN, hai là các mô hình dựa trên Transformer đang làm mưa làm gió với những mô hình mạnh mẽ như Vision Transformer hay Swin Transformer.\nGiữa tình hình đó, một mô hình được công bố mà khi nghe qua là ta đã thấy khó mà tin được là nó tốt đến thế. MLP-Mixer, mô hình chỉ sử dụng các fully connected layer, hệt như thời “xa xưa” ta dùng để xây dựng các Deep Neural Network (DNN), hay còn lại là Multi-layer Perceptrons (MLP) 😀\nMulti-layer Perceptron Trước hết, ta cùng nhắc lại đến những hạn chế khiến DNN trở nên không phù hợp với computer vision và gần như đã bị bỏ quên khi CNN được phát triển:\nDNN dễ bị overfitting Số lượng tham số của một mô hình DNN là rất lớn Nó khó học được những đặc trưng liên quan sự dịch chuyển vị trí trong không gian Ví dụ, xét 4 bức ảnh chứa cùng một con chó như ở bên dưới thì khi các ảnh này được đưa vào DNN để tính toán, ta sẽ có các vector rất khác nhau. Trong khi đó, với CNN, qua các lần sử dụng filter thì ta sẽ thu về được các feature map tương tự nhau. Ví dụ về sự quan trọng của các đặc trưng liên quan đến sự dịch chuyển vị trí trong không quan Như vậy, ta dễ hiểu rằng mô hình MLP-Mixer mà bài viết này giới thiệu sẽ có các cách để khắc phục được những vấn đề đó. Lưu ý rằng, MLP-Mixer không phải là SOTA khi nó được công bố và nó vẫn còn kém một chút so với Vision Transformer. Tuy nhiên, xét đến tốc độ thì MLP-Mixer nhanh hơn khá nhiều.\nSo sánh MLP-Mixer và Vision Transformer trên tập ImageNet Chia ảnh input thành các phần nhỏ Nếu đã từng xem qua các mô hình dựa trên Transformer trong Computer Vision thì ta cũng đã rất quen thuộc với bước này. Ảnh input $H \\times W \\times C$ của mô hình sẽ được chia thành các patch (hay là token) nhỏ có kích thước bằng nhau là $P \\times P \\times C$. Giá trị của $H$, $W$ và $P$ thường được chọn sao cho ta chia vừa đủ số patch, và $H$ thường bằng $W$. Như vậy số patch mà ta có được là\n$$ S = \\frac{H \\times W}{P^2} $$\nChia ảnh input thành các tokens (hay patches) Sau đó, ta sẽ duỗi những patch có được này để làm input cho mô hình. Lúc này, một ma trận input của ta sẽ có shape là $(S, P \\times P \\times C)$, với mỗi hàng là “đặc trưng” ban đầu của mỗi patch. Ở các phần tiếp theo, ta gọi $P \\times P \\times C$ là số “channel” của một token, và ta kí hiệu nó là $C$ (hơi lú chút 😅)\nMinh họa ma trận input Channel-mixing và token-mixing Ý nghĩa Trong MLP-Mixer, các tác giả giới thiệu hai block đặc biệt là channel-mixing và token-mixing. Đây cũng chính là những gì tinh hoa nhất trong MLP-Mixer.\nTa có thể mô tả hai block này như sau:\nChannel-mixing: Liên quan đến quan hệ giữa các channel trong cùng một token, tức là các hàng của ma trận input. Nó thực hiện phép toán kết hợp những giá trị trên các channel của cùng một patch và có sự độc lập giữa các patch, do đó ta gọi nó là channel-mixing. Token-mixing: Liên quan đến quan hệ giữa các giá trị ở cùng một vị trí trong các token, tức là các cột của ma trận input. Ta thấy rằng các giá trị tại cùng một vị trí trên các patch (token) được kết hợp với nhau và nó độc lập giữa các channel, do đó ta gọi nó là token-mixing. Sự khác biệt giữa channel-mixing và token-mixing trong khi cài đặt chỉ đơn giản là input của chúng. Với channel-mixing, ta dùng luôn ma trận input có shape là $(S, C)$, còn với token-mixing thì ta cần chuyển vị ma trận input thành shape $(C, S)$.\nMinh họa input của Channel-mixing Minh họa input của Token-mixing Block channel-mixing và token-mixing sẽ bao gồm một MLP với hai fully connected layer và chúng sử dụng GELU activation function (sẽ được ở phần tiếp theo). Trước khi tiến hành tính toán, ta sẽ đưa ma trận input qua một layer chuẩn hóa để đưa ma trận này về phân phối chuẩn tắc. Hơn nữa, ta cũng áp dụng skip connection để tăng hiệu quả training.\nKiến trúc của về channel-mixing và token-mixing như sau:\nKiến trúc của Channel-mixing Kiến trúc của Token-mixing Có một số điểm cần lưu ý như sau:\nCác phần “MLP1” và “MLP2” ở hai hình trên được ghi theo từng khối dọc có nghĩa là ta đã chia sẻ trọng số giữa các vector input. Yếu tố này sẽ được đề cập ở những phần sauSS Trong token-mixing, trước khi tính toán thì ta cần chuyển vị ma trận input Vấn đề tiếp theo ta cần quan tâm là channel-mixing và token-mixing giúp cho MLP-Mixer đạt ược những gì.\nKết hợp đặc trưng Các mô hình trong Computer Vision thường kết hợp các đặc trưng như sau với nhau:\nĐặc trưng tại một vị trí nào đó trong không gian Đặc trưng giữa các vị trí khác nhau trong không gian Ví dụ:\nVới CNN điều này được thực hiện nhờ vào các phép toán convolution với filter $K \\times K$ và pooling, mà cụ thể hơn thì ta thực hiện được (2) khi $K \u0026gt; 1$, và thực hiện được (1) khi $K = 1$. Với mô hình dựa trên Transformer thì cả 2 đều được thực hiện nhờ các self-attention layers. Khi xét đến MLP thông thường, ta chỉ thực hiện được (1) Ý tưởng đằng sau MLP-Mixer là sử dụng channel-mixing cho yếu tố (1), thật sự là vậy vì channel-mixing tính toán trên từng patch độc lập; và sử dụng token-mixing cho yếu tố (2), token-mixing đã tính toán trên các patch.\nChia sẻ trọng số Chia sẻ trọng số (typing weights) là một yếu tố giúp cho MLP-Mixer giảm được đáng kể số lượng trọng số của mô hình, đồng thời góp phần hỗ trợ việc học các đặc trưng liên quan đến tính không gian.\nXét phép toán convolution, ta sẽ thấy rằng phép toán này có sự chia sẻ trọng số trong từng channel như sau:\nVới mỗi channel của input, ta sẽ dùng một filter $K \\times K$ (có $K^2$ trọng số) và lần lượt di chuyển nó qua các vị trí trên channel này để tính toán ra một feature map. Như vậy, toàn bộ các vùng trên input đều được áp dụng cùng một bộ trọng số để tính ra feature map. Nhờ vào việc chia sẻ trọng số, nếu một đối tượng xuất hiện ở những vị trí khác nhau trên ảnh input thì feature map ta thu được cũng sẽ rất tương tự nhau. Phép toán convolution Để ý rằng, ta sẽ không tìm thấy sự chia sẻ trọng số ở MLP thông thường. Lý do là vì mỗi neuron trong một layer của mô hình sẽ nối với toàn bộ các neuron ở layer phía trước, do đó ta có một ma trận trọng số với mỗi dòng là một vector trọng số ứng với một neuron.\nVậy trong MLP-Mixer thì chia sẻ trọng số xảy ra ở đâu?\nVới ý tưởng rất giống với filter trong CNN, channel-mixing sẽ được áp dụng chia sẻ trọng số. Việc kết hợp đặc trưng trên từng patch sẽ được tiến hành bởi cùng một bộ trọng số (ở đây là vector). Đối với token-mixing thì MLP-Mixer cũng áp dụng chia sẻ trọng số. Tuy nhiên, yếu tố này là rất hiếm thấy trong các nghiên cứu trước đó. Lí do chính của điều này là nhằm giảm số lượng trọng số của mô hình. Các tác giả cugx cho biết rằng họ đã thử nghiệm cả hướng không áp dụng sự chia sẻ trọng số cho token-mixing và kết quả thì rất xấp xỉ nhau. Do đó, ở phần giới thiệu channel-mixing và token-mixing thì ta quan sát thấy cách biểu diễn phần MLP theo từng khối “MLP2” và “MLP1” như vậy.\nGELU activation function Một điểm đáng chú ý ở trong MLP-Mixer là mô hình này có sử dụng activation function GELU. Đây là một hàm sử dụng hàm phân phối tích lũy chuẩn tắc $N(0, 1)$. Có thể nói GELU là một phiên bản “trơn hơn” của ReLU.\nVới giả sử input $X$ tuân theo phân phối $N(0, 1)$, ta có\n$$ \\text{GELU}\\left(x\\right) = x{P}\\left(X\\leq{x}\\right) = x\\Phi\\left(x\\right) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}(x/\\sqrt{2})\\right], $$\nSo sánh ReLU, ELU và GELU Mixer block Mixer block (hay Mixer Layer) là thành phần chính trong kiến trúc của MLP-Mixer. Trong block này, ta sẽ sử dụng cả channel-mixing và token-mixing, tạo nên kiến trúc như sau:\nKiến trúc của Mixer block Như vậy, ban đầu ta sẽ áp dụng token-mixing và sau đó dùng kết quả để làm input cho channel-mixing.\nKiến trúc MLP-Mixer Mô hình MLP-Mixer được xây dựng bằng cách áp dụng nhiều Mixer Block (hay Mixer Layer). Trước đó, ta chia ảnh input thành các patch và đưa các patch này qua một fully connected layer để giảm số channel mỗi patch. Lưu ý rằng, ở layer này thì ta cũng áp dụng sự chia sẻ trọng số, tức là toàn bộ patch đều được giảm số channel với cùng một bộ trọng số.\nKiến trúc của mô hình MLP-Mixer Ngoài ra, tùy vào kích thước của một batch và số lượng Mixer Block được sử dụng mà ta sẽ có các phiên bản MLP-Mixer khác nhau. Bên cạnh các siêu tham số đó thì ta cũng có một vài siêu tham số khác như trong bảng dưới đây:\nCác phiên bản MLP-Mixer. Trong đó, S, B, L và H lần lượt là Small, Base, Large và Huge. Các siêu tham số này được gán giá trị cho ảnh input với độ phân giải là 224 x 224. Tài liệu tham khảo ProtonX: AI Papers Reading and Coding - MLP-Mixer: An all-MLP Architecture for Vision Paper MLP-Mixer: https://arxiv.org/pdf/2105.01601.pdf MLP-Mixer - Hướng giải quyết các bài toán Computer Vision mới bên cạnh CNN và Transformer ","date":"2023-05-16T11:37:12+07:00","permalink":"https://htrvu.github.io/post/mlp_mixer/","title":"MLP Mixer"},{"content":"Note. Vì mình cũng đang trong quá trình tìm hiểu về diffusion models nên tạm thời blog sẽ ngừng các bài viết trong chủ đề NLP lại một thời gian để tập trung cho diffusion models nhé 😀\nGiới thiệu về diffusion models Trong thời gian gần đây, xu hướng “AI vẽ tranh\u0026quot; đang rất là hot và các mô hình sinh ảnh nổi tiếng đó hầu hết là dựa trên diffusion models, đặc biệt là Stable Diffusion.\nMinh họa ảnh sinh bởi Stable Diffusion\nNguồn: Stable Diffusion Online Bài toán Image Generation, hay Image Synthesis, không phải là bài toán mới mà ta đã có khá nhiều họ mô hình được nghiên cứu và công bố. Cơ bản nhất là Autoencoder, Variational Autoencoder (VAE), Normalizing Flow và nổi tiếng nhất là Generative Adversarial Network. Trong image generation thì ta có một cái gọi là generative trilemma. Ý nghĩa của cái này là các mô hình sinh ảnh sẽ chỉ đạt được nhiều nhất là 2 trong 3 tiêu chí sau: Thời gian sinh ảnh nhanh, chất lượng ảnh rõ nét và nội dung ảnh đa dạng.\nThe generative trilemma\nNguồn: Tanishq Abraham GAN thì sinh ảnh nhanh và chất lượng ảnh rõ nét nhưng các ảnh nó sinh ra thường trông khá giống nhau, tức là thiếu sự đa dạng VAE và Normalizing Flow thì đạt được mặt tốc độ và đa dạng nhưng chất lượng ảnh thì không tốt lắm. Với diffusion models, ta thường đạt được tiêu chí chất lượng và sự đa dạng nhưng tốc độ thì lại khá chậm. Do đó, hầu hết các cải tiến trong difusion models là liên quan đến việc tăng tốc quá trình sinh ảnh.\nÝ tưởng chung của diffusion models là ta từng bước thêm nhiễu vào ảnh ban đầu để “phá hủy” phân phối của dữ liệu, sau đó học cách khôi phục lại cấu trúc của ảnh gốc. Sau đó, để sinh ảnh thì ta xuất phát từ một ảnh nhiễu hoàn toàn và từ từ khử nhiễu để có được ảnh kết quả.\nÝ tưởng chung của diffusion models Mô hình Diffusion Probability Model được giới thiệu đầu tiên trong paper năm 2015 là Deep Unsupervised Learning using Nonequilibrium Thermodynamics. Phần toán của mô hình này rất là nặng và nó ít được cộng đồng để ý tới khi mà kết quả lúc được công bố thì cũng không có gì nổi bật. Mãi đến năm 2020, Denoising Diffusion Probability Model (DDPM) được công bố và có thể nói đây là sự kiện quan trọng khi nhờ paper này mà diffusion models mới trở thành một chủ đề nghiên cứu được nhiều người quan tâm.\nNói đến các mô hình ảo diệu như Stable Diffusion thì ta còn có nhiều chi tiết khác nữa nhưng nó sẽ không nằm trong phạm vi bài viết này. Mình sẽ tập trung vào phần lý thuyết toán của mô hình để làm nền tảng cho các bài viết sau.\nForward diffusion Giả sử data sample (“ảnh” ban đầu) được lấy mẫu từ phân phối dữ liệu thật sự $\\bold{x}_0 \\sim q(\\bold{x})$. Feed forward là quá trình ta thêm một lượng nhỏ Gaussian noise vào data sample $\\bold{x}_0$ thông qua $T$ bước, từ đó có các noisy samples $\\bold{x}_1, \\bold{x}_2,\u0026hellip;, \\bold{x}_T$. Phân bố của data sample $\\bold{x}_t$ chỉ phụ thuộc vào $\\bold{x}_{t-1}$ như sau:\n$$ \\begin{equation} q(\\bold{x}_t | \\bold{x}_{t-1}) = \\mathcal{N}\\left(\\bold{x}_{t-1}; \\sqrt{1-\\beta_t} \\bold{x}_{t-1}, \\beta_t \\bold{I} \\right) \\end{equation} $$\nCông thức $(1)$ có nghĩa là “ảnh” tại bước thứ $t$ được sample từ một conditional Gaussian distribution với mean $\\mu_t = \\sqrt{1 - \\beta_t} \\bold{x}_{t-1}$ và variance $\\sigma^2 = \\beta_t$. Với giả thiết sự phụ thuộc, ta cũng có thể xem đây là một xích Markov.\nLưu ý. $q$ là probability density fuction (hàm mật độ xác suất) của phân phối chuẩn. Minh họa quá trình forward\nNguồn: Steins Bằng re-parameterization trick, ta có thể sample $\\bold{x}_t$ như sau:\n$$ \\begin{equation} \\bold{x}_t = \\sqrt{1 - \\beta_t} \\bold{x}_{t-1} + \\epsilon_{t-1} \\sqrt {\\beta_t} \\end{equation} $$\nvới $\\epsilon_{t-1} \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$.\nCác giá trị phương sai $\\beta_t$ là được định nghĩa trước và nó được gọi là variance scheduler. Ta sẽ đề cập kĩ hơn về scheduler ở phần 4. Trước hết thì có một số nhận xét như sau:\n$0 \u0026lt; \\beta_1 \u0026lt; \\beta_2 \u0026lt; \u0026hellip; \u0026lt; \\beta_T \u0026lt; 1$ Có thể hiểu rằng càng đến các bước sau thì ta thêm càng nhiều nhiễu vào data sample (vì variance ngày càng lớn). Khi $t \\to \\infty$, phân bố của $\\bold{x}_T$ sẽ tương đương với isotropic Gaussian distribution, tức là $\\bold{x}_T \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$. Quay trở lại với công thức xác định phân phối của $\\bold{x}_t$. Để xác định phân phối của $\\bold{x}_T$ thì ta sẽ tính dần từng bước như sau:\n$$ \\begin{equation} q(\\bold{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod^T_{t=1} q(\\bold{x}_t \\vert \\bold{x}_{t-1}) \\end{equation} $$\nTính như trên thì trông có vẻ là khá lâu!\nMột tính chất thú vị của quá trình forward là ta có thể sample được ngay $\\bold{x}_t$ với bất kì timestep $t$ nào. Đặt $\\alpha_t = 1 - \\beta_t$ và $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$. Vì $\\beta_t$ đều được xác định sẵn nên ta cũng sẽ tính trước được $\\bar{\\alpha_t}$. Từ công thức $(2)$ ta có\n$$ \\begin{aligned} \\mathbf{x}_t \u0026amp;= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} \\\\ \u0026amp;= \\sqrt{\\alpha_t \\alpha_{t-1}} \\bold{x}_{t-2} + \\sqrt{\\alpha_t - \\alpha_t \\alpha_{t-1}} \\epsilon_{t-2} + \\sqrt{1 - \\alpha_t} \\epsilon_{t-1} \\end{aligned} $$\nKhi ta gộp hai Gaussion distribution $\\mathcal{N}(\\bold{0}, \\sigma_1^2 \\bold{I})$ và $\\mathcal{N}(\\bold{0}, \\sigma_2^2 \\bold{I})$ (ý nói đến hai đại lượng chứa $\\epsilon_{t-2}$ và $\\epsilon_{t-1}$ ở trên) thì phân phối thu được sẽ là $\\mathcal{N}(\\bold{0}, (\\sigma_1^2 + \\sigma_2^2) \\bold{I})$. Do đó, từ công thức trên ta có thể viết lại thành\n$$ \\mathbf{x}_t = \\sqrt{\\alpha_t \\alpha_{t-1}} \\bold{x}_{t-2} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{\\epsilon}_{t-2} $$\nCứ tiếp tục biến đổi thì ta sẽ có\n$$ \\mathbf{x}_t = \\sqrt{\\bar{\\alpha_t}} \\bold{x}_{0} + \\sqrt{1 - \\bar{\\alpha_t}} \\epsilon $$\nvới $\\epsilon \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$.\nTóm lại, trong quá trình forward, ta có thể sample $\\bold{x}_t$ dựa vào phân phối sau:\n$$ q(\\mathbf{x}_t \\vert \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I}) $$\nReverse diffusion Đặt vấn đề Quá trình forward diffusion dần dần thêm các Gaussian noise vào sample dựa theo các phân phối $q(\\bold{x}_t|\\bold{x}_{t-1})$. Nếu ta có thể đi ngược lại, tức là xuất phát từ một sample $\\bold{x}_{T} \\sim q(\\bold{x}_T) = \\mathcal{N}(\\bold{0}, \\bold{I})$ và dần sample $\\bold{x}_{t-1}$ theo phân phối $q(\\bold{x}_{t-1} | \\bold{x}_t)$ nào đó thì khi thực hiện cho đến $\\bold{x}_0$, ta đã có thể denoise (khử nhiễu) để thu lại được sample xấp xỉ sample ban đầu của quá trình forward.\nMinh họa quá trình forward và reverse\nTuy nhiên, việc xác định $q(\\bold{x}_{t-1} | \\bold{x}_t)$ là rất khó vì nó liên quan đến phân phối của toàn bộ dữ liệu. Do đó, ta sẽ tìm cách xấp xỉ phân phối này.\nKĩ hơn về lý do khó xác định $q(\\bold{x}_{t-1} | \\bold{x}_t)$: Vì có rất nhiều khả năng có thể xảy ra đối với $\\bold{x}_{t-1}$ nên phương sai của phân phối này cũng sẽ rất lớn. Giả sử ta cần xác định $q(\\bold{x}_{t-1} | \\bold{x}_t, \\bold{x}_0)$ thì mọi chuyện sẽ dễ dàng hơn. Khi biết trước thêm $\\bold{x}_0$ nữa thì ta có thể hình dung được $\\bold{x}_{t-1}$ nên trông như thế nào. Kĩ thuật này gọi là variance reduction step. Một cách trực giác, ta đoán được những gì cần làm khi biết thêm $\\bold{x}_0$ là thực hiện nội suy $\\bold{x}_{t-1}$ dựa vào $\\bold{x}_0$ và $\\bold{x}_{t}$. Ta sẽ quay lại với nhận xét này sau. Để ý rằng với $\\bold{x}_T \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$ và $\\beta_T$ đủ nhỏ thì $q(\\bold{x}_{T-1}|\\bold{x}_T)$ cũng là một Gaussian distribution. Một cách đệ quy, ta có thể xem toàn bộ $q(\\bold{x}_{t-1} | \\bold{x}_t)$ là Gaussian distribution luôn 😀 Để xấp xỉ $q(\\bold{x}_{t-1} | \\bold{x}_t)$, ta sẽ sử dụng một mô hình $p_{\\theta}$ để dự đoán giá trị mean $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$ và variance $\\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)$. Từ đó xấp xỉ được\n$$ p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_\\theta(\\mathbf{x}_t, t)) $$\nLưu ý rằng các giá trị đầu vào của mô hình bao gồm cả $\\bold{x}_t$ và timestep $t$ (cho biết mức độ noise được dùng để tạo ra $\\bold{x}_t$ trong quá trình forward). Mô hình $p_\\theta$ thường là U-Net. Tạm thời ta sẽ bỏ qua chi tiết về kiến trúc của mô hình này. Minh họa mô hình U-Net trong quá trình reverse\nNguồn: Steins Trong quá trình forward, ta đã đề cập là các giá trị variance đã được xác định trước dựa vào scheluder nên ở đây thì ta không cần dự đoán variance. Khi đó $$ p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\beta_t \\bold{I}) $$\nGiả sử ta đã học được mô hình $p_{\\theta}$ như trên thì từ $\\bold{x}_T$ ta có thể xấp xỉ được phân phối của $\\bold{x}_0$ như sau:\n$$ p_\\theta (\\bold{x}_{0:T}) = q (\\bold{x}_T) \\prod_{t=1}^T p_\\theta (\\bold{x}_{t-1} | \\bold{x}_t) $$\nVấn đề đặt ra là ta huấn luyện mô hình $p_\\theta$ như thế nào. Vậy thì ta cần phải tìm loss function!\nXác định loss function Mục tiêu của ta là minimize the negative log likelihood $-\\log p_\\theta (\\bold{x}_0)$. Biến đổi một chút như sau:\n$$ \\begin{aligned} -\\log p_\\theta(\\mathbf{x}_0) \u0026amp;\\leq - \\log p_\\theta(\\mathbf{x}_0) + D_\\text{KL}(q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\| \\color{red}p_\\theta(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0) \\color{default} ) \\\\ \u0026amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_{\\mathbf{x}_{1:T}\\sim q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)} \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{\\color{red}p_\\theta(\\mathbf{x}_{0:T}) / p_\\theta(\\mathbf{x}_0)} \\Big] \\\\ \u0026amp;= -\\log p_\\theta(\\mathbf{x}_0) + \\mathbb{E}_q \\Big[ \\log\\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} + \\log p_\\theta(\\mathbf{x}_0) \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\end{aligned} $$\nĐặt $L_{VLB} = \\mathbb{E}_q \\Big[ \\log \\frac{q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\Big]$ (variational lower bound). Nhận xét rằng nếu minimize được $L_{VLB}$ thì giá trị của negative log likelihood cũng sẽ nhỏ 😀 Do đó, hàm mục tiêu của ta sẽ là $L_{VLB}$.\nTiếp tục biến đổi $L_{VLB}$, ta có\n$$ \\begin{aligned} L_\\text{VLB} \u0026amp;= \\mathbb{E}_{q(\\mathbf{x}_{0:T})} \\Big[ \\log\\frac{\\color{red}q(\\mathbf{x}_{1:T}\\vert\\mathbf{x}_0)}{\\color{blue}p_\\theta(\\mathbf{x}_{0:T})} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\log\\frac{\\color{red}\\prod_{t=1}^T q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{\\color{blue} p_\\theta(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t) } \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=1}^T \\log \\frac{q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{\\color{green}q(\\mathbf{x}_t\\vert\\mathbf{x}_{t-1})}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\Big( \\frac{\\color{green}q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)}\\cdot \\frac{\\color{green}q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{\\color{green}q(\\mathbf{x}_{t-1}\\vert\\mathbf{x}_0)} \\Big) + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\end{aligned} $$\nTrong dòng biến đổi ở trên, áp dụng công thức Bayes thì ta có\n$$ q(\\bold{x}_t | \\bold{x}_{t-1}) = \\frac{q(\\bold{x}_{t-1} | \\bold{x}_t) \\cdot q(\\bold{x}_t)}{q(\\bold{x}_{t-1})} $$\nTa có nhận xét ảo ma rằng nếu ta cho thêm một điều kiện là $\\bold{x}_0$ thì các giá trị trên sẽ dễ tính hơn rất nhiều (có thể tận dụng quá trình forward). Vậy thì “assume” luôn là\n$$ q(\\bold{x}_t | \\bold{x}_{t-1}) = \\frac{q(\\bold{x}_{t-1} | \\bold{x}_t, \\bold{x}_0) \\cdot q(\\bold{x}_t | \\bold{x}_0)}{q(\\bold{x}_{t-1} | \\bold{x}_0)} $$\nOK, tiếp tục biến đổi $L_{VLB}$:\n$$ \\begin{aligned} L_\\text{VLB} \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\color{red}\\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_t \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0)} \\color{default} + \\log\\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big] \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ -\\log p_\\theta(\\mathbf{x}_T) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\color{red}\\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)} \\color{default} + \\log \\frac{q(\\mathbf{x}_1 \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\Big]\\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\color{green}-\\log p_\\theta(\\mathbf{x}_T) \\color{default} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} + \\log\\frac{\\color{green}q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{\\color{blue}p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)} \\\\ \u0026amp;= \\mathbb{E}_q \\Big[ \\color{green}\\log\\frac{q(\\mathbf{x}_T \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_T)} \\color{default} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)} - \\color{blue}\\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)\\color{default} \\Big] \\end{aligned} $$\nVà bước cuối cùng:\n$$ \\begin{aligned}L_{LVB} \u0026amp;= \\mathbb{E}_q [\\color{red}\\underbrace{D_\\text{KL}(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_T))}_{L_T} \\color{default} + \\color{blue}\\sum_{t=2}^T \\underbrace{D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t))}_{L_{t-1}} \\color{green} \\underbrace{- \\log p_\\theta(\\mathbf{x}_0 \\vert \\mathbf{x}_1)}_{L_0} ] \\\\ \u0026amp;= L_T + \\left (L_1 + L_2 + \\cdots + L_{T-1}\\right) + L_0 \\end{aligned} $$\nNhìn chung thì đây toàn là loss liên quan đến khoảng cách các phân phối trong từng timestep. Vì $\\bold{x}_T \\sim \\mathcal{N}(\\bold{0}, \\bold{I})$ nên $L_T$ là một hằng số (khi $T \\to \\infty$ thì $L_T \\to 0$) và ta có thể bỏ qua nó. Ngoài ra, vì giá trị phương sai $\\beta_1$ là rất nhỏ và thường thì “ảnh\u0026quot; $\\bold{x}_0$ và $\\bold{x}_1$ trông sẽ rất giống nhau (vì lượng nhiễu thêm vào là rất ít). Vì lý do này mà các tác giả của DDPM thật sự đã bỏ qua nó trong quá trình huấn luyện.\nNhư vậy ta còn lại mỗi $L_t$ với $1 \\leq t \\leq T - 1$. Đối với cái này thì cần có một số ma thuật.\nTham số hóa $L_t$ Các giá trị $L_t$ với $1 \\leq t \\leq T - 1$ liên quan đến khoảng cách giữa phân phối được xấp xỉ bởi model $p_\\theta$ và một phiên bản “dễ tính hơn” của $q(\\bold{x}_{t-1} | \\bold{x}_t)$ là $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)$ . Ta sẽ đi tính mean và variance của phân phối này.\nĐầu tiên, áp dụng quy tắc Bayes thì\n$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}, \\mathbf{x}_0) \\frac{ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_0) }{ q(\\mathbf{x}_t \\vert \\mathbf{x}_0)} $$\nCả 3 phân phối trên đều là Gaussian distribution trong quá trình forward và ta đã biết được mean, variance của chúng. Nhắc lại một chút: Công thức hàm mật độ xác suất của Gaussian distribution $\\cal{N}(\\mu,\\sigma)$ là\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left( -\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2\\right) $$\nDo đó\n$$ \\begin{aligned} q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \u0026amp;\\propto \\exp \\left(-\\frac{1}{2} \\left(\\frac{(\\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1})^2}{\\beta_t} + \\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\\\ \u0026amp;= \\exp \\left(-\\frac{1}{2} \\left(\\frac{\\mathbf{x}_t^2 - 2\\sqrt{\\alpha_t} \\mathbf{x}_t \\color{blue}{\\mathbf{x}_{t-1}} \\color{default}{+ \\alpha_t} \\color{red}{\\mathbf{x}_{t-1}^2} }{\\beta_t} + \\frac{ \\color{red}{\\mathbf{x}_{t-1}^2} \\color{default}{- 2 \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0} \\color{blue}{\\mathbf{x}_{t-1}} \\color{default}{+ \\bar{\\alpha}_{t-1} \\mathbf{x}_0^2} }{1-\\bar{\\alpha}_{t-1}} - \\frac{(\\mathbf{x}_t - \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0)^2}{1-\\bar{\\alpha}_t} \\right) \\right) \\\\ \u0026amp;= \\exp \\left( -\\frac{1}{2} \\left( \\left( \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right) \\color{red} \\bold{x}_{t-1}^2 \\color{default} - \\left( \\frac{2\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{2 \\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t-1}} \\bold{x}_0 \\right) \\color{blue}\\bold{x}_{t-1} \\color{default} + C(\\bold{x}_t, \\bold{x}_0)\\right)\\right)\\\\ \\end{aligned} $$\nvới $C(\\bold{x}_t, \\bold{x}_0)$ là những đại lượng không liên quan đến $\\bold{x}_{t-1}$ và có thể được bỏ qua.\nTa cần biến đổi công thức trên về dạng của một Gaussian distribution. Đặt\n$$ \\tilde{\\beta}_t = 1/\\left(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\\right) = 1/\\left(\\frac{\\alpha_t - \\bar{\\alpha}_t + \\beta_t}{\\beta_t(1 - \\bar{\\alpha}_{t-1})}\\right) = {\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t} $$\nvà\n$$ \\begin{aligned} \\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, \\bold{x}_0) \u0026amp;= \\left(\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0\\right)/\\left(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}}\\right) \\\\ \u0026amp;= \\left(\\frac{\\sqrt{\\alpha_t}}{\\beta_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1} }}{1 - \\bar{\\alpha}_{t-1}} \\mathbf{x}_0\\right) {\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\beta_t} \\\\ \u0026amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\color{green}\\beta_t}{1 - \\bar{\\alpha}_t} \\color{red}\\mathbf{x}_0 \\end{aligned} $$\nTừ quá trình forward, ta có thể suy ra rằng $\\bold{x}_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon})$ với $\\boldsymbol{\\epsilon} \\sim \\cal{N}(\\bold{0}, \\bold{I})$. Do đó\n$$ \\begin{aligned} \\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t) \u0026amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\color{green}(1 - \\alpha_t)}{1 - \\bar{\\alpha}_t} \\color{red}\\frac{1}{\\sqrt{\\bar{\\alpha}_t}}(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}) \\\\ \u0026amp;= \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon} \\right) \\end{aligned} $$\nKhi đó ta có\n$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\propto \\exp \\left( -\\frac{1}{2} \\frac{\\left( \\bold{x}_{t-1} - \\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t) \\right)^2}{\\tilde{\\beta}_t} \\right) $$\n, tức là\n$$ q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\cal{N}(\\bold{x}_{t-1};\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t), \\tilde{\\beta}_t\\bold{I}) $$\nMặt khác, trong phần đặt vấn đề của quá trình reverse thì ta cần huấn luyện mô hình $p_\\theta$ để dự đoán giá trị mean $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$, sao cho\n$$ p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t), \\beta_t \\bold{I}) $$\nĐiều này nghĩa là thứ ta cần quan tâm về $\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)$ là dự đoán được $\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t)$, mà cụ thể hơn là dự đoán giá trị $\\epsilon \\sim \\cal{N}(\\bold{0}, \\bold{I})$ (vì toàn bộ những giá trị khác trong $\\tilde{\\boldsymbol{\\mu}}_t (\\mathbf{x}_t, t)$ đều đã biết vì chúng là input của quá trình reverse). Từ đó, ta có thể biểu diễn\n$$ \\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t) = {\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\color{red}\\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t) \\color{default} \\right)} $$\nDự đoán $\\epsilon_t$ cũng có nghĩa là đi dự đoán lượng nhiễu được thêm vào $\\bold{x}_{t-1}$ để tạo ra $\\bold{x}_{t}$ trong quá trình forward trước đó. Như vậy, thành phần loss $L_{t-1}$ sẽ trở thành\n$$ \\begin{aligned} L_{t-1} \u0026amp;= D_\\text{KL}(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) \\parallel p_\\theta(\\mathbf{x}_{t-1} \\vert\\mathbf{x}_t)) \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\| \\beta_t \\bold{I} \\|^2_2} \\| \\color{blue}{\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0)} - \\color{green}{\\boldsymbol{\\mu}_\\theta(\\mathbf{x}_t, t)} \\color{default} \\|^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{1}{2 \\|\\beta_t \\bold{I} \\|^2_2} \\| \\color{blue}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon} \\Big)} - \\color{green}{\\frac{1}{\\sqrt{\\alpha_t}} \\Big( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\boldsymbol{\\epsilon}}_\\theta(\\mathbf{x}_t, t) \\Big)} \\color{default}\\|^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{\\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\frac{ (1 - \\alpha_t)^2 }{2 \\alpha_t (1 - \\bar{\\alpha}_t) \\| \\beta_t \\bold{I} \\|^2_2} \\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\end{aligned} $$\nCác tác giả của DDPM cho rằng lược bỏ phần trọng số sẽ giúp mô hình học tốt hơn. Khi đó ta có phiên bản đơn giản hơn của $L_{t-1}$ là\n$$ \\begin{aligned} L_{t-1}^\\text{simple} \u0026amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\|^2 \\Big] \\\\ \u0026amp;= \\mathbb{E}_{t \\sim [1, T], \\mathbf{x}_0, \\boldsymbol{\\epsilon}} \\Big[\\|\\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t)\\|^2 \\Big] \\end{aligned} $$\n, hay diễn tả dễ hiểu hơn thì đây là Mean Squared Error giữa lượng nhiễu dự đoán và lượng nhiễu thật sự.\nVariance scheduler Trong các phần trước, ta có đề cập đến việc các giá trị variance $\\beta_t$ là cố định và chúng được xác định trước bằng cách sử dụng variance chueduler.\nVì sao cần dùng đến scheduler?\nViệc sử dụng schedule làm cho quá trình forward của diffusion model là cố định và có thể xác định trước. Điều này có thể giúp cho các tính toán của quá trình reverse trở nên gọn nhẹ hơn mà không làm ảnh hưởng gì đến độ hiệu quả của mô hình. Có nhiều dạng schedule khác nhau như linear, cosine. Trong paper DDPM 2020 thì các tác giả sử dụng linear variance scheduler với $\\beta_1 = 10^{-4}$ và $\\beta_T = 0.02$.\nMinh họa cho linear variance scheduler Trong paper Improved Denoising Diffusion Probabilistic Models - 2021, người ta cho rằng sử dụng cosine variance scheduler sẽ mang lại kết quả tốt hơn. Vấn đề chủ yếu nằm ở chỗ với linear thì ở những timesteps phía sau thì hầu như ảnh đã trở thành “isotropic Gaussian distribution” (có vẻ ý là linear thì thêm nhiễu hơi bị nhanh). Ở hình bên dưới thì hàng trên là linear, hàng dưới là cosine.\nLinear scheduler và Cosine scheduler Quá trình training và sampling Dù các nền tảng lý thuyết bên dưới của diffusion model trông rất rối nhưng quá trình training và sampling (inference) thì rất đơn giản.\nĐầu tiên, ta xét quá trình training:\nTa cần train mô hình dự đoán nhiễu. Có một chi tiết là các tác giả của paper DDPM cho rằng việc training sẽ hiệu quả hơn khi ta chọn ngẫu nhiên các timestep $t$ và chỉ dự đoán nhiễu tại timestep đó, thay vì phải dự đoán ở toàn bộ timestep như những gì được biểu diễn trong hàm loss ở phần 3.2. Mã giả cho quá trình training Trong quá trình sampling (inference) thì ta sẽ chọn trước số lượng timestep tối đa $T$. Bắt đầu từ $\\bold{x}_T \\sim \\cal{N}(\\bold{0}, \\bold{I})$. Từ đó, dần qua các bước reverse thì:\nDự đoán nhiễu $\\epsilon_\\theta(\\bold{x}_t, t)$. Sample $\\bold{x}_{t-1}$ với bằng re-parameterization trick Mã giả cho quá trình sampling Lưu ý. Trong mã giả của quá trình sampling, khi sampling $\\bold{x}_0$ thì ta không thêm nhiễu. Điều này khớp với chi tiết bỏ qua thành phần loss $L_0$ ở trên :D Minh họa trực quan cho quá trình training và sampling:\nNhận xét Chỉ với những chi tiết trên thì có thể nói rằng diffusion models chưa thể vượt mặt GAN được về tốc độ sampling cũng như chất lượng ảnh. Ta vẫn còn nhiều cải tiến cho difusion models và những cải tiến này đã cùng nhau đưa diffusion models lên cạnh tranh vị trí top 1 trong bài toán Image Generation, trong đó nổi bật là sự xuất hiện của Stable Diffusion với những ma thuật như text-to-image, image-to-image và hơn thế nữa!.\nTa sẽ đề cập đến chúng trong các bài viết sau.\nTài liệu tham khảo Jascha Sohl-Dickstein et al. “Deep Unsupervised Learning using Nonequilibrium Thermodynamics.” ICML 2015. Jonathan Ho et al. “Denoising diffusion probabilistic models.” arxiv Preprint arxiv:2006.11239 (2020). Lilian Wenge, What are diffusion models? The AI Summer, Diffusion models HuggingFace\u0026rsquo;s Blog, Annotated Diffusion Steins, Diffusion Model Clearly Explained! Outlier, Diffusion Models | Paper Explanation | Math Explained Tanishq Abraham, Diffusions Study Group ","date":"2023-04-11T22:52:57+07:00","permalink":"https://htrvu.github.io/post/diffusion-models/","title":"Lý thuyết về diffusion models"},{"content":"Bài toán Machine Translation Giới thiệu Machine Translation (dịch máy) là bài toán rất phổ biến trong lĩnh vực NLP. Sản phẩm Google Dịch mà chúng ta vẫn dùng hằng ngày chính là một mô hình dịch máy khá tốt và nó được huấn luyện bởi Google 😀\nThật ra bài toán Machine Translation đã ra đời từ rất lâu. Tất nhiên rồi, vì nó đóng vai trò rất quan trọng trong giao tiếp. Kể từ khi Statistical Machine Learning (máy học thống kê) rồi cho đến Deep Learning phát triển mạnh thì dần càng có nhiều nghiên cứu về Machine Translation được thực hiện và độ chính xác của các mô hình đã được cải thiện một cách đáng kể.\nNguồn: FreeCodeCamp Rule-based Machine Translation Ban đầu, Machine Translation được giải quyết bằng cách dựa vào những cách như dịch trực tiếp từng từ một dựa vào từ điển, sau đó là dịch từng cụm, chuyển từ các từ hay cụm từ thành một dạng biểu diễn trung gian (ví dụ như ảnh), rồi từ đó tính ra từ của ngôn ngữ khác. Những phương pháp này gọi chung là Rule-based Machine Translation. Tất nhiên là dịch như vậy thì độ chính xác sẽ khó mà cao được rồi 😀\nMinh họa cho Rule-based Machine Translation\nNguồn: FreeCodeCamp Statistical Machine Translation Với Statistical Machine Translation (SMT), người ta xây dựng khá nhiều các phương pháp dựa trên cơ sở là các mô hình thống kê. Ta có thể kể ra một vài phương pháp như Word-based (bag-of-words, word-alignment), Phase-based, Syntax-based.\nLấy ví dụ với bài toán dịch Tiếng Anh sang Tiếng Việt. Với câu input Tiếng Anh là $x$, ta sẽ tìm câu Tiếng Việt $y_0$ sao cho xác suất $y_0$ là câu dịch của $x$ là cao nhất:\n$$ y_0 = \\argmax_{y} P(y|x) $$\nSử dụng quy tắc Bayes, ta có\n$$ P(y|x)= P(x|y) P(y) $$\nDo đó, trong mô hình SMT, ta sẽ có sự góp mặt của hai thành phần: Translation Model (cho $P(x|y)$) và Language Model (cho $P(y)$)\nTranslation Model (TM) liên quan đến việc dịch các từ và cụm từ giữa hai ngôn ngữ (fidelity - sự chính xác trong dịch thuật). Để huấn luyện TM thì ta cần sử dụng tập dữ liệu “song ngữ”, tức là tập các cặp câu Anh-Việt tương ứng. Kỹ thuật thường được dùng trong huấn luyện TM là word alignment. Mình sẽ không đề cập đến nó ở trong bài viết này 😀 Language Model (LM) sẽ tập trung vào sự trôi chảy của câu được dịch ra (fluency). Để huấn luyện LM thì ta chỉ cần dùng tập dữ liệu đơn ngữ. Có thể kể đến một số cách huấn luyện cơ bản như là Mô hình Markov (thuần túy dựa vào xác suất và thống kê), hoặc là về sau thì có thêm Recurrent Neural Network. Translation Malde và Language Model\nNguồn: VietAI Để sử dụng mô hình SMT trong thực tế, tất nhiên là ta không thể đi thử toàn bộ câu output $y$ để tính xác suất rồi so sánh được. Ta sẽ sử dung một thuật toán heuristic search để tìm ra câu dịch phù hợp.\nCho đến trước năm 2016 thì Google Dịch vẫn sử dụng mô hình SMT, trước khi nó chuyển hoàn toàn sang Neural Machine Translation (phần kế tiếp). Ta có thể liệt kê một số hạn chế của SMT như sau:\nHệ thống thật sự sẽ rất phức tạp với nhiều thành phần tách rời nhau Cần thực hiện quá trình feature engineering rất nhiều để có thể nắm bắt được đặc trưng của từng ngôn ngữ Chi phí duy trì và phát triển rất tốn kém. Neural Machine Translation Khi Deep Learning dần phát triển mạnh, ta có nhiều mô hình được xây dựng để giải quyết bài toán Machine Translation. Chúng được gọi chung là Neural Machine Translation (NMT)\nMinh họa cho các mô hình trong nhóm Neural Machine Translation\nNguồn: FreeCodeCamp Mục đích của Machine Translation là ta đi dịch một văn bản từ ngôn ngữ X sang ngôn ngữ Y, tức là input của bài toán này là một chuỗi và output cũng là một chuỗi. Và với các bài toán có dữ liệu dạng chuỗi thì ta thường nghĩ ngay đến Recurrent Neural Network!\nLoại mô hình RNN thường được sử dụng trong bài toán này là many-to-many. Trong bài viết về RNN, mình có đề cập đến hai dạng khác nhau của mô hình many-to-many như sau:\nXét dạng mô hình many-to-many phía bên phải. Ta thấy rằng nó đang hoạt động theo kiểu như dịch dần từng chữ một, và có vẻ đây không phải là cách mà con người sử dụng để dịch văn bản 😀 Đối với phía bên trái, mô hình hoạt động theo hướng là đọc hiểu toàn bộ input rồi sau đó mới bắt đầu dịch. Nghe rất hợp lý! Dạng kiến trúc này thường được gọi là Encoder-Decoder, trong đó: Encoder sẽ rút trích các đặc trưng ở trong câu input. Sau khi hoàn thành, nó sẽ chuyển thông tin này cho decoder. Decoder là một Language Model sinh ra các từ cho câu output, dựa trên các từ đã sinh trước đó và lượng thông tin đến từ encoder. Nếu mô tả ngắn gọn thì ta sẽ có sơ đồ như sau:\nSơ đồ của kiến trúc Encoder-Decoder\nNguồn: Dive into DL Cụ thể hơn một chút với dạng mô hình RNN Encoder-Decoder, “thông tin” mà encoder gửi cho decoder chính là hidden state của giai đoạn cuối cùng trong encoder.\nMinh họa mô hình RNN Encoder-Decoder\nNguồn: VietAI Ta cũng có thể xem NMT như là một SMT với khả năng tính toán trực tiếp xác suất $P(y|x)$:\n$$ P(y|x) = P(y_1|x) \\times P(y_2|y_1, x) \\times \\cdots P(y_T | y_{T -1}, \\cdots, y_1, x) $$\nSo với SMT, NMT có một số điểm mạnh hơn như sau:\nHiệu năng tốt hơn: Dịch chính xác, trôi chảy hơn và câu văn đa dạng hơn Dễ tối ưu mô hình hơn (huấn luyện end-to-end) Con người không cần phải can thiệp quá nhiều vào thao tác feature engineering Bên cạnh đó, NMT cũng có một hạn chế quan trọng là mô hình này khó để “có thể giải thích được”, các hoạt động bên traong như là một blackbox (xem thêm bài viết về XAI tại đây).\nMô hình Sequence to Sequence Sequence to Sequence (seq2seq) là mô hình dịch máy có kiến trúc dạng Encoder-Decoder. Nó được các nhà nghiên cứu tại Google nghiên cứu và công bố vào năm 2016, cũng là năm mà Google Dịch chuyển từ SMT sang NMT 😀\nSeq2seq đơn giản Ở phiên bản đơn giản nhất của seq2seq, kiến trúc mô hình sẽ giống với hình minh họa của Encoder-Decoder ở trên. Trong đó, ta có sử dụng embedding layer và các cell có thể là RNN cell, LSTM cell hoặc GRU cell (xem thêm bài viết về LSTM và GRU tại đây). Ta có thể mô tả kiến trúc này như hình bên dưới:\nKiến trúc Seq2seq đơn giản\nNguồn: VietAI Lưu ý. Trong Seq2seq, ta hoàn toàn có thể dùng Bidirectional RNN. Khi đó, lượng “thông tin”, hay là trạng thái S mà encoder gửi cho decoder có thể được tính bằng trung bình của trạng thái cuối cùng của mỗi hướng truyền. Nếu xét về mặt công thức của Encoder và Decoder thì nó sẽ giống với trong RNN thông thường. Chỉ đặc biệt ở một phần là thời điểm đầu tiên của Decoder sẽ có trạng thái ẩn truyền vào là khác 0 (nhận được từ Encoder).\nDeep Seq2seq Từ RNN, ta có Deep RNN. Vậy thì với Seq2seq cũng như thế 😀 Deep Seq2seq là dạng kiến trúc mà Encoder và Decoder có nhiều recurrent layer liên tiếp nhau. Khi đó, số lượng trạng thái mà Encoder truyền qua Decoder cũng sẽ nhiều lên. Ví dụ như sau:\nMinh họa kiến trúc Deep Seq2seq\nNguồn: VietAI Để ý rằng, recurrent layer thứ $i$ trong Decoder sẽ nhận trạng thái đầu từ recurrent layer tương ứng của Encoder.\nChỉ bằng cách đơn giản là chồng thêm nhiều recurrent layer trong kiến trúc mô hình, Deep Seq2seq đã đạt độ hiệu quả rất vượt trội 😀\nKỹ thuật Teacher Forcing và đảo ngược câu input Đầu tiên, ta thấy rằng để huấn luyện được một mô hình Seq2seq thì ta cần có input cho cả 2 thành phần là Encoder và Decoder. Với Encoder thì chắc chắn input chính là đoạn văn bản cần dịch. Còn Decoder thì sao?\nInput của Decoder được tạo ra bằng một kỹ thuật gọi là Teacher Forcing. Ví dụ, câu văn bản input là “Hôm nay tôi đi học” và label của nó là “Today I go to school”. Khi đó, input và label của Decoder sẽ là:\nInput: “Today I go to” Label: “I go to school” Nhìn vào thì ta sẽ thấy ngay ý tưởng của Teacher Forcing 😀\nĐối với quá trình dịch (hay là dự đoán) thì Decoder hoạt động giống với mô hình RNN thông thường: Sử dụng output của thời điểm liền trước để làm input cho thời điểm hiện tại Bên cạnh Teacher Forcing, các tác giả của Seq2seq còn sử dụng một kỹ thuật để giúp Seq2seq đạt được một hiệu năng ấn tượng là đảo ngược câu input (label thì giữ nguyên). Nghe rất ảo nhưng… it works! Ví dụ:\nMinh họa kỹ thuật đảo ngược câu input trong Seq2seq\nNguồn: VietAI Dù không đưa ra được lời giải thích chặt chẽ là vì sao kỹ thuật này lại mang đến kết quả rất tốt nhưng các tác giả của Seq2seq cũng có nêu ra một số lí do thiên về phần trực giác. Lí do chính là vì nhờ cách làm này mà mô hình có thể học được thêm các mối quan hệ phụ thuộc giữa các từ trong câu input và label.\nKhi ta “dịch xuôi”, với những input có độ dài lớn thì sau khi Encoder tính toán xong, đến với Decoder thì Decoder đang đi dịch cho một từ cách thời điểm hiện tại một khoảng cách rất xa, và với các từ sau cũng vậy (khoảng cách của từng cặp là xấp xỉ nhau). Trong khi đó, nếu đảo ngược input thì trung bình khoảng cách những cặp từ sẽ gần như không đổi nhưng sẽ có những cặp ở rất gần nhau. Từ đó nó góp phần làm giảm hiện tượng vanishing gradient (hay còn gọi tên khác là time lag mà các tác giả sử dụng trong paper). Các ứng dụng khác của Seq2seq Dù được phát triển cho bài toán Machine Translation được Seq2seq có thể được áp dụng vào rất nhiều bài toán khác nhau, và chúng đều là các bài toán rất thú vị và liên quan đến nhiều mảng khác nhau trong Deep Learning. Trong đó có hai bài toán nổi bật là Image Captioning và Speech Recognition.\nNhìn vào kiến trúc của Seq2seq thì ta có nhận xét rằng nếu Encoder đủ tốt để rút trích các đặc trưng từ input và truyền vào cho Decoder thì Decoder có thể làm rất nhiều điều.\nĐối với Image Captioning, Encoder sẽ rút trích đặc trưng của ảnh và truyền vector này vào Decoder là ta đã có khả năng sinh ra câu mô tả cho tấm ảnh đó Image Captioning sử dụng ý tưởng Seq2seq\nNguồn: Analytics Vidhya Speech Recognition là bài toán sinh ra đoạn văn bản được nói lên trong file âm thanh. Như vậy, chỉ cần một Encoder rút trích được đặc trưng của âm thanh rồi truyền vào Decoder là ta đã có thể có một giải pháp cho bài toán này. Speech Recognition sử dụng ý tưởng Seq2seq\nNguồn: Research Gate Độ đo BLEU Để biết được một mô hình Machine Translation có hoạt động đủ tốt hay không thì ta cần có một độ đo. BLEU (Bilingual Evaluation Understudy) chính là một trong những độ đo cơ bản và phổ biến nhất.\nBLEU sẽ đánh giá một câu dịch dựa theo các n-grams của câu đó với các câu label có trong tập dữ liệu. Ví dụ:\nCâu input là $x$ = “Con mèo nằm ở trên bàn”\nOutput của mô hình là $\\hat{y}$ = “The cat on table”\n1-grams (hay là unigrams): The, cat, on, table 2-grams (bigrams): The cat, cat on, on table Tương tự với các giá trị n khác Giả sử input $x$ có hai câu label trong tập dữ liệu:\n$o_1$ = “The cat is on a table” $o_2$ = “The cat lies on a desk” Đặt $N_0$ là tập các $n_0$-grams của câu output $\\hat{y}$. Khi đó, giá trị điểm BLEU (hay là BLEU score) của $\\hat{y}$ tính theo $n_0$-grams là\n$$ s_n = \\frac{\\sum_{n_0\\text{-gram} \\in N_0} \\text{count}_{\\text{clip}}(n_0\\text{-gram})}{\\sum_{n_0\\text{-gram} \\in N_0} \\text{count}(n_0\\text{-gram})} $$\ntrong đó:\nTử số được tính theo các câu label của input $x$, với $\\text{count}_{\\text{clip}} (n_0\\text{-gram})$ là số lần xuất hiện lớn nhất của gram này ở trong các câu label. Với ví dụ trên, ta có hai câu label là $o_1$ và $o_2$. Giả sử xét một 2-gram “The cat” thì gram này đều xuất hiện 1 lần ở trong mỗi câu label nên giá trị $\\text{count}_\\text{clip}$ của nó là 1. Mẫu số được tính tại chính câu output $\\hat{y}$, với $\\text{count}(n_0\\text{-gram})$ là số lần xuất hiện của gram này ở trong câu output. Như vậy, tất nhiên là $s_n \\leq 1$ và $s_n$ càng lớn thì câu output $\\hat{y}$ càng “gần” với các câu label trong tập dữ liệu.\nĐể tính được điểm BLEU thật sự của câu $\\hat{y}$, ta sẽ tính $s_n$ với một số giá trị $n$ và sau đó tính trung bình theo một công thức khá đặc biệt:\n$$ BLEU(\\hat{y}) = BP \\times \\exp \\left ( \\frac{1}{m} \\sum_{n=1}^m s_n \\right ) $$\ntrong đó $BP$ là BLEU penalties, với ý nghĩa là nếu mô hình cho ra những câu output quá ngắn (ngắn hơn các câu trong tập dữ liệu) thì sẽ bị phạt (giảm điểm BLEU):\n$$ \\begin{aligned} BP = \\left\\{\\begin{matrix} 1, \u0026amp; \\text{if } len(\\hat{y}) \u0026gt; \\min(len(o_i)) \\\\ \\exp \\left ( 1 - \\frac{\\min(len(o_i))}{len(\\hat{y})} \\right ), \u0026amp; \\text{otherwise} \\end{matrix}\\right. \\end{aligned} $$\nVì sao cần phải có BLEU penalties?\nĐể ý rằng, nếu mô hình chỉ cho ra một câu output chứa đúng một gram luôn xuất hiện trong các câu label thì ta luôn có $s_n = 1$ 😀 Nếu không phạt thì hỏng! Tài liệu tham khảo FreeCodeCamp, A history of machine translation from the Cold War to deep learning Dive into Deep Learning, Encoder-Decoder Seq2Seq for Machine Translation VietAI, Deep Learning Foundation Course 2019, Lecture 14 - Machine Translation and Sequence to Sequence model DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-03-19T00:25:41+07:00","permalink":"https://htrvu.github.io/post/mt_seq2seq/","title":"Bài toán Machine Translation, mô hình Sequence to Sequence và độ đo BLEU"},{"content":"Giới thiệu Qua các bài viết về RNN truyền thống, LSTM và GRU thì mình đều trình bày về các mô hình với duy nhất một cell trong kiến trúc (recurrent cell, LSTM cell hoặc là GRU cell). Ngoài ra, ta thấy các hidden state cũng được truyền theo một hướng cố định là từ trái sang phải (thời điểm $t$ đến thời điểm $t + 1$). Nếu bỏ qua chi tiết về các \u0026ldquo;thời điểm\u0026rdquo; thì nhìn chúng sẽ không khác gì một mô hình MLP cơ bản trong Machine Learning.\nCác dạng mô hình RNN truyền thống\nNguồn: Javatpoint Đây chỉ mới là sự khởi đầu của RNN. Để đạt được hiệu năng tốt nhất có thể trong các bài toán, ta cần phải có những cải tiến nhất định. Hai cải tiến, hay là hai biến thể, phổ biến của RNN mà mình giới thiệu trong bài viết này là Deep RNN (dùng nhiều cell trong kiến trúc) và Bidirectional RNN (truyền hidden state theo cả hai hướng).\nLưu ý. Để cho đơn giản, các cell được sử dụng trong kiến trúc mô hình mà mình trình bày bên dưới đều là recurrent cell. Ta hoàn toàn có thể thay thế nó bằng LSTM cell, GRU cell.\nDeep RNN Tất nhiên, Deep Learning mà, dùng nhiều cell (hay là hidden layer) ngay 😜 Trong các mô hình RNN mà mình đã trình bày cho đến trước bài viết này thì chúng chỉ có duy nhất một recurrent cell và cell này cứ nhận vào input, tính ra hidden state và output. Nếu chúng ta dùng nhiều cell thì sao?\nKết quả sẽ có dạng như hình bên dưới. Trong đó, input của recurrent cell thứ $l$ là hidden state của recurrent cell thứ $l - 1$.\nMinh họa mô hình Deep RNN\nNguồn: Dive into DL Ta kí hiệu:\n$L$ là số recurrent cell của mô hình Tại recurrent cell thứ $l$ thì $\\bold{H}_t^{(l)} \\in \\mathbb{R}^{h}$ là hidden state tại thời điểm $t$ (quy ước $\\bold{H}_t^{(0)} = \\bold{X}_t$) Các ma trận trọng số lần lượt là $(\\bold{W}_{xh}^{(l)}, \\bold{W}_{hh}^{(l)})$ (với $l \u0026lt; L$). Activation function dùng để tính hidden state là $\\phi_l$. $\\bold{O}_t \\in \\mathbb{R}^{o}$ là output tại thời điểm $t$ của mô hình. Tại recurrent cell thứ $L$, ma trận trọng số để tính ra output là $\\bold{W}_{ho}$ và activation function là $\\phi_o$. Khi đó, quá trình feed-forward trong Deep RNN được mô tả như sau: Tại thời điểm $t$ thì\nQua từng recurrent cell thứ $l = 1, 2, \u0026hellip;, L$, ta có $$\\bold{H}_t^{(l)} = \\phi_l(\\bold{W}_{xh}^{(l)} \\bold{H}_t^{(l-1)} + \\bold{W}_{hh}^{(l)}\\bold{H}_{t-1}^{(l)})$$\nOutput tại thời điểm $t$ là $$\\bold{O}_t = \\phi_o (\\bold{W}_{ho} \\bold{H}_t^{(L)})$$\nBidirectional RNN (BiRNN) Ý tưởng của Bidirectional RNN (BiRNN) rất là tự nhiên và giống với cách con người đọc hiểu ngôn ngữ. Đầu tiên, ta xét bài toán Name Entity Recognition với câu sau:\nCan you see that? Teddy bears are on sales. He said that Teddy Rooosevelt was a great president. Ở câu (2) thì ta có thể gán cho Teddy thuộc lớp Name, và nó đúng là tên của một người thật. Tuy nhiên, trong câu (1) mà gán như thế là sai. Để gán đúng với câu (1) thì ta cần biết được từ phía sau đó nữa (và ta phải gán nguyên cụm Teddy bears). Như vậy, RNN truyền thống sẽ thất bại trong ví dụ (1), vì khi xét tới Teddy thì ta chưa có bất kì thông tin gì về các từ phía sau nó.\nTheo cách con người đọc hiểu ngôn ngữ, ở câu (1) thì ta cũng cần phải đọc thêm từ \u0026ldquo;bears\u0026rdquo; ở phía sau để biết được từ \u0026ldquo;Teddy\u0026rdquo; ở trước mang ý nghĩa gì. Như vậy, trong BiRNN, các trạng thái ẩn sẽ được truyền theo cả hai chiều (xuôi và ngược). Kiến trúc của nó sẽ có dạng như hình bên dưới (để cho đơn giản thì ta chỉ xét với một recurrent cell 😜).\nMinh họa mô hình BiRNN\nNguồn: Dive into DL Ta kí hiệu:\nTrong mỗi hướng truyền xuôi và ngược thì: Hướng truyền xuôi: Hidden state là $\\overrightarrow{\\mathbf{H}}_t$ và hai ma trận trọng số là $(\\bold{W}_{xh}^{(f)}, \\bold{W}_{hh}^{(f)})$. Hướng truyền ngược: Hidden state là $\\overleftarrow{\\mathbf{H}}_t$ và hai ma trận trọng số là $(\\bold{W}_{xh}^{(b)}, \\bold{W}_{hh}^{(b)})$. Ma trận trọng số để tính ra output là $\\bold{W}_{ho}$. Quá trình feed-forward của BiRNN sẽ diễn ra như sau:\nLần lượt theo các hướng truyền xuôi, ta tính được hidden state theo các công thức $$\\overrightarrow{\\bold{H}}_t = \\phi_h(\\bold{W}_{xh}^{(f)}\\bold{X}_t + \\bold{W}_{hh}^{(f)}\\overrightarrow{\\bold{H}}_{t-1})$$\n$$\\overleftarrow{\\bold{H}}_t = \\phi_h(\\bold{W}_{xh}^{(b)}\\bold{X}_t + \\bold{W}_{hh}^{(b)}\\overleftarrow{\\bold{H}}_{t+1})$$\nSau khi đã tính xong hidden state tại toàn bộ các thời điểm, ta tính output: $$\\bold{O}_t = \\phi_o \\left ( \\bold{W}_{ho} \\left [ \\overrightarrow{\\bold{H}}_t , \\overleftarrow{\\bold{H}}_t \\right ] \\right )$$\n, trong đó $\\left [ \\overrightarrow{\\bold{H}}_t , \\overleftarrow{\\bold{H}}_t \\right ]$ nghĩa là nối hai hidden state với nhau (concatenate).\nNhận xét.\nQua quá trình feed-forward của BiRNN, ta thấy rằng mô hình phải thực hiện tính toán hidden state tại toàn bộ các thời điểm rồi mới bắt đầu đưa ra output của mỗi thời điểm. Do đó, đôi khi BiRNN sẽ không thực sự phù hợp cho các bài toán real-time như speech recognition. Tài liệu tham khảo Dive into DL, Deep Recurrent Neural Network Dive into DL, Bidirectional Recurrent Neural Network DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-02-24T13:48:53+07:00","permalink":"https://htrvu.github.io/post/deep-rnn_birnn/","title":"Deep RNN và Bidirectional RNN"},{"content":"Vanishing gradient và long-term, short-term dependency Trong bài viết về mô hình RNN truyền thống, mình đã có đề cập đến vấn đề vanishing gradient của nó dựa vào công thức của quá trình BPPT (Back-propagation Through Time). Hệ quả của vấn đề này là RNN gặp khó khăn trong việc ghi nhớ thông tin trong những câu có nhiều từ.\nĐể minh họa rõ hơn về hệ quả của vanishing gradient, ta xét ví dụ với mô hình RNN dùng để sinh ra văn bản. Giả sử đoạn văn bản đang được sinh ra như sau:\nTrưa hôm nay, trời đã mưa rất to và tôi thì lại để quên áo mưa ở nhà. Vì sao quên thì là do sáng nay ngủ dậy muộn nên tôi chỉ tập trung nhanh chóng vệ sinh cá nhân, soạn sách vở rồi ăn sáng để đến lớp thôi. …(vài câu gì đó nữa)…. Kết quả là lúc về đến nhà, cả người tôi đã bị _ Từ tiếp theo được sinh ra ở vị trí của kí tự _ lúc này nên là “ướt”, nhưng những thông tin liên quan đến vấn đề bị ướt này thì lại cách vị trí hiện tại rất xa, ở tận phía đầu của đoạn văn bản (trời mưa, quên áo mưa). Khi đó, RNN sẽ khó mà nhớ được những chi tiết này, dẫn đến từ sinh ra sẽ không phù hợp.\nTa có thể gọi sự phụ thuộc giữa từ “ướt” nên được sinh ra và các chi tiết ở đầu đoạn văn là long-term dependency. Mô hình cần phải nhớ được những chi tiết đó thì ở sau nó mới có thể sinh ra được từ hợp lý. Như vậy, vì gặp vấn đề vanishing gradient mà mô hình RNN truyền thống gặp khó khăn trong việc ghi nhớ các long-term dependency. Đây là một điểm yếu rõ rệt nhất của RNN.\nNgược với long-term thì ta có short-term dependency. Sự phụ thuộc này chỉ những mối tương quan giữa những từ ở gần nhau trong đoạn văn bản. Với RNN truyền thống thì nó hoàn toàn có thể nhớ được các sự phụ thuộc này.\nLong Short-Term Memory (LSTM, 1997) và Gated Recurrent Unit (GRU, 2014) là các cải tiến của mô hình RNN truyền thống, nhằm tập trung khắc phục điểm yếu của nó trong vấn đề ghi nhớ các long-term dependency.\nLưu ý.\nTrước khi đi đến các phần sau, ta quy ước rằng output của các cell (LSTM cell, GRU cell) tại thời điểm $t$ sẽ được ký hiệu là $\\bold{Y}_t$ (tránh nhầm với $\\bold{O}_t$ trong RNN truyền thống). Nói chung là thay O thành Y 😜 Trong bài viết này, mình cũng sẽ bỏ qua các giá trị bias, tương tự như trong bài viết về RNN. Long Short-Term Memory (LSTM) Ý tưởng về internal state Trước tiên, ta nhắc lại quá trình feed-forward của RNN truyền thống một chút. Tại thời điểm $t$, từ input $\\bold{X}_t$ và hidden state $\\bold{H}_{t-1}$ thì ta có\n$$ \\bold{H}_t = \\phi_h (\\bold{W}_{xh} \\bold{X}_t + \\bold{W}_{hh} \\bold{H}_{t-1})$$ $$\\bold{Y}_t = \\phi_y (\\bold{W}_{hy} \\bold{H}_t) $$\nHidden state $\\bold{H}_t$ chính là thành phần “nhớ” các short-term dependency trong RNN. Hơn nữa, cũng vì lý do ta tính $\\bold{H}_t$ dựa vào $\\bold{H}_{t-1}$ theo công thức như trên nên RNN mới gặp vấn đề vanishing gradient descent.\nÝ tưởng của Long Short-Term Memory (LSTM) xuất phát từ việc xây dựng thêm một thành phần tại mỗi thời điểm để ghi nhớ long-term dependency, nó được gọi là internal state. Đối với short-term thì ta vẫn sẽ ghi nhớ chúng bằng hidden state như trong RNN truyền thống.\nKí hiệu internal state tại thời điểm $t$ là $\\bold{C}_t$. Ta có các thao tác liên quan đến $\\bold{C}_t$ như sau:\nCập nhật internal state $\\bold{C}_t$:\nLoại bỏ một số thông tin không cần thiết trong long-term dependency nhớ được từ $t-1$ thời điểm trước (hay là quên bớt thông tin trong $\\bold{C}_{t-1}$)\nThêm vào các thông tin cần thiết từ các input của thời điểm hiện tại vào internal state (hay là cập nhật thêm thông tin). Ta thường kí hiệu lượng thông tin này là $\\tilde{\\bold{C}}_t$.\nInput của thời điểm hiện tại bao gồm $\\bold{H}_{t-1}$ (short-term dependency) và $\\bold{X}_t$ $\\tilde{\\bold{C}}_t$ còn được gọi là candiate internal state. Trong đó, ta có hai giá trị điều chỉnh tỉ lệ loại bỏ và thêm vào tại thời điểm $t$, nó sẽ kiểu như\n$$ \\bold{C}_t = \\alpha_t \\bold{C}_{t-1} + \\beta_t \\tilde{\\bold{C}}_t $$\nTừ internal state $\\bold{C}_t$, ta chắt lọc các thông tin có vai trò như là những short-term dependency mà mô hình nên nhớ ở thời điểm hiện tại, tức là tính ra $\\bold{H}_t$.\nNgoài ra, nếu cần tính ra output $\\bold{Y}_t$ thì ta cũng sẽ dựa vào $\\bold{C}_t$.\nNhư vậy, input của LSTM cell tại thời điểm $t$ sẽ có tổng cộng 3 phần là $\\bold{X}_t$, $\\bold{H}_{t-1}$, $\\bold{C}_{t-1}$ và output sẽ bao gồm $\\bold{H}_t$, $\\bold{C}_t$ (có thể có thêm $\\bold{Y}_t$).\nNhận xét.\nTrong LSTM, short-term dependency và long-term dependency được tách ra và nó sử dụng hai cổng để ghi nhớ. Từ công thức tính $\\bold{C}_t$ ở trên, nếu $\\alpha_t = 0$ thì có nghĩa là ta sẽ quên hết các thông tin phía trước luôn, chỉ tập trung hiện tại thôi, còn $\\beta_t = 0$ thì xem như ta không quan tâm hiện tại, chỉ dùng đúng những gì đã biết trong quá khứ. Thật ra $\\alpha_t$ và $\\beta_t$ là các ma trận và phép nhân được thực hiện là element-wise. Nhờ tính internal state $\\bold{C}_t$ theo ý tưởng của LSTM, ta đã có thể hạn chế vấn đề vanishing gradient (hạn chế thôi, vẫn có thể gặp phải nhưng hiếm hơn 😀) Forget gate, input gate, output gate và candidate internal state Trong các thao tác liên quan đến $\\bold{C}_t$ ở trên, quan trọng nhất là các chi tiết về loại bỏ, thêm vào và chắt lọc. Chúng sẽ lần lượt ứng với ba “cổng” là forget gate $\\bold{F}_t$, input gate $\\bold{I}_t$ và output gate $\\bold{O}_t$trong LSTM cell.\nLưu ý rằng các giá trị này là tỉ lệ, liên quan đến thao tác loại bỏ, thêm vào và chắt lọc các thông tin truyền từ bên ngoài vào nên ta sẽ dùng activation function $\\sigma$ (sigmoid) Ngoài ra, thao tác tính toán thông tin candidate internal state $\\tilde{\\bold{C}}_t$ dựa vào $\\bold{X}_t$ và $\\bold{H}_{t-1}$ sẽ được biểu diễn thông qua thành phần input node. Activation function được dùng để tính giá tị này là $\\tanh$.\nKhi đó, những thành phần này được biểu diễn trong LSTM cell như sau:\nMinh họa các thành phần trong LSTM cell\nNguồn: Dive into DL Như vậy thì ta đã có kha khá ký hiệu được sử dụng để biểu diễn cho các giá trị. Điều này cũng có nghĩa là sẽ có rất nhiều ma trận trọng số 😀 Cụ thể hơn, với mỗi thành phần $\\bold{F}_t$, $\\bold{I}_t$, $\\bold{O}_t$ và $\\tilde{\\bold{C}}_t$ thì ta sẽ có hai ma trận trọng số, ví dụ như $\\bold{W}_{xf}, \\bold{W}_{hf}$ đối với thành phần $\\bold{F}_t$.\nQuá trình feed-forward Tổng quan quá trình feed-forward trong LSTM cell được thể hiện trong hình ảnh bên dưới\nQuá trình feed-forward trong LSTM cell\nNguồn: Dive into DL Ta sẽ có khá nhiều phép tính, trước hết là tính các “cổng” $\\bold{F}_t$, $\\bold{I}_t$, $\\bold{O}_t$ dựa vào $\\bold{X}_t$ và $\\bold{H}_{t-1}$:\n$$ \\mathbf{I}_t = \\sigma( \\mathbf{W}_{xi}\\mathbf{X}_t + \\mathbf{W}_{hi}\\mathbf{H}_{t-1} )$$ $$\\mathbf{F}_t = \\sigma( \\mathbf{W}_{xf} \\mathbf{X}_t + \\mathbf{W}_{hf} \\mathbf{H}_{t-1})$$ $$\\mathbf{O}_t = \\sigma( \\mathbf{W}_{xo}\\mathbf{X}_t + \\mathbf{W}_{ho} \\mathbf{H}_{t-1}) $$\nBên cạnh đó, ta tính $\\tilde{\\bold{C}}_t$ bằng công thức\n$$ \\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{W}_{xc} \\mathbf{X}_t + \\mathbf{W}_{hc} \\mathbf{H}_{t-1}) $$\nTừ đó, internal state $\\bold{C}_t$ và hidden state $\\bold{H}_t$ sẽ được tính như sau:\n$$ \\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$$ $$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t) $$\nNếu ở thời điểm này cần tính ra output $\\bold{Y}_t$ thì sẽ tính theo $\\bold{C}_t$ cùng với ma trận trọng số $\\bold{W}_{cy}$.\nGated Recurrent Units Ý tưởng đơn giản hóa LSTM Gated Recurrent Units (GRU - 2014) là có thể nói là một phiên bản tối ưu hơn của LSTM về mặt độ phức tạp, trong khi về hiệu năng thì có thể nói là hai mô hình này ngang ngửa nhau. Do đó, ta thường thấy GRU được sử dụng nhiều hơn.\nÝ tưởng mấu chốt của GRU là chỉ sử dụng hidden state $\\bold{H}_t$ để vừa nhớ cả short-term và long-term dependency. Trong khi đó, ở LSTM thì ta có sự phân tách giữa hai thông tin này.\nBên cạnh đó, nhìn vào công thức tính internal state $\\bold{C}_t$ dựa vào hai giá trị tỉ lệ $\\bold{F}_t$ và $\\bold{I}_t$ trong LSTM cell, ta có thể có “cảm giác” là thông thường thì tổng của chúng hay bằng 1, nên là thôi bỏ một cái đi 😀 GRU làm theo đúng như thế.\nNgoài ra, trong LSTM có candidate internal state $\\tilde{\\bold{C}}_t$ dùng để tính ra các thông tin cần thiết từ các input $\\bold{X}_t$ và $\\bold{H}_{t-1}$. Với GRU thì ta cũng có thành phần tương tự là candidate hidden state $\\tilde{\\bold{H}}_t$ với cùng mục đích như thế.\nReset gate, update gate và candidate hidden state Thay vì sử dụng ba “cổng” như LSTM thì GRU sẽ dùng hai là reset gate $\\bold{R}_t$ và update gate $\\bold{Z}_t$. Một thành phần nữa cũng rất quan trọng trong GRU là candidate hidden state $\\tilde{\\bold{H}}_t$. Trong đó:\n$\\bold{R}_t$ sẽ đóng vai trò loại bỏ một số thông tin không cần thiết về các short-term dependency trong hidden state của thời điểm trước là $\\bold{H}_{t-1}$. Ta sẽ dùng nó để tính $\\tilde{\\bold{H}}_t$. Do đó, candidate hidden state $\\tilde{\\bold{H}}_t$ sẽ chứa các thông tin có ích về short-term dependency. $\\bold{Z}_t$ sẽ thay thế cho cả $\\bold{F}_t$ và $\\bold{I}_t$ trong việc điều chỉnh tỉ lệ loại bỏ thông tin không cần thiết về long-term dependency trong $\\bold{H}_{t-1}$ và thêm vào các thông tin cần thiết về short-term dependency (chính là $\\tilde{\\bold{H}}_t$). Minh họa các thành phần trong GRU cell\nNguồn: Dive into DL Ta thấy rằng, số ma trận trọng số cần sử dụng trong GRU cell sẽ ít hơn LSTM cell hai ma trận. Cụ thể hơn thì với mỗi thành phần $\\bold{R}_t$, $\\bold{Z}_t$ và $\\tilde{\\bold{H}}_t$ thì ta đều cần hai ma trận trọng số.\nQuá trình feed-forward Tổng quan quá trình feed-forward trong GRU cell được thể hiện trong hình ảnh bên dưới.\nMinh họa các thành phần trong GRU cell\nNguồn: Dive into DL Trước hết, ta tính các “cổng” $\\bold{R}_t$, $\\bold{Z}_t$ dựa vào $\\bold{X}_t$ và $\\bold{H}_{t-1}$:\n$$ \\mathbf{R}_t = \\sigma( \\mathbf{W}_{xr} \\mathbf{X}_t + \\mathbf{W}_{hr} \\mathbf{H}_{t-1})$$ $$\\mathbf{Z}_t = \\sigma(\\mathbf{W}_{xz} \\mathbf{X}_t + \\mathbf{W}_{hz} \\mathbf{H}_{t-1} ) $$\nTừ $\\bold{R}_t$, ta tính được $\\tilde{\\bold{H}}_t$ như sau:\n$$ \\tilde{\\mathbf{H}}_t = \\tanh( \\mathbf{W}_{xh} \\mathbf{X}_t + \\mathbf{W}_{hh}\\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) ) $$\nSử dụng $\\bold{Z}_t$ và $\\tilde{\\bold{H}}_t$, hidden state tại thời điểm $t$ được xác định bằng\n$$ \\bold{H}_t = \\bold{Z}_t \\odot \\bold{H}_{t-1} + (1 - \\bold{Z}_t) \\odot \\tilde{\\bold{H}}_t $$\nTài liệu tham khảo Dive into DL, Long Short-Term Memory Dive into DL, Gated Recurrent Units Rian Dolphin, LSTM Networks | A Detailed Explanation DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-02-24T00:51:43+07:00","permalink":"https://htrvu.github.io/post/lstm-gru/","title":"Long Short-Term Memory (LSTM) và Gated Recurrent Unit (GRU)"},{"content":"Trong bài viết về Recurrent Neural Network, mình đã đề cập khá kỹ về mô hình này nhưng để ứng dụng được nó vào các bài toán thì ta cần phải làm thêm bước “số hóa” dữ liệu từ văn bản sao cho máy tính có thể hiểu được.\nNếu máy tính hiểu được càng nhiều về các từ thì nghĩa là cách số hóa càng có hiệu quả. Do đó, ta cần quan tâm đến vấn đề “hiểu”. Hiểu như thế nào là đủ tốt? 😀 Đối với NLP, ta có những phương pháp (hay có thể nói là kỹ thuật) biểu diễn từ phổ biến là One-hot Encoding, TF-IDF và Word Embedding. Nội dung của bài viết này sẽ tập trung vào One-hot Encoding và Word Embedding.\nOne-hot Encoding Ý tưởng Từ điển (vocabulary) là một thành phần không thể thiếu của mọi hệ thống ngôn ngữ. Những từ ta dùng thường ngày hầu như là sẽ nằm ở một vị trí nào đó trong từ điển (có thể các từ địa phương thì sẽ không có). One-hot Encoding là phương pháp biểu diễn từ bằng chính thông tin vị trí này.\nVới những từ không có trong từ điển thì ta thường sử dụng một giá trị vị trí đặc biệt để cho biết từ đó là unknown. Giả sử tập từ điển của chúng ta có $S$ từ và không có từ trên các văn bản là không có trong từ điển. Khi đó, mỗi từ sẽ được biểu diễn bằng một vector nhị phân có $S$ chiều, với duy nhất một phần tử bằng 1 tại chiều ứng với vị trí của từ đó trong từ điển và các phần tử còn lại là 0. Ví dụ:\nMinh họa phương pháp One-hot Encoding với kích thước từ điển là 9\nNguồn: Shane Lynn Khi kết hợp phương pháp One-hot Encoding vào mô hình RNN để giải quyết các bài toán thì ở trong mỗi giai đoạn ta sẽ có:\nInput và label sẽ là các vector nhị phân tương ứng với các từ Output là một vector thể hiện một phân bố xác suất, với phần tử thứ $i$ là xác suất mà từ output là từ ở vị trí thứ $i$ trong từ điển (do đó activation function thường dùng ở đây chính là softmax) Ví dụ áp dụng One-hot Encoding vào RNN trong bài toán sinh văn bản theo từng kí tự\nNguồn: Stanford - Natural Language Processing Vì sao ta lại sử dụng vector nhị phân để biểu diễn các từ mà không dùng luôn giá trị số thực là vị trí của từ trong từ điển?\nCâu hỏi này cũng giống như hỏi rằng trong bài toán image classification thì vì sao ta không cài đặt output là một số thực và sau đó làm tròn để có kết quả mà lại là một vector phân bố xác suất. Tất nhiên là nếu làm theo cách đó thì mọi thứ vẫn CÓ THỂ ổn, quá trình huấn luyện cũng có thể được thành công. Tuy nhiên, ta có những điều cần lưu tâm như sau: Với output là số thực như vậy thì cost function hầu như chắc chắn là MSE (Mean Square Error). Khi đó, quá trình huấn luyện sẽ rất dễ rơi vào vị trí tối ưu cục bộ. Nếu mà số từ trong từ điển là rất nhiều thì kết quả của các phép tính trong cách biểu diễn dùng số thực là rất lớn. Để ý rằng, trong các biểu diễn One-hot Encoding thì khoảng cách giữa một từ với các từ khác nó sẽ bằng hằng số là $\\sqrt{2}$. Trong khi đó, với cách biểu diễn dùng duy nhất số thực thì lại không, có những cặp từ rất gần nhau và có những cặp từ cực kì xa nhau, trong khi ta chưa có bất cứ điều gì thể hiện được rằng từ này nên gần với một từ hơn so với từ kia. Hạn chế One-hot Encoding Trong cách biểu diễn One-hot Encoding, ta thấy rằng máy tính đã có thể phân biệt được các từ với nhau, có thể biết được từ được dùng trong câu input là từ gì và có thể cho biết từ mà nó tính ra được ở output là từ gì. Nói chung là máy tính đã hiểu được “mặt trước” của các từ.\nTuy nhiên, ta vẫn chưa thể biểu diễn được mối quan hệ giữa các từ với nhau. Như đã đề cập ở phần trước, khoảng cách giữa hai cặp từ phân biệt bất kỳ đều bằng $\\sqrt{2}$, trong khi những từ có nghĩa gần gần nhau như “good” và “nice” thì nên có khoảng cách gần nhau, còn những từ trái nghĩa nhau như “good” và “bad” thì cũng nên cách nhau rất xa. Chính vì yếu tố này mà thường thì việc áp dụng One-hot Encoding vào RNN khó có thể mang lại kết quả như mong muốn.\nBên cạnh đó, cách biểu diễn One-hot Encoding thật sự là rất tốn kém về mặt bộ nhớ 😀 Nếu mà kích thước từ điển rất lớn thì cứ mỗi từ như vậy ta lại cần một vector có số chiều khổng lồ để biểu diễn. Một cách khắc phục vấn đề này là sử dụng ma trận thưa (sparse matrix), nhưng mà việc cài đặt thì cũng không phải đơn giản.\nTừ các hạn chế của One-hot Encoding, ta có một phương pháp tốt hơn, vừa có thể biểu diễn được mối quan hệ giữa các từ và vừa tiết kiệm được bộ nhớ, đó là Word Embedding!\nWord Embedding Ý tưởng Đầu tiên, embedding nói chung là phương pháp đưa một vector có số chiều lớn (thường ở dạng thưa, tức là hầu hết các phần tử đều bằng 0), về một vector có số chiều nhỏ hơn (và không thưa).\nTa thấy ngay rằng one-hot vector để biểu diễn các từ trong một tập từ điển lớn chính là vector có số chiều lớn và ở dạng thưa 😀 Embedding có thể được áp dụng ở nhiều mảng khác nhau chứ không phải mỗi xử lý ngôn ngữ, ví dụ như hình ảnh cũng có. Word Embedding là một phương pháp biểu diễn các từ bằng một vector đặc trưng. Ví dụ, với các từ {man, woman, king, queen, apple, orange} và tập các đặc trưng {gender, age, food} thì ta có thể biểu diễn mỗi từ bằng một vector 3 chiều như sau:\nman woman king queen apple orange gender -1 1 -0.9 0.97 0.0 0.01 age 0.3 0.25 0.7 0.69 0.02 0.0 food 0.01 0.0 0.005 0.015 0.97 0.96 Trong bảng trên, mỗi từ trong từ điển ban đầu đã được ánh xạ thành một vector 3 chiều (còn one-hot vector để biểu diễn chúng thì có 6 chiều). Trong đó, giá trị vector ứng với mỗi từ sẽ chứa những nét đặc trưng về mặt ngữ nghĩa của từ đó. Kí hiệu $e_{word}$ là embedding vector của từ $word$. Ta có một số nhận xét sau: $e_{apple}$ và $e_{orange}$ có giá trị tại đặc trưng food rất cao và hai đặc trưng còn lại thì không. $e_{man}$ có đặc trưng gender là -1 còn $e_{woman}$ là 1, hàm ý rằng giới tính “man” và “woman” là trái ngược nhau. $e_{man}$ với $e_{king}$ có giá trị tại đặc trưng gender rất giống nhau, đối với age thì có sự khác biệt, hàm ý rằng “king” thì thường lớn tuổi hơn “man”. Ta có nhân xét tương tự với “woman” và “king”. Nếu ta tính thử độ tương đồng (similarity) giữa các vector (thường là khoảng cách Cosine hoặc khoảng cách Euclid), thì kết quả sẽ có ý nghĩa như sau: Hai vector $e_{man}$ và $e_{king}$ rất gần nhau. Tương tự với $e_{woman}$ và $e_{queen}$, $e_{apple}$ và $e_{orange}$. Điều này thể hiện rằng các từ trong mỗi cặp có quan hệ gần gũi với nhau về ngữ nghĩa. Hai vector $e_{man}$ và $e_{woman}$ có hướng gần như là ngược nhau, thể hiện rằng hai từ này có quan hệ trái ngược nhau. Thông thường, người ta thường sử dụng phương pháp t-SNE để giảm chiều các embedding vector xuống 2 chiều và trực quan hóa chúng để có góc nhìn rõ hơn về Word Embedding. Ví dụ như hình bên dưới, với các từ có nghĩa tương tự nhau thì ta sẽ thấy chúng có xu hướng cùng thuộc về một cụm:\nSử dụng t-SNE để trực quan hóa các embedding vector\nNguồn: Neptune AI Đối với trực quan hóa trong không gian 3 chiều thì các bạn có thể truy cập vào trang này của Tensorflow. Trong trang web đó, nếu tìm kiếm từ “soccer” thì ta sẽ thấy các vector được highlight lên là vector ứng với các từ có nghĩa rất tương tự, và hầu hết là liên quan đến thể thao.\nNhư vậy, phương pháp Word Embedding đã có thể khắc phục được hạn chế của One-hot Encoding trong việc thể hiện mối quan hệ giữa các từ.\nTính chất của Word Embedding Trong khả năng biểu diễn các từ bằng vector đặc trưng và thể hiện được mối quan hệ giữa từ đó với những từ khác, ta có một tính chất thú vị liên quan đến Analogy Reasoning (suy diễn tương tự).\nVí dụ: Cho trước 3 từ “man”, “woman” và “king”. Trong đó, “man” đã có một quan hệ nhất định với “woman”. Ta cần tìm một từ sao cho quan hệ giữa “king” với từ này cũng tương tự như quan hệ giữa “man” và “woman”. Nếu một hệ thống Word Embedding đủ tốt thì ta sẽ có tính chất rằng những cặp từ $(w_{i1}, w_{i2})$ mà có quan hệ giữa hai từ trong một cặp là rất tương tự nhau thì các vector $x_i = e_{w_{i1}} - e_{w_{i2}}$ sẽ có hướng cũng rất tương tự. Ví dụ:\nMinh họa Analogy Reasoning\nNguồn: Polakowo Dựa vào tính chất này, ta có thể giải quyết câu hỏi đặt ra ở phía trên rằng từ cần tìm sẽ là “queen”. Để kiểm chứng, hãy xét lại bảng ở phần 2.1, ta có:\n$e_{man} -e_{woman} = \\begin{bmatrix}-2 \u0026amp; 0.05 \u0026amp; 0.01\\end{bmatrix}^\\top$ $e_{king} - e_{queen} = \\begin{bmatrix}-1.87 \u0026amp; 0.01 \u0026amp; -0.01 \\end{bmatrix}^\\top$ Để biểu diễn bài toán analogy reasoning như ví dụ bên trên một cách hình thức hơn, ta có thể phát biểu như sau:\nTìm từ $w$ sao cho\n$$w = \\argmax_w \\left ( \\text{sim} ( e_w, e_{man} - e_{woman} + e_{king} )\\right )$$\nvà kết quả là $w = queen$.\nSử dụng Word Embedding trong RNN Trong phương pháp One-hot Encoding, ta sử dụng các one-hot vector ở input và output của RNN. Đối với word-embedding thì ta sẽ thay đổi một chút ở input, còn output thì vẫn dùng One-hot Encoding để biết được mô hình dự đoán từ nào. 😜\nTại sao lại như thế?\nĐối với input, đưa vào RNN embedding vector thì chắc chắn mô hình có thể học tốt hơn so với one-hot vector rồi. Trong output, ta thấy rằng việc mô hình tính ra một vector dạng phân bố xác suất và sau đó xác định từ tương ứng bằng cách softmax thì sẽ dễ hơn nhiều so với việc output ra một embedding vector rồi từ vector này đi tìm từ gốc. Thật ra thì mình chưa thấy ai thực hiện tìm từ dựa vào embedding vector cả. 😀 Như vậy, để sử dụng Word Embedding trong RNN thì ta sẽ dùng embedding vector của các từ để làm input cho RNN.\nVí dụ: mượn tạm ảnh của các pháp sư Trung Hoa z =)) Minh họa sử dụng Word Embedding trong RNN\nNguồn: Pháp sư nào đó Lưu ý.\nKhi sử dụng Word Embedding trong RNN thì thường ta sẽ dùng theo hướng transfer learning hoặc fine-tuning. Điều này có nghĩa là các embedding vector của mỗi từ có thể đã được cung cấp sẵn, ta chỉ việc đem vào dùng trong mô hình là đủ và nếu cần thiết thì cũng sẽ tiếp tục huấn luyện trên nền tảng đã có. Embedding matrix Ở các phần trên thì ta chỉ mới nêu sơ lược về phương pháp Word Embedding chứ chưa đề cập đến việc làm thế nào để xây dựng được các vector biểu diễn từ như vậy. Đầu tiên, thứ chúng ta cần xây dựng trong Word Embedding được gọi là embedding matrix (kí hiệu là $E$), với số dòng là số đặc trưng được dùng để mô tả cho mỗi từ và số cột bằng với số từ trong từ điển.\nĐể minh họa, ta sẽ dùng lại ví dụ ở phần 2.1. embedding matrix $E$ sẽ là\n$$ E = \\begin{bmatrix}-1 \u0026amp; 1 \u0026amp; -0.9 \u0026amp; 0.97 \u0026amp; 0.0 \u0026amp; 0.01 \\\\ 0.3 \u0026amp; 0.25 \u0026amp; 0.7 \u0026amp; 0.69 \u0026amp; 0.02 \u0026amp; 0.0 \\\\ 0.01 \u0026amp; 0.0 \u0026amp; 0.005 \u0026amp; 0.015 \u0026amp; 0.97 \u0026amp; 0.96\\end{bmatrix} $$\nVới từ $man$, one-hot vector của từ này là\n$$o_{man} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\end{bmatrix}^\\top$$\nEmbedding vector của $man$ sẽ được tính bằng công thức\n$$e_{man} = E \\cdot o_{man} = \\begin{bmatrix} -1 \u0026amp; 0.3 \u0026amp; 0.01 \\end{bmatrix}^\\top$$\nĐể xây dựng các embedding matrix, ta có hai hướng phổ biến như sau:\nWord2vec: Huấn luyện một mô hình MLP. embedding matrix sẽ là một ma trận trọng số trong mô hình MLP sau khi đã huấn luyện xong. GloVe (Global Vector for word representations): Mạnh hơn Word2vec, có sử dụng thêm những kỹ thuật liên quan đến xác suất thống kê. Trong bài viết này, mình sẽ tập trung vào word2vec.\nWord2vec Word2vec là một mô hình rất đơn giản và nổi tiếng trong việc tạo embedding matrix. Ý tưởng của word2vec xuất phát từ hai nhận xét sau:\nHai từ thường xuất hiện trong các ngữ cảnh tương tự nhau thì có thể có quan hệ gần gũi nhau về mặt ngữ nghĩa. Khi cho biết trước các từ xung quanh từ bị thiếu trong câu, ta có thể dự đoán ra được từ đó. Ví dụ, với câu “Husky là một … chó rất ngáo” thì từ trong dấu ba chấm có khả năng cao là “loài”. Đây là ví dụ với câu ngắn, còn nếu câu dài thì đôi khi ta chỉ cần xét tầm 10 từ xung quanh từ cần dự đoán là đã đủ để đoán ra được. Đầu tiên, ta đề cập đến khái niệm từ mục tiêu (target word) và từ ngữ cảnh (context word). Có thể nói từ mục tiêu là từ ta đang xem xét và từ ngữ cảnh là các từ xuất hiện xung quanh từ mục tiêu ở trong các đoạn văn bản của kho dữ liệu, với phạm vi là cách từ mục tiêu không quá $\\dfrac{C}{2}$ từ. Vùng phạm vi này còn lại là cửa sổ trượt (sliding window).\nVới câu ví dụ ở trên thì, từ “loài” là target word. Nếu xét cửa sổ trượt có kích thước $C = 4$ thì các context word sẽ bao gồm “là”, “một”, “chó”, “rất”. Để có cái nhìn rõ hơn về các khái niệm này, ta xét ví dụ bên dưới với kích thước sliding window là 4. Từ màu xanh là target word, các từ trong ô màu trắng là context word\nNguồn: Machine Learning cho dữ liệu dạng bảng Lưu ý.\nTừ phương pháp word2vec, ta sẽ có hai embedding vector cho mỗi từ, ứng với hai trường hợp là từ đó đóng vai trò target word và context word. Lý do là vì trong mỗi tình huống thì ngữ nghĩa của nó có thể sẽ khác nhau. Trong word2vec, ta sẽ đi xây dựng một mô hình MLP (Multi-layer Perceptron, hay nói cách khác là Neural Network) chỉ gồm 1 hidden layer, với mục đích có thể là:\nDựa vào target word để dự đoán context word Dựa vào các context word để dự đoán target word Tùy vào mục đích mà ta sẽ có một kiến trúc MLP khác nhau. Với mục đích (1) thì ta có Skip-gram, mục đích (2) là CBoW (Continuous Bag of Word)\nQuay lại với hai nhận xét đã mở ra ý tưởng cho word2vec thì nhận xét thứ nhất được thể hiện rõ hơn ở trong Skip-gram và nhận xét thứ hai thì ở trong CBoW 😀 Để có cái nhìn tổng quan về sự khác biệt giữa Skip-gram và CBoW, ta có hình ảnh so sánh như bên dưới:\nSự khác biệt giữa CBoW và skip-gram trong một câu với target word là W(t), context word là W(t-2), W(t-1), W(t+1), W(t+2)\nNguồn: https://i.stack.imgur.com/ShJJX.png Skip-gram Skip-gram là cách xây dựng mô hình MLP theo hướng dự đoán context word dựa vào target word. Về mặt toán học thì ta sẽ đi tìm xác suất xảy ra các context word khi biết trước target word.\nVí dụ, kho dữ liệu ta có hai câu là {“em ấy học toán tốt”, “em ấy học toán giỏi”}. Với từ mục tiêu “học” và kích thước sliding window là $C=4$, ta sẽ tìm xác suất\n$$ P_0 = P(\\text{\u0026ldquo;em\u0026rdquo;}, \\text{\u0026ldquo;ấy\u0026rdquo;}, \\text{\u0026ldquo;toán\u0026rdquo;}, \\text{\u0026ldquo;tốt\u0026rdquo;}, \\text{\u0026ldquo;giỏi\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) $$\nGiả sử các từ trên là độc lập với nhau, khi đó\n$$ P_0 = P(\\text{\u0026ldquo;em\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;ấy\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;toán\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;tốt\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;giỏi\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) $$\nTập từ điển sẽ có 6 từ nên input của mô hình MLP sẽ là vector 6 chiều. Ta sử dụng hidden layer với 300 neuron. Xét hai cặp (context, target) lần lượt là (học, tốt) và (học, giỏi). Khi đó, mô hình skip-gram sẽ có dạng như hình bên dưới, với $\\bold{U}$ và $\\bold{V}$ lần lượt là ma trận trọng số giữa layer input-hidden và hidden-output.\nTham khảo: ProtonX - Word2vec Ta thấy rằng, hai từ “tốt” và “giỏi” cùng xuất hiện trong một ngữ cảnh, do đó chúng nên có quan hệ nào đó về mặt ngữ nghĩa 😀 Lưu ý.\nShape của $\\bold{U}$ có thể là (embedding_dim x vocab_size), tức là $(300 \\times 6)$, hoặc là (vocab_size x embedding_dim), tức là $(6 \\times 300)$. Với bài viết này thì mình sử dụng (embedding_dim x vocab_size). Tương tự như với $\\bold{V}$. Khi lần đầu tìm hiểu về Skip-gram, mình có một thắc mắc mà mình nghĩ là cũng rất nhiều người có cùng thắc mắc như thế 😜 Trong hình trên, ta thấy rằng mô hình cùng nhận vào một input là one-hot vector của từ học, sử dụng cùng hai ma trận trọng số $\\bold{U}$ và $\\bold{V}$, vì sao label lại có những giá trị khác nhau?\nThực chất, trong quá trình huấn luyện skip-gram, ta sẽ sử dụng optimizer SGD (stochastic gradient descent), tại mỗi thời điểm thì ta sẽ chọn ngẫu nhiên một cặp (target, context) rồi tiến hành cập nhật các ma trận trọng số một chút dựa theo cặp được chọn. Trong đó, phép chọn này không nên tuân theo phân phối đều mà nên có heuristic một chút, ví dụ như cặp nào xuất hiện càng nhiều thì có xác suất được chọn càng cao. Do đó, cặp (target, context) nào xuất hiện càng nhiều thì mô hình càng “học” được nhiều thứ về nó. Kết quả là sau quá trình huấn luyện, từ dự đoán của một context word sẽ là một phân bố xác suất “đủ gần” với tất cả các target word của nó trong kho dữ liệu. Ngoài ra, trong quá trình huấn luyện skip-gram thì chúng ta thường sử dụng loss function là cross-entropy (có dùng đến $\\text{softmax}$ activation function).\nCBoW (Continuous Bag of Words) Có thể nói CBoW là một phiên bản ngược lại của Skip-gram. Trong CBoW, ta sẽ sử dụng các context word để dự đoán target word. Về mặt toán học thì ta sẽ đi tìm xác suất xảy ra target word khi biết trước các context word.\nVới cùng ví dụ như phần về Skip-gram, ta sẽ tìm xác suất $$P_0 = P(\\text{\u0026ldquo;học\u0026rdquo;} | \\text{\u0026ldquo;em\u0026rdquo;}, \\text{\u0026ldquo;ấy\u0026rdquo;}, \\text{\u0026ldquo;toán\u0026rdquo;}, \\text{\u0026ldquo;tốt\u0026rdquo;}, \\text{\u0026ldquo;giỏi\u0026rdquo;})$$\nLúc này, ta thường tính một “từ trung bình” của các context word (embedding vector trung bình), sau đó thay vào biểu thức trên $$P_0 = P(\\text{\u0026ldquo;học\u0026rdquo;} | \\overline{word})$$\nĐể dễ minh họa, ta xét hai context words “em”, “toán” của target word “học”. Khi đó, mô hình CBoW sẽ có dạng như hình bên dưới, với $\\bold{V}$ và $\\bold{U}$ là ma trận trọng số giữa layer input-hidden và hidden-output. Lưu ý rằng, sau khi tính ra output tại hidden layer của các context word thì ta sẽ thực hiện thao tác tính trung bình để có một vector trung bình, sau đó mới tính ra predicted word. Tham khảo: ProtonX - Word2vec Trong huấn luyện mô hình, ta cũng loss function là cross-entropy và optimizer SGD giống như Skip-gram. Trong đó, ở mỗi bước của SGD thì thứ ta chọn ngẫu nhiên là một câu ngắn trong kho dữ liệu.\nTrích xuất embedding matrix Sau khi huấn luyện xong các mô hình như Skip-gram và CBoW thì ta thu được các ma trận trọng số $\\bold{U}$ và $\\bold{V}$. Nếu các bạn để ý thì trong mỗi mô hình, thứ tự mình sử dụng kí hiệu $\\bold{U}$ và $\\bold{V}$ là khác nhau.\nTrong Skip-gram, $\\bold{U}$ là ma trận trọng số nối giữa input-hidden, liên quan đến target word và $\\bold{V}$ thì nối giữa hidden-output và nó liên quan đến context word. Cũng vì sự “liên quan” giữa ma trận trọng số là các từ, trong CBoW thì $\\bold{V}$ được đưa lên thành ma trận trọng số giữa input-hidden, tương tự cho $\\bold{U}$. Ta biết rằng ma trận trọng số nối giữa layer input-hidden có nhiệm vụ chính là “học” các đặc trưng của từ input, còn ma trận trọng số nối giữa hidden-output thì có nhiệm vụ chính là dự đoán từ. Như vậy, rõ ràng là ta nên dùng ma trận trọng số đầu tiên để làm embedding matrix. Tuy nhiên, có sự khác biệt nào giữa các ma trận thu được từ Skip-gram và CBoW?\nĐối với Skip-gram, $\\bold{U}$ liên quan trực tiếp đến target word. Khi đó, embedding vector của mỗi từ tính được dựa vào $\\bold{U}$ sẽ mang nhiều thông tin về mặt ngữ nghĩa hơn. Ngược lại, trong CBoW, $\\bold{V}$ liên quan trực tiếp đến các context word nên embedding vector của mỗi từ tính được sẽ nghiêng về phía ngữ pháp. Ví dụ, với từ “cat”, ta tính embedding vector của nó theo cả hai ma trận $\\bold{U}$ trong Skip-gram và $\\bold{V}$ trong CBoW. Tiếp đến thì ta sẽ tìm từ tương đồng với “cat” nhất . Khi đó, sử dụng $\\bold{U}$ thì kết quả có thể là “dog”, còn dùng $\\bold{V}$ thì rất có thể sẽ là “cats” 😀.\nNhận xét Hai hướng tiếp cận Skip-gram và CBoW đều có những điểm mạnh và yếu của riêng nó (ví dụ như về mặt ngữ nghĩa và ngữ pháp) nhưng nhìn chung thì chúng đều cho ta những embedding vector đủ tốt để sử dụng trong các bài toán khác.\nTuy nhiên, ta có một điểm yếu khá quan trọng trong việc huấn luyện Skip-gram và CBoW. Về hàm loss function thì mình đã đề cập là chúng đều sử dụng cross-entropy và cần đến $\\text{softmax}$ activation function. Trong trường hợp từ điển có rất nhiều từ thì thao tác tính $\\text{softmax}$ này sẽ rất rất lâu 😀\nLĐể khắc phục, ta có một số cách như là sử dụng Hierarchy Softmax hoặc là Negative Sampling. Trong bài viết này thì mình sẽ không đề cập đến chúng 😜\nVấn đề thiên vị trong Word Embdding Nghe rất là ảo, nhưng mà nó có tồn tại 😅 Điều này xảy ra phần lớn là do kho dữ liệu văn bản mà chúng ta sử dụng để xây dựng embedding matrix.\nĐể lấy ví dụ, mình sẽ xét trường hợp liên quan đến giới tính. Cùng quay lại bài toán Analogy Reasoning trong phần 2.2:\nVới 3 từ “man”, “woman” và “king” thì ta có thể tìm được từ “queen” sao cho quan hệ giữa “king” và “queen” sẽ tương tự như giữa “man\u0026quot; và “woman”. Bây giờ giả sử ta có “man”, “doctor”, “woman” và cần tìm từ X sao cho quan hệ giữa “woman” và X tương tự như giữa “man\u0026quot; và “doctor”. Nếu kho dữ liệu liên quan phần lớn đến việc người đàn ông là trụ cột trong gia đình (xã hội thời xa xưa) thì kết quả X rất có thể là “babysitter” (người trông trẻ). Như vậy, đã có vấn đề thiên vị cho nam giới. Để hạn chế vấn đề này, ta có một số phương pháp như Hard Debiasing, Soft Debiasing. Mình sẽ không đề cập đến những phương pháp này ở đây, các bạn có thể tự tìm đọc nhé 😀 Nó phần lớn là liên quan đến các phép biến đổi toán học.\nDưới đây là minh họa cho vấn đề thiên vị mà mình đã lấy ví dụ ở trên để cho các bạn dễ hình dung:\nĐầu tiên, hướng thay đổi giới tính là từ trái qua phải. Vì vấn đề thiên vị đang xảy ra liên quan đến giới tính nên ta gọi đây là bias direction. Trước khi điều chỉnh, ta thấy embedding vector của doctor nghiêng về phía bên nam giới hơn, tương tự như babysitter. Sau khi hạn chế vấn đề thiên vị điều chỉnh, các embedding vector của doctor và baby sitter nên nghiêng về phía “công bằng” hơn đối với hai giới tính, tức là hướng trực giao với hướng bias direction tại vị trí trung bình. Trước khi điều chỉnh (embedding vector gốc)\nSau khi thực hiện debiasing\nNguồn: Vagdevik\nTài liệu tham khảo Vũ Hữu Tiệp, Machine Learning cho dữ liệu dạng bảng - Word2vec Kavita Gane, Word2Vec: A Comparison Between CBOW, SkipGram \u0026amp; SkipGramSI Dive into Deep Learning, Word2vec ","date":"2023-02-19T10:51:57+07:00","permalink":"https://htrvu.github.io/post/word-embedding/","title":"Word Embedding"},{"content":"Đây là bài viết đầu tiên của mình trong lĩnh vực Natural Language Processing (NLP - Xử lý ngôn ngữ tự nhiên). Nó sẽ khá dài một chút, mong các bạn đọc hết nhé! 😀\nSơ lược về Natural Language Processing Bên cạnh Computer Vision (CV - Thị giác máy tính) thì Natural Language Processing (NLP - Xử lý ngôn ngữ tự nhiên) cũng là một mảng rất quan trọng và được nghiên cứu rộng rãi trong Deep Learning. Các sản phẩm nổi tiếng trên thế giới liên quan đến chữ viết, giọng nói, âm thanh như Google Dịch, Google Assistant, Siri, Alexa, ChatGPT… đều là các thành quả của việc áp dụng NLP vào thực tế.\nNguồn: Data Science Dojo Tuy NLP và CV là hai lĩnh vực nghiên cứu khác nhau nhưng chúng có những mối quan hệ rất đặc biệt và thú vị. Ta có thể đem ý tưởng của CV qua NLP, ví dụ như sử dụng phép toán convolution trong xử lý tính toán với văn bản, và ngược lại là đem ý tưởng của NLP qua CV, mà đặc biệt nổi tiếng gần đây là sử dụng Attention, Transformer trong CV. Việc kết hợp NLP và CV đã tạo ra nhiều kết quả rất nổi bật trong các ứng dụng như Image Captioning, Text-to-Image.\nNếu mà kể tên ra thì ta có thể nhắc đến ngay mô hình rất ảo diệu là Stable Diffusion 😀 Nền móng của NLP bắt nguồn từ những mô hình toán học và xác suất, đặc biệt là mô hình Markov. Khi Deep Learning bắt đầu phát triển mạnh mẽ, ta đã có thêm những mô hình khác với độ hiệu quả rất tuyệt vời như Recurrent Neural Network, Sequence to Sequence, Attention Mechanism và Transformer. Trong đó, Recurrent Neural Network, hay là RNN, là sự khởi đầu thú vị của Deep Learning trong NLP. Mô hình RNN cũng là sẽ chủ đề của bài viết này.\nSequence data, sequence models Sequence data Ta có thể hiểu sequence data (dữ liệu dạng chuỗi) là dạng dữ liệu mà các giá trị trong đó được sắp xếp theo một trình tự không gian/thời gian nào đó và chúng có những mối liên hệ với nhau. Ta khó có thể chỉ dựa vào một giá trị cụ thể để biết được dữ liệu có ý nghĩa là gì mà phải sử dụng quan hệ giữa các giá trị với nhau. Ví dụ:\nVăn bản: Đây là dạng sequence data rất phổ biến. Các từ trong câu tất nhiên là được sắp xếp theo một trình tự nhất định để tạo ra được một câu có nghĩa Âm thanh: Một đoạn ghi âm giọng nói, một bản nhạc Video: Các frame (ảnh) của video theo các thời điểm liên tiếp nhau Sinh học: Trình tự của một đoạn gen, dãy protein,… Time series: Các dữ liệu thu thập theo thời gian như thị trường chứng khoán Văn bản\nÂm thanh\nVideo\nTime series\nLưu ý:\nTùy theo dạng dữ liệu mà ta sẽ có cách “số hóa\u0026quot; chúng sao cho các mô hình có thể tiến hành “học” được. Vì sao cần phải làm vậy? Vì máy tính chỉ hiểu được những con số, mà cụ thể hơn là chỉ hiểu 0 và 1 😀\nTrong bài viết này, mình sẽ tạm thời chưa đề cập đến điều này. Đối với dữ liệu dạng văn bản, các bạn có thể xem bài viết tiếp theo về Word Embeddings nhé.\nSequence models Khác với các dữ liệu dạng hình ảnh mà ta thường thấy trong Computer Vision, Natural Language Processing sẽ tập trung vào việc xử lý các dữ liệu dạng chuỗi. Do đó, các mô hình trong NLP thường được gọi là sequence model.\nĐể có sự phân biệt rõ hơn giữa sequence model và các model trong CV, ta thường xét đến input và các thức tính toán của chúng.\nVới model trong CV, input của ta sẽ là một ảnh xám hoặc là RGB (ma trận nhiều chiều). Trong quá trình tính toán, ta có thể thực hiện tính toán song song trên các giá trị đầu vào. Trong khi đó, sequence model sẽ nhận vào input là dữ liệu ở các giai đoạn khác nhau, mỗi giai đoạn thì ta sẽ có một vector hay một ma trận nhiều chiều. Khi tính toán, ta sẽ tính toán tuần tự từng giai đoạn một. Ví dụ, với input là một câu văn bản thì từng giai đoạn sẽ ứng với từng từ, mỗi từ có thể được biểu diễn bởi một số hoặc một vector. Minh họa sequence model Nguồn: Jeddy92 Sequence models được sử dụng cho nhiều bài toán phổ biến trong thực tế như sau:\nNguồn: deeplearning.ai Recurrent Neural Network (RNN) Để cho dễ diễn đạt, ta xét một bài toán trong NLP với input là một câu có độ dài $T_x$, từ ứng với vị trí thứ $i$ được biểu diễn bằng vector $x^{\u0026lt; i \u0026gt;}$ có $D$ chiều, output là một câu có độ dài $T_y$ và những từ tương ứng được biểu diễn là $y^{\u0026lt; i \u0026gt;}$ (tương tự như $x^{\u0026lt; i \u0026gt;}$).\nThông thường thì ta sẽ giả sử luôn $T_x = T_y$ để bài toán đơn giản hơn một chút. Để thực hiện được thì chỉ đơn giản là padding/truncate để chúng bằng nhau thôi 😜 Nếu bạn đang thắc mắc là có bài toán nào dạng như này thì hãy nghĩ đến Name Entity Recognition (phân loại các thành phần trong câu thành các nhóm như tên người, địa điểm, thời gian, số lượng,\u0026hellip;) Hạn chế của mô hình Multi-layers Percentron Một cách tự nhiên, ta hoàn toàn có thể xây dựng một mô hình MLP (Multi-layers Perceptron) với input layer có $T_x \\times D$ neurons, output layer cũng có $T_y \\times D$ neurons và ở giữa là các hidden layer.\nNhằm mục đích minh họa, mình sẽ chỉ biểu diễn mỗi vector $x^{\u0026lt; i \u0026gt;}$ và $y^{\u0026lt; i \u0026gt;}$ là một neuron. Khi đó, kiến trúc của mô hình MLP sẽ có dạng như sau:\nMinh họa sử dụng MLP cho bài toán đặt ra\nNguồn: deeplearning.ai Với MLP thì ta đã thực hiện tính toán song song, tức là tính luôn trên toàn bộ input và cho ra output. Tuy nhiên, cách tiếp này có những hạn chế khá nghiêm trọng:\nKhông phải câu input nào cũng có độ dài $T_x$ như nhau, output cũng vậy. Nếu các câu này có nhiều từ và mỗi từ được biểu diễn bởi vector có số chiều lớn thì mô hình sẽ có rất rất nhiều trọng số. Mô hình không học được sự “chia sẻ đặc trưng” giữa các vị trí khác nhau trong câu. Ví dụ, cụm từ “tôi đi học” xuất hiện trong câu input thì cho dù nó bắt đầu ở vị trí nào đi nữa, ta vẫn nên học được các đặc trưng rất tương tự nhau. Lưu ý. Đây cũng là một trong những vấn đề dẫn đến mô hình CNN được áp dụng nhiều hơn trong lĩnh vực Computer Vision chứ không phải là MLP. Ý tưởng của RNN Vì sequence data có một đặc điểm là thứ tự của các giá trị trong input là rất quan trọng nên ta thường thiên về hướng lần lượt xử lý trên từng vị trí một. Đồng thời, khi đi đến các vị trí sau thì ta cũng nên có thông tin đã trích xuất được từ các vị trí trước. Ý tưởng của RNN chính là như vậy.\nSơ lược về cách hoạt động của RNN được mô tả như sau:\nVới điều kiện giả sử $T_x=T_y$, ta sẽ thực hiện tính toán $T_x$ lần, tại thời điểm (hay là vị trí) thứ $i$ thì từ $x^{\u0026lt; i \u0026gt;}$ (input) ta tính ra $y^{\u0026lt; i \u0026gt;}$ (output). Hơn nữa, để thực hiện thao tác lưu giữ các thông tin cho đến thời điểm hiện tại và đưa nó qua các thời điểm sau, ta cũng cần tính thêm một giá trị là $h^{\u0026lt; i \u0026gt;}$ (giả sử luôn $h^{\u0026lt;0\u0026gt;} = 0$). Lúc này, ta gọi $h^{\u0026lt; i \u0026gt;}$ là trạng thái ẩn (hidden state). Với vai trò của hidden state $h^{\u0026lt; i \u0026gt;}$ thì ta thấy rằng output $y^{\u0026lt; i \u0026gt;}$ chắc chắn là nên được tính dựa trên $h^{\u0026lt; i \u0026gt;}$. 😃 Như vậy, tại mỗi thời điểm,ta cần tính ra $h^{\u0026lt; i \u0026gt;}$ và $y^{\u0026lt; i \u0026gt;}$, dựa vào 2 input là $x^{\u0026lt; i \u0026gt;}$ và thông tin từ những thời điểm trước được tổng hợp tại $h^{\u0026lt;i - 1\u0026gt;}$. Minh họa ban đầu cho RNN\nNguồn: deeplearning.ai Nhìn vào hình trên, ta có thể suy ra rằng những trọng số mà RNN cần học là ma trận trọng số của hidden layer ở các thời điểm.\nĐể thuận tiện cho các phần sau, ta sẽ quy ước kí hiệu như sau:\nVector input, output và hidden state tại thời điểm $t$ lần lượt là $\\bold{X}_t \\in \\mathbb{R}^d$, $\\bold{O}_t \\in \\mathbb{R}^o$ và $\\bold{H}_t \\in \\mathbb{R}^h$. Ma trận trọng số cho phép tính liên quan giữa $\\bold{X}_t$ và $\\bold{H}_t$ là $\\bold{W}^{t}_{ xh } \\in \\mathbb{R}^{ h \\times d }$. Tương tự như trên, ta có $\\bold{W}^{t}_{hh} \\in \\mathbb{R}^{h \\times h}$ và $\\bold{W}^{t}_{ho} \\in \\mathbb{R}^{o \\times h}$. Khi đó, ta có thể biểu diễn RNN như sau:\nNguồn: Dive into DL Thông thường, ta gọi phần xử lý tính toán từ $\\bold{X}_t$, $\\bold{H}_{t-1}$ ra $\\bold{H}_t$ và $\\bold{O}_t$ ở các thời điểm (ô vuông được vẽ ở hình trên) là recurrent cell.\nSự chia sẻ trọng số giữa các thời điểm Qua mô tả về cách hoạt động của RNN ở phần trước, ta thấy rằng nếu ở mỗi thời điểm mà ta cần dùng một bộ trọng số khác nhau thì lượng tham số của mô hình RNN sẽ lớn không kém gì MLP ở phần 3.1 😀\nTrong RNN, giữa các thời điểm sẽ có sự chia sẻ trọng số, tức là mọi thời điểm đều dùng cùng một bộ trọng số $(\\bold{W}_{xh}, \\bold{W}_{hh}, \\bold{W}_{ho})$ để tính toán $\\bold{O}_t$ và $\\bold{H}_t$. Lợi ích của việc chia sẻ trọng số bao gồm:\nSố lượng trọng số trong RNN sẽ giảm đi rất nhiều lần so với MLP Ta cũng có thể khắc phục được nhược điểm của MLP trong việc “chia sẻ đặc trưng” giữa các vị trí khác nhau trong câu. Khi đó, từ hình ở phần 3.2, sau khi chú thích vị trí các ma trận trọng số được sử dụng thì ta có hình sau:\nThay vì phải biểu diễn đủ các thời điểm, ta có thể viết gọn lại RNN như hình bên dưới:\nBiểu diễn gọn hơn của Recurrent Neural Network Quá trình feed-forward Đầu tiên, ta sẽ xem hidden state ban đầu là $\\bold{H}_0 = \\bold{0}$. Để đơn giản, ta sẽ bỏ qua các giá trị bias ứng với $\\bold{H}_t$ và $\\bold{O}_t$. Khi đó, quá trình feed-forward tại thời điểm $t \u0026gt; 0$ diễn ra như sau:\n$$\\begin{equation} \\bold{H}_t = \\phi_h (\\bold{W}_{xh} \\bold{X}_t + \\bold{W}_{hh} \\bold{H}_{t-1}) \\end{equation}$$ $$\\begin{equation} \\bold{O}_t = \\phi_o (\\bold{W}_{ho} \\bold{H}_t) \\end{equation}$$\n, với $\\phi_h$ và $\\phi_o$ là các activation function. Thông thường, $\\phi_h$ là $ReLU$ hoặc $\\tanh$ và $\\phi_o$ thường là $\\text{softmax}$.\nNhư vậy, từ $\\bold{X}_{t}$ và $\\bold{H}_{t-1}$ ta sẽ tính được $\\bold{H}_{t}$, và từ $\\bold{H}_{t}$ thì ta sẽ tính được $\\bold{O}_{t}$.\nNhận xét.\nTa có thể nhận thấy rất rõ sự khác biệt giữa feed-forward trong RNN so với MLP. Với MLP thì nó chỉ cần hai phép toán (xem như phần hidden layers chỉ có 1 layer) là có luôn kết quả cuối cùng:\n$$\\begin{equation*} \\bold{H} = \\phi_h (\\bold{X} \\bold{W}_{xh}^\\top ) \\end{equation*}$$ $$\\begin{equation*} \\bold{O} = \\phi_o (\\bold{H} \\bold{W}^\\top _{ho} ) \\end{equation*}$$\n, với $\\bold{X} \\in \\mathbb{R}^{n \\times d}$ và $\\bold{H} \\in \\mathbb{R}^{n \\times h}$ là các ma trận ứng với toàn bộ giá trị input và hidden state (trong RNN thì $\\bold{X}_t$ và $\\bold{H}_t$ là các vector).\nBack-propagation Through Time (BPTT) Khi mà feed-forward trong RNN diễn ra khác với MLP thì tất nhiên là back-propagation cũng khác 😀 Thuật toán back-propagation trong sequence model được gọi là Back-propagation Through Time (BPTT).\nĐầu tiên, ta sẽ đề cập đến cost function. Giả sử output của RNN tại thời điểm $t$ là $\\bold{O}_t$ và label là $\\bold{Y}_t$. Kí hiệu $l(\\bold{O}_t, \\bold{Y}_t)$ là loss tại thời điểm $t$. Khi đó cost function của ta là\n$$\\begin{equation} L= \\sum_{i=1}^{T} l(\\bold{O}_t, \\bold{Y}_t) \\end{equation}$$\nThực ra là ta có chia $L$ cho $T$ nữa nhưng để cho gọn thì thôi bỏ qua 😜 Những gì ta cần thực hiện trong quá trình BPTT là tính đạo hàm của $L$ theo các ma trận trọng số $\\bold{W}_{xh}$, $\\bold{W}_{hh}$ và $\\bold{W}_{ho}$. Điều đặc biệt ở đây là trong quá trình feed-forward thì $\\bold{H}_{t-1}$ sẽ lại được dùng để tính $\\bold{H}_t$ chứ nó không đi một “mạch\u0026quot; từ $\\bold{X}$ đến $\\bold{H}$ rồi từ $\\bold{H}$ đến $\\bold{O}$ như trong feed-forward của MLP thông thường.\nĐể công thức được gọn nhẹ hơn, ta sẽ giả sử luôn các activation function $\\phi_h$ và $\\phi_o$ là hàm đồng nhất, tức là\n$$\\phi_h(\\bold{x}) = \\phi_o(\\bold{x}) = \\bold{x}$$\nQuá trình BPPT diễn ra như sau:\nĐầu tiên, dễ nhất là tính đạo hàm $L$ theo $\\bold{W}_{ho}$ 😀 Từ biểu thức $(3)$ thì ta có ngay\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{O}_{t}} = \\frac{\\partial l(\\bold{O}_t, \\bold{Y}_t)}{\\partial \\bold{O}_{t}} \\end{equation}$$\nDo đó, kết hợp $(2)$ và $(4)$ thì\n$$ \\frac{\\partial L}{\\partial \\bold{W}_{ho}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{O}_{t}} \\frac{\\partial \\bold{O}_t}{\\partial \\bold{W}_{ho}} \\right ) = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{O}_{t}}\\bold{H}_t^\\top \\right ) $$\nLưu ý. Cách tính giá trị của $\\dfrac{\\partial l(\\bold{O}_t, \\bold{Y}_t)}{\\partial \\bold{O}_{t}}$ trong biểu thức $(4)$ sẽ phụ thuộc vào hàm $l$ và thường thì nó rất dễ tính 😜\nTiếp theo, ta có nhận xét sau:\n$\\bold{H}_T$ chỉ tham gia vào một biểu thức trong quá trình feed-forward (để tính ra $\\bold{O}_T$) $\\bold{H}_{t}$ với $t \u0026lt; T$ thì tham gia vào hai biểu thức (tính $\\bold{H}_{t +1}$ và $\\bold{O}_t$) Do đó, cách tính $\\dfrac{\\partial L}{\\partial \\bold{H}_{t}}$ sẽ có sự khác biệt tùy theo giá trị $t$.\nVới $t = T$: Từ $(2)$ và $(4)$ ta có\n$$ \\frac{\\partial L}{\\partial \\bold{H}_{T}} = \\frac{\\partial L}{\\partial \\bold{O}_{T}} \\frac{\\partial \\bold{O}_T}{\\partial \\bold{H}_{T}} = \\bold{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\bold{O}_{T}} $$\nVới $t \u0026lt; T$: Từ $(1), (2)$ và $(4)$ thì\n$$ \\frac{\\partial L}{\\partial \\bold{H}_{t}} = \\frac{\\partial L}{\\partial \\bold{O}_{t}} \\frac{\\partial \\bold{O}_t}{\\partial \\bold{H}_{t}} + \\frac{\\partial L}{\\partial \\bold{H}_{t+1}} \\frac{\\partial \\bold{H}_{t+1}}{\\partial \\bold{H}_{t}} = \\bold{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\bold{O}_{t}} + \\bold{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\bold{H}_{t+1}} $$\nCứ tiếp tục biến đổi tiếp với $\\dfrac{\\partial L}{\\partial \\bold{H}_{t+1}}$ và cứ như thế cho đến $T$, ta sẽ có\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\mathbf{H}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_{hh}^\\top\\right)}^{T-i} \\mathbf{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\mathbf{O}_{T+t-i}} \\end{equation}$$\nĐể ý rằng biểu thức $(5)$ cũng đúng với $t = T$.\nVậy từ $(1)$ và $(5)$ thì\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{W}_{xh}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\frac{\\partial \\bold{H}_t}{\\partial \\bold{W}_{xh}} \\right ) = = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\bold{X}_t^\\top \\right ) \\end{equation}$$\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{W}_{hh}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\frac{\\partial \\bold{H}_t}{\\partial \\bold{W}_{hh}} \\right ) = = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\bold{H}_{t-1}^\\top \\right ) \\end{equation}$$\nVấn đề vanishing và exploding gradients trong RNN Qua các biểu thức $(6)$ và $(7)$, ta thấy rằng nếu giá trị $T$ lớn (tức là câu input gồm rất nhiều từ) thì sẽ có hai trường hợp xảy ra đối với các giá trị gradient $\\dfrac{\\partial L}{\\partial \\bold{W}_{xh}}$ và $\\dfrac{\\partial L}{\\partial \\bold{W}_{ho}}$:\nVanishing: Trong quá trình tính $\\left ( \\bold{W}_{hh}^\\top \\right ) ^ {T-i}$ có nhiều giá trị nhỏ hơn 1 được nhân với nhau. Exploding: Ngược lại (lớn hơn 1). Ta có một giải pháp để hạn chế hiện tượng này là Truncated BPTT, tức là ta chỉ lan truyền gradients đến một trạng thái cách trạng thái hiện tại một khoảng nào đó thôi chứ không lan truyền toàn bộ. Một phương pháp cũng khá phổ biến trong việc hạn chế exploding gradient là gradient clipping cũng co thể được áp dụng.\nĐối với hiện tượng vanishing gradient, ta có thể diễn đạt nó một cách văn vở hơn là mô hình đã bị “quên” những thông tin tích lũy từ phía trước.\nVí dụ, ta có hai câu sau:\nThe cat, which always … (very long descriptions) …, was very cute The cats, which always … (very long descriptions) …, were very cute Thông thường, động từ to-be ở trước “very cute” sẽ phụ thuộc vào danh từ ở đầu câu (”cat” hay “cats”). Tuy nhiên, nếu RNN bị vanishing gradient thì nó sẽ không thể nhớ được trước đó là một con mèo hay nhiều con mèo để mà chọn động từ to-be cho đúng. 😀\nYếu tố này đã mở ra một hướng phát triển cho RNN là ta sẽ cố gắng tính toán thêm những phép toán khác để duy trì được các thông tin từ trước, từ phía xa mà RNN hiện tại không thể ghi nhớ được. Từ đó, ta có sự ra đời của Long Short-Term Memory (LSTM) và Gated Recurrent Unit (GRU).\nQuá trình huấn luyện và sử dụng mô hình RNN trong thực tế Đối với các mô hình ML hay CNN thông thường, những thao tác diễn ra trong bước huấn luyện (training) và sử dụng trong thực tế (testing) là rất giống nhau: Từ một intput cho ra một output và chỉ như vậy là xong (ta tạm bỏ qua back-propagation).\nNguồn: Towards Data Science Tuy nhiên, trong RNN thì training và testing sẽ có sự khác biệt khá rõ rệt. Đối với training, ở mỗi thời điểm thì ta sẽ luôn có input và label ứng với thời điểm đó. Tuy nhiên, trong testing thì ta chỉ có duy nhất input cho thời điểm đầu tiên và input của những thời điểm sau chính là output của thời điểm trước đó.\nTa xét ví dụ với bài toán xây dựng mô hình RNN sinh ra đoạn văn bản như sau:\nNguồn: ML Lectures Trong training, từ một câu có $T_x$ từ là $x_1, x_2,\u0026hellip;, x_{T_x}$ thì ta sẽ tạo ra được một training sample với $(T_x + 1)$ thời điểm, ở thời điểm $t$ thì input và label lần lượt là $x_t$ và $x_{t+1}$ (giả sử $x_0$ và $x_{T_x + 1}$ lần lượt là các từ đặc biệt nhằm báo hiệu bắt đầu và kết thúc đoạn).\nTrong testing, từ một từ ban đầu là $x_1$ (thường là từ bắt đầu đoạn văn bản), ta sẽ có testing sample với 1 thời điểm. Sau khi qua RNN, ta có thêm từ mới là $y_1$. Sau đó, $y_1$ được sử dụng như là input của thời điểm thứ hai, đưa qua RNN và có tiếp từ $y_2$. Quá trình cứ lặp lại cho đến khi đã sinh ra đủ số từ chúng ta cần hoặc là từ được sinh ra chính là từ kết thúc đoạn.\nCác dạng mô hình RNN Chúng ta để ý rằng dạng mô hình RNN mình đã trình bày ở phần trước đang nhận input là một câu và output của nó cũng là một câu. Dạng mô hình này còn gọi là many-to-many. Tùy vào bài toán cần giải quyết mà ta có các dạng như sau:\nCác dạng của mô hình RNN\nNguồn: Javatpoint One-to-one: Cái này thì rất thường thấy, ví dụ như Image Classification. One-to-many: Có thể lấy ví dụ như bài toán sinh ra văn bản hoặc âm nhạc. Ta cung cấp input là một từ bất kì và mô hình sẽ tạo ra từ kế tiếp, ta lại đem từ này vào làm input để có từ tiếp theo. Many-to-one: Các bài toán như Sentiment Analysis, Mail fitlering,… Trong dạng mô hình này, chỉ có thời điểm cuối cùng là có tính ra output, các thời điểm trước thì chỉ có tính hidden state. Many-to-many: Trong hình trên, ta thấy rằng có hai loại \u0026ldquo;many-to-many\u0026rdquo;: Đầu tiên là hình thứ 4, khi mà từ thời điểm cuối cùng của input thì ta bắt đầu sinh ra các từ của output, theo nguyên tắc giống như one-to-many. Đây thực ra là dạng của mô hình đầu tiên đạt được độ chính xác khá ấn tượng trong bài toán Machine Translation, mô hình này có tên là Sequence to Sequence (seq2seq). Tiếp theo là hình thứ 5, với kiến trúc giống như mô hình RNN mà mình đã lấy ví dụ ở các phần trên. Bài toán điển hình cho dạng này là Name Entity Recognition. Tài liệu tham khảo Robin M. Schmidtm, Recurrent Neural Networks (RNNs): A gentle Introduction and Overview Dive into DL, Recurrent Neural Network DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-02-15T21:42:44+07:00","permalink":"https://htrvu.github.io/post/rnn/","title":"Recurrent Neural Network (RNN)"},{"content":"Giới thiệu Ta biết rằng, hầu hết các mô hình CNN thường được xây dựng từ một phiên bản ban đầu (có thể là dựa theo một nguồn tài nguyên nào đó), sau đó chúng được scale dần lên để đạt được độ chính xác tốt hơn, và tất nhiên là độ phức tạp cũng tăng theo\nVí dụ: Với ResNet thì ta có ResNet18 cho đến ResNet152, DenseNet thì DenseNet121 cho đến 201, MobileNet thì ta có siêu tham số width multiplier để điều chỉnh số channel trong từng layer và resolutiom multiplier để điều chỉnh kích thước tại các layer,… Những cách làm đó gọi là model scaling. Tuy nhiên, ta nhận thấy rằng những thao tác model scaling trước đó chỉ tập trung vào một trong 3 yếu tố: depth - $d$ (số layer), width - $w$ (số channel) và resolution - $r$. Hơn nữa, việc điều chỉnh cũng không theo một nguyên tắc nào mà còn mang đậm tính chất ngẫu nhiên, “hên xui”, cần phải thử nghiệm rất nhiều lần mới có thể đạt được một độ chính xác mong muốn. Khi đó thì số lượng tham số của các mô hình cũng tăng chóng mặt!\nCác tác giả của paper đã cho thấy kết quả thực nghiệm rằng việc điều chỉnh một trong 3 yếu tố có thể tăng độ chính xác nhưng chỉ tăng đến một mức nào đó thôi, sau đó nó sẽ bị bão hòa. Ví dụ như ở hình bên dưới:\nTa có thể đưa ra các nhận xét như sau:\nNếu mô hình có width lớn (mỗi layer có nhiều channel) thì nó có thể học được nhiều loại đặc trưng khác nhau. Nhưng nếu mô hình không đủ sâu thì các đặc trưng đó cũng chưa phải đặc trưng ở mức high-level (nổi bật cho đối tượng) Nếu mô hình có depth lớn thì nó có thể học được các đặc trưng high-level nhưng nếu không có width lớn thì cũng không học được nhiều loại đặc trưng* Về mặt trực giác, nếu ta đưa vào mô hình một bức ảnh có resolution cao thì mô hình nên có depth lớn để có thể dần học các đặc trưng từ các feature maps có resolution lớn, đồng thời cũng vì sẽ có nhiều đặc trưng hơn nên ta cần width lớn. Do đó, model scaling nên tập trung vào việc điều chỉnh đồng thời cả 3 yếu tố $d$, $w$, $r$. Paper công bố EfficientNet có tên là “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”. Các tác giả tập trung vào việc đi tìm một phương pháp model scaling hiệu quả, có nguyên tắc, điều chỉnh đồng thời cả 3 yếu tố như đã đề cập. Phương pháp được giới thiệu có tên là compound scaling.\n\u0026ldquo;Nguyên tắc\u0026rdquo; trong phương pháp này rất đơn giản, ta sẽ cùng điều chỉnh $d$, $w$, $r$ của toàn bộ network theo cùng một hệ số gọi là compound coefficient (kí hiệu là $\\phi$).\nMinh họa cho các phương pháp model scaling. (a) là mô hình ban đầu. (b)-(d) thực hiện điều chỉnh một trong ba yếu tố. (e) là phương pháp được đề xuất, nó tiến hành điều chỉnh cả ba. Tất nhiên là model scaling chỉ phát huy tác dụng khi mà mô hình ban đầu là đủ tốt. Từ phương pháp compound scaling, các tác giả đã áp dụng nó cho ResNet, MobileNet để chứng tỏ độ hiệu quả của phương pháp. Sau đó, một họ mô hình mới được đề xuất là EfficientNet, vớ 8 phiên bản từ B0 đến B7 với độ phức tạp và độ chính xác tăng dần trên tập ImageNet. EfficientNet-B7 đã trở thành SOTA (state-of-the-art) với độ phức tạp nhỏ hơn rất nhiều lần so với mô hình SOTA trước đó.\nBài toán model scaling Giả sử conv layer thứ $i$ được định nghĩa là hàm số $Y_i = F_i(X_i)$, với input $X_i$ có shape là $\\left (H_i, W_i, C_i \\right)$. Khi đó, một CNN $N$ có thể được biểu diễn là\n$$ N = F_k \\bigodot F_{k-1} \\bigodot \\cdots F_1(X_1) = \\bigodot_{j=1,\u0026hellip;,k} F_j(X_1) $$\nThông thường, các mạng CNN thường được xây dựng theo kiểu gồm nhiều giai đoạn, mỗi giai đoạn là sự lặp lại các block có cùng dạng cấu trúc, chỉ khác nhau một số chi tiết như số layer trong block, kích thước của filter,… Ví dụ, ResNet được xây dựng dựa trên các residual block, MobileNet thì là các depthwise separable block,… Do đó, ta có thể viết lại $N$ thành\n$$ N = \\bigodot_{i=1,\u0026hellip;,s} F_i ^ {L_i}(X_{(H_i, W_i, C_i}) $$\n, với $F_i$ là layer được lặp lại $L_i$ lần trong giai đoạn thứ $i$, với input là $X$ có shape $(H_i, W_i, C_i)$.\nBài toán model scaling sẽ cố định layer $F_i$ và đi điều chỉnh các giá trị $L_i, H_i, W_i, C_i$, sao cho mô hình thỏa mãn các ràng buộc về tài nguyên và đạt độ chính xác cao nhất có thể.\nĐiều chỉnh $L_i$ $\\Leftrightarrow$ Điều chỉnh depth Điều chỉnh $C_i$ $\\Leftrightarrow$ Điều chỉnh width Đều chỉnh $H_i, W_i$ $\\Leftrightarrow$ Điều chỉnh resolution Để giảm không gian tìm kiếm, ta sẽ điều chỉnh các giá trị trên của toàn bộ layer trong mô hình theo cùng một tỉ lệ. Khi đó, bài toán của ta là bài toán tối ưu như sau:\n, với $d, w, r$ là hệ số để điều chỉnh depth, width, resolution; $\\hat{F_i}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i}$ là các giá trị ban đầu của mô hình baseline.\nPhương pháp compound scaling Phương pháp này sử dụng compound coefficient $\\phi$ để điều chỉnh depth, width, resolution theo nguyên tắc như sau:\nVới mô hình baseline ban đầu, ta thực hiện grid search để tìm ra bộ 3 giá trị tỉ lệ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tốt nhất có thể, sao cho\n$$ \\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\text{ và } \\alpha \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 $$\nSau đó, ta sẽ scale mô hình lên theo hệ số $\\phi$ với\n$$ d = \\alpha ^ \\phi, ; w = \\beta^\\phi, ; r = \\gamma^\\phi $$\nVề mặt trực giác, ta có thể xem $\\phi$ như là cách mà chúng ta cho biết lượng tài nguyên dành cho model scaling là bao nhiêu, còn các giá trị $\\alpha, \\beta, \\gamma$ là cách chúng ta phân phối tài nguyên đó cho depth, width và resolution. Giải thích cho các ràng buộc cho $\\alpha, \\beta, \\gamma$ được trình bày như sau:\nTất nhiên là để thực hiện được việc scale mô hình lên thì giá trị của chúng phải không nhỏ hơn 1 Ngoài ra, một phép toán convolution sẽ có độ phức tạp tỉ lệ thuận với $d, w^2, r^2$. Do đó, nếu ta scale model lên theo hệ số $\\phi$ thì độ phức tạp sẽ tăng lên một lượng bằng $(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2)^\\phi$. Các tác giả mong muốn độ phức tạp tăng khoảng $2^\\phi$, do đó ta có ràng buộc $\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2$. Họ mô hình EfficientNet Neural architecture search EfficientNet là một họ các mô hình rất đặc biệt:\nThứ nhất, chúng được xây dựng bằng “máy” 😀 Vào năm 2017, một ma thuật đã được công bố trong paper “Neural architecture search with reinforcement learning” của chính tác giả Quoc V. Le, nó giúp chúng ta xây một kiến trúc phù hợp nhất có thể dựa theo độ chính xác, độ phức tạp mà chúng ta yêu cầu.\nVới họ EfficientNet, các tác giả tập trung vào việc giới hạn độ phức tạp (cụ thể là FLOPS). Mục tiêu tối ưu của reinforcement learning là\n$$ ACC(m) \\times \\left ( \\frac{FLOPS(m)}{T} \\right )^w $$\n, với $m$ là mô hình, $ACC$ và $FLOPS$ là độ chính xác và độ phức tạp, $T$ là FLOPS mong muốn và nó bằng $400 \\times 10^6$, $w=-0.07$ là hằng số điều chỉnh trade-off giữa $ACC$ và $FLOPS$\nThứ hai, từ một mô hình ban đầu là EfficientNet-B0, ta tiến hành scale theo 2 bước:\nBước 1: Cố định $\\phi = 1$, giả sử lượng tài nguyên mà ta có thể sử dụng là nhiều gấp đôi hiện tại. Khi đó, thực hiện grid search để tìm các giá trị $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tốt nhất Bước 2: Từ các giá trị $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tìm được, tiến hành scale theo các giá trị $\\phi$ lớn hơn để có được các phiên bản B1-B7. Ở đây, ta hoàn toàn có thể tăng $\\phi$ lên rồi lại grid search nhưng lúc này chi phí thực hiện là rất lớn. Do đó, các tác giả chỉ grid search một lần rồi sau đó chỉ cần tăng $\\phi$.\nKiến trúc mô hình Đầu tiên, tổng quan kiến trúc của EfficientNet-B0 như sau:\nTrong đó: MBConv chính là inverted residual block trong MobileNetV2, cùng với một số cải tiến như trong paper “Squeeze-and-excitation networks” Các mô hình EfficientNet-B1 cho đến B7 chính là kết quả của việc áp dụng compound scaling lên EfficientNet-B0.\nTài liệu tham khảo Paper EfficientNet: https://arxiv.org/abs/1905.11946 ","date":"2023-02-15T11:17:46+07:00","permalink":"https://htrvu.github.io/post/efficientnet/","title":"EfficientNet (2020)"},{"content":"Giới thiệu Từ sự thành công của MobileNet (2017) trong việc triển khai các mô hình Deep Learning trên các thiết bị biên (smartphone, embedded,…) nhờ vào việc sử dụng hiệu quả phép toán depthwise separable convolution, nhiều nghiên cứu dựa trên hướng phát triển này đã được tiến hành.\nDựa theo các “kinh nghiệm” có được của bản thân, nhìn vào MobileNet thì ta sẽ thấy ngay rằng, nó chưa có cái skip connection nào cả 😀 Đúng z, skip connection đã cho thấy được sự hiệu quả của mình trong các mô hình như ResNet, Inception-ResNet, DenseNet,… tại sao ta không thử thêm vào MobileNet? Boom, thêm ngay!\nMobileNetV2 được công bố với sự kế thừa từ MobileNet và bổ sung thêm skip connection. Tất nhiên là không chỉ dừng ở đó 🙂 Các tác giả xây dựng MobileNetV2 dựa trên các inverted residual block, nơi mà các skip connection dùng để kết nối các bottleneck layer với nhau! Hơn nữa, ta còn có một điểm rất thú vị là các bottleneck layer này sử dụng activation function là linear!\nMobileNetV2 đạt được độ chính xác cao hơn MobileNet trên tập ImageNet, với số tham số ít hơn, lượng bộ nhớ cần dùng tại mỗi layer là ít hơn. Từ sự ra đời của mô hình này, người ta cũng đã phát triển các mô hình hiệu quả trong bài toán Object Detection như SSDLite, hay Semantic Segmentation như Mobile DeepLabv3\nBàn về ReLU Ta biết rằng từ khi paper AlexNet giới thiệu activation function ReLU thì nó đã trở thành một activation function rất phổ biến và được dùng thường xuyên trong các mô hình Deep Learning với điểm mạnh quan trọng là đạo hàm của nó rất đơn giản. Công thức của ReLU là\n$$ReLU(x) = \\max(x, 0)$$\n, tức là nó sẽ “vứt” những giá trị bé hơn 0 trong input. Điều này nghĩa là ta sẽ bị mất thông tin!. Nếu input truyền vào là một channel thì ta sẽ bị mất một lượng thông tin nhỏ (hoặc có thể là lớn) trên channel đó.\nActivation function ReLU Nguồn: Research Gate Vậy tại sao trước giờ ReLU vẫn luôn được sử dụng?\nĐiều quan trọng là chúng ta có rất nhiều channel và giữa các channel này có những mối liên hệ nhất định. Do đó, việc mất thông tin ở một channel này có thể được channel khác bù đắp. Như vậy là ok. Để minh họa cho yếu tố làm mất thông tin, các tác giả đưa ra ví dụ sau:\nBan đầu, input của ta ở không gian 2 chiều. Qua phép biến đổi bằng một ma trận $T$ bất kì và áp dụng ReLU, ta có các output ở các không gian có số chiều khác nhau là 2, 3, 5, 15, 30. Để xác định xem thông tin có bị mất hay không, ta chiếu các output này về lại không gian 2 chiều bằng cách dùng ma trận nghịch đảo $T^{-1}$. Khi đó, kết quả thu được là các hình tương ứng ở trên. Rõ ràng là tính chất ban đầu của input đã bị mất. Gỉa sử từ một input $D_F \\times D_F \\times M$, qua một số layer thì ta có output $D_F \\times D_F \\times N$ và ta chuẩn bị áp dụng ReLU cho output. Các tác giả chứng minh được rằng ReLU sẽ không làm mất thông tin ban đầu của input nếu như $N \u0026lt; M$. Điều này có thể phát biểu bằng lời là nếu một input có thể được embedded (hay là nén) vào một không gian ít chiều hơn (số channel ít hơn) thì việc áp dụng ReLU lên kết quả nén đó sẽ không làm mất thông tin.\nỞ ví dụ phía trên thì ta đã có $N \\geq M$ và thông tin thật sự là đã bị mất. Từ nhận xét trên, ta thấy rằng không phải lúc nào xài ReLU cũng tốt. Nếu ngẫm lại, trong các kiến trúc như VGG, ResNet, Inception, MobileNet thì số channel của chúng hầu như luôn tăng qua từng block (chính là cụm “một số layer”) nhưng activation function được sử dụng luôn là ReLU. Điều này là vì chúng có rất nhiều channel (tăng theo bội 2) nên mọi thứ vẫn ổn 😀\nNếu bạn thắc mắc là vì sao số channel thường tăng như vậy thì trong CNN, những conv layer đầu thường sẽ học những đặc trưng đơn giản như cạnh ngang, dọc, chéo, vị trí của đối tượng trong ảnh,… càng về sau thì sẽ có các đặc trưng cụ thể, nổi bật lên của đối tượng (ví dụ như tai mèo, mắt mèo, mũi mèo,…). Do đó, càng về sau thì ta nên có càng nhiều channel đễ học được nhiều đặc trưng. Linear bottleneck Đầu tiên, ta sẽ nhắc lại về bottleneck layer. Đây là dạng layer thường được dùng với mục đích là “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán).\nNhững phát hiện về ReLU như đã đề cập là nguồn gốc của các linear bottleneck được sử dụng trong MobileNetV2.\nTại sao lại là linear mà không tiếp tục dùng ReLU rồi tăng số channel như những block trước?\nViệc giảm số channel trong các layer sẽ giảm lượng tham số của mô hình, từ đó giảm được độ phức tạp tính toán. Do đó, nếu xây dựng được một kiến trúc mà số lượng channel trong mỗi layer là nhỏ thì nó sẽ rất phù hợp cho các thiết bị biên. Các tác giả hướng đến việc giữ cho số channel của input và output của các block là nhỏ, tức là số channel chưa chắc đủ nhiều để đảm bảo rằng ReLU không làm mất thông tin 😜.\nNếu mà toàn các output có channel nhỏ như vậy thì làm sao mà mô hình đạt hiệu quả được? Lí do là vì nó là output của các bottleneck nên vẫn ok 😀 Số channel trong các layer của MobileNetV2\nNguồn: Machine Think Inverted residual block và expansion factor Inverted residual block là thành phần chính xây dựng nên MobileNetV2. Trong block này, ta sẽ áp dụng cả dethwise separable convolution, linear bottleneck và skip connection.\nTuy nhiên, lưu ý rằng số channel của input và output của các block này là rất nhỏ. Qua hình ở trên thì ta thấy chúng chỉ quanh quẩn 16, 24, 32. Do đó, trước khi áp dụng depthwise separable convolution lên input thì các tác giả thực hiện giải nén (expansion) lượng kiến thức trong input (input này là output của một block trước đó, nơi mà kiến thức đã được nén lại bởi linear bottleneck).\nVề mặt trực giác, lí do của thao tác này có thể hiểu là ta sẽ thực hiện convolution trên thông tin đầy đủ hơn để có thể phát hiện được càng nhiều đặc trưng càng tốt).\nViệc giải nén thực chất là ta sử dụng một conv layer $1 \\times 1$, với số lượng filter sẽ bằng với số lượng channel của input nhân với một siêu tham số. Siêu tham số này gọi là expansion factor và được kí hiệu là $t$.\nTiếp đến, theo sau phép toán depthwise separable convolution thì ta sẽ sử dụng linear bottleneck để tính ra output của block.\nNhư vậy, điểm qua các layer sẽ có trong inverted residual block sẽ bao gồm:\nLưu ý. Layer “linear 1x1 conv2d” chính là linear bottleneck. Tùy theo giá trị số channel $k$ và $k\u0026rsquo;$ có bằng nhau hay không mà ta sẽ áp dụng thêm skip connection. Hơn nữa, các skip connection trong inverted residual block được dùng để nối các bottleneck layer với nhau!\nVÌ sao lại là nối bottleneck chứ không phải nối các layer khác? Output của bottleneck là các “kiến thức” đã được cô đọng, đây là những gì mà mô hình đã học được và được biểu diễn trong một không gian ít chiều hơn. Do đó, ta vừa tiết kiệm được tài nguyên và vừa liên kết được các kiến thức quan trọng với nhau. Trong cài đặt, tùy theo giá trị stride của depthwise conv layer mà ta sẽ áp dụng skip connection hoặc là không. Cụ thể như sau:\nInverted Residual Block với stride=1 (có skip connection) và stride=2 (không có) Ngoài ra, ta có thể thấy trong inverted residual block thì activation được dùng cho 2 layer đầu tiên là ReLU6. Đây là một biến thể của ReLU, nó giới hạn giá trị output nằm trong đoạn $[0, 6]$ nhằm đảm bảo sự ổn định trong tính toán với số chậm động.\nActivation function RELU6\nNguồn: Mmuratarat Để dễ hình dung hơn về inverted residual block, ta cùng xem một ví dụ cho quá trình tính toán với expansion factor là 6:\nNguồn: Machine Think Lưu ý.\nCác tác giả có đề cập thêm đến luồng truyền thông tin của MobileNetV2, yếu tố mở ra những hướng phát triển tiếp theo trong tương lai. Ta thấy rằng inverted residual block đã tạo ra được sự độc lập giữa số channel của intput/output của block và của các layer nằm bên trong block:\nPhần bên trong được gọi là layer transformation với những phép biến đổi phi tuyến. Ta hoàn toàn có thể nghiên cứu thêm những cách xây dựng bộ phận này để tăng độ hiệu quả của mô hình. Nếu expansion factor của ta \u0026lt; 1 thì block này sẽ rất giống với block trong ResNet:\nKiến trúc MobileNetV2 MobileNetV2 được xây dựng dựa trên việc sử dụng nhiều inverted residual block. Kiến trúc tổng quan của nó như sau:\nTrong đó:\n$t$ là expansion factor. $c$ là số output channel của phần bottleneck trong inverted residual block. $n$ là số lần sử dụng block. $s$ là stride của block đầu tiên trong dãy $t$ block liên tiếp nhau, các block còn lại trong dãy có stride 1. Toàn bộ filter được sử dụng đều là $3 \\times 3$. Cài đặt Các bạn có thể tham khảo phần cài đặt MobileNetV2 bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper MobileNetV2: https://arxiv.org/abs/1801.04381 MachineThink, MobileNet version 2 ","date":"2023-02-13T11:32:55+07:00","permalink":"https://htrvu.github.io/post/mobilenet_v2/","title":"MobileNet V2 (2019)"},{"content":"Skip connection và concatenate Trước đó, kiến trúc ResNet được công bố và nó đã cho thấy được sức mạnh của các skip connection khi chúng được thêm vào các mô hình từ sâu cho đến rất sâu (ví dụ như ResNet152). Ta thấy rằng những kiến trúc áp dụng skip connection trước đây đều có một điểm chung là trong một block thì ta sẽ có những điểm nối 1 feature map vào làm input của một layer sau đó, và chúng đều sử dụng phép toán cộng.\nResidual block trong ResNet sử dụng skip connection với phép toán cộng Nguồn: Idiot Developer Công thức về skip connection trong block trên có thể được viết như sau:\n$$ x_l = H_l(x_{l - 1}) + x_{l-1} $$\n, với $H_l$ là phép biến đổi phi tuyến ở layer thứ $l$, $x_l$ là output của layer thứ $l$.\nPaper DenseNet giới thiệu một kiến trúc với ý tưởng là feature-map tại layer $l$ sẽ sử dụng toàn bộ feature-maps ở phía trước (layer $l - 1, l - 2,\u0026hellip;$) để làm input, và chúng sử dụng concatenate (thay vì phép toán cộng như ResNet). Với tư tưởng như vậy, các feature-maps ta có được có thể xem là một trạng thái có phạm vi toàn cục và bất kì layer nào cũng có thể sử dụng trạng thái này trong việc tính toán ra feature-maps của nó. Nếu viết theo kiểu công thức thì ta sẽ có\n$$ x_l = H_l([x_0, x_1,\u0026hellip;, x_{l-1}]) $$\nLưu ý. Để thực hiện phép toán concatenate thì các feature-maps phải có cùng size, hay là width và height.\nMột ví dụ cho kiến trúc DenseNet như sau:\nTrong hình trên, với $L$ layer, ta có $\\dfrac{L(L+1)}{2}$ kết nối trực tiếp giữa các layer. Các kết nối được tạo ra là rất dày đặc (dense). Từ đó, tên của kiến trúc được đặt là Dense Convolutional Network (DenseNet). Bàn về cách tổ chức các liên kết như vậy một chút:\nNhóm tác giả cho rằng kiến trúc như DenseNet sẽ đảm bảo lượng thông tin cũng như gradient truyền qua các layer là nhiều nhất có thể , từ đó mô hình sẽ có thể học được từ nhiều thông tin hơn, và tất nhiên là nó sẽ tạo ra hiệu ứng làm dịu bớt hiện tượng vanishing gradient.\nĐồng thời, việc sử phép toán concatenate có mang đến cho ta trực giác là có sự phân biệt rõ hơn giữa input trực tiếp từ layer ở ngay phía trước nó với các thông tin được “lưu trữ” và truyền đến từ các layer ở phía trước nữa. Nếu sử dụng phép toán cộng, những yếu tố này đã bị pha lẫn vào nhau.\nCó một chi tiết mà ta thường nghĩ đến ở các mô hình có kiến trúc rất sâu (nhiều layer) là số lượng tham số của nó sẽ rất lớn. Tuy nhiên, với DenseNet thì điều này không phải là vấn đề. Số feature-maps của các layer trong DenseNet sẽ rất nhỏ (chỉ tầm không quá 60), với lý do là để tính toán cho layer kế tiếp thì ta đã dùng toàn bộ feature-maps ở các phía trước rồi chứ không phải chỉ mỗi layer liền trước nó như hầu hết các mô hình khác, nên tại mỗi layer ta chỉ cần tầm đó là đủ rồi 😀\nKết quả so sánh giữa DenseNet và ResNet trên dataste ImageNet được các tác giả công bố như hình bên dưới. Ta thấy rằng DenseNet có số lượng tham số và số phép toán ít hơn ResNet, cùng với độ hiệu quả cao hơn.\nDense block, transition layer và growth rate Dense block và transition layer Ta thấy rằng nếu áp dụng ý tưởng kết nối dày đặt của DenseNet cho toàn bộ layer trong mô hình thì toàn bộ feature-maps trong tất cả layer này đều phải có cùng size (do phép toán được sử dụng là concatnerate).\nTuy nhiên, nếu toàn bộ các layer trong kiến trúc đều có cùng size như vậy thì ta khó mà down-sampling feature-maps về các size nhỏ hơn và rồi sau đó sử dụng các layer như Average Pooling, Dense để cho ra output như các kiến trúc khác được. Và việc “cô đọng” kiến thức của mô hình cũng sẽ gặp khó khăn.\nDo đó, ý tưởng kết nối dày đặt được các tác giả áp dụng trong từng khối (gọi là Dense block), việc down-sampling sẽ được thực hiện trong các khớp nối các Dense block với nhau (gọi là Transition layer).\nCó tổng cộng 4 dạng Dense block như sau:\nDense block cơ bản:\nHàm $H_l$ trong block này là sự kết hợp theo thứ tự 3 phép toán: $$ BN \\to ReLU \\to Conv (3 \\times 3) $$\nNgoài ra, ta có thể thêm dropout vào sau Conv để giảm overfitting.\nDense-B block (bottleneck):\nĐể tăng hiệu suất về mặt tính toán, ta có thể thêm một phép toán Conv $1 \\times 1$ vào $H_l$ để giảm bớt số lượng feature-maps input. Lúc này, thứ tự các phép toán sẽ là $$ BN \\to ReLU \\to Conv (1 \\times 1) \\to BN \\to ReLU \\to Conv (3 \\times 3) $$\nDense-C block (compression):\nTa sẽ giảm số lượng output feature-maps của các Dense block theo tham số $0 \u0026lt; \\theta \\leq 1$: từ $m$ feature-maps thành $\\lfloor \\theta m \\rfloor$ Thông thường, phần cài đặt của thao tác compression được ghép vào transition layer. Dense-BC block:\nKết hợp bottleneck và compression vào Dense block. Trong kiến trúc tổng thể, nếu trước đó ta dùng Dense-C hoặc Dense-BC block thì theo sau nó sẽ có thêm layer bottleneck (Conv $1 \\times 1) và thành phần này gọi là transition layer. Bên cạnh conv layer, thành phần không thể thiếu trong transition layer là một lớp Pooling (các tác giả sử dụng Average Pooling) để thực hiện down-sampling các feature-maps. Thứ tự các phép toán trong transition layer sẽ là\n$$ BN \\to ReLU \\to Conv (1 \\times 1) \\to AvgPool (2 \\times 2) $$\nGrowth rate Như đã đề cập ở phần ý tưởng, lượng tham số trong DenseNet được tối thiểu hóa là nhờ vào chi tiết số lượng feature-maps tại các layer trong DenseNet là nhỏ. Các tác giả xem số lượng feature-maps $k$ tại các layer là một siêu tham số của DenseNet, và nó được gọi là growth rate.\nThực nghiệm cho thấy rằng các giá trị $k$ mang lại kết quả tốt trên các dataset thường không quá lớn. Về mặt trực giác, ta có thể hiểu $k$ điều chỉnh lượng thông tin mới mà một layer có thể đóng góp vào trạng thái toàn cục (đóng góp một lượng vừa đủ thì sẽ tốt hơn là quá nhiều hay quá ít).\nMinh họa Dense Block với growth rate là 4 Nguồn: https://reliablecho-programming.tistory.com/3 Kiến trúc DenseNet Tùy vào loại Dense block được sử dụng, ta cũng có các tên gọi khác nhau cho DenseNet (DenseNet, DenseNet-B, Denset-C, DenseNet-BC). Kiến trúc DenseNet-C (hoặc DenseNet-BC) với 3 Dense block được mô tả trong hình bên dưới: Trước khi đến với quá trình tính toán qua các Dense block và Transition layer, ta có một layer Conv (và có thể có thêm Pooling, BN) như đa số các kiến trúc CNN khác. Các layer của của mô hình cũng sử dụng Global Pooling và Dense cùng activation softmax để tạo ra vector output. Các mô hình được nhóm tác giả thử nghiệm với dataset ImageNet được tóm tắt như sau:\nQuan sát bảng trên, ta có nhận xét là các mô hình trên đều thuộc loại DenseNet-BC. Ngoài ra, các tác giả cho biết giá trị growth rate được sử dụng là $k=32$.\nCài đặt Các bạn có thể tham khảo phần cài đặt DenseNet bằng Tensorflow và Pytorch tại repo sau.\nKhi cài đặt DenseNet, ta thường sẽ hơi phân vân về cách cài đặt các Dense block. Làm sao để cài đặt các kết nối dày đặc như vậy?\nThực ra cách cài đặt là rất đơn giản. Ta gọi khối gồm (Conv $1 \\times 1$, Conv $3 \\times 3$) như bảng trên là bottleneck block (bb). Khi đó 1 dense block với 4 bottleneck block sẽ có dạng như sau:\n$$x \\to bb_1 \\to x_1 \\to bb_2 \\to x_2 \\to bb_3 \\to x_3 \\to bb_4 \\to x_4 (output) $$\nTrong đó:\nbb_1.input = [ x ] bb_2.input = [x1, x] bb_3.input = [x2, x1, x] bb_4.input = [x3, x2, x1, x] Mã giả cho cách cài đặt dense block này như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def bottleneck_block(input, k): x = BN(input) x = ReLU(x)\tx = conv_1x1(x) x = BN(x) x = ReLU(x) x = conv_3x3(x) return x def dense_block(input, k): c = input x1 = bottleneck_block(c, k) c = concatenate(x1, c) # c = [x1, input] x2 = bottleneck_block(c, k) c = concatenate(x2, c) # c = [x2, x1, input] x3 = bottleneck_block(c, k) c = concatenate(x3, c) # c = [x3, x2, x1, input] x4 = bottleneck_block(c, k) c = concatenate(x4, c) # c = [x4, x3, x2, x1, input] return c # done! Như vậy, ta hoàn toàn có thể dùng một vòng lặp để cài đặt dense block:\n1 2 3 4 5 6 def dense_block(input, k): c = input for i in range(4): x = bottleneck_block(c, k) c = concatenate(x, c) return c Tài liệu tham khảo Paper DenseNet: https://arxiv.org/abs/1608.06993 ","date":"2023-02-11T18:09:08+07:00","permalink":"https://htrvu.github.io/post/densenet/","title":"DenseNet (2018)"},{"content":"Giới thiệu Class Activation Map (CAM) là phương pháp phổ biến trong việc giải thích sự hoạt động của CNN. Nó cho ta biết rằng CNN sẽ tập trung vào những phần nào của ảnh input để dự đoán xác suất ảnh đó thụôc về một class nào đó. Thông thường, CAM còn được gọi là Attention Map.\nĐể dễ hình dung hơn về CAM, ta có 2 ví dụ như sau:\nCNN tập trung vào phần đầu của con chó để đưa ra xác suất mà bức ảnh thuộc class “dog” CNN tập trung vào phần đầu của con chó để đưa ra xác suất mà bức ảnh thuộc class “dog”\nCNN tập trung vào con mèo khi đưa ra xác suất mà bức ảnh thuộc class “cat”\nNguồn: GlassBoxMedicine Các phương pháp này được xếp vào nhóm post-hoc, tức là ta chỉ tiến hành sinh ra CAM để giải thích sự hoạt động của CNN sau khi mô hình này đã được huấn luyện và có một bộ trọng số cố định.\nViệc giải thích CNN bằng CAM là rất hợp lý, vì:\nTa có thể biết được mô hình của mình có đang thật sự hoạt động tốt hay không (tập trung vào đúng phần quan trọng trong ảnh), tức là có chứng cứ rõ ràng cho các dự đoán Nó giúp ta kịp thời phát hiện những đặc trưng mà mô hình “hiểu lầm” khi học về một đối tượng nào đó. Ví dụ, nhờ CAM thì ta thấy rằng mô hình học cách nhận dạng tàu hỏa dựa vào các đường ray trong ảnh thì rõ ràng là nó đã học sai đặc trưng. Trường hợp này hoàn toàn có thể xảy ra vì phần lớn bức ảnh có tàu hỏa thì cũng có đường ray, nhưng ngược lại thì không. Ta có thể có một chiếc xe ô tô chạy ngang đường ray 😀 Trong các phương pháp sinh ra CAM cho một CNN (theo từng input) thì ta có các phương pháp nổi bật như Default CAM, Grad-CAM, Score-CAM. Ta sẽ lần lượt đề cập đến các phương pháp đó.\nCác phương pháp sinh CAM Default CAM (2016) Phương pháp này được đề xuất ngay từ khi các ý tưởng về CAM được công bố. Ta sẽ dựa vào output của conv layer cuối cùng trong kiến trúc, ngay trước fully connected layer sinh ra output của mô hình và các trọng số trong output layer.\nĐầu tiên, để mô tả về ý nghĩa của các feature maps trong output của conv layer cuối cùng thì ta xét ví dụ sau: Trong hình ảnh bên dưới, conv layer cuối cùng của ta là “Conv Layer n”. Output của nó có $k$ channel, hay là $k$ feature maps, mỗi feature map sẽ liên quan đến một đặc trưng nào đó trong ảnh input. Giả sử như feature map $1$ sẽ phát hiện mặt người trong ảnh, feature map $2$ sẽ phát phiện lông của con chó,…, feature map $k$ sẽ phát hiện tai của con chó. Minh họa ý nghĩa các feature maps Nguồn: Johfischer Các trọng số trong output layer sẽ mang ý nghĩa là tầm quan trọng của feature map tương ứng vơi trọng số đó trong việc đưa ra xác suất dự đoán ảnh input thuộc một class nào đó. Ví dụ, ta xét class 2 như hình ảnh bên dưới Khi đó, với $k$ feature maps $F_i$, ta có $k$ trọng số $w_i$ và bằng cách tính tổ hợp tuyến tính của $F_i$ và $w_i$ thì ta sẽ có CAM của ảnh input ứng với class 2. Phép tổ hợp tuyến tính giữa các feature maps Nguồn: Johfischer $$CAM_2 = w_1 * F_1 + w_2 * F_2 + \u0026hellip; + w_k * F_k$$\nLưu ý: Kích thước width và height của CAM đang bằng với các feature maps và nó thường nhỏ hơn nhiều so với ảnh input. Để có thể visualize được như trên, ta chỉ cần upsample CAM lên bằng kích thước của ảnh input. Với mỗi class khác nhau thì CAM tính được là khác nhau Sau khi trình bày ý tưởng của CAM thì ta thấy ngay một điều kiện mà CNN cần thỏa mãn để có thể áp dụng phương pháp này là sau các conv layer thì nó chỉ có duy nhất một fully connected layer để sinh ra output, và điểm nối giữa hai phần này là một global average pooling layer (GAP). Ví dụ như hình bên dưới:\nNguồn: GlassBoxMedicine Trong kiến trúc trên, ta có một CNN với output của conv layer cuối cùng là 3 feature map là A1, A2, A3. Qua GAP thì ta thu được một layer với 3 giá trị số thực. Theo sau đó là một fully connected layer sinh ra output của mô hình. Vì sao ta cần có duy nhất một fully connected layer? Ta đã đề cập rằng các trọng số trong output layer sẽ mang ý nghĩa là tầm quan trọng của feature map tương ứng. Do đó, nối ngay ở đây thì nó mới “đúng ý” (với mỗi neuron trong output layer, ta có đúng $k$ trọng số liên quan trực tiếp đến $k$ feature maps) Vì sao ta cần GAP? Nó sẽ tạo ra cầu nối giữa các feature map và output layer và đảm bảo rằng số channel trong input của output layer đúng bằng số feature maps của conv layer cuối cùng. Viết một cách tổng quát và “formal” hơn, ta sẽ có như sau:\nXét một CNN thỏa điều kiện áp dụng CAM với conv layer cuối cùng có $k$ feature maps $F_i$, output layer có $C$ neurons ứng với $C$ class. Ma trận trọng số tại output layer là $W_{C \\times k}$. Khi đó, với input $X$, CAM được sinh ra cho class $c$ là\n$$CAM_c = W_{c,1} * F_1 + W_{c,2} * F_2 + \u0026hellip; + W_{c,k} * F_k$$\nGrad-CAM (2016) Gradient-weighted Class Activation Mapping (Grad-CAM) là một phiên bản cải tiến của Default CAM với hai yếu tố sau:\nKhông ràng buộc điều kiện đối với kiến trúc của mô hình. Phần fully connected layers được phép ở bất kì cách tổ chức nào. Lưu ý là ta vẫn dùng GAP (global average pooling). Thay vì phải áp dụng cho conv layer cuối cùng thì ta áp dụng cho layer nào cũng được, nhưng thường thì người ta vẫn hay cùng conv layer cuối hơn 😜. “Tầm quan trọng” của các feature maps sẽ được tính theo cách khác, và cách tính này sẽ dựa vào gradient! Grad-CAM thường cho ra kết quả CAM tốt hơn, tập trung vào đúng vùng quan trọng hơn trong ảnh input khi so sánh với Default CAM. Vi dụ:\nSo sánh Default CAM và Grad-CAM Nguồn: MDPI Xét một CNN với conv layer cuối cùng có $k$ feature maps $F_i$, output layer có $C$ neurons ứng với $C$ class. Gọi giá trị output cho class $c$ là $y_c$. Với ảnh input $X$, ta đặt:\nTầm quan trọng của feature map $F_i$ trong việc đưa ra xác suất $X$ thuộc class $c$ được tính dựa vào gradient của output $Y_c$ theo $F_i$, tức là\n$$ \\alpha_{c,i} = GAP(\\frac{\\partial Y_c}{\\partial F_i}) $$\n, với GAP là global average pooling.\nĐể ý rằng, $\\dfrac{\\partial Y_c}{\\partial F_i}$ có cùng shape với $F_i$, ta tiến hành tính GAP để có được giá trị số thực $\\alpha_{c,i}$\nTừ đó, CAM cho class $c$ sẽ được tính bằng tổ hợp tuyến tính giữa $F_i$ và $\\alpha_{c,i}$:\n$$CAM_{c} = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nGrad-CAM đã hoạt động rất tốt và được dùng nhiều trong việc giải thích cho CNN. Tuy nhiên, đến năm 2021 thì có 2 tác giả đã chỉ ra rằng có những trường hợp Grad-CAM cho ra kết quả không thật sự đúng, khi mà CAM được sinh ra tập trung không đúng vào các phần quan trọng. Vi dụ trong y học mà các tác giả đưa ra như sau:\nLý do chính dẫn đến yếu tố này là ở phép toán GAP trong việc tính $\\alpha_{c,i}$. Có những tình huống mà GAP sẽ làm mất đi một số điểm nổi bật ở trong một feature map. Phương pháp mới được giới thiệu là HiresCam (2021), bằng cách thay thế GAP thành phép toán tích element-wise. Các bạn có thể tự tìm hiểu về cái này nhé. Score-CAM (2019) Score-CAM là một phương pháp dựa chỉ dựa vào score của các class (vector output của output layer) để tính tầm quan trọng của các feature maps, từ đó sinh ra CAM. Kết quả ta có được là các CAM tốt hơn phương pháp Grad-CAM, khi mà vùng được chú ý trong ảnh input là đúng hơn, “gọn” hơn (không lan rộng ra những thứ không liên quan lắm ở các phía xung quanh). Ví dụ:\nSo sánh Score-CAM và Grad-CAM Nguồn: Paper Score-CAM - Figure 1 Lưu ý.\nTa xét score của class khi chưa áp dụng softmax để đưa về xác suất. Score-CAM cũng không có ràng buộc gì về kiến trúc của CNN và ta có thể sinh CAM cho các feature map tại bất kỳ layer nào như Grad-CAM. Để trình bày phương pháp này thì ta phải dùng các kí hiệu toán học hơi nhiều một chút 😀\nIncrease of Confidence: Giả sử ta có hàm số $y = f(X)$ với input $X$ là vector $[x_1, x_2,\u0026hellip;, x_n]^T$ và $y$ là số thực. Với một input cơ sở $X_b$ đã biết nào đó, tầm quan trọng của thành phần $x_i$ đối với việc tính ra giá trị $y$ sẽ bằng với độ chênh lệch của output khi ta thay đổi phần thứ $i$ trong $X_b$ bằng cách nhân thêm $x_i$, tức là\n$$ c_i = f(X_b \\circ H_i) - f(X_b) $$\n, với $\\circ$ là element-wise product, $H_i = [1, \u0026hellip;, 1, x_i, 1, \u0026hellip; 1]^T$ (thành phần thứ $i$ là $x_i$)\nBây giờ, áp dụng Increase of Confidence trên với việc ta tính tầm quan trọng của các feature maps: Giả sử ta có một CNN $Y = f(X)$ với input $X$ là ảnh, $Y$ là vector có $C$ phần tử, ứng với score của $C$ class (chưa áp dụng softmax). Ta xét conv layer thứ $k$, feature map thứ $i$ tại layer này được kí hiệu là $F_{k, i}$. Với một input cơ sở $X_b$, tầm quan trọng của $F_{k, i}$ đối với score của class $c$ $( Y_c = f_c(X))$ là\n$$ C(F_{k, i}) = f_c(X_b \\circ H_{k, i}) - f_c(X_b) $$\n, với $H_{k, i} = s(Up(F_{k, i})$ là ta upsample $F_{k, i}$ lên cùng shape với ảnh input $X$, sau đó normalize nó theo công thức $Z \\leftarrow \\dfrac{Z - \\min_Z}{\\max_Z - \\min_Z}$\nSau khi tính các giá trị $C(F_{k, i})$, ta đưa chúng qua softmax để có các giá trị tầm quan trọng với tổng bằng 1:\n$$ \\alpha_{c,i} = \\frac{\\exp(C(F_{k,i}))}{\\sum \\exp(C(F_{k,i})) } $$\nKhi đó, ta có thể sinh ra CAM cho class $c$ như sau:\n$$CAM_c = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nNhư vậy, ta đã xác định được tầm quan trọng của các feature map theo cách là chỉ dựa vào output của CNN (score của các class).\nNhận xét Qua các phương pháp đã trình bày là Default CAM, Grad-CAM, Score-CAM, ta có thể thấy rằng điểm khác biệt lớn nhất giữa chúng là cách tính tầm quan trọng của các feature maps trong việc sinh ra output của CNN.\nCài đặt Các bạn có thể tham khảo notebook sau: Google Colab\nNội dung của notebook trên là sinh ra CAM cho các mô hình CNN như ResNet, DenseNet, EfficientNet và so sánh output của CAM, Grad-CAM và Score-CAM. Các kết quả chạy được trong notebook trên như sau:\nCAM của các mô hình CNN khác nhau: So sánh CAM, Grad-CAM và Score-CAM của mô hình ResNet50: Tài liệu tham khảo GlassBox, CNN Heat Maps: Class Activation Mapping (CAM) GlassBox, Grad-CAM: Visual Explanations from Deep Networks Paper Grad-CAM: https://arxiv.org/abs/1610.02391 Paper Score-CAM: https://arxiv.org/abs/1910.01279 ","date":"2023-02-09T18:30:41+07:00","permalink":"https://htrvu.github.io/post/cam/","title":"CAM, Grad-CAM và Score-CAM trong CNN"},{"content":"XAI là gì? Hầu hết các mô hình AI nói chung hay Deep Learning nói riêng luôn được người ta ví như là một chiếc hộp đen (black-box). Chúng ta xây dựng các mô hình với rất nhiều layer, từ convolution cho đến fully connected, sau đó sử dụng các optimizer như Adam, RMSprop,… (hoặc nói chung chung là gradient descent) để tối ưu mô hình, tức là tìm ra bộ trọng số sao cho hàm mất mát có giá trị nhỏ nhất có thể. Tuy nhiên, nếu ta nhìn lại mô hình và tìm cách giải thích là vì sao các mô hình hoạt động được tốt như vậy thì đây luôn là một câu hỏi khó, Việc đưa những chứng minh chặt chẽ, rõ ràng là không đề hơn giản. Từ đó, ta có một hướng nghiên cứu về các phương pháp giải thích sự hoạt động của các mô hình AI và lĩnh vực này được gọi là Explainable AI (XAI).\nNguồn: https://impact.nuigalway.ie/wp-content/uploads/2022/01/blackboxpng.png Vì sao chúng ta cần phải tìm cách giải thích các mô hình AI?\nTa lấy một ví dụ về mô hình chẩn đoán ung thư dạ dày dựa vào hình ảnh chụp nội soi được sử dụng ở các bệnh viện. Lúc này, tính chính xác của mô hình sẽ trở nên đặc biệt nghiêm trọng, nó có thể ảnh hưởng đến sức khỏe và cả tính mạng của bệnh nhân. Nếu mô hình chẩn đoán là ung thư thì ta cũng cần nó đưa ra những “chứng cứ” cho chẩn đoán đó, tất nhiên là chứng cứ phải đúng, mang tính thuyết phục cao thì mới chấp nhân được. Ngoài lĩnh vực y tế thì ta còn có các ví dụ khác như trong hệ thống bảo mật của ngân hàng,…\nNguồn: Webflow\nNguồn: MicroAI\nKhi AI càng được ứng dụng nhiều vào cuộc sống thì nhu cầu giải thích các mô hình AI cũng sẽ dần nhiều lên. Điều đó dẫn đến sự phát triển mạnh của XAI trong thời gian gần đây.\nDiễn giải một mô hình AI Khả năng diễn giải mô hình (interpretability) là mức độ hiểu biết của chúng ta về cách mô hình hoạt động, mà cụ thể hơn là về quá trình đưa ra dự đoán của mô hình. Ta có hai hướng tiếp cận chính đối với việc diễn giải mô hình là intrinsic và post-hoc.\nNguồn: Kemal Erdem Intrinsic (dựa vào bản chất của mô hình): Cách tiếp cận này thường dùng cho những mô hình thuộc nhóm white-box, đặc biệt là những mô hình Machine Learning như Linear Regresion, Decision Tree, SVM,… Đằng sau những mô hình đó là các lý thuyết toán chặt chẽ, ta có thể tìm được ngay công thức tính ra trọng số tối ưu của bài. Nói cách khác, khi chưa cần huấn luyện thì ta cũng có thể giải thích rằng mô hình sẽ hoạt động theo cách như thế này, như thế kia. Decision Tree Nguồn: javatpoint\nSVM Nguồn: Wikipedia\nPost-hoc: Đây là cách tiếp cận chúng ta thường dùng khi diễn giải các mô hình Deep Learning, và đặc biệt là nó được tiến hành sau khi mô hình đã được huấn luyện với một bộ trọng số đủ tốt. Vì việc giải thích, chứng minh chặt chẽ, chính xác về quá trình hoạt động của các mô hình Deep Learning là rất khó khăn nên post-hoc là hướng tiếp cận được ưu tiên hơn. Trong post-hoc, ta có 2 cách diễn giải là model-agnostic và model-specific.\nModel-agnostic: Cách này nghĩa là chúng ta có thể áp dụng cùng một phương pháp để diễn giải cho toàn bộ các mô hình mà không cần quan tâm đến kiến trúc của chúng. Như vậy, ta chỉ dựa vào input và output của mô hình để đưa ra cách diễn giải. Model-specific: Với cách này thì tùy theo những mô hình, hay là họ các mô hình, mà ta sẽ đưa ra cách diễn giải tương ứng. Ta có thể thấy rằng model-specific có thể dễ tiến hành hơn model-agnostic rất nhiều.\nNhững phương pháp trong XAI mà mình trình bày trong tương lai sẽ chủ yếu thuộc về hướng post-hoc.\nVì sao chúng ta bàn nhiều về khả năng diễn giải mô hình (interpretability) nhưng lĩnh vực này lại gọi là Explainable AI (thiên về khả năng giải thích mô hình)?\n2 thuật ngữ diễn giải và giải thích có thể xem là mang ý nghĩa tương tự và có thể dùng thay thế cho nhau. Tuy nhiên, có một vài quan điểm cho rằng khả năng diễn giải là nói đến một tính chất bị động của mô hình và nó cần con người chúng ta can thiệp vào, còn khả năng giải thích là thiên về chủ động, tức là mô hình có thể tự giải thích cho chính nó. Ở đây, con người chúng ta đang tìm cách giải thích các mô hình, do đó ta ưu tiên gọi là diễn giải. Đánh giá phương pháp XAI Một vấn đề khác mà người ta thường quan tâm đến là cách đánh giá một phương pháp XAI, tức là xét xem cách diễn giải mô hình A đã thuyết phục, đã đúng hay chưa. Hiện tại, ta chưa có một độ đo nào để có thể so sánh các phương pháp với nhau. Phần lớn thì nó nằm ở các nhận xét của con người thông qua việc quan sát 😀\nTài liệu tham khảo Mobiquity, An introduction to Explainable Artificial Intelligence (XAI) Erdem, XAI Methods - The Introduction ","date":"2023-02-09T15:30:31+07:00","permalink":"https://htrvu.github.io/post/intro-xai/","title":"Giới thiệu về XAI"},{"content":"Giới thiệu Qua các mô hình đã được giới thiệu như VGG, GoogLeNet hay ResNet thì ta thấy rằng chúng đều được phát triển theo hướng tăng dần độ sâu và độ phức tạp tính toán của mô hình để đạt được độ chính xác cao hơn, kể từ khi AlexNet được công bố. Số lượng tham số của chúng là rất lớn.\nTuy nhiên, các ứng dụng AI trong thực tế như robotics, xe tự hành thì các phép tính toán của mô hình cần được thực hiện trong một khoảng thời gian giới hạn, cùng với tài nguyên phần cứng hạn chế. Do đó, ta phải đối mặt với một trade-off giữa độ chính xác và độ trễ, kích thước mô hình.\nVào thời điểm này, có 2 hướng giải pháp chính để có thể đưa các mô hình vào ứng dụng thực tế như sau:\nNén các mô hình phức tạp lại thông qua các phương pháp như lượng tử hóa (quantization), hashing, cắt tỉa mô hình Xây dựng và huấn luyên các mô hình nhỏ, độ phức tạp thấp ngay từ đầu. MobileNet được phát triển theo hướng thứ 2, trong đó, nó tập trung vào các yếu tố:\nVừa đảm bảo kích thước mô hình đủ nhỏ, tốc độ suy diễn đủ nhanh (độ trễ thấp) và với độ chính xác đủ cao. Cung cấp hai siêu tham số cho phép ta điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước mô hình: width multiplier (liên quan đến số channel trong từng layer) và resolution multiplier (width và height trong từng layer) Depthwise separable convolutions MobileNet được xây dựng từ các layer convolution khá đặc biệt, chúng được gọi là depthwise separable convolutions. Depthwise separable convolution được tạo ra từ hai phép toán:\nDepthwise convolution: Áp dụng từng filter cho từng channel của input. Nếu input có bao nhiêu channel thì ta sẽ có bấy nhiêu filter. Pointwise convolution: Đây thực chất là convolution layer thông thường với filter 1 x 1. Nó được dùng để tổng hợp các kết quả từ phép toán depthwise convolution và tính ra output, thông qua các phép toán tổ hợp tuyến tính. Nguồn: Research Gate Ta có thể thấy ngay sự khác biệt giữa depthwise separable convolution và convolution thông thường như sau:\nConvolution thông thường: Mỗi filter sẽ tương tác với toàn bộ channel của input. Giả sử input của ta là $D_F \\times D_F \\times M$, một filter $3 \\times 3$ được áp dụng thì filter này sẽ trở thành một tensor với shape $3 \\times 3 \\times M$, ta thực hiện convolution trên từng channel và sau đó cộng $M$ ma trận lại với nhau, thu được kết quả $D_F \\times D_F$. Nếu sử dụng $N$ filter để tính thì ta sẽ có kết quả cuối cùng là $D_F \\times D_F \\times N$. Depthwise separable convolution: Ban đầu, các channel được tính toán độc lập với từng filter riêng, sau đó mới kết hợp lại sau nhờ vào pointwise convolution. Với input $D_F \\times D_F \\times M$ thì khi đưa qua depthwise convotution, ta sẽ có kết quả là $D_F \\times D_F \\times M$. Nếu pointwise convolution sử dụng $N$ filter $1 \\times 1$ thì ta có kết quả cuối cùng là $D_F \\times D_F \\times N$ Vấn đề đặt ra là tại sao sử dụng depthwise separable convolution lại có thể giúp cho MobileNet gọn nhẹ hơn, tính toán nhanh hơn và có độ chính xác đủ tốt, không hề kém cạnh các mô hình to lớn khác. Ta sẽ đặt tính một chút:\nGiả sử input của ta là feature maps $\\bold{F}: D_F \\times D_F \\times M$, output cuối cùng là $\\bold{G}: D_F \\times D_F \\times N$.\nVới convolution thông thường: Giả sử ta dùng $N$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding sao cho kích thước width và height không đổi. Khi đó, độ phức tạp tính toán sẽ là\n$$ O_1 = D_K \\times D_K \\times D_F \\times D_F \\times M \\times N $$\n, vì với mỗi filter thì: mỗi lần tính toán ta phải thực hiện $D_K \\times D_K$ phép toán nhân, sau đó cộng chúng lại; ta tính tại $D_F \\times D_F$ vị trí trên $M$ channel của input, và ta sử dụng $N$ filter.\nVới depthwise separable convolution: Ở bước depthwise convolution thì ta dùng $M$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding phù hợp. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times M $$\n, vì ta chỉ đơn giản là áp dụng đơn lẻ từng filter cho từng channels\nVới pointwise convolution thì ta dùng $N$ filter $\\bold{K}: 1 \\times 1$, stride là 1, padding 0. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_F \\times D_F \\times M \\times N $$\nDo đó, ta có độ phức tạp tính toán là\n$$ O_2 = D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N $$\nLúc này, đem chia cho nhau thì ta có tỉ lệ\n$$ \\frac{O_2}{O_1}=\\frac{D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N}{D_K \\times D_K \\times D_F \\times D_F \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_K^2} $$\nNhư vậy, độ phức tạp tính toán khi sử dụng depthwise separable convolution đã giảm khoảng $D_K^2$ lần so với convolution thông thường. MobileNet sử dụng các filter $3 \\times 3$, từ đó giảm được độ phức tạp tính toán đi khoảng 8 đến 9 lần, trong khi độ chính xác chỉ giảm đi một phần nhỏ.\nSiêu tham số điều chỉnh trade-off Để có thể hỗ trợ tốt hơn việc áp dụng MobileNet vào các thiết bị biên trong các ứng dụng thực tế, các tác giả còn cung cấp thêm cho ta hai siêu tham số để điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước của mô hình\nWidth multiplier Tham số width multiplier (kí hiệu là $\\alpha$) sẽ tác động lên giá trị số channel của các layer. Với những công thức ở trên thì số channel chính là $M$ và $N$. Giá trị $\\alpha \\in (0, 1]$ và ta thường đặt là $1, 0.75, 0.5, 0.25.$ Khi đó, thứ thật sự được thay đổi chính là số lượng filter mà ta dùng trong các phép toán pointwise convolution.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng width multiplier $\\alpha$ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times \\alpha M + D_F \\times D_F \\times \\alpha M \\times \\alpha N $$\nResolution multiplier Tham số resolution muiltiplier (kí hiệu là $\\rho$ ) liên quan đến kích thước width và height (chính là $D_F$ trong các công thức trên). Miền giá trị của nó cũng sẽ tương tự như $\\alpha$. Thực chất thì ta sẽ chỉ áp dụng nó vào input ban đầu của mô hình (ảnh). Các kích thước input mà ta thường sử dụng với mô hình MobileNet là 224, 192, 160 hoặc 128.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng thêm resolution multiplier $\\rho$ là\n$$ D_K \\times D_K \\times \\rho D_F \\times \\rho D_F \\times \\alpha M + \\rho D_F \\times \\rho D_F \\times \\alpha M \\times \\alpha N $$\nKiến trúc mô hình Depthwise Separable block Mô hình MobileNet V1 được tạo thành bởi các thành phần chính là depthwise separable block. Chúng bao gồm hai phép toán như ta đã đề cập là depthwise convolution và pointwise convolution. Đi kèm với các layer đó là batch norm và activation ReLU.\nNguồn: Research Gate Kiến trúc MobileNet MobileNet sử dụng tất cả gồm 13 depthwise separable block. Tổng thể kiến trúc của MobileNet được thể hiện ở bảng sau:\nTrong đó:\nConv dw là depthwise convolution. Ta có thể thấy các filter shape của chúng luôn có cùng số channel trong input size. Các conv layer với filter shape $1 \\times 1$ chính là pointwise convolution. s1 tức là stride = 1, tương tự với s2. Toàn bộ các conv layer trong mô hình đều có padding sao cho kích thước width và height của input và output của layer đó là như nhau. Note:\nTrong bảng trên có vẻ có một chỗ gõ nhầm. Để ý đến layer “Conv dw / s2” đầu tiên từ phía dưới lên, nếu đây là s2 thì input size của layer “Conv / s1” tiếp theo phải bị giảm size chứ không phải $7 \\times 7$. Do đó, trong cài đặt mô hình ở bên dưới thì mình đã đổi nó thành “Conv dw / s1”. Cài đặt Các bạn có thể tham khảo phần cài đặt MobileNet bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper MobileNet: https://arxiv.org/abs/1704.04861 ","date":"2023-02-08T18:13:00+07:00","permalink":"https://htrvu.github.io/post/mobilenet/","title":"MobileNet (2017)"},{"content":"Giới thiệu Các mô hình thuộc họ Inception-ResNet được phát triển dựa trên ý tưởng là kết hợp skip connection vào các Inception block (các ý tưởng từ ResNet và GoogLeNet). Vì paper này chỉ mang tính thực nghiệm là chính nên mình sẽ không trình bày chi tiết 👀.\nKiến trúc mô hình Inception-ResNet V1 Về mặt tổng quan, Inception-ResNet V1 có kiến trúc như sau:\nKiến trúc Inception-ResNet V1\nStem của Inception-ResNet V1\nTa sẽ đề cập đến các loại Inception-ResNet block và Reduction:\nInception-ResNet block: Ta thấy rằng phần “Inception” trong các block này là đơn giản hơn khá nhiều so với các Inception block nguyên mẫu. Inception-ResNet-A V1\nInception-ResNet-B V1\nInception-ResNet-C V1\nReduction: Chúng thực hiện nhiệm vụ giảm kích thước (width, height) của các tensor đi một nửa. Kiến trúc của chúng rất giống với các Inception block Reduction-A\nReduction-B\nInception-ResNet V2 Phiên bản thứ hai của Inception-ResNet có kiến trúc tổng thể giống hệt với phiên bản đầu tiên, ta chỉ có một số thay đổi ở các block Inception-ResNet và Reduction\nTài liệu tham khảo Paper Inception-ResNet: https://arxiv.org/abs/1602.07261 ","date":"2023-02-08T18:03:46+07:00","permalink":"https://htrvu.github.io/post/inception-resnet/","title":"Inception-Reset (2016)"},{"content":"Khó khăn trong huấn luyện mô hình lớn Ta biết rằng, việc tạo ra các mô hình có độ sâu lớn (nhiều layer) chưa chắc đã mang lại hiệu quả tốt hơn những mô hình “cạn” hơn. Ví dụ, với tập CIFAR10 thì ta có một kết quả thử nghiệm cho thấy rằng mô hình sâu hơn lại có độ hiệu quả kém hơn:\nĐối với việc huấn luyện các mô hình có độ sâu lớn thì ta có thể sẽ bị gặp phải các vấn đề sau:\nOverfitting: Mô hình càng sâu thường sẽ càng phức tạp nên nó rất dễ bị overfitting. Vanishing/exploding gradient: Vấn đề này thì ta đã có một số cách giải quyết phổ biến như thay đổi activation function, các phương pháp khởi tạo trọng số như He Initialization. Trong He Initialization, tại mỗi layer thì ta khởi tạo các giá trị bias với gái trị 0, các trọng số sẽ tuân theo phân phối chuẩn với kỳ vọng 0, phương sai $\\dfrac{2}{D_h}$, trong đó $D_h$ là số units của layer liền trước layer hiện tại. Ngoài 2 vấn đề trên, ta có một vấn đề đặc biệt hơn là degradation: Accuracy tăng dần cho đến một độ sâu nhất định thì ngừng tăng (bão hòa) rồi sau đó sẽ giảm dần.\nLưu ý rằng, nhiều trường hợp degradation không phải do overfitting gây ra. Khi mô hình có độ phức tạp đủ lớn, những sự thay đổi dù là rất nhỏ trong trọng số cũng sẽ gây ra sự biến thiên lớn trong giá trị của gradient, điều này dẫn đến các bước cập nhật trọng số qua những lần chạy gradient descent sẽ không mang lại lợi ích gì nhiều mà còn có thể khiến quá trình huấn luyện \u0026ldquo;đi lạc\u0026rdquo;.\nSự biến thiên của gradient khi trọng số thay đổi trong các mô hình\n(a) Mô hình chỉ có 1 hidden layer; (b) Mô hình có độ sâu lớn với 24 hidden layers\nNguồn: Understanding Deep Learning - Simon J.D. Prince Degradation cho chúng ta thấy rằng việc tối ưu các mô hình có độ sâu lớn là không hề dễ dàng. Trong quá trình xây dựng mô hình, từ một mô hình ban đầu, sau khi thêm một số layer vào thì tất nhiên là ta mong rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc. Tuy nhiên, khi xảy ra degradation thì mong muốn đó đã không thể thành sự thật được. 😀\nResNet được công bố nhằm giải quyết vấn đề degradation đối với các mô hình có độ sâu lớn. Khi nhắc đến ResNet thì ta sẽ lập tức nghĩ đến những mô hình với độ sâu rất khủng, thậm chỉ là lên đến 100, 200 layer.\nHàm phần dư và skip connection Ý tưởng về hàm phần dư Nhắc lại về cái mong muốn ở phần trước, rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc, ta có thể nghĩ ngay đến một phương pháp cực kì đơn giản: các layer phía sau sẽ là identity mapping, tức là input và output của nó sẽ giống nhau. Với cách làm này thì hiển nhiên là ta đạt được mong muốn rồi, vì độ hiệu quả của mô hình mới và mô hình gốc rõ ràng sẽ y hệt nhau.\nTuy nhiên, nếu chỉ dừng lại ở đó thôi thì thêm layer vào làm gì :v Ta muốn đạt được kết quả tốt hơn! Các tác giả của paper ResNet giới thiệu một phương pháp gọi là deep residual learning (học phần dư).\nGiả sử ta có một block $B$ các layer, input của nó là $\\bold{x}$. Với một mô hình thông thường, ta sẽ “học” một hàm số đầu ra mong muốn là $f(\\bold{x})$. Lúc này, ban đầu thì ta hoàn toàn chưa có một thông tin gì về $f(\\bold{x})$ cả, việc “học” sẽ xuất phát từ một đại lượng ngẫu nhiên.\nĐối với phương pháp deep residual learning, output của block $B$ sẽ có dạng\n$$h(\\bold{x}) = \\bold{x} + f(\\bold{x})$$\nvà ta sẽ đi học $f(\\bold{x})$. Lúc này, $f(\\bold{x})$ được gọi là residual function (hàm phần dư)\nTa có các nhận xét sau:\nTrong deep residual learning, ta đã có sự “gợi ý” cho hàm mong muốn thông qua giá trị input $\\bold{x}$. Đối với thông thường thì không có sự gợi ý nào được đưa ra cả. Việc học $f(\\bold{x})$ như là một hàm phần dư là dễ hơn so với việc học hàm mong muốn. Nếu identity mapping là kết quả tối ưu thì ta có luôn $f(\\bold{x})=0$ Về mặt bản chất, với phương pháp deep residual learning, ta đã tác động vào lớp hàm chứa hàm mong muốn, sao cho lớp mới là lớn hơn (bao gồm) lớp cũ. Nguồn: Dive into DL Để minh họa cho yếu tố \u0026ldquo;giúp việc học trở nên dễ hơn\u0026rdquo;, ta có ví dụ như sau:\nĐồ thị của loss function cũng \"trơn\" hơn rất nhiều khi có sử dụng skip connection, từ đó ta sẽ dễ huấn luyện mô hình hơn Nguồn: Jeremy Jordan Skip connection Như vậy, điểm nhấn của ResNet là ta đi học các hàm phần dư, thông qua việc “gợi ý” cho hàm số mong muốn một giá trị bằng với chính giá trị input ban đầu. Trong cài đặt, thao tác này được thực hiện thông qua một kết nối gọi là skip connection. Ta sẽ cộng input $\\bold{x}$ vào output của block thông thường.\nĐối với việc cộng như vậy thì thực chất là ta đang đi cộng hai ma trận. Khi đó, một vấn đề có thể nảy sinh là về shape của chúng, mà thường là số channel. Nếu số channel không khớp thì ta cần phải tiến hành “điều chỉnh”. Vì các block được sử dụng thường gồm các conv layer với stride và padding phù hợp để giữ nguyên width và height nên ta sẽ chỉ xét đến số channel. Nếu số channel của $\\bold{x}$ và output ban đầu là như nhau thì ta gọi skip connection này là identity skip connection. Ngược lại, ta sẽ dùng conv layer $1 \\times 1$ để điều chỉnh số channel của $\\bold{x}$. Lúc này, skip connection được gọi là projection skip connection. Sử dụng skip connection (identity) Nguồn: Dive into DL Ngoài ra, ta còn có một điểm mạnh quan trọng của skip connection là nó giúp cho gradient được lan truyền tốt hơn trong quá trình backpropagation, từ đó góp phần làm giảm hiện tượng vanishing gradient.\nQuan sát hình phía trên, ta thấy rằng khi backpropagation thì layer ở ngay trước block nhận được gradient từ hai layer phía sau nó (một layer liền trước nó và một layer được kết nối thông qua skip connection). Vấn đề exploding gradient Như mình đã đề cập ở đầu bài viết, với các mô hình có độ sâu lớn thì hai vấn đề đối với gradient mà ta thường gặp là vanishing và exploding. Đầu tiên, ta có những cách khởi tạo trọng số để hạn chế các vấn đề này như He Initialization. Ngoài ra, với vanishing thì cũng đã được phần nào hạn chế đi bằng skip connection. Tuy nhiên, nếu dùng He Initialization cùng với skip connection thì skip connection lại có khả năng gây ra exploding gradient 😃\nLý do của điều này nằm ở thao tác cộng giá trị $\\bold{x}$ với giá trị tính được của hàm $f(\\bold{x})$. Ta hoàn toàn có thể xem $\\bold{x}$ và $f(\\bold{x})$ là hai biến ngẫu nhiên độc lập, khi đó\n$$\\text{var}(\\bold{x} + f(\\bold{x})) = \\text{var} (\\bold{x}) + \\text{var} (f(\\bold{x}))$$\nHơn nữa, khi ta dùng ReLU và He Initialization thì $\\text{var}(f(\\bold{x})) = \\text{var}(\\bold{x})$. Do đó, có thể nói là ta đã làm cho phương sai của input $\\bold{x}$ tăng gấp đôi sau khi đi qua một block có dùng skip connection. Như vậy, khi dần qua nhiều block thì phương sai sẽ cứ tăng lên theo số mũ 2 và có thể dẫn đến hiện tượng exploding gradient.\nĐể hạn chế vấn đề exploding như trên, ta có hai cách phổ biến:\nSau khi tính xong $h(\\bold{x}) = \\bold{x} + f(\\bold{x})$ thì ta sẽ scale $h(\\bold{x})$ bằng cách nhân nó với $\\dfrac{1}{\\sqrt{2}}$. Sử dụng Batch Normalization với hai trọng số $(\\gamma, \\delta) = (1, 0)$ để chuẩn hóa $\\bold{x}$ trước khi tính $f(\\bold{x})$ (không thay đổi giá trị $\\bold{x}$ gốc vì giá trị gốc sẽ được sử dụng cho phép cộng ở sau. Bằng cách này, kết hợp với He Initialization thì $f(\\bold{x})$ cũng sẽ có phương sai là 1. Từ đó, phương sai của tổng $h(\\bold{x})$ sẽ chỉ tăng tuyến tính theo số lượng block được sử dụng. Minh họa hai cách hạn chế exploding trong skip connection\nNguồn: Understanding Deep Learning - Simon J.D. Prince Cách giải quyết số 2 được các tác giả sử dụng trong kiến trúc của ResNet. Đây là lý do ta thấy các layer Batch Normalization được sử dụng trong residual block ở phần tiếp theo.\nResidual block Các loại residual block Residual block (khối phần dư) được tạo ra bằng cách thêm một skip connection vào một block thông thường trong các mô hình CNN như VGG block. Ta có hai loại residual block như sau:\nBasic: Các conv layer trong block này có filter $3 \\times 3$. Block dạng này thường được dùng cho các mô hình có độ sâu vừa phải. Bottleneck: Nhằm giảm bớt số lượng tham số của các mô hình có độ sâu lớn, các tác giả sử dụng dạng block này với hai conv layer $1 \\times 1$ với vai trò là giảm/tăng số channel, tạo ra một hình dáng giống như nút thắt cổ chai. Hai layer conv $1 \\times 1$ như vậy được gọi là bottleneck layer. Để cho dễ hình dùng thì ta có thể xem đây như là thao tác “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán). Basic Residual Block và Bottleneck Residual Block Ngoài ra, ta còn có 2 loại skip connection là identity và projection. Lúc này, tùy vào số channels của input và output ban đầu có khớp hay không mà block tương ứng sẽ chứa loại skip connection phù hợp.\nBasic residual block với identity và projection skip connection Nguồn: Dive into DL Thứ tự thực hiện các phép toán Quan sát kỹ hình ảnh mô tả residual block ở phía trên, ta sẽ thấy rằng thao tác cộng input $\\bold{x}$ và output của layer Batch Norm thứ hai (chính là $f(\\bold{x})$) được thực hiện trước khi đi qua ReLU. Vì sao lại như thế?\nLý do rất đơn giản. Nếu ta đưa $f(\\bold{x})$ qua ReLU trước rồi mới cộng vào $\\bold{x}$ thì $\\bold{x}$ đang được cộng với một lượng không âm. Cứ nhiều lần như thế thì giá trị model tích lũy được sẽ chỉ có tăng dần lên chứ không có giảm 😃 Điều này chắc chắn là không ổn và có thể khiến mô hình không thể học được đặc trưng gì có ích cả.\nKiến trúc ResNet ResNet được tạo nên bằng cách sử dụng nhiều residual block liên tiếp nhau, tương tự như những gì mà GoogLeNet hay VGG đã thực hiện. Các tác giả của paper ResNet tạo ra nhiều phiên bản ResNet khác nhau với độ sâu tăng dần.\nDựa vào số lượng layer có trọng số thì ta sẽ có tên các mô hình như ResNet18 (18 layer có trọng số), ResNet34,… Các phiên bản ResNet Ta có một số nhận xét như sau:\nCác phiên bản có độ sâu lớn như 50, 101 và 152 sử dụng Bottleneck Residual Block. Với 18 và 34 thì chúng dùng Basic block Trong các kiến trúc trên thì ta sử dụng cả 2 loại skip connection: identity và projection Ví dụ, với ResNet18 thì trong cụm Basic block đầu tiên của cụm conv3_x sẽ có số channel là 64, còn output ban đầu của block này thì là 128 nên ta phải áp dụng projection vào input Đối với height và width thì các tác giả cho biết việc downsampling input sẽ được thực hiện tại conv layer đầu tiên của các cụm conv3_x, conv4_x, conv5_x (với stride là 2). Các conv layer khác thì đều có stride 1. Ta sẽ cần chú ý đến chi tiết này khi tiến hành cài đặt ResNet. Cài đặt Các bạn có thể tham khảo phần cài đặt ResNet bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into DL, ResNet Simon J.D. Prince, Understanding Deep Learning ","date":"2023-02-08T17:41:09+07:00","permalink":"https://htrvu.github.io/post/resnet/","title":"Resnet (2015)"},{"content":" Cá nhân mình thấy GoogLeNet là một paper khó đọc. Khi viết ra bài này thì mình vẫn đang cảm thấy hơi lú về nội dung của nó 😀\nGiới thiệu Từ khi AlexNet được công bố vào năm 2012 và đặt nền tảng cho các mạng Deep CNN, GoogLeNet, hay Inception V1 (2014), là một trong những kiến trúc có cách thiết kế rất thú vị khi nó tận dụng hiệu quả các conv layer, đặt nền móng cho nhiều mô hình sau này.\n“Inception” có thể dịch là “sự khởi đầu”, nghe có vẻ rất hợp lý 😀. Ngoài ra, ở trong bài viết về VGG, mình có đề cập đến vấn đề thiết kế kiến trúc mô hình theo hướng có sự lặp lại các khuôn mẫu. GoogLeNet cũng sẽ được thiết kế như vậy.\nLưu ý. Tên của mô hình này là GoogLeNet, không phải GoogleNet =)) Tác giả cho biết ý nghĩa của cái tên này là “This name is a homage to Yann LeCuns pioneering LeNet 5 network.”\nGoogLeNet được xây dựng từ những mục tiêu của nghiên cứu như sau:\nNâng cao khả năng tận dụng tài nguyên tính toán Cho phép tăng chiều rộng và chiều sâu của kiến trúc mô hình mà vẫn đảm bảo được độ phức tạp tính toán là ở mức chấp nhận được. GoogLeNet thật sự đã đạt được những điều đó, và nó được xây dựng dựa trên nguyên lý Hebbian: “neurons that fire together, wire together”. Paper GoogLeNet đã đưa ra các nền tảng lý thuyết rất “căng thẳng” để cho thấy rằng mô hình CNN có thể hoạt động “đủ tốt”, điều mà ta chưa được biết đến ở trong các paper trước đó 😀\nNgoài ra, có một quan điểm khá thú vị khi mô tả về kiến trúc của GoogLeNet như sau: Khi xây dựng kiến trúc CNN, thay vì phải suy nghĩ xem trong các mạng CNN ta nên áp dụng filter với kích thước bao nhiêu, hãy áp dụng luôn nhiều filter với kích thước khác nhau và tổng hợp kết quả lại 😜\nMình cũng có đồng tình với quan điểm này. Tuy nhiên, nguồn gốc đằng sau nó có vẻ không chỉ đơn giản là như vậy. Trong paper, các tác giả đã tiến hành phân tích và thử nghiệm nhiều lắm cho ra được cái kiến trúc của GoogLeNet. Kết nối thưa và CNN Đầu tiên, tại thời điểm trước khi Inception được công bố, chúng ta có thể cải thiện một mô hình DNN bằng những cách như sau:\nTăng chiều sâu của mô hình (tức là số layer) Tăng chiều rộng (số channel trong mỗi layer) Tuy nhiên, với hai cách trên thì sẽ có những vấn đề mà ta cần lưu tâm là hiện tượng overfitting và sự gia tăng độ phức tạp của mô hình.\nCác tác giả có đề cập đến một hướng đi có thể giảm bớt hai vấn đề trên là sử dụng kiến trúc kết nối thưa (sparsely connected architectures). Ta có thể hiểu đơn giản như sau:\nVới hai fully connected layer liên tiếp nhau, mỗi neuron trong layer sau sẽ kết nối với tất cả các unit trong layer trước. Nếu như ta thay đổi đi một chút, mỗi neuron trong layer sau sẽ chỉ kết nối đến một vài unit trong layer trước, kiến trúc có được sẽ trở nên “thưa” hơn. Kết nối thưa trong fully connected layer Đối với conv layer thì ta đã đạt được tính chất thưa như vậy. Ta biết rằng, với mỗi phần tử trên feature map của layer hiện tại thì ta tính nó dựa vào một vùng nhỏ trên feature map output của layer trước đó. Giả sử hai layer này đều chỉ có 1 channel thì ta có thể biểu diễn nó như hình bên dưới: Kết nối thưa trong conv layer Kiến trúc thưa được các tác giả mô tả là mô phỏng lại hệ thống sinh học (ví dụ như khi ta nhìn vào một đối tượng thì ta thường chỉ chú ý một số điểm trên đối tượng đó thôi, khó mà chú ý tất cả được).\nNgoài ra, có một nền tàng lý thuyết rất trâu bò về kiến trúc thưa của Arora như sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Thật sự mình cũng chưa hiểu hết ý của câu trên… Nhưng đại ý của nó có vẻ là nếu ta biểu diễn được phân phối các điểm dữ liệu của một dataset bằng một kiến trúc DNN lớn và thưa thì từ đó ta có thể xây nên được một kiến trúc tối ưu (về cả độ chính xác và độ phức tạp), bằng cách xây từng layer một 😀\nTuy nhiên, với các tài nguyên phần cứng trong thời gian này thì rất khó để ta có thể tính toán trên các mạng DNN thưa. Do đó, các tác giả đi theo hướng tìm một mô hình là có tận dụng một số thông tin về tính chất thưa nhưng vẫn thực hiện tính toán trên các ma trận đầy đủ. Đấy chính là hướng sử dụng các phép toán convolution!.\nCũng vì lý do này, ở phần kiến trúc của GoogLeNet thì ta sẽ thấy nó chỉ có duy nhất một fully connected layer để sinh ra output, còn lại chỉ toàn conv layer thôi 😗 Nói đến việc áp dụng các filter trong conv layer, các tác giả cho rằng:\nMỗi phần tử trong feature map của một layer sẽ có mối tương quan với một vùng nào đó trên ảnh input (receptive field). Ta sẽ gom các phần tử cùng tương quan với một vùng vào cùng một cụm. Để ý rằng, trong những layer ở gần ảnh input thì ta sẽ có nhiều cụm và kích thước mỗi cụm là nhỏ. Càng đi qua các layer CNN thì số lượng cụm sẽ ít lại và kích thước cụm sẽ lan rộng ra. Tất nhiên là vẫn có thể có những cụm có kích thước nhỏ tại những layer đó. Do vậy, ta nên có các filter với kích thước lớn hơn để học các đặc trưng tại các cụm lớn, đồng thời cũng cần có filter kích thước nhỏ đối với các cụm nhỏ hơn. Qua một loạt các thử nghiệm, các tác giả đã chọn 3 filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Một khối chứa 3 filter trên được đặt tên là Inception module.\nBây giờ quay lại với lý thuyết của Arora, ở ý xây dựng mạng tối ưu qua từng layer một. GoogLeNet được tạo nên bằng đúng ý tưởng như vậy, ta nối Inception module - by - Inception module 😀\nP/s: Các bạn có thể tìm đọc phần Motivation trong paper gốc và tự cảm nhận nó nhé.\nInception module Inception module Qua các mô tả ở phần trên, ta có thể liên tưởng đến kiến trúc của Inception module là một thứ gì đó giống với cái ở hình (a), với đủ 3 loại filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Các tác giả dùng thêm cả một layer max pooling trong đó nữa, với lí do rất đơn giản là vì ở thời điểm đó thì max pooling thường mang lại hiệu quả tốt trong các kiến trúc mạng CNN =))\nTruy nhiên, cách cài đặt như hình (a) sẽ dẫn đến số lượng tham số của mô hình là rất lớn. Thay vào đó, ta có thể tạo ra kiến trúc dạng “bottleneck” bằng cách sử dụng thêm các layer convolution $1 \\times 1$ như hình (b), nhằm mục đích chính là giảm số channel. Số lượng phép tính lúc này sẽ được giảm một cách đáng kể.\nLưu ý rằng, ở cuối module ta có phép toán concatenate, tức là các feature-maps của toàn bộ layer convolution đều phải có cùng size (tức là width và height)\nKiến trúc GoogLeNet GoogLeNet sử dụng tổng cộng 9 Inception module, gồm có 22 layer có trọng số (tính cả pooling là 27), được tóm tắt qua bảng sau:\nMột điểm đặc biệt khi train GoogLeNet là các tác giả sử dụng các auxiliary classifiers (xem hình bên dưới). Các thành phần này sẽ khá giống như giống hệt với phần cuối của mô hình (bộ classifier). Như vậy, ta có thể xem GoogLeNet có 3 bộ classifier. Auxiliary classifiers được sử dụng với các ý nghĩa như sau:\nHạn chế vanishing gradient Tăng regularization Lưu ý:\nLoss khi huấn luyện sẽ cộng loss của cả ba lại với nhau. Khi test thì ta thường chỉ quan tâm đến bộ classifer cuối cùng. Một cách làm khác là ta xài cả 3, sau đó lấy kết quả trung bình. Nguồn: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png Cài đặt Các bạn có thể tham khảo phần cài đặt GoogLeNet bằng Tensorflow và Pytorch tại repo sau. Trong cách cài đặt này, mình sẽ bỏ qua auxiliary classifiers.\nTài liệu tham khảo Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"https://htrvu.github.io/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giới thiệu Dựa trên sự thành công của AlexNet vào năm 2012, nhiều nghiên cứu đã được tiến hành nhằm tìm ra các phương pháp hay kiến trúc mới để đạt được kết quả tốt hơn, ví dụ như:\nThay đổi (tăng, giảm) kích thước của conv filter Thay đổi stride, padding của conv layer Train và test trên các input với nhiều độ phân giải (resolution) ảnh khác nhau Trong năm 2014, VGG là một trong những kết quả nghiên cứu nổi bật nhất, và nó tập trung vào một vấn đề khác với các hướng trên là độ sâu (depth, hay là số layer) của mô hình. VGG đã đạt được các kết quả tốt nhất vào thời điểm nó ra mới trên dataset ImageNet và các dataset khác, trong các task như classification, localization,…\nNgoài ra, ta có thể đưa ra nhận xét như sau về AlexNet:\nDù AlexNet đã chứng minh được CNN có thể đạt độ hiệu quả tốt, nó lại không cung cấp một khuôn mẫu nào cho việc nghiên cứu, thiết kế các mạng mới. Theo thời gian, các nhà nghiên cứu đã thay đổi suy nghĩ từ quy mô những neuron riêng lẻ sang các tầng, rồi sau đó là các khối (block) gồm các tầng lặp lại theo khuôn mẫu. Kiến trúc của VGG là một trong những kiến trúc phổ biến đầu tiên được xây dựng theo ý tưởng như vậy.\nVGG block Điểm nổi bật của VGG là ta chỉ dùng duy nhất một kích thước filter trong mọi conv layer là $3 \\times 3$, và ta dần tăng độ sâu của mô hình bằng các conv layer. Hơn nữa, ta còn áp dụng nhiều conv layer liền nhau rồi mới dùng đến max pooling. Ta có thể gọi một block gồm những layer như thế là VGG block.\nCác tác giả có đề cập đến một vấn đề cho cách áp dụng này như sau: Việc dùng nhiều conv layer 3 x 3 liền nhau như vậy so với dùng một conv layer với filter lớn hơn (ví dụ 7 x 7) như hầu hết các mô hình đã được công bố vào thời điểm trước đó thì có gì “tốt” hơn, khi mà features map sau cùng ta thu được có thể có cùng kích thước? Để trả lời, ta có 2 ý chính như sau: Giảm số lượng tham số của mô hình (đặt tính là biết nhaa 😜) Dùng nhiều conv layer thì ta có khả năng sẽ phát hiện được nhiều feature có ích hơn (hai conv layer sẽ tạo thành một \u0026ldquo;hàm hợp\u0026rdquo;), từ đó decision function sẽ ok hơn. Ngoài ra, VGG block sử dụng padding 1 (giữ nguyên kích thước input), theo sau đó là max pooling với pool size $2 \\times 2$ và stride 2 (giảm kích thước input đi một nửa). Kiến trúc của nó có thể được mô tả như hình bên dưới:\nVGG block Nguồn: Dive intro AI Trong các bài toán áp dụng VGG, đôi khi ta có thể gặp VGG block với một conv layer $1 \\times 1$ ở trước max pooling. Block dạng này thường được sử dụng với mục đích chính là bổ sung thêm một phép biến đổi tuyến tính nữa. Tuy nhiên, trong thực nghiệm thì các tác giả đã cho thấy rằng việc sử dụng block dạng này không hiệu quả hơn so với toàn các conv layer với filter $3 \\times 3$ (cùng số lượng layer).\nKiến trúc VGG Bằng cách kết hợp nhiều VGG block với nhau, các tác giả đã tạo ra nhiều phiên bản VGG khác nhau, với số layer có trọng số là trong đoạn 11 - 19. Trong paper VGG, ta có 6 kiến trúc với độ sâu tăng dần và tiến hành so sánh với nhau. Điểm chung của các kiến trúc này là phần fully connected đều có 3 layer, và toàn bộ layer đều sử dụng activation ReLU.\nCác phiên bản VGG Ta có một số nhận xét như sau:\nSố lượng conv layer trong các VGG block của các phiên bản là tăng dần. Để ý đến B, C và D thì: C = B + conv layer $1 \\times 1$ trong mỗi VGG block D = C + đổi conv layer $1 \\times 1$ thành $3 \\times 3$ Khi thực nghiệm, ta có B \u0026lt; C \u0026lt; D. Do đó, việc thêm phép biến đổi tuyến tính bằng conv layer $1 \\times 1$ giúp mô hình hoạt động tốt hơn, nhưng nó vẫn không bằng với việc là ta thêm luôn conv layer $3 \\times 3$ 🤔. Độ rộng (số channel) trong từng block được tăng theo bội 2. Ý tưởng này được sử dụng rất rộng rãi trong thời điểm này và cả về sau Để hạn chế overfitting, ta có thể sử dụng thêm dropout cho hai tầng fully connected đầu tiên. Hai kiến trúc phổ biến nhất trong việc áp dụng VGG vào các bài toán khác là D (VGG16) và E (VGG19). Để trực quan hơn, ta có thể thể biểu diễn VGG16 như sau:\nNguồn: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png Cài đặt Các bạn có thể tham khảo phần cài đặt VGG bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"https://htrvu.github.io/post/vgg/","title":"VGG (2014)"}]
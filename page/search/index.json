[{"content":"Giới thiệu Ta biết rằng, việc tạo ra các mô hình có độ sâu lớn (nhiều layer) chưa chắc đã mang lại hiệu quả tốt hơn những mô hình “cạn” hơn. Ví dụ, với tập CIFAR10 thì ta có một kết quả thử nghiệm cho thấy rằng mô hình sâu hơn lại có độ hiệu quả kém hơn:\nĐối với việc huấn luyện các mô hình có độ sâu lớn thì ta có thể sẽ bị gặp phải các vấn đề sau:\nOverfitting Vanishing/exploding gradient. Vấn đề này thì ta đã có một số cách giải quyết phổ biến như thay đổi activation function, các phương pháp khởi tạo trọng số Đặc biêt hơn là vấn đề degradation: Accuracy tăng dần cho đến một độ sâu nhất định thì ngừng tăng (bão hòa) rồi sau đó sẽ giảm dần.\nLưu ý. Nhiều trường hợp degradation không phải do overfitting gây ra.\nDegradation cho chúng ta thấy rằng việc tối ưu các mô hình có độ sâu lớn là không hề dễ dàng.\nTrong quá trình xây dựng mô hình, từ một mô hình ban đầu, ta thêm một số layer vào thì tất nhiên là ta mong rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc. Tuy nhiên, khi xảy ra degradation thì mong muốn đó đã không thể thành sự thật được. 😀\nResNet được công bố nhằm giải quyết vấn đề degradation đối với các mô hình có độ sâu lớn. Khi nhắc đến ResNet thì ta sẽ lập tức nghĩ đến những mô hình với độ sâu rất khủng, thậm chỉ là lên đến 100, 200 layer.\nHàm phần dư và skip connection Nhắc lại về cái mong muốn ở trên, rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc, ta có thể nghĩ ngay đến một phương pháp cực kì đơn giản: các layer phía sau sẽ là identity mapping, tức là input và output của nó sẽ giống hệt nhau. Với cách làm này thì hiển nhiên là ta đạt được mong muốn rồi, vì độ hiệu quả của mô hình mới và mô hình gốc sẽ y hệt nhau.\nTuy nhiên, nếu chỉ muốn ngang đó thôi thì thêm layer vào làm gì :v Ta muốn đạt được kết quả tốt hơn! Các tác giả của paper ResNet giới thiệu một phương pháp gọi là deep residual learning (học phần dư).\nGiả sử ta có một block B các layer, input của nó là $x$. Với một mô hình thông thường, ta sẽ “học” một hàm số đầu ra mong muốn là $f(x)$. Lúc này, ban đầu thì ta hoàn toàn chưa có một thông tin gì về $f(x)$ cả, việc “học” sẽ xuất phát từ một đại lượng ngẫu nhiên. Đối với phương pháp deep residual learning, output của block B sẽ có dạng $h(x) = x + f(x)$, và ta sẽ đi học $f(x)$. Lúc này, ta gọi $f(x)$ là residual function (hàm phần dư) Ta có các nhận xét sau: Trong deep residual learning, ta đã có sự “gợi ý” cho hàm mong muốn thông qua giá trị input $x$. Đối với thông thường thì không có sự gợi ý nào, học từ một cái hoàn toàn ngẫu nhiên. Việc học $f(x)$ như là một hàm phần dư là dễ hơn so với việc học hàm mong muốn. Nếu identity mapping là kết quả tối ưu thì ta có luôn $f(x)=0$ Về mặt bản chất, với phương pháp deep residual learning, ta đã tác động vào lớp hàm chứa hàm mong muốn, sao cho lớp mới là lớn hơn (bao gồm) lớp cũ. Lớp hàm chứa hàm mong muốn của phương pháp thông thường và phương pháp deep residual learning Nguồn: Dive into AI Như vậy, điểm nhấn của ResNet là ta đi học các hàm phần dư, thông qua việc “gợi ý” cho hàm số mong muốn một giá trị bằng với giá trị input. Trong cài đặt, thao tác này được thực hiện thông qua một kết nối gọi là skip connection. Ta sẽ cộng input $x$ vào output của block thông thường.\nĐối với việc cộng như vậy thì thực chất là ta đang đi cộng hai ma trận. Khi đó thì có nảy sinh một vấn đề về kích thước của chúng, mà thường là số channel. Nếu số channel không khớp thì ta cần phải tiến hành “điều chỉnh”. Chi tiết về phần này sẽ được đề cập trong phần về Residual block. Vì các block được sử dụng thường giữ nguyên kích thước của input (width x height) nên ta chỉ nhắc đến số channel. Khi width và height được giữ nguyên thì skip connection được gọi là identity skip connection. Trong trường hợp width và height của output cũng khác input $x$ thì ta cần phải biến đổi $x$ trước. Phép biến đổi này thực chất là dùng conv layer $1 \\times 1$ để thay đổi số channel. Lúc này, skip connection được gọi là projection skip connection. Sử dụng skip connection (identity) Nguồn: Dive into AI Bây giờ, khi quan sát vào hình trên thì ta còn nhận thấy một lợi ích khác của skip connection là giúp cho gradient được lan truyền tốt hơn trong quá trình backpropagation, từ đó góp phần làm giảm hiện tượng vanishing gradient.\nResidual block Residual block (khối phần dư) được tạo ra bằng cách thêm một skip connection vào một block thông thường trong các mô hình CNN như VGG block. Ta có hai loại residual block như sau:\nBasic: Các conv layer trong block này có filter $3 \\times 3$. Block dạng này thường được dùng cho các mô hình có độ sâu vừa phải. Bottleneck: Nhằm giảm bớt số lượng tham số của các mô hình có độ sâu lớn, các tác giả sử dụng dạng block này với hai conv layer $1 \\times 1$ với vai trò là giảm/tăng số channel, tạo ra một hình dáng giống như nút thắt cổ chai. Hai layer conv $1 \\times 1$ như vậy được gọi là bottleneck layer. Để cho dễ hình dùng thì ta có thể xem đây như là thao tác “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán). Basic Residual Block và Bottleneck Residual Block Lưu ý. Có một chi tiết này khá quan trọng trong cài đặt là ta sẽ tiến hành cộng ma trận trước rồi mới đưa kết quả qua activation function.\nNgoài ra, ta còn có 2 loại skip connection là identity và projection. Lúc này, tùy vào số channels của input và output ban đầu có khớp hay không mà block tương ứng sẽ chứa loại skip connection phù hợp.\nBasic residual block với identity và projection skip connection Nguồn: Dive into AI Kiến trúc ResNet ResNet được tạo nên bằng cách sử dụng nhiều residual block liên tiếp nhau, tương tự như những gì mà GoogLeNet hay VGG đã thực hiện, Các tác giả của paper ResNet tạo ra nhiều phiên bản ResNet khác nhau với độ sâu tăng dần.\nDựa vào số lượng layer thì ta sẽ có tên các mô hình như ResNet18 (18 layer có trọng số), ResNet34,… Các phiên bản ResNet Ta có một số nhận xét như sau:\nCác phiên bản có độ sâu lớn như 50, 101 và 152 sử dụng Bottleneck Residual Block. Với 18 và 34 thì chúng dùng Basic block Trong các kiến trúc trên thì ta sử dụng cả 2 loại skip connection: identity và projection Ví dụ, với ResNet18 thì trong cụm Basic block đầu tiên của cụm conv3_x sẽ có số channel là 64, còn output ban đầu của block này thì là 128 nên ta phải áp dụng projection vào input Đối với height và width thì các tác giả cho biết việc downsampling input sẽ được thực hiện tại conv layer đầu tiên của các cụm conv3_x, conv4_x, conv5_x (với stride là 2). Các conv layer khác thì đều có stride 1. Ta sẽ cần chú ý đến chi tiết này khi tiến hành cài đặt ResNet. Cài đặt Các bạn có thể tham khảo phần cài đặt ResNet bằng Tensorflow và Pytorch tại repo sau\nTài liệu tham khảo Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into AI, ResNet ","date":"2023-02-08T17:41:09+07:00","permalink":"http://example.org/post/resnet/","title":"Resnet (2015)"},{"content":" Cá nhân mình thấy GoogLeNet là một paper khó đọc. Khi viết ra bài này thì mình vẫn đang cảm thấy hơi lú về nội dung của nó 😀\nGiới thiệu Từ khi AlexNet được công bố vào năm 2012 và đặt nền tảng cho các mạng Deep CNN, GoogLeNet, hay Inception V1 (2014), là một trong những kiến trúc có cách thiết kế rất thú vị khi nó tận dụng hiệu quả các conv layer, đặt nền móng cho nhiều mô hình sau này.\n“Inception” có thể dịch là “sự khởi đầu”, nghe có vẻ rất hợp lý 😀. Ngoài ra, ở trong bài viết về VGG, mình có đề cập đến vấn đề thiết kế kiến trúc mô hình theo hướng có sự lặp lại các khuôn mẫu. GoogLeNet cũng sẽ được thiết kế như vậy.\nLưu ý. Tên của mô hình này là GoogLeNet, không phải GoogleNet =)) Tác giả cho biết ý nghĩa của cái tên này là “This name is a homage to Yann LeCuns pioneering LeNet 5 network.”\nCác tác giả của paper GoogLeNet có tóm tắt các mục tiêu của nghiên cứu như sau:\nNâng cao khả năng tận dụng tài nguyên tính toán Cho phép tăng chiều rộng và chiều sâu của kiến trúc mô hình mà vẫn đảm bảo được độ phức tạp tính toán là ở mức chấp nhận được. GoogLeNet thật sự đã đạt được những điều đó, và nó được xây dựng dựa trên nguyên lý Hebbian: “neurons that fire together, wire together”. Paper GoogLeNet đã đưa ra các nền tảng lý thuyết rất “căng thẳng” để cho thấy rằng mô hình CNN có thể hoạt động “đủ tốt”, điều mà ta chưa được biết đến ở trong các paper trước đó 😀\nNgoài ra, có một quan điểm khá thú vị khi mô tả về kiến trúc của GoogLeNet như sau: Khi xây dựng kiến trúc CNN, thay vì phải suy nghĩ xem trong các mạng CNN ta nên áp dụng filter với kích thước bao nhiêu, hãy áp dụng luôn nhiều filter với kích thước khác nhau và tổng hợp kết quả lại 😜\nMình cũng có đồng tình với quan điểm, nhưng khi đọc paper thì có vẻ là các tác giả cũng đã tiến hành phân tích và thử nghiệm nhiều lắm mới cho ra được cái kiến trúc như vậy. Kết nối thưa và CNN Đầu tiên, tại thời điểm trước khi Inception được công bố, chúng ta có thể cải thiện một mô hình DNN bằng những cách như sau:\nTăng chiều sâu của mô hình (tức là số layer) Tăng chiều rộng (số channel trong mỗi layer) Tuy nhiên, với hai cách trên thì sẽ có những vấn đề mà ta cần lưu tâm là hiện tượng overfitting và sự gia tăng độ phức tạp của mô hình.\nCác tác giả có đề cập đến một hướng đi có thể giảm bớt hai vấn đề trên là sử dụng kiến trúc kết nối thưa (sparsely connected architectures). Ta có thể hiểu đơn giản như sau:\nVới hai fully connected layer liên tiếp nhau, mỗi neuron trong layer sau sẽ kết nối với tất cả các unit trong layer trước. Nếu như ta thay đổi đi một chút, mỗi neuron trong layer sau sẽ chỉ kết nối đến một vài unit trong layer trước, kiến trúc có được sẽ trở nên “thưa” hơn. Kết nối thưa trong fully connected layer Đối với conv layer thì ta đã đạt được tính chất thưa như vậy. Ta biết rằng, với mỗi phần tử trên feature map của layer hiện tại thì ta tính nó dựa vào một vùng nhỏ trên feature map output của layer trước đó. Giả sử hai layer này đều chỉ có 1 channel thì ta có thể biểu diễn nó như hình bên dưới: Kết nối thưa trong conv layer Kiến trúc thưa được các tác giả mô tả là mô phỏng lại hệ thống sinh học (ví dụ như khi ta nhìn vào một đối tượng thì ta thường chỉ chú ý một số điểm trên đối tượng đó thôi, khó mà chú ý tất cả được).\nNgoài ra, có một nền tàng lý thuyết rất trâu bò về kiến trúc thưa của Arora như sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Thật sự mình cũng chưa hiểu hết ý của câu trên… Nhưng đại ý của nó có thể là nếu ta biểu diễn được phân phối các điểm dữ liệu của một dataset bằng một kiến trúc DNN lớn và thưa thì từ đó ta có thể xây nên được một kiến trúc tối ưu (về cả độ chính xác và độ phức tạp), bằng cách xây từng layer một 😀\nTuy nhiên, với các tài nguyên phần cứng trong thời gian này thì rất khó để ta có thể tính toán trên các mạng DNN thưa. Do đó, các tác giả đi theo hướng tìm một mô hình là có tận dụng một số thông tin về tính chất thưa nhưng vẫn thực hiện tính toán trên các ma trận đầy đủ. Đấy chính là hướng sử dụng các phép toán convolution!.\nCũng vì lý do này, ở phần kiến trúc của GoogLeNet thì ta sẽ thấy nó chỉ có duy nhất một fully connected layer để sinh ra output, còn lại chỉ toàn conv layer thôi 😗 Nói đến việc áp dụng các filter trong conv layer, các tác giả cho rằng:\nMỗi phần tử trong feature map của một layer sẽ có mối tương quan với một vùng nào đó trên ảnh input (receptive field). Ta sẽ gom các phần tử cùng tương quan với một vùng vào cùng một cụm. Để ý rằng, trong những layer ở gần ảnh input thì ta sẽ có nhiều cụm và kích thước mỗi cụm là nhỏ. Càng đi qua các layer CNN thì số lượng cụm sẽ ít lại và kích thước cụm sẽ lan rộng ra. Tất nhiên là vẫn có thể có những cụm có kích thước nhỏ tại những layer đó. Do vậy, ta nên có các filter với kích thước lớn hơn để học các đặc trưng tại các cụm lớn, đồng thời cũng cần có filter kích thước nhỏ đối với các cụm nhỏ hơn. Qua một loạt các thử nghiệm, các tác giả đã chọn 3 filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Một khối chứa 3 filter trên được đặt tên là Inception module.\nBây giờ quay lại với lý thuyết của Arora, ở ý xây dựng mạng tối ưu qua từng layer một. GoogLeNet được tạo nên bằng đúng ý tưởng như vậy, ta nối Inception module - by - Inception module 😀\nP/s: Các bạn có thể tìm đọc phần Motivation trong paper gốc và tự cảm nhận nó nhé.\nInception module Inception module Qua các mô tả ở phần trên, ta có thể liên tưởng đến kiến trúc của Inception module là một thứ gì đó giống với cái ở hình (a), với đủ 3 loại filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Các tác giả dùng thêm cả một layer max pooling trong đó nữa, với lí do rất đơn giản là vì ở thời điểm đó thì max pooling thường mang lại hiệu quả tốt trong các kiến trúc mạng CNN =))\nTruy nhiên, cách cài đặt như hình (a) sẽ dẫn đến số lượng tham số của mô hình là rất lớn. Thay vào đó, ta có thể tạo ra kiến trúc dạng “bottleneck” bằng cách sử dụng thêm các layer convolution $1 \\times 1$ như hình (b), nhằm mục đích chính là giảm số channel. Số lượng phép tính lúc này sẽ được giảm một cách đáng kể.\nLưu ý rằng, ở cuối module ta có phép toán concatenate, tức là các feature-maps của toàn bộ layer convolution đều phải có cùng size (tức là width và height)\nKiến trúc GoogLeNet GoogLeNet sử dụng tổng cộng 9 Inception module, gồm có 22 layer có trọng số (tính cả pooling là 27), được tóm tắt qua bảng sau:\nMột điểm đặc biệt khi train GoogLeNet là các tác giả sử dụng các auxiliary classifiers (xem hình bên dưới). Các thành phần này sẽ khá giống như giống hệt với phần cuối của mô hình (bộ classifier). Như vậy, ta có thể xem GoogLeNet có 3 bộ classifier. Auxiliary classifiers được sử dụng với các ý nghĩa như sau:\nHạn chế vanishing gradient Tăng regularization Lưu ý:\nLoss khi huấn luyện sẽ cộng loss của cả ba lại với nhau. Khi test thì ta thường chỉ quan tâm đến bộ classifer cuối cùng. Một cách làm khác là ta xài cả 3, sau đó lấy kết quả trung bình. Nguồn: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png Cài đặt Các bạn có thể tham khảo phần cài đặt GoogLeNet bằng Tensorflow và Pytorch tại repo sau. Trong cách cài đặt này, mình sẽ bỏ qua auxiliary classifiers.\nTài liệu tham khảo Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"http://example.org/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giới thiệu Dựa trên sự thành công của AlexNet vào năm 2012, nhiều nghiên cứu đã được tiến hành nhằm tìm ra các phương pháp hay kiến trúc mới để đạt được kết quả tốt hơn, ví dụ như:\nThay đổi (tăng, giảm) kích thước của conv filter Thay đổi stride, padding của conv layer Train và test trên các input với nhiều kích thước ảnh khác nhau Trong năm 2014, VGG là một trong những kết quả nghiên cứu nổi bật nhất, và nó tập trung vào một vấn đề khác với các hướng trên là độ sâu (depth, hay là số layer) của mô hình. VGG đã đạt được các kết quả rất tốt trên tập ImageNet và các dataset khác, trong các task như classification, localization,…\nNgoài ra, ta có thể đưa ra nhận xét như sau về AlexNet:\nDù AlexNet đã chứng minh được CNN có thể đạt độ hiệu quả tốt, nó lại không cung cấp một khuôn mẫu nào cho việc nghiên cứu, thiết kế các mạng mới. Theo thời gian, các nhà nghiên cứu đã thay đổi suy nghĩ từ quy mô những neuron riêng lẻ sang các tầng, rồi sau đó là các khối gồm các tầng lặp lại theo khuôn mẫu. Kiến trúc của VGG là một trong những kiến trúc phổ biến đầu tiên được xây dựng theo tiêu chí như vậy.\nVGG block Điểm nổi bật của VGG là ta chỉ dùng duy nhất một kích thước filter trong mọi conv layer là 3 x 3, và ta dần tăng độ sâu của mô hình bằng các conv layer. Hơn nữa, ta còn áp dụng nhiều conv layer liền nhau rồi mới dùng đến max pooling (ta có thể gọi đây là VGG block)\nCác tác giả có đề cập đến một vấn đề cho cách áp dụng này như sau: Việc dùng nhiều conv layer 3 x 3 liền nhau như vậy so với dùng một conv layer với filter lớn hơn (ví dụ 7 x 7) như hầu hết các mô hình đã được công bố vào thời điểm trước đó thì có gì “tốt” hơn, khi mà features map sau cùng ta thu được có thể có cùng kích thước? Để trả lời, ta có 2 ý chính như sau: Giảm số lượng tham số của mô hình (đặt tính là biết nhaa 😜) Dùng nhiều conv layer thì ta có khả năng sẽ phát hiện được nhiều feature có ích hơn, từ đó decision function sẽ ok hơn. Ngoài ra, VGG block sử dụng padding 1 (giữ nguyên kích thước input), theo sau đó là max pooling với pool size 2 x 2 và stride 2 (giảm kích thước input đi một nửa). Kiến trúc của nó có thể được mô tả như hình bên dưới:\nVGG block Nguồn: Dive intro AI Trong các bài toán áp dụng VGG, đôi khi ta có thể gặp VGG block với một conv layer 1 x 1 ở trước max pooling. Block dạng này thường được sử dụng với mục đích chính là bổ sung thêm một phép biến đổi tuyến tính nữa. Tuy nhiên, trong thực nghiệm thì các tác giả đã cho thấy rằng việc sử dụng block dạng này không hiệu quả hơn so với toàn các conv layer với filter 3 x 3 (cùng số lượng layer).\nKiến trúc VGG Bằng cách kết hợp nhiều VGG block với nhau, các tác giả đã tạo ra nhiều phiên bản VGG khác nhau, với số layer có trọng số là trong đoạn 11 - 19. Trong paper VGG, các tác giả đã đưa ra 6 kiến trúc với độ sâu tăng dần và tiến hành so sánh với nhau. Điểm chung của các kiến trúc này là phần fully connected đều có 3 layer, và toàn bộ layer đều sử dụng activation ReLU.\nCác phiên bản VGG Ta có một số nhận xét như sau:\nSố lượng conv layer trong các VGG block của các phiên bản là tăng dần. Để ý đến B, C và D thì: C = B + conv layer 1 x 1 D = C + đổi conv layer 1 x 1 thành 3 x 3 Khi test, ta có B \u0026lt; C \u0026lt; D. Do đó, việc thêm phép biến đổi tuyến tính bằng conv layer 1 x 1 giúp mô hình hoạt động tốt hơn, nhưng nó vẫn không bằng với việc là ta thêm luôn conv layer 3 x 3. Độ rộng (số channel) trong từng block được tăng theo bội 2. Ý tưởng này được sử dụng rất rộng rãi trong thời điểm này và cả về sau Để hạn chế overfitting, ta có thể sử dụng thêm dropout cho hai tầng fully connected đầu tiên. Hai kiến trúc phổ biến nhất trong việc áp dụng VGG vào các bài toán khác là D (VGG16) và E (VGG19). Để trực quan hơn, ta có thể thể biểu diễn VGG16 như sau:\nNguồn: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png Cài đặt Các bạn có thể tham khảo phần cài đặt VGG bằng Tensorflow và Pytorch tại repo sau\nTài liệu tham khảo Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"http://example.org/post/vgg/","title":"VGG (2014)"}]
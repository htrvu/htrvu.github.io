[{"content":"Giới thiệu Ta biết rằng, hầu hết các mô hình CNN thường được xây dựng từ một phiên bản ban đầu (có thể là dựa theo một nguồn tài nguyên nào đó), sau đó chúng được scale dần lên để đạt được độ chính xác tốt hơn, và tất nhiên là độ phức tạp cũng tăng theo\nVí dụ: Với ResNet thì ta có ResNet18 cho đến ResNet152, DenseNet thì DenseNet121 cho đến 201, MobileNet thì ta có siêu tham số width multiplier để điều chỉnh số channel trong từng layer và resolutiom multiplier để điều chỉnh kích thước tại các layer,… Những cách làm đó gọi là model scaling. Tuy nhiên, ta nhận thấy rằng những thao tác model scaling trước đó chỉ tập trung vào một trong 3 yếu tố: depth - $d$ (số layer), width - $w$ (số channel) và resolution - $r$. Hơn nữa, việc điều chỉnh cũng không theo một nguyên tắc nào mà còn mang đậm tính chất ngẫu nhiên, “hên xui”, cần phải thử nghiệm rất nhiều lần mới có thể đạt được một độ chính xác mong muốn. Khi đó thì số lượng tham số của các mô hình cũng tăng chóng mặt!\nCác tác giả của paper đã cho thấy kết quả thực nghiệm rằng việc điều chỉnh một trong 3 yếu tố có thể tăng độ chính xác nhưng chỉ tăng đến một mức nào đó thôi, sau đó nó sẽ bị bão hòa. Ví dụ như ở hình bên dưới:\nTa có thể đưa ra các nhận xét như sau:\nNếu mô hình có width lớn (mỗi layer có nhiều channel) thì nó có thể học được nhiều loại đặc trưng khác nhau. Nhưng nếu mô hình không đủ sâu thì các đặc trưng đó cũng chưa phải đặc trưng ở mức high-level (nổi bật cho đối tượng) Nếu mô hình có depth lớn thì nó có thể học được các đặc trưng high-level nhưng nếu không có width lớn thì cũng không học được nhiều loại đặc trưng* Về mặt trực giác, nếu ta đưa vào mô hình một bức ảnh có resolution cao thì mô hình nên có depth lớn để có thể dần học các đặc trưng từ các feature maps có resolution lớn, đồng thời cũng vì sẽ có nhiều đặc trưng hơn nên ta cần width lớn. Do đó, model scaling nên tập trung vào việc điều chỉnh đồng thời cả 3 yếu tố $d$, $w$, $r$. Paper công bố EfficientNet có tên là “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”. Các tác giả tập trung vào việc đi tìm một phương pháp model scaling hiệu quả, có nguyên tắc, điều chỉnh đồng thời cả 3 yếu tố như đã đề cập. Phương pháp được giới thiệu có tên là compound scaling.\n\u0026ldquo;Nguyên tắc\u0026rdquo; trong phương pháp này rất đơn giản, ta sẽ cùng điều chỉnh $d$, $w$, $r$ của toàn bộ network theo cùng một hệ số gọi là compound coefficient (kí hiệu là $\\phi$).\nMinh họa cho các phương pháp model scaling. (a) là mô hình ban đầu. (b)-(d) thực hiện điều chỉnh một trong ba yếu tố. (e) là phương pháp được đề xuất, nó tiến hành điều chỉnh cả ba. Tất nhiên là model scaling chỉ phát huy tác dụng khi mà mô hình ban đầu là đủ tốt. Từ phương pháp compound scaling, các tác giả đã áp dụng nó cho ResNet, MobileNet để chứng tỏ độ hiệu quả của phương pháp. Sau đó, một họ mô hình mới được đề xuất là EfficientNet, vớ 8 phiên bản từ B0 đến B7 với độ phức tạp và độ chính xác tăng dần trên tập ImageNet. EfficientNet-B7 đã trở thành SOTA (state-of-the-art) với độ phức tạp nhỏ hơn rất nhiều lần so với mô hình SOTA trước đó.\nBài toán model scaling Giả sử conv layer thứ $i$ được định nghĩa là hàm số $Y_i = F_i(X_i)$, với input $X_i$ có shape là $\\left (H_i, W_i, C_i \\right)$. Khi đó, một CNN $N$ có thể được biểu diễn là\n$$ N = F_k \\bigodot F_{k-1} \\bigodot \\cdots F_1(X_1) = \\bigodot_{j=1,\u0026hellip;,k} F_j(X_1) $$\nThông thường, các mạng CNN thường được xây dựng theo kiểu gồm nhiều giai đoạn, mỗi giai đoạn là sự lặp lại các block có cùng dạng cấu trúc, chỉ khác nhau một số chi tiết như số layer trong block, kích thước của filter,… Ví dụ, ResNet được xây dựng dựa trên các residual block, MobileNet thì là các depthwise separable block,… Do đó, ta có thể viết lại $N$ thành\n$$ N = \\bigodot_{i=1,\u0026hellip;,s} F_i ^ {L_i}(X_{(H_i, W_i, C_i}) $$\n, với $F_i$ là layer được lặp lại $L_i$ lần trong giai đoạn thứ $i$, với input là $X$ có shape $(H_i, W_i, C_i)$.\nBài toán model scaling sẽ cố định layer $F_i$ và đi điều chỉnh các giá trị $L_i, H_i, W_i, C_i$, sao cho mô hình thỏa mãn các ràng buộc về tài nguyên và đạt độ chính xác cao nhất có thể.\nĐiều chỉnh $L_i$ $\\Leftrightarrow$ Điều chỉnh depth Điều chỉnh $C_i$ $\\Leftrightarrow$ Điều chỉnh width Đều chỉnh $H_i, W_i$ $\\Leftrightarrow$ Điều chỉnh resolution Để giảm không gian tìm kiếm, ta sẽ điều chỉnh các giá trị trên của toàn bộ layer trong mô hình theo cùng một tỉ lệ. Khi đó, bài toán của ta là bài toán tối ưu như sau:\n, với $d, w, r$ là hệ số để điều chỉnh depth, width, resolution; $\\hat{F_i}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i}$ là các giá trị ban đầu của mô hình baseline.\nPhương pháp compound scaling Phương pháp này sử dụng compound coefficient $\\phi$ để điều chỉnh depth, width, resolution theo nguyên tắc như sau:\nVới mô hình baseline ban đầu, ta thực hiện grid search để tìm ra bộ 3 giá trị tỉ lệ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tốt nhất có thể, sao cho\n$$ \\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\text{ và } \\alpha \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 $$\nSau đó, ta sẽ scale mô hình lên theo hệ số $\\phi$ với\n$$ d = \\alpha ^ \\phi, ; w = \\beta^\\phi, ; r = \\gamma^\\phi $$\nVề mặt trực giác, ta có thể xem $\\phi$ như là cách mà chúng ta cho biết lượng tài nguyên dành cho model scaling là bao nhiêu, còn các giá trị $\\alpha, \\beta, \\gamma$ là cách chúng ta phân phối tài nguyên đó cho depth, width và resolution. Giải thích cho các ràng buộc cho $\\alpha, \\beta, \\gamma$ được trình bày như sau:\nTất nhiên là để thực hiện được việc scale mô hình lên thì giá trị của chúng phải không nhỏ hơn 1 Ngoài ra, một phép toán convolution sẽ có độ phức tạp tỉ lệ thuận với $d, w^2, r^2$. Do đó, nếu ta scale model lên theo hệ số $\\phi$ thì độ phức tạp sẽ tăng lên một lượng bằng $(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2)^\\phi$. Các tác giả mong muốn độ phức tạp tăng khoảng $2^\\phi$, do đó ta có ràng buộc $\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2$. Họ mô hình EfficientNet Neural architecture search EfficientNet là một họ các mô hình rất đặc biệt:\nThứ nhất, chúng được xây dựng bằng “máy” 😀 Vào năm 2017, một ma thuật đã được công bố trong paper “Neural architecture search with reinforcement learning” của chính tác giả Quoc V. Le, nó giúp chúng ta xây một kiến trúc phù hợp nhất có thể dựa theo độ chính xác, độ phức tạp mà chúng ta yêu cầu.\nVới họ EfficientNet, các tác giả tập trung vào việc giới hạn độ phức tạp (cụ thể là FLOPS). Mục tiêu tối ưu của reinforcement learning là\n$$ ACC(m) \\times \\left ( \\frac{FLOPS(m)}{T} \\right )^w $$\n, với $m$ là mô hình, $ACC$ và $FLOPS$ là độ chính xác và độ phức tạp, $T$ là FLOPS mong muốn và nó bằng $400 \\times 10^6$, $w=-0.07$ là hằng số điều chỉnh trade-off giữa $ACC$ và $FLOPS$\nThứ hai, từ một mô hình ban đầu là EfficientNet-B0, ta tiến hành scale theo 2 bước:\nBước 1: Cố định $\\phi = 1$, giả sử lượng tài nguyên mà ta có thể sử dụng là nhiều gấp đôi hiện tại. Khi đó, thực hiện grid search để tìm các giá trị $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tốt nhất Bước 2: Từ các giá trị $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tìm được, tiến hành scale theo các giá trị $\\phi$ lớn hơn để có được các phiên bản B1-B7. Ở đây, ta hoàn toàn có thể tăng $\\phi$ lên rồi lại grid search nhưng lúc này chi phí thực hiện là rất lớn. Do đó, các tác giả chỉ grid search một lần rồi sau đó chỉ cần tăng $\\phi$.\nKiến trúc mô hình Đầu tiên, tổng quan kiến trúc của EfficientNet-B0 như sau:\nTrong đó: MBConv chính là inverted residual block trong MobileNetV2, cùng với một số cải tiến như trong paper “Squeeze-and-excitation networks” Các mô hình EfficientNet-B1 cho đến B7 chính là kết quả của việc áp dụng compound scaling lên EfficientNet-B0.\nTài liệu tham khảo Paper EfficientNet: https://arxiv.org/abs/1905.11946 ","date":"2023-02-15T11:17:46+07:00","permalink":"https://htrvu.github.io/post/efficientnet/","title":"EfficientNet (2020)"},{"content":"Giới thiệu Từ sự thành công của MobileNet (2017) trong việc triển khai các mô hình Deep Learning trên các thiết bị biên (smartphone, embedded,…) nhờ vào việc sử dụng hiệu quả phép toán depthwise separable convolution, nhiều nghiên cứu dựa trên hướng phát triển này đã được tiến hành.\nDựa theo các “kinh nghiệm” có được của bản thân, nhìn vào MobileNet thì ta sẽ thấy ngay rằng, nó chưa có cái skip connection nào cả 😀 Đúng z, skip connection đã cho thấy được sự hiệu quả của mình trong các mô hình như ResNet, Inception-ResNet, DenseNet,… tại sao ta không thử thêm vào MobileNet? Boom, thêm ngay!\nMobileNetV2 được công bố với sự kế thừa từ MobileNet và bổ sung thêm skip connection. Tất nhiên là không chỉ dừng ở đó 🙂 Các tác giả xây dựng MobileNetV2 dựa trên các inverted residual block, nơi mà các skip connection dùng để kết nối các bottleneck layer với nhau! Hơn nữa, ta còn có một điểm rất thú vị là các bottleneck layer này sử dụng activation function là linear!\nMobileNetV2 đạt được độ chính xác cao hơn MobileNet trên tập ImageNet, với số tham số ít hơn, lượng bộ nhớ cần dùng tại mỗi layer là ít hơn. Từ sự ra đời của mô hình này, người ta cũng đã phát triển các mô hình hiệu quả trong bài toán Object Detection như SSDLite, hay Semantic Segmentation như Mobile DeepLabv3\nBàn về ReLU Ta biết rằng từ khi paper AlexNet giới thiệu activation function ReLU thì nó đã trở thành một activation function rất phổ biến và được dùng thường xuyên trong các mô hình Deep Learning với điểm mạnh quan trọng là đạo hàm của nó rất đơn giản. Công thức của ReLU là\n$$ReLU(x) = \\max(x, 0)$$\n, tức là nó sẽ “vứt” những giá trị bé hơn 0 trong input. Điều này nghĩa là ta sẽ bị mất thông tin!. Nếu input truyền vào là một channel thì ta sẽ bị mất một lượng thông tin nhỏ (hoặc có thể là lớn) trên channel đó.\nActivation function ReLU Nguồn: Research Gate Vậy tại sao trước giờ ReLU vẫn luôn được sử dụng?\nĐiều quan trọng là chúng ta có rất nhiều channel và giữa các channel này có những mối liên hệ nhất định. Do đó, việc mất thông tin ở một channel này có thể được channel khác bù đắp. Như vậy là ok. Để minh họa cho yếu tố làm mất thông tin, các tác giả đưa ra ví dụ sau:\nBan đầu, input của ta ở không gian 2 chiều. Qua phép biến đổi bằng một ma trận $T$ bất kì và áp dụng ReLU, ta có các output ở các không gian có số chiều khác nhau là 2, 3, 5, 15, 30. Để xác định xem thông tin có bị mất hay không, ta chiếu các output này về lại không gian 2 chiều bằng cách dùng ma trận nghịch đảo $T^{-1}$. Khi đó, kết quả thu được là các hình tương ứng ở trên. Rõ ràng là tính chất ban đầu của input đã bị mất. Gỉa sử từ một input $D_F \\times D_F \\times M$, qua một số layer thì ta có output $D_F \\times D_F \\times N$ và ta chuẩn bị áp dụng ReLU cho output. Các tác giả chứng minh được rằng ReLU sẽ không làm mất thông tin ban đầu của input nếu như $N \u0026lt; M$. Điều này có thể phát biểu bằng lời là nếu một input có thể được embedded (hay là nén) vào một không gian ít chiều hơn (số channel ít hơn) thì việc áp dụng ReLU lên kết quả nén đó sẽ không làm mất thông tin.\nỞ ví dụ phía trên thì ta đã có $N \\geq M$ và thông tin thật sự là đã bị mất. Từ nhận xét trên, ta thấy rằng không phải lúc nào xài ReLU cũng tốt. Nếu ngẫm lại, trong các kiến trúc như VGG, ResNet, Inception, MobileNet thì số channel của chúng hầu như luôn tăng qua từng block (chính là cụm “một số layer”) nhưng activation function được sử dụng luôn là ReLU. Điều này là vì chúng có rất nhiều channel (tăng theo bội 2) nên mọi thứ vẫn ổn 😀\nNếu bạn thắc mắc là vì sao số channel thường tăng như vậy thì trong CNN, những conv layer đầu thường sẽ học những đặc trưng đơn giản như cạnh ngang, dọc, chéo, vị trí của đối tượng trong ảnh,… càng về sau thì sẽ có các đặc trưng cụ thể, nổi bật lên của đối tượng (ví dụ như tai mèo, mắt mèo, mũi mèo,…). Do đó, càng về sau thì ta nên có càng nhiều channel đễ học được nhiều đặc trưng. Linear bottleneck Đầu tiên, ta sẽ nhắc lại về bottleneck layer. Đây là dạng layer thường được dùng với mục đích là “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán).\nNhững phát hiện về ReLU như đã đề cập là nguồn gốc của các linear bottleneck được sử dụng trong MobileNetV2.\nTại sao lại là linear mà không tiếp tục dùng ReLU rồi tăng số channel như những block trước?\nViệc giảm số channel trong các layer sẽ giảm lượng tham số của mô hình, từ đó giảm được độ phức tạp tính toán. Do đó, nếu xây dựng được một kiến trúc mà số lượng channel trong mỗi layer là nhỏ thì nó sẽ rất phù hợp cho các thiết bị biên. Các tác giả hướng đến việc giữ cho số channel của input và output của các block là nhỏ, tức là số channel chưa chắc đủ nhiều để đảm bảo rằng ReLU không làm mất thông tin 😜.\nNếu mà toàn các output có channel nhỏ như vậy thì làm sao mà mô hình đạt hiệu quả được? Lí do là vì nó là output của các bottleneck nên vẫn ok 😀 Số channel trong các layer của MobileNetV2\nNguồn: Machine Think Inverted residual block và expansion factor Inverted residual block là thành phần chính xây dựng nên MobileNetV2. Trong block này, ta sẽ áp dụng cả dethwise separable convolution, linear bottleneck và skip connection.\nTuy nhiên, lưu ý rằng số channel của input và output của các block này là rất nhỏ. Qua hình ở trên thì ta thấy chúng chỉ quanh quẩn 16, 24, 32. Do đó, trước khi áp dụng depthwise separable convolution lên input thì các tác giả thực hiện giải nén (expansion) lượng kiến thức trong input (input này là output của một block trước đó, nơi mà kiến thức đã được nén lại bởi linear bottleneck).\nVề mặt trực giác, lí do của thao tác này có thể hiểu là ta sẽ thực hiện convolution trên thông tin đầy đủ hơn để có thể phát hiện được càng nhiều đặc trưng càng tốt).\nViệc giải nén thực chất là ta sử dụng một conv layer $1 \\times 1$, với số lượng filter sẽ bằng với số lượng channel của input nhân với một siêu tham số. Siêu tham số này gọi là expansion factor và được kí hiệu là $t$.\nTiếp đến, theo sau phép toán depthwise separable convolution thì ta sẽ sử dụng linear bottleneck để tính ra output của block.\nNhư vậy, điểm qua các layer sẽ có trong inverted residual block sẽ bao gồm:\nLưu ý. Layer “linear 1x1 conv2d” chính là linear bottleneck. Tùy theo giá trị số channel $k$ và $k\u0026rsquo;$ có bằng nhau hay không mà ta sẽ áp dụng thêm skip connection. Hơn nữa, các skip connection trong inverted residual block được dùng để nối các bottleneck layer với nhau!\nVÌ sao lại là nối bottleneck chứ không phải nối các layer khác? Output của bottleneck là các “kiến thức” đã được cô đọng, đây là những gì mà mô hình đã học được và được biểu diễn trong một không gian ít chiều hơn. Do đó, ta vừa tiết kiệm được tài nguyên và vừa liên kết được các kiến thức quan trọng với nhau. Trong cài đặt, tùy theo giá trị stride của depthwise conv layer mà ta sẽ áp dụng skip connection hoặc là không. Cụ thể như sau:\nInverted Residual Block với stride=1 (có skip connection) và stride=2 (không có) Ngoài ra, ta có thể thấy trong inverted residual block thì activation được dùng cho 2 layer đầu tiên là ReLU6. Đây là một biến thể của ReLU, nó giới hạn giá trị output nằm trong đoạn $[0, 6]$ nhằm đảm bảo sự ổn định trong tính toán với số chậm động.\nActivation function RELU6\nNguồn: Mmuratarat Để dễ hình dung hơn về inverted residual block, ta cùng xem một ví dụ cho quá trình tính toán với expansion factor là 6:\nNguồn: Machine Think Lưu ý.\nCác tác giả có đề cập thêm đến luồng truyền thông tin của MobileNetV2, yếu tố mở ra những hướng phát triển tiếp theo trong tương lai. Ta thấy rằng inverted residual block đã tạo ra được sự độc lập giữa số channel của intput/output của block và của các layer nằm bên trong block:\nPhần bên trong được gọi là layer transformation với những phép biến đổi phi tuyến. Ta hoàn toàn có thể nghiên cứu thêm những cách xây dựng bộ phận này để tăng độ hiệu quả của mô hình. Nếu expansion factor của ta \u0026lt; 1 thì block này sẽ rất giống với block trong ResNet:\nKiến trúc MobileNetV2 MobileNetV2 được xây dựng dựa trên việc sử dụng nhiều inverted residual block. Kiến trúc tổng quan của nó như sau:\nTrong đó:\n$t$ là expansion factor. $c$ là số output channel của phần bottleneck trong inverted residual block. $n$ là số lần sử dụng block. $s$ là stride của block đầu tiên trong dãy $t$ block liên tiếp nhau, các block còn lại trong dãy có stride 1. Toàn bộ filter được sử dụng đều là $3 \\times 3$. Cài đặt Các bạn có thể tham khảo phần cài đặt MobileNetV2 bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper MobileNetV2: https://arxiv.org/abs/1801.04381 MachineThink, MobileNet version 2 ","date":"2023-02-13T11:32:55+07:00","permalink":"https://htrvu.github.io/post/mobilenet_v2/","title":"MobileNet V2 (2019)"},{"content":"Skip connection và concatenate Trước đó, kiến trúc ResNet được công bố và nó đã cho thấy được sức mạnh của các skip connection khi chúng được thêm vào các mô hình từ sâu cho đến rất sâu (ví dụ như ResNet152). Ta thấy rằng những kiến trúc áp dụng skip connection trước đây đều có một điểm chung là trong một block thì ta sẽ có những điểm nối 1 feature map vào làm input của một layer sau đó, và chúng đều sử dụng phép toán cộng.\nResidual block trong ResNet sử dụng skip connection với phép toán cộng Nguồn: Idiot Developer Công thức về skip connection trong block trên có thể được viết như sau:\n$$ x_l = H_l(x_{l - 1}) + x_{l-1} $$\n, với $H_l$ là phép biến đổi phi tuyến ở layer thứ $l$, $x_l$ là output của layer thứ $l$.\nPaper DenseNet giới thiệu một kiến trúc với ý tưởng là feature-map tại layer $l$ sẽ sử dụng toàn bộ feature-maps ở phía trước (layer $l - 1, l - 2,\u0026hellip;$) để làm input, và chúng sử dụng concatenate (thay vì phép toán cộng như ResNet). Với tư tưởng như vậy, các feature-maps ta có được có thể xem là một trạng thái có phạm vi toàn cục và bất kì layer nào cũng có thể sử dụng trạng thái này trong việc tính toán ra feature-maps của nó. Nếu viết theo kiểu công thức thì ta sẽ có\n$$ x_l = H_l([x_0, x_1,\u0026hellip;, x_{l-1}]) $$\nLưu ý. Để thực hiện phép toán concatenate thì các feature-maps phải có cùng size, hay là width và height.\nMột ví dụ cho kiến trúc DenseNet như sau:\nTrong hình trên, với $L$ layer, ta có $\\dfrac{L(L+1)}{2}$ kết nối trực tiếp giữa các layer. Các kết nối được tạo ra là rất dày đặc (dense). Từ đó, tên của kiến trúc được đặt là Dense Convolutional Network (DenseNet). Bàn về cách tổ chức các liên kết như vậy một chút:\nNhóm tác giả cho rằng kiến trúc như DenseNet sẽ đảm bảo lượng thông tin cũng như gradient truyền qua các layer là nhiều nhất có thể , từ đó mô hình sẽ có thể học được từ nhiều thông tin hơn, và tất nhiên là nó sẽ tạo ra hiệu ứng làm dịu bớt hiện tượng vanishing gradient.\nĐồng thời, việc sử phép toán concatenate có mang đến cho ta trực giác là có sự phân biệt rõ hơn giữa input trực tiếp từ layer ở ngay phía trước nó với các thông tin được “lưu trữ” và truyền đến từ các layer ở phía trước nữa. Nếu sử dụng phép toán cộng, những yếu tố này đã bị pha lẫn vào nhau.\nCó một chi tiết mà ta thường nghĩ đến ở các mô hình có kiến trúc rất sâu (nhiều layer) là số lượng tham số của nó sẽ rất lớn. Tuy nhiên, với DenseNet thì điều này không phải là vấn đề. Số feature-maps của các layer trong DenseNet sẽ rất nhỏ (chỉ tầm không quá 60), với lý do là để tính toán cho layer kế tiếp thì ta đã dùng toàn bộ feature-maps ở các phía trước rồi chứ không phải chỉ mỗi layer liền trước nó như hầu hết các mô hình khác, nên tại mỗi layer ta chỉ cần tầm đó là đủ rồi 😀\nKết quả so sánh giữa DenseNet và ResNet trên dataste ImageNet được các tác giả công bố như hình bên dưới. Ta thấy rằng DenseNet có số lượng tham số và số phép toán ít hơn ResNet, cùng với độ hiệu quả cao hơn.\nDense block, transition layer và growth rate Dense block và transition layer Ta thấy rằng nếu áp dụng ý tưởng kết nối dày đặt của DenseNet cho toàn bộ layer trong mô hình thì toàn bộ feature-maps trong tất cả layer này đều phải có cùng size (do phép toán được sử dụng là concatnerate).\nTuy nhiên, nếu toàn bộ các layer trong kiến trúc đều có cùng size như vậy thì ta khó mà down-sampling feature-maps về các size nhỏ hơn và rồi sau đó sử dụng các layer như Average Pooling, Dense để cho ra output như các kiến trúc khác được. Và việc “cô đọng” kiến thức của mô hình cũng sẽ gặp khó khăn.\nDo đó, ý tưởng kết nối dày đặt được các tác giả áp dụng trong từng khối (gọi là Dense block), việc down-sampling sẽ được thực hiện trong các khớp nối các Dense block với nhau (gọi là Transition layer).\nCó tổng cộng 4 dạng Dense block như sau:\nDense block cơ bản:\nHàm $H_l$ trong block này là sự kết hợp theo thứ tự 3 phép toán: $$ BN \\to ReLU \\to Conv (3 \\times 3) $$\nNgoài ra, ta có thể thêm dropout vào sau Conv để giảm overfitting.\nDense-B block (bottleneck):\nĐể tăng hiệu suất về mặt tính toán, ta có thể thêm một phép toán Conv $1 \\times 1$ vào $H_l$ để giảm bớt số lượng feature-maps input. Lúc này, thứ tự các phép toán sẽ là $$ BN \\to ReLU \\to Conv (1 \\times 1) \\to BN \\to ReLU \\to Conv (3 \\times 3) $$\nDense-C block (compression):\nTa sẽ giảm số lượng output feature-maps của các Dense block theo tham số $0 \u0026lt; \\theta \\leq 1$: từ $m$ feature-maps thành $\\lfloor \\theta m \\rfloor$ Thông thường, phần cài đặt của thao tác compression được ghép vào transition layer. Dense-BC block:\nKết hợp bottleneck và compression vào Dense block. Trong kiến trúc tổng thể, nếu trước đó ta dùng Dense-C hoặc Dense-BC block thì theo sau nó sẽ có thêm layer bottleneck (Conv $1 \\times 1) và thành phần này gọi là transition layer. Bên cạnh conv layer, thành phần không thể thiếu trong transition layer là một lớp Pooling (các tác giả sử dụng Average Pooling) để thực hiện down-sampling các feature-maps. Thứ tự các phép toán trong transition layer sẽ là\n$$ BN \\to ReLU \\to Conv (1 \\times 1) \\to AvgPool (2 \\times 2) $$\nGrowth rate Như đã đề cập ở phần ý tưởng, lượng tham số trong DenseNet được tối thiểu hóa là nhờ vào chi tiết số lượng feature-maps tại các layer trong DenseNet là nhỏ. Các tác giả xem số lượng feature-maps $k$ tại các layer là một siêu tham số của DenseNet, và nó được gọi là growth rate.\nThực nghiệm cho thấy rằng các giá trị $k$ mang lại kết quả tốt trên các dataset thường không quá lớn. Về mặt trực giác, ta có thể hiểu $k$ điều chỉnh lượng thông tin mới mà một layer có thể đóng góp vào trạng thái toàn cục (đóng góp một lượng vừa đủ thì sẽ tốt hơn là quá nhiều hay quá ít).\nMinh họa Dense Block với growth rate là 4 Nguồn: https://reliablecho-programming.tistory.com/3 Kiến trúc DenseNet Tùy vào loại Dense block được sử dụng, ta cũng có các tên gọi khác nhau cho DenseNet (DenseNet, DenseNet-B, Denset-C, DenseNet-BC). Kiến trúc DenseNet-C (hoặc DenseNet-BC) với 3 Dense block được mô tả trong hình bên dưới: Trước khi đến với quá trình tính toán qua các Dense block và Transition layer, ta có một layer Conv (và có thể có thêm Pooling, BN) như đa số các kiến trúc CNN khác. Các layer của của mô hình cũng sử dụng Global Pooling và Dense cùng activation softmax để tạo ra vector output. Các mô hình được nhóm tác giả thử nghiệm với dataset ImageNet được tóm tắt như sau:\nQuan sát bảng trên, ta có nhận xét là các mô hình trên đều thuộc loại DenseNet-BC. Ngoài ra, các tác giả cho biết giá trị growth rate được sử dụng là $k=32$.\nCài đặt Các bạn có thể tham khảo phần cài đặt DenseNet bằng Tensorflow và Pytorch tại repo sau.\nKhi cài đặt DenseNet, ta thường sẽ hơi phân vân về cách cài đặt các Dense block. Làm sao để cài đặt các kết nối dày đặc như vậy?\nThực ra cách cài đặt là rất đơn giản. Ta gọi khối gồm (Conv $1 \\times 1$, Conv $3 \\times 3$) như bảng trên là bottleneck block (bb). Khi đó 1 dense block với 4 bottleneck block sẽ có dạng như sau:\n$$x \\to bb_1 \\to x_1 \\to bb_2 \\to x_2 \\to bb_3 \\to x_3 \\to bb_4 \\to x_4 (output) $$\nTrong đó:\nbb_1.input = [ x ] bb_2.input = [x1, x] bb_3.input = [x2, x1, x] bb_4.input = [x3, x2, x1, x] Mã giả cho cách cài đặt dense block này như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def bottleneck_block(input, k): x = BN(input) x = ReLU(x)\tx = conv_1x1(x) x = BN(x) x = ReLU(x) x = conv_3x3(x) return x def dense_block(input, k): c = input x1 = bottleneck_block(c, k) c = concatenate(x1, c) # c = [x1, input] x2 = bottleneck_block(c, k) c = concatenate(x2, c) # c = [x2, x1, input] x3 = bottleneck_block(c, k) c = concatenate(x3, c) # c = [x3, x2, x1, input] x4 = bottleneck_block(c, k) c = concatenate(x4, c) # c = [x4, x3, x2, x1, input] return c # done! Như vậy, ta hoàn toàn có thể dùng một vòng lặp để cài đặt dense block:\n1 2 3 4 5 6 def dense_block(input, k): c = input for i in range(4): x = bottleneck_block(c, k) c = concatenate(x, c) return c Tài liệu tham khảo Paper DenseNet: https://arxiv.org/abs/1608.06993 ","date":"2023-02-11T18:09:08+07:00","permalink":"https://htrvu.github.io/post/densenet/","title":"DenseNet (2018)"},{"content":"Giới thiệu Class Activation Map (CAM) là phương pháp phổ biến trong việc giải thích sự hoạt động của CNN. Nó cho ta biết rằng CNN sẽ tập trung vào những phần nào của ảnh input để dự đoán xác suất ảnh đó thụôc về một class nào đó. Thông thường, CAM còn được gọi là Attention Map.\nĐể dễ hình dung hơn về CAM, ta có 2 ví dụ như sau:\nCNN tập trung vào phần đầu của con chó để đưa ra xác suất mà bức ảnh thuộc class “dog” CNN tập trung vào phần đầu của con chó để đưa ra xác suất mà bức ảnh thuộc class “dog”\nCNN tập trung vào con mèo khi đưa ra xác suất mà bức ảnh thuộc class “cat”\nNguồn: GlassBoxMedicine Các phương pháp này được xếp vào nhóm post-hoc, tức là ta chỉ tiến hành sinh ra CAM để giải thích sự hoạt động của CNN sau khi mô hình này đã được huấn luyện và có một bộ trọng số cố định.\nViệc giải thích CNN bằng CAM là rất hợp lý, vì:\nTa có thể biết được mô hình của mình có đang thật sự hoạt động tốt hay không (tập trung vào đúng phần quan trọng trong ảnh), tức là có chứng cứ rõ ràng cho các dự đoán Nó giúp ta kịp thời phát hiện những đặc trưng mà mô hình “hiểu lầm” khi học về một đối tượng nào đó. Ví dụ, nhờ CAM thì ta thấy rằng mô hình học cách nhận dạng tàu hỏa dựa vào các đường ray trong ảnh thì rõ ràng là nó đã học sai đặc trưng. Trường hợp này hoàn toàn có thể xảy ra vì phần lớn bức ảnh có tàu hỏa thì cũng có đường ray, nhưng ngược lại thì không. Ta có thể có một chiếc xe ô tô chạy ngang đường ray 😀 Trong các phương pháp sinh ra CAM cho một CNN (theo từng input) thì ta có các phương pháp nổi bật như Default CAM, Grad-CAM, Score-CAM. Ta sẽ lần lượt đề cập đến các phương pháp đó.\nCác phương pháp sinh CAM Default CAM (2016) Phương pháp này được đề xuất ngay từ khi các ý tưởng về CAM được công bố. Ta sẽ dựa vào output của conv layer cuối cùng trong kiến trúc, ngay trước fully connected layer sinh ra output của mô hình và các trọng số trong output layer.\nĐầu tiên, để mô tả về ý nghĩa của các feature maps trong output của conv layer cuối cùng thì ta xét ví dụ sau: Trong hình ảnh bên dưới, conv layer cuối cùng của ta là “Conv Layer n”. Output của nó có $k$ channel, hay là $k$ feature maps, mỗi feature map sẽ liên quan đến một đặc trưng nào đó trong ảnh input. Giả sử như feature map $1$ sẽ phát hiện mặt người trong ảnh, feature map $2$ sẽ phát phiện lông của con chó,…, feature map $k$ sẽ phát hiện tai của con chó. Minh họa ý nghĩa các feature maps Nguồn: Johfischer Các trọng số trong output layer sẽ mang ý nghĩa là tầm quan trọng của feature map tương ứng vơi trọng số đó trong việc đưa ra xác suất dự đoán ảnh input thuộc một class nào đó. Ví dụ, ta xét class 2 như hình ảnh bên dưới Khi đó, với $k$ feature maps $F_i$, ta có $k$ trọng số $w_i$ và bằng cách tính tổ hợp tuyến tính của $F_i$ và $w_i$ thì ta sẽ có CAM của ảnh input ứng với class 2. Phép tổ hợp tuyến tính giữa các feature maps Nguồn: Johfischer $$CAM_2 = w_1 * F_1 + w_2 * F_2 + \u0026hellip; + w_k * F_k$$\nLưu ý: Kích thước width và height của CAM đang bằng với các feature maps và nó thường nhỏ hơn nhiều so với ảnh input. Để có thể visualize được như trên, ta chỉ cần upsample CAM lên bằng kích thước của ảnh input. Với mỗi class khác nhau thì CAM tính được là khác nhau Sau khi trình bày ý tưởng của CAM thì ta thấy ngay một điều kiện mà CNN cần thỏa mãn để có thể áp dụng phương pháp này là sau các conv layer thì nó chỉ có duy nhất một fully connected layer để sinh ra output, và điểm nối giữa hai phần này là một global average pooling layer (GAP). Ví dụ như hình bên dưới:\nNguồn: GlassBoxMedicine Trong kiến trúc trên, ta có một CNN với output của conv layer cuối cùng là 3 feature map là A1, A2, A3. Qua GAP thì ta thu được một layer với 3 giá trị số thực. Theo sau đó là một fully connected layer sinh ra output của mô hình. Vì sao ta cần có duy nhất một fully connected layer? Ta đã đề cập rằng các trọng số trong output layer sẽ mang ý nghĩa là tầm quan trọng của feature map tương ứng. Do đó, nối ngay ở đây thì nó mới “đúng ý” (với mỗi neuron trong output layer, ta có đúng $k$ trọng số liên quan trực tiếp đến $k$ feature maps) Vì sao ta cần GAP? Nó sẽ tạo ra cầu nối giữa các feature map và output layer và đảm bảo rằng số channel trong input của output layer đúng bằng số feature maps của conv layer cuối cùng. Viết một cách tổng quát và “formal” hơn, ta sẽ có như sau:\nXét một CNN thỏa điều kiện áp dụng CAM với conv layer cuối cùng có $k$ feature maps $F_i$, output layer có $C$ neurons ứng với $C$ class. Ma trận trọng số tại output layer là $W_{C \\times k}$. Khi đó, với input $X$, CAM được sinh ra cho class $c$ là\n$$CAM_c = W_{c,1} * F_1 + W_{c,2} * F_2 + \u0026hellip; + W_{c,k} * F_k$$\nGrad-CAM (2016) Gradient-weighted Class Activation Mapping (Grad-CAM) là một phiên bản cải tiến của Default CAM với hai yếu tố sau:\nKhông ràng buộc điều kiện đối với kiến trúc của mô hình. Phần fully connected layers được phép ở bất kì cách tổ chức nào. Lưu ý là ta vẫn dùng GAP (global average pooling). Thay vì phải áp dụng cho conv layer cuối cùng thì ta áp dụng cho layer nào cũng được, nhưng thường thì người ta vẫn hay cùng conv layer cuối hơn 😜. “Tầm quan trọng” của các feature maps sẽ được tính theo cách khác, và cách tính này sẽ dựa vào gradient! Grad-CAM thường cho ra kết quả CAM tốt hơn, tập trung vào đúng vùng quan trọng hơn trong ảnh input khi so sánh với Default CAM. Vi dụ:\nSo sánh Default CAM và Grad-CAM Nguồn: MDPI Xét một CNN với conv layer cuối cùng có $k$ feature maps $F_i$, output layer có $C$ neurons ứng với $C$ class. Gọi giá trị output cho class $c$ là $y_c$. Với ảnh input $X$, ta đặt:\nTầm quan trọng của feature map $F_i$ trong việc đưa ra xác suất $X$ thuộc class $c$ được tính dựa vào gradient của output $Y_c$ theo $F_i$, tức là\n$$ \\alpha_{c,i} = GAP(\\frac{\\partial Y_c}{\\partial F_i}) $$\n, với GAP là global average pooling.\nĐể ý rằng, $\\dfrac{\\partial Y_c}{\\partial F_i}$ có cùng shape với $F_i$, ta tiến hành tính GAP để có được giá trị số thực $\\alpha_{c,i}$\nTừ đó, CAM cho class $c$ sẽ được tính bằng tổ hợp tuyến tính giữa $F_i$ và $\\alpha_{c,i}$:\n$$CAM_{c} = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nGrad-CAM đã hoạt động rất tốt và được dùng nhiều trong việc giải thích cho CNN. Tuy nhiên, đến năm 2021 thì có 2 tác giả đã chỉ ra rằng có những trường hợp Grad-CAM cho ra kết quả không thật sự đúng, khi mà CAM được sinh ra tập trung không đúng vào các phần quan trọng. Vi dụ trong y học mà các tác giả đưa ra như sau:\nLý do chính dẫn đến yếu tố này là ở phép toán GAP trong việc tính $\\alpha_{c,i}$. Có những tình huống mà GAP sẽ làm mất đi một số điểm nổi bật ở trong một feature map. Phương pháp mới được giới thiệu là HiresCam (2021), bằng cách thay thế GAP thành phép toán tích element-wise. Các bạn có thể tự tìm hiểu về cái này nhé. Score-CAM (2019) Score-CAM là một phương pháp dựa chỉ dựa vào score của các class (vector output của output layer) để tính tầm quan trọng của các feature maps, từ đó sinh ra CAM. Kết quả ta có được là các CAM tốt hơn phương pháp Grad-CAM, khi mà vùng được chú ý trong ảnh input là đúng hơn, “gọn” hơn (không lan rộng ra những thứ không liên quan lắm ở các phía xung quanh). Ví dụ:\nSo sánh Score-CAM và Grad-CAM Nguồn: Paper Score-CAM - Figure 1 Lưu ý.\nTa xét score của class khi chưa áp dụng softmax để đưa về xác suất. Score-CAM cũng không có ràng buộc gì về kiến trúc của CNN và ta có thể sinh CAM cho các feature map tại bất kỳ layer nào như Grad-CAM. Để trình bày phương pháp này thì ta phải dùng các kí hiệu toán học hơi nhiều một chút 😀\nIncrease of Confidence: Giả sử ta có hàm số $y = f(X)$ với input $X$ là vector $[x_1, x_2,\u0026hellip;, x_n]^T$ và $y$ là số thực. Với một input cơ sở $X_b$ đã biết nào đó, tầm quan trọng của thành phần $x_i$ đối với việc tính ra giá trị $y$ sẽ bằng với độ chênh lệch của output khi ta thay đổi phần thứ $i$ trong $X_b$ bằng cách nhân thêm $x_i$, tức là\n$$ c_i = f(X_b \\circ H_i) - f(X_b) $$\n, với $\\circ$ là element-wise product, $H_i = [1, \u0026hellip;, 1, x_i, 1, \u0026hellip; 1]^T$ (thành phần thứ $i$ là $x_i$)\nBây giờ, áp dụng Increase of Confidence trên với việc ta tính tầm quan trọng của các feature maps: Giả sử ta có một CNN $Y = f(X)$ với input $X$ là ảnh, $Y$ là vector có $C$ phần tử, ứng với score của $C$ class (chưa áp dụng softmax). Ta xét conv layer thứ $k$, feature map thứ $i$ tại layer này được kí hiệu là $F_{k, i}$. Với một input cơ sở $X_b$, tầm quan trọng của $F_{k, i}$ đối với score của class $c$ $( Y_c = f_c(X))$ là\n$$ C(F_{k, i}) = f_c(X_b \\circ H_{k, i}) - f_c(X_b) $$\n, với $H_{k, i} = s(Up(F_{k, i})$ là ta upsample $F_{k, i}$ lên cùng shape với ảnh input $X$, sau đó normalize nó theo công thức $Z \\leftarrow \\dfrac{Z - \\min_Z}{\\max_Z - \\min_Z}$\nSau khi tính các giá trị $C(F_{k, i})$, ta đưa chúng qua softmax để có các giá trị tầm quan trọng với tổng bằng 1:\n$$ \\alpha_{c,i} = \\frac{\\exp(C(F_{k,i}))}{\\sum \\exp(C(F_{k,i})) } $$\nKhi đó, ta có thể sinh ra CAM cho class $c$ như sau:\n$$CAM_c = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nNhư vậy, ta đã xác định được tầm quan trọng của các feature map theo cách là chỉ dựa vào output của CNN (score của các class).\nNhận xét Qua các phương pháp đã trình bày là Default CAM, Grad-CAM, Score-CAM, ta có thể thấy rằng điểm khác biệt lớn nhất giữa chúng là cách tính tầm quan trọng của các feature maps trong việc sinh ra output của CNN.\nCài đặt Các bạn có thể tham khảo notebook sau: Google Colab\nNội dung của notebook trên là sinh ra CAM cho các mô hình CNN như ResNet, DenseNet, EfficientNet và so sánh output của CAM, Grad-CAM và Score-CAM. Các kết quả chạy được trong notebook trên như sau:\nCAM của các mô hình CNN khác nhau: So sánh CAM, Grad-CAM và Score-CAM của mô hình ResNet50: Tài liệu tham khảo GlassBox, CNN Heat Maps: Class Activation Mapping (CAM) GlassBox, Grad-CAM: Visual Explanations from Deep Networks Paper Grad-CAM: https://arxiv.org/abs/1610.02391 Paper Score-CAM: https://arxiv.org/abs/1910.01279 ","date":"2023-02-09T18:30:41+07:00","permalink":"https://htrvu.github.io/post/cam/","title":"CAM, Grad-CAM và Score-CAM trong CNN"},{"content":"XAI là gì? Hầu hết các mô hình AI nói chung hay Deep Learning nói riêng luôn được người ta ví như là một chiếc hộp đen (black-box). Chúng ta xây dựng các mô hình với rất nhiều layer, từ convolution cho đến fully connected, sau đó sử dụng các optimizer như Adam, RMSprop,… (hoặc nói chung chung là gradient descent) để tối ưu mô hình, tức là tìm ra bộ trọng số sao cho hàm mất mát có giá trị nhỏ nhất có thể. Tuy nhiên, nếu ta nhìn lại mô hình và tìm cách giải thích là vì sao các mô hình hoạt động được tốt như vậy thì đây luôn là một câu hỏi khó, Việc đưa những chứng minh chặt chẽ, rõ ràng là không đề hơn giản. Từ đó, ta có một hướng nghiên cứu về các phương pháp giải thích sự hoạt động của các mô hình AI và lĩnh vực này được gọi là Explainable AI (XAI).\nNguồn: https://impact.nuigalway.ie/wp-content/uploads/2022/01/blackboxpng.png Vì sao chúng ta cần phải tìm cách giải thích các mô hình AI?\nTa lấy một ví dụ về mô hình chẩn đoán ung thư dạ dày dựa vào hình ảnh chụp nội soi được sử dụng ở các bệnh viện. Lúc này, tính chính xác của mô hình sẽ trở nên đặc biệt nghiêm trọng, nó có thể ảnh hưởng đến sức khỏe và cả tính mạng của bệnh nhân. Nếu mô hình chẩn đoán là ung thư thì ta cũng cần nó đưa ra những “chứng cứ” cho chẩn đoán đó, tất nhiên là chứng cứ phải đúng, mang tính thuyết phục cao thì mới chấp nhân được. Ngoài lĩnh vực y tế thì ta còn có các ví dụ khác như trong hệ thống bảo mật của ngân hàng,…\nNguồn: Webflow\nNguồn: MicroAI\nKhi AI càng được ứng dụng nhiều vào cuộc sống thì nhu cầu giải thích các mô hình AI cũng sẽ dần nhiều lên. Điều đó dẫn đến sự phát triển mạnh của XAI trong thời gian gần đây.\nDiễn giải một mô hình AI Khả năng diễn giải mô hình (interpretability) là mức độ hiểu biết của chúng ta về cách mô hình hoạt động, mà cụ thể hơn là về quá trình đưa ra dự đoán của mô hình. Ta có hai hướng tiếp cận chính đối với việc diễn giải mô hình là intrinsic và post-hoc.\nNguồn: Kemal Erdem Intrinsic (dựa vào bản chất của mô hình): Cách tiếp cận này thường dùng cho những mô hình thuộc nhóm white-box, đặc biệt là những mô hình Machine Learning như Linear Regresion, Decision Tree, SVM,… Đằng sau những mô hình đó là các lý thuyết toán chặt chẽ, ta có thể tìm được ngay công thức tính ra trọng số tối ưu của bài. Nói cách khác, khi chưa cần huấn luyện thì ta cũng có thể giải thích rằng mô hình sẽ hoạt động theo cách như thế này, như thế kia. Decision Tree Nguồn: javatpoint\nSVM Nguồn: Wikipedia\nPost-hoc: Đây là cách tiếp cận chúng ta thường dùng khi diễn giải các mô hình Deep Learning, và đặc biệt là nó được tiến hành sau khi mô hình đã được huấn luyện với một bộ trọng số đủ tốt. Vì việc giải thích, chứng minh chặt chẽ, chính xác về quá trình hoạt động của các mô hình Deep Learning là rất khó khăn nên post-hoc là hướng tiếp cận được ưu tiên hơn. Trong post-hoc, ta có 2 cách diễn giải là model-agnostic và model-specific.\nModel-agnostic: Cách này nghĩa là chúng ta có thể áp dụng cùng một phương pháp để diễn giải cho toàn bộ các mô hình mà không cần quan tâm đến kiến trúc của chúng. Như vậy, ta chỉ dựa vào input và output của mô hình để đưa ra cách diễn giải. Model-specific: Với cách này thì tùy theo những mô hình, hay là họ các mô hình, mà ta sẽ đưa ra cách diễn giải tương ứng. Ta có thể thấy rằng model-specific có thể dễ tiến hành hơn model-agnostic rất nhiều.\nNhững phương pháp trong XAI mà mình trình bày trong tương lai sẽ chủ yếu thuộc về hướng post-hoc.\nVì sao chúng ta bàn nhiều về khả năng diễn giải mô hình (interpretability) nhưng lĩnh vực này lại gọi là Explainable AI (thiên về khả năng giải thích mô hình)?\n2 thuật ngữ diễn giải và giải thích có thể xem là mang ý nghĩa tương tự và có thể dùng thay thế cho nhau. Tuy nhiên, có một vài quan điểm cho rằng khả năng diễn giải là nói đến một tính chất bị động của mô hình và nó cần con người chúng ta can thiệp vào, còn khả năng giải thích là thiên về chủ động, tức là mô hình có thể tự giải thích cho chính nó. Ở đây, con người chúng ta đang tìm cách giải thích các mô hình, do đó ta ưu tiên gọi là diễn giải. Đánh giá phương pháp XAI Một vấn đề khác mà người ta thường quan tâm đến là cách đánh giá một phương pháp XAI, tức là xét xem cách diễn giải mô hình A đã thuyết phục, đã đúng hay chưa. Hiện tại, ta chưa có một độ đo nào để có thể so sánh các phương pháp với nhau. Phần lớn thì nó nằm ở các nhận xét của con người thông qua việc quan sát 😀\nTài liệu tham khảo Mobiquity, An introduction to Explainable Artificial Intelligence (XAI) Erdem, XAI Methods - The Introduction ","date":"2023-02-09T15:30:31+07:00","permalink":"https://htrvu.github.io/post/intro-xai/","title":"Giới thiệu về XAI"},{"content":"Giới thiệu Qua các mô hình đã được giới thiệu như VGG, GoogLeNet hay ResNet thì ta thấy rằng chúng đều được phát triển theo hướng tăng dần độ sâu và độ phức tạp tính toán của mô hình để đạt được độ chính xác cao hơn, kể từ khi AlexNet được công bố. Số lượng tham số của chúng là rất lớn.\nTuy nhiên, các ứng dụng AI trong thực tế như robotics, xe tự hành thì các phép tính toán của mô hình cần được thực hiện trong một khoảng thời gian giới hạn, cùng với tài nguyên phần cứng hạn chế. Do đó, ta phải đối mặt với một trade-off giữa độ chính xác và độ trễ, kích thước mô hình.\nVào thời điểm này, có 2 hướng giải pháp chính để có thể đưa các mô hình vào ứng dụng thực tế như sau:\nNén các mô hình phức tạp lại thông qua các phương pháp như lượng tử hóa (quantization), hashing, cắt tỉa mô hình Xây dựng và huấn luyên các mô hình nhỏ, độ phức tạp thấp ngay từ đầu. MobileNet được phát triển theo hướng thứ 2, trong đó, nó tập trung vào các yếu tố:\nVừa đảm bảo kích thước mô hình đủ nhỏ, tốc độ suy diễn đủ nhanh (độ trễ thấp) và với độ chính xác đủ cao. Cung cấp hai siêu tham số cho phép ta điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước mô hình: width multiplier (liên quan đến số channel trong từng layer) và resolution multiplier (width và height trong từng layer) Depthwise separable convolutions MobileNet được xây dựng từ các layer convolution khá đặc biệt, chúng được gọi là depthwise separable convolutions. Depthwise separable convolution được tạo ra từ hai phép toán:\nDepthwise convolution: Áp dụng từng filter cho từng channel của input. Nếu input có bao nhiêu channel thì ta sẽ có bấy nhiêu filter. Pointwise convolution: Đây thực chất là convolution layer thông thường với filter 1 x 1. Nó được dùng để tổng hợp các kết quả từ phép toán depthwise convolution và tính ra output, thông qua các phép toán tổ hợp tuyến tính. Nguồn: Research Gate Ta có thể thấy ngay sự khác biệt giữa depthwise separable convolution và convolution thông thường như sau:\nConvolution thông thường: Mỗi filter sẽ tương tác với toàn bộ channel của input. Giả sử input của ta là $D_F \\times D_F \\times M$, một filter $3 \\times 3$ được áp dụng thì filter này sẽ trở thành một tensor với shape $3 \\times 3 \\times M$, ta thực hiện convolution trên từng channel và sau đó cộng $M$ ma trận lại với nhau, thu được kết quả $D_F \\times D_F$. Nếu sử dụng $N$ filter để tính thì ta sẽ có kết quả cuối cùng là $D_F \\times D_F \\times N$. Depthwise separable convolution: Ban đầu, các channel được tính toán độc lập với từng filter riêng, sau đó mới kết hợp lại sau nhờ vào pointwise convolution. Với input $D_F \\times D_F \\times M$ thì khi đưa qua depthwise convotution, ta sẽ có kết quả là $D_F \\times D_F \\times M$. Nếu pointwise convolution sử dụng $N$ filter $1 \\times 1$ thì ta có kết quả cuối cùng là $D_F \\times D_F \\times N$ Vấn đề đặt ra là tại sao sử dụng depthwise separable convolution lại có thể giúp cho MobileNet gọn nhẹ hơn, tính toán nhanh hơn và có độ chính xác đủ tốt, không hề kém cạnh các mô hình to lớn khác. Ta sẽ đặt tính một chút:\nGiả sử input của ta là feature maps $\\bold{F}: D_F \\times D_F \\times M$, output cuối cùng là $\\bold{G}: D_F \\times D_F \\times N$.\nVới convolution thông thường: Giả sử ta dùng $N$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding sao cho kích thước width và height không đổi. Khi đó, độ phức tạp tính toán sẽ là\n$$ O_1 = D_K \\times D_K \\times D_F \\times D_F \\times M \\times N $$\n, vì với mỗi filter thì: mỗi lần tính toán ta phải thực hiện $D_K \\times D_K$ phép toán nhân, sau đó cộng chúng lại; ta tính tại $D_F \\times D_F$ vị trí trên $M$ channel của input, và ta sử dụng $N$ filter.\nVới depthwise separable convolution: Ở bước depthwise convolution thì ta dùng $M$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding phù hợp. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times M $$\n, vì ta chỉ đơn giản là áp dụng đơn lẻ từng filter cho từng channels\nVới pointwise convolution thì ta dùng $N$ filter $\\bold{K}: 1 \\times 1$, stride là 1, padding 0. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_F \\times D_F \\times M \\times N $$\nDo đó, ta có độ phức tạp tính toán là\n$$ O_2 = D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N $$\nLúc này, đem chia cho nhau thì ta có tỉ lệ\n$$ \\frac{O_2}{O_1}=\\frac{D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N}{D_K \\times D_K \\times D_F \\times D_F \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_K^2} $$\nNhư vậy, độ phức tạp tính toán khi sử dụng depthwise separable convolution đã giảm khoảng $D_K^2$ lần so với convolution thông thường. MobileNet sử dụng các filter $3 \\times 3$, từ đó giảm được độ phức tạp tính toán đi khoảng 8 đến 9 lần, trong khi độ chính xác chỉ giảm đi một phần nhỏ.\nSiêu tham số điều chỉnh trade-off Để có thể hỗ trợ tốt hơn việc áp dụng MobileNet vào các thiết bị biên trong các ứng dụng thực tế, các tác giả còn cung cấp thêm cho ta hai siêu tham số để điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước của mô hình\nWidth multiplier Tham số width multiplier (kí hiệu là $\\alpha$) sẽ tác động lên giá trị số channel của các layer. Với những công thức ở trên thì số channel chính là $M$ và $N$. Giá trị $\\alpha \\in (0, 1]$ và ta thường đặt là $1, 0.75, 0.5, 0.25.$ Khi đó, thứ thật sự được thay đổi chính là số lượng filter mà ta dùng trong các phép toán pointwise convolution.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng width multiplier $\\alpha$ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times \\alpha M + D_F \\times D_F \\times \\alpha M \\times \\alpha N $$\nResolution multiplier Tham số resolution muiltiplier (kí hiệu là $\\rho$ ) liên quan đến kích thước width và height (chính là $D_F$ trong các công thức trên). Miền giá trị của nó cũng sẽ tương tự như $\\alpha$. Thực chất thì ta sẽ chỉ áp dụng nó vào input ban đầu của mô hình (ảnh). Các kích thước input mà ta thường sử dụng với mô hình MobileNet là 224, 192, 160 hoặc 128.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng thêm resolution multiplier $\\rho$ là\n$$ D_K \\times D_K \\times \\rho D_F \\times \\rho D_F \\times \\alpha M + \\rho D_F \\times \\rho D_F \\times \\alpha M \\times \\alpha N $$\nKiến trúc mô hình Depthwise Separable block Mô hình MobileNet V1 được tạo thành bởi các thành phần chính là depthwise separable block. Chúng bao gồm hai phép toán như ta đã đề cập là depthwise convolution và pointwise convolution. Đi kèm với các layer đó là batch norm và activation ReLU.\nNguồn: Research Gate Kiến trúc MobileNet MobileNet sử dụng tất cả gồm 13 depthwise separable block. Tổng thể kiến trúc của MobileNet được thể hiện ở bảng sau:\nTrong đó:\nConv dw là depthwise convolution. Ta có thể thấy các filter shape của chúng luôn có cùng số channel trong input size. Các conv layer với filter shape $1 \\times 1$ chính là pointwise convolution. s1 tức là stride = 1, tương tự với s2. Toàn bộ các conv layer trong mô hình đều có padding sao cho kích thước width và height của input và output của layer đó là như nhau. Note:\nTrong bảng trên có vẻ có một chỗ gõ nhầm. Để ý đến layer “Conv dw / s2” đầu tiên từ phía dưới lên, nếu đây là s2 thì input size của layer “Conv / s1” tiếp theo phải bị giảm size chứ không phải $7 \\times 7$. Do đó, trong cài đặt mô hình ở bên dưới thì mình đã đổi nó thành “Conv dw / s1”. Cài đặt Các bạn có thể tham khảo phần cài đặt MobileNet bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper MobileNet: https://arxiv.org/abs/1704.04861 ","date":"2023-02-08T18:13:00+07:00","permalink":"https://htrvu.github.io/post/mobilenet/","title":"MobileNet (2017)"},{"content":"Giới thiệu Các mô hình thuộc họ Inception-ResNet được phát triển dựa trên ý tưởng là kết hợp skip connection vào các Inception block (các ý tưởng từ ResNet và GoogLeNet). Vì paper này chỉ mang tính thực nghiệm là chính nên mình sẽ không trình bày chi tiết 👀.\nKiến trúc mô hình Inception-ResNet V1 Về mặt tổng quan, Inception-ResNet V1 có kiến trúc như sau:\nKiến trúc Inception-ResNet V1\nStem của Inception-ResNet V1\nTa sẽ đề cập đến các loại Inception-ResNet block và Reduction:\nInception-ResNet block: Ta thấy rằng phần “Inception” trong các block này là đơn giản hơn khá nhiều so với các Inception block nguyên mẫu. Inception-ResNet-A V1\nInception-ResNet-B V1\nInception-ResNet-C V1\nReduction: Chúng thực hiện nhiệm vụ giảm kích thước (width, height) của các tensor đi một nửa. Kiến trúc của chúng rất giống với các Inception block Reduction-A\nReduction-B\nInception-ResNet V2 Phiên bản thứ hai của Inception-ResNet có kiến trúc tổng thể giống hệt với phiên bản đầu tiên, ta chỉ có một số thay đổi ở các block Inception-ResNet và Reduction\nTài liệu tham khảo Paper Inception-ResNet: https://arxiv.org/abs/1602.07261 ","date":"2023-02-08T18:03:46+07:00","permalink":"https://htrvu.github.io/post/inception-resnet/","title":"Inception-Reset (2016)"},{"content":"Giới thiệu Ta biết rằng, việc tạo ra các mô hình có độ sâu lớn (nhiều layer) chưa chắc đã mang lại hiệu quả tốt hơn những mô hình “cạn” hơn. Ví dụ, với tập CIFAR10 thì ta có một kết quả thử nghiệm cho thấy rằng mô hình sâu hơn lại có độ hiệu quả kém hơn:\nĐối với việc huấn luyện các mô hình có độ sâu lớn thì ta có thể sẽ bị gặp phải các vấn đề sau:\nOverfitting: Mô hình càng sâu thường sẽ càng phức tạp nên nó rất dễ bị overfitting. Vanishing/exploding gradient. Vấn đề này thì ta đã có một số cách giải quyết phổ biến như thay đổi activation function, các phương pháp khởi tạo trọng số như Xavier, He Initialization. Ngoài 2 vấn đề trên, ta có một vấn đề đặc biệt hơn là degradation: Accuracy tăng dần cho đến một độ sâu nhất định thì ngừng tăng (bão hòa) rồi sau đó sẽ giảm dần.\nLưu ý. Nhiều trường hợp degradation không phải do overfitting gây ra. Degradation cho chúng ta thấy rằng việc tối ưu các mô hình có độ sâu lớn là không hề dễ dàng. Trong quá trình xây dựng mô hình, từ một mô hình ban đầu, sau khi thêm một số layer vào thì tất nhiên là ta mong rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc. Tuy nhiên, khi xảy ra degradation thì mong muốn đó đã không thể thành sự thật được. 😀\nResNet được công bố nhằm giải quyết vấn đề degradation đối với các mô hình có độ sâu lớn. Khi nhắc đến ResNet thì ta sẽ lập tức nghĩ đến những mô hình với độ sâu rất khủng, thậm chỉ là lên đến 100, 200 layer.\nHàm phần dư và skip connection Nhắc lại về cái mong muốn ở trên, rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc, ta có thể nghĩ ngay đến một phương pháp cực kì đơn giản: các layer phía sau sẽ là identity mapping, tức là input và output của nó sẽ giống nhau. Với cách làm này thì hiển nhiên là ta đạt được mong muốn rồi, vì độ hiệu quả của mô hình mới và mô hình gốc rõ ràng sẽ y hệt nhau.\nTuy nhiên, nếu chỉ dừng lại ở đó thôi thì thêm layer vào làm gì :v Ta muốn đạt được kết quả tốt hơn! Các tác giả của paper ResNet giới thiệu một phương pháp gọi là deep residual learning (học phần dư).\nGiả sử ta có một block B các layer, input của nó là $x$. Với một mô hình thông thường, ta sẽ “học” một hàm số đầu ra mong muốn là $f(x)$. Lúc này, ban đầu thì ta hoàn toàn chưa có một thông tin gì về $f(x)$ cả, việc “học” sẽ xuất phát từ một đại lượng ngẫu nhiên. Đối với phương pháp deep residual learning, output của block B sẽ có dạng $h(x) = x + f(x)$, và ta sẽ đi học $f(x)$. Lúc này, $f(x)$ được gọi là residual function (hàm phần dư) Ta có các nhận xét sau: Trong deep residual learning, ta đã có sự “gợi ý” cho hàm mong muốn thông qua giá trị input $x$. Đối với thông thường thì không có sự gợi ý nào được đưa ra cả. Việc học $f(x)$ như là một hàm phần dư là dễ hơn so với việc học hàm mong muốn. Nếu identity mapping là kết quả tối ưu thì ta có luôn $f(x)=0$ Về mặt bản chất, với phương pháp deep residual learning, ta đã tác động vào lớp hàm chứa hàm mong muốn, sao cho lớp mới là lớn hơn (bao gồm) lớp cũ. Nguồn: Dive into AI Để minh họa cho yếu tố \u0026ldquo;giúp việc học trở nên dễ hơn\u0026rdquo;, ta có ví dụ như sau:\nSố điểm cực tiểu địa phương sẽ ít hơn hẳn, đồ thị cũng sẽ \"trơn\" hơn khi có sử dụng skip connection Nguồn: Jeremy Jordan Như vậy, điểm nhấn của ResNet là ta đi học các hàm phần dư, thông qua việc “gợi ý” cho hàm số mong muốn một giá trị bằng với giá trị input. Trong cài đặt, thao tác này được thực hiện thông qua một kết nối gọi là skip connection. Ta sẽ cộng input $x$ vào output của block thông thường.\nĐối với việc cộng như vậy thì thực chất là ta đang đi cộng hai ma trận. Khi đó, một vấn đề có thể nảy sinh là về shape của chúng, mà thường là số channel. Nếu số channel không khớp thì ta cần phải tiến hành “điều chỉnh”. Vì các block được sử dụng thường gồm các conv layer với stride và padding phù hợp để giữ nguyên width và height nên ta sẽ chỉ xét đến số channel. Nếu số channel của $x$ và output ban đầu là như nhau thì ta gọi skip connection này là identity skip connection Ngược lại, ta sẽ dùng conv layer $1 \\times 1$ để điều chỉnh số channel của $x$. Lúc này, skip connection được gọi là projection skip connection. Sử dụng skip connection (identity) Nguồn: Dive into AI Ngoài ra, ta còn có một điểm mạnh quan trọng của skip connection là nó giúp cho gradient được lan truyền tốt hơn trong quá trình backpropagation, từ đó góp phần làm giảm hiện tượng vanishing gradient\nQuan sát hình phía trên, ta thấy rằng khi backpropagation thì layer ở ngay trước block nhận được gradient từ hai layer phía sau nó (một layer liền trước nó và một layer được kết nối thông qua skip connection). Residual block Residual block (khối phần dư) được tạo ra bằng cách thêm một skip connection vào một block thông thường trong các mô hình CNN như VGG block. Ta có hai loại residual block như sau:\nBasic: Các conv layer trong block này có filter $3 \\times 3$. Block dạng này thường được dùng cho các mô hình có độ sâu vừa phải. Bottleneck: Nhằm giảm bớt số lượng tham số của các mô hình có độ sâu lớn, các tác giả sử dụng dạng block này với hai conv layer $1 \\times 1$ với vai trò là giảm/tăng số channel, tạo ra một hình dáng giống như nút thắt cổ chai. Hai layer conv $1 \\times 1$ như vậy được gọi là bottleneck layer. Để cho dễ hình dùng thì ta có thể xem đây như là thao tác “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán). Basic Residual Block và Bottleneck Residual Block Lưu ý. Có một chi tiết này khá quan trọng trong cài đặt là ta sẽ tiến hành cộng ma trận trước rồi mới đưa kết quả qua activation function.\nNgoài ra, ta còn có 2 loại skip connection là identity và projection. Lúc này, tùy vào số channels của input và output ban đầu có khớp hay không mà block tương ứng sẽ chứa loại skip connection phù hợp.\nBasic residual block với identity và projection skip connection Nguồn: Dive into AI Kiến trúc ResNet ResNet được tạo nên bằng cách sử dụng nhiều residual block liên tiếp nhau, tương tự như những gì mà GoogLeNet hay VGG đã thực hiện. Các tác giả của paper ResNet tạo ra nhiều phiên bản ResNet khác nhau với độ sâu tăng dần.\nDựa vào số lượng layer có trọng số thì ta sẽ có tên các mô hình như ResNet18 (18 layer có trọng số), ResNet34,… Các phiên bản ResNet Ta có một số nhận xét như sau:\nCác phiên bản có độ sâu lớn như 50, 101 và 152 sử dụng Bottleneck Residual Block. Với 18 và 34 thì chúng dùng Basic block Trong các kiến trúc trên thì ta sử dụng cả 2 loại skip connection: identity và projection Ví dụ, với ResNet18 thì trong cụm Basic block đầu tiên của cụm conv3_x sẽ có số channel là 64, còn output ban đầu của block này thì là 128 nên ta phải áp dụng projection vào input Đối với height và width thì các tác giả cho biết việc downsampling input sẽ được thực hiện tại conv layer đầu tiên của các cụm conv3_x, conv4_x, conv5_x (với stride là 2). Các conv layer khác thì đều có stride 1. Ta sẽ cần chú ý đến chi tiết này khi tiến hành cài đặt ResNet. Cài đặt Các bạn có thể tham khảo phần cài đặt ResNet bằng Tensorflow và Pytorch tại repo sau\nTài liệu tham khảo Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into AI, ResNet ","date":"2023-02-08T17:41:09+07:00","permalink":"https://htrvu.github.io/post/resnet/","title":"Resnet (2015)"},{"content":" Cá nhân mình thấy GoogLeNet là một paper khó đọc. Khi viết ra bài này thì mình vẫn đang cảm thấy hơi lú về nội dung của nó 😀\nGiới thiệu Từ khi AlexNet được công bố vào năm 2012 và đặt nền tảng cho các mạng Deep CNN, GoogLeNet, hay Inception V1 (2014), là một trong những kiến trúc có cách thiết kế rất thú vị khi nó tận dụng hiệu quả các conv layer, đặt nền móng cho nhiều mô hình sau này.\n“Inception” có thể dịch là “sự khởi đầu”, nghe có vẻ rất hợp lý 😀. Ngoài ra, ở trong bài viết về VGG, mình có đề cập đến vấn đề thiết kế kiến trúc mô hình theo hướng có sự lặp lại các khuôn mẫu. GoogLeNet cũng sẽ được thiết kế như vậy.\nLưu ý. Tên của mô hình này là GoogLeNet, không phải GoogleNet =)) Tác giả cho biết ý nghĩa của cái tên này là “This name is a homage to Yann LeCuns pioneering LeNet 5 network.”\nGoogLeNet được xây dựng từ những mục tiêu của nghiên cứu như sau:\nNâng cao khả năng tận dụng tài nguyên tính toán Cho phép tăng chiều rộng và chiều sâu của kiến trúc mô hình mà vẫn đảm bảo được độ phức tạp tính toán là ở mức chấp nhận được. GoogLeNet thật sự đã đạt được những điều đó, và nó được xây dựng dựa trên nguyên lý Hebbian: “neurons that fire together, wire together”. Paper GoogLeNet đã đưa ra các nền tảng lý thuyết rất “căng thẳng” để cho thấy rằng mô hình CNN có thể hoạt động “đủ tốt”, điều mà ta chưa được biết đến ở trong các paper trước đó 😀\nNgoài ra, có một quan điểm khá thú vị khi mô tả về kiến trúc của GoogLeNet như sau: Khi xây dựng kiến trúc CNN, thay vì phải suy nghĩ xem trong các mạng CNN ta nên áp dụng filter với kích thước bao nhiêu, hãy áp dụng luôn nhiều filter với kích thước khác nhau và tổng hợp kết quả lại 😜\nMình cũng có đồng tình với quan điểm này. Tuy nhiên, nguồn gốc đằng sau nó có vẻ không chỉ đơn giản là như vậy. Trong paper, các tác giả đã tiến hành phân tích và thử nghiệm nhiều lắm cho ra được cái kiến trúc của GoogLeNet. Kết nối thưa và CNN Đầu tiên, tại thời điểm trước khi Inception được công bố, chúng ta có thể cải thiện một mô hình DNN bằng những cách như sau:\nTăng chiều sâu của mô hình (tức là số layer) Tăng chiều rộng (số channel trong mỗi layer) Tuy nhiên, với hai cách trên thì sẽ có những vấn đề mà ta cần lưu tâm là hiện tượng overfitting và sự gia tăng độ phức tạp của mô hình.\nCác tác giả có đề cập đến một hướng đi có thể giảm bớt hai vấn đề trên là sử dụng kiến trúc kết nối thưa (sparsely connected architectures). Ta có thể hiểu đơn giản như sau:\nVới hai fully connected layer liên tiếp nhau, mỗi neuron trong layer sau sẽ kết nối với tất cả các unit trong layer trước. Nếu như ta thay đổi đi một chút, mỗi neuron trong layer sau sẽ chỉ kết nối đến một vài unit trong layer trước, kiến trúc có được sẽ trở nên “thưa” hơn. Kết nối thưa trong fully connected layer Đối với conv layer thì ta đã đạt được tính chất thưa như vậy. Ta biết rằng, với mỗi phần tử trên feature map của layer hiện tại thì ta tính nó dựa vào một vùng nhỏ trên feature map output của layer trước đó. Giả sử hai layer này đều chỉ có 1 channel thì ta có thể biểu diễn nó như hình bên dưới: Kết nối thưa trong conv layer Kiến trúc thưa được các tác giả mô tả là mô phỏng lại hệ thống sinh học (ví dụ như khi ta nhìn vào một đối tượng thì ta thường chỉ chú ý một số điểm trên đối tượng đó thôi, khó mà chú ý tất cả được).\nNgoài ra, có một nền tàng lý thuyết rất trâu bò về kiến trúc thưa của Arora như sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Thật sự mình cũng chưa hiểu hết ý của câu trên… Nhưng đại ý của nó có vẻ là nếu ta biểu diễn được phân phối các điểm dữ liệu của một dataset bằng một kiến trúc DNN lớn và thưa thì từ đó ta có thể xây nên được một kiến trúc tối ưu (về cả độ chính xác và độ phức tạp), bằng cách xây từng layer một 😀\nTuy nhiên, với các tài nguyên phần cứng trong thời gian này thì rất khó để ta có thể tính toán trên các mạng DNN thưa. Do đó, các tác giả đi theo hướng tìm một mô hình là có tận dụng một số thông tin về tính chất thưa nhưng vẫn thực hiện tính toán trên các ma trận đầy đủ. Đấy chính là hướng sử dụng các phép toán convolution!.\nCũng vì lý do này, ở phần kiến trúc của GoogLeNet thì ta sẽ thấy nó chỉ có duy nhất một fully connected layer để sinh ra output, còn lại chỉ toàn conv layer thôi 😗 Nói đến việc áp dụng các filter trong conv layer, các tác giả cho rằng:\nMỗi phần tử trong feature map của một layer sẽ có mối tương quan với một vùng nào đó trên ảnh input (receptive field). Ta sẽ gom các phần tử cùng tương quan với một vùng vào cùng một cụm. Để ý rằng, trong những layer ở gần ảnh input thì ta sẽ có nhiều cụm và kích thước mỗi cụm là nhỏ. Càng đi qua các layer CNN thì số lượng cụm sẽ ít lại và kích thước cụm sẽ lan rộng ra. Tất nhiên là vẫn có thể có những cụm có kích thước nhỏ tại những layer đó. Do vậy, ta nên có các filter với kích thước lớn hơn để học các đặc trưng tại các cụm lớn, đồng thời cũng cần có filter kích thước nhỏ đối với các cụm nhỏ hơn. Qua một loạt các thử nghiệm, các tác giả đã chọn 3 filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Một khối chứa 3 filter trên được đặt tên là Inception module.\nBây giờ quay lại với lý thuyết của Arora, ở ý xây dựng mạng tối ưu qua từng layer một. GoogLeNet được tạo nên bằng đúng ý tưởng như vậy, ta nối Inception module - by - Inception module 😀\nP/s: Các bạn có thể tìm đọc phần Motivation trong paper gốc và tự cảm nhận nó nhé.\nInception module Inception module Qua các mô tả ở phần trên, ta có thể liên tưởng đến kiến trúc của Inception module là một thứ gì đó giống với cái ở hình (a), với đủ 3 loại filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Các tác giả dùng thêm cả một layer max pooling trong đó nữa, với lí do rất đơn giản là vì ở thời điểm đó thì max pooling thường mang lại hiệu quả tốt trong các kiến trúc mạng CNN =))\nTruy nhiên, cách cài đặt như hình (a) sẽ dẫn đến số lượng tham số của mô hình là rất lớn. Thay vào đó, ta có thể tạo ra kiến trúc dạng “bottleneck” bằng cách sử dụng thêm các layer convolution $1 \\times 1$ như hình (b), nhằm mục đích chính là giảm số channel. Số lượng phép tính lúc này sẽ được giảm một cách đáng kể.\nLưu ý rằng, ở cuối module ta có phép toán concatenate, tức là các feature-maps của toàn bộ layer convolution đều phải có cùng size (tức là width và height)\nKiến trúc GoogLeNet GoogLeNet sử dụng tổng cộng 9 Inception module, gồm có 22 layer có trọng số (tính cả pooling là 27), được tóm tắt qua bảng sau:\nMột điểm đặc biệt khi train GoogLeNet là các tác giả sử dụng các auxiliary classifiers (xem hình bên dưới). Các thành phần này sẽ khá giống như giống hệt với phần cuối của mô hình (bộ classifier). Như vậy, ta có thể xem GoogLeNet có 3 bộ classifier. Auxiliary classifiers được sử dụng với các ý nghĩa như sau:\nHạn chế vanishing gradient Tăng regularization Lưu ý:\nLoss khi huấn luyện sẽ cộng loss của cả ba lại với nhau. Khi test thì ta thường chỉ quan tâm đến bộ classifer cuối cùng. Một cách làm khác là ta xài cả 3, sau đó lấy kết quả trung bình. Nguồn: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png Cài đặt Các bạn có thể tham khảo phần cài đặt GoogLeNet bằng Tensorflow và Pytorch tại repo sau. Trong cách cài đặt này, mình sẽ bỏ qua auxiliary classifiers.\nTài liệu tham khảo Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"https://htrvu.github.io/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giới thiệu Dựa trên sự thành công của AlexNet vào năm 2012, nhiều nghiên cứu đã được tiến hành nhằm tìm ra các phương pháp hay kiến trúc mới để đạt được kết quả tốt hơn, ví dụ như:\nThay đổi (tăng, giảm) kích thước của conv filter Thay đổi stride, padding của conv layer Train và test trên các input với nhiều độ phân giải (resolution) ảnh khác nhau Trong năm 2014, VGG là một trong những kết quả nghiên cứu nổi bật nhất, và nó tập trung vào một vấn đề khác với các hướng trên là độ sâu (depth, hay là số layer) của mô hình. VGG đã đạt được các kết quả tốt nhất vào thời điểm nó ra mới trên dataset ImageNet và các dataset khác, trong các task như classification, localization,…\nNgoài ra, ta có thể đưa ra nhận xét như sau về AlexNet:\nDù AlexNet đã chứng minh được CNN có thể đạt độ hiệu quả tốt, nó lại không cung cấp một khuôn mẫu nào cho việc nghiên cứu, thiết kế các mạng mới. Theo thời gian, các nhà nghiên cứu đã thay đổi suy nghĩ từ quy mô những neuron riêng lẻ sang các tầng, rồi sau đó là các khối (block) gồm các tầng lặp lại theo khuôn mẫu. Kiến trúc của VGG là một trong những kiến trúc phổ biến đầu tiên được xây dựng theo ý tưởng như vậy.\nVGG block Điểm nổi bật của VGG là ta chỉ dùng duy nhất một kích thước filter trong mọi conv layer là $3 \\times 3$, và ta dần tăng độ sâu của mô hình bằng các conv layer. Hơn nữa, ta còn áp dụng nhiều conv layer liền nhau rồi mới dùng đến max pooling. Ta có thể gọi một block gồm những layer như thế là VGG block.\nCác tác giả có đề cập đến một vấn đề cho cách áp dụng này như sau: Việc dùng nhiều conv layer 3 x 3 liền nhau như vậy so với dùng một conv layer với filter lớn hơn (ví dụ 7 x 7) như hầu hết các mô hình đã được công bố vào thời điểm trước đó thì có gì “tốt” hơn, khi mà features map sau cùng ta thu được có thể có cùng kích thước? Để trả lời, ta có 2 ý chính như sau: Giảm số lượng tham số của mô hình (đặt tính là biết nhaa 😜) Dùng nhiều conv layer thì ta có khả năng sẽ phát hiện được nhiều feature có ích hơn (hai conv layer sẽ tạo thành một \u0026ldquo;hàm hợp\u0026rdquo;), từ đó decision function sẽ ok hơn. Ngoài ra, VGG block sử dụng padding 1 (giữ nguyên kích thước input), theo sau đó là max pooling với pool size $2 \\times 2$ và stride 2 (giảm kích thước input đi một nửa). Kiến trúc của nó có thể được mô tả như hình bên dưới:\nVGG block Nguồn: Dive intro AI Trong các bài toán áp dụng VGG, đôi khi ta có thể gặp VGG block với một conv layer $1 \\times 1$ ở trước max pooling. Block dạng này thường được sử dụng với mục đích chính là bổ sung thêm một phép biến đổi tuyến tính nữa. Tuy nhiên, trong thực nghiệm thì các tác giả đã cho thấy rằng việc sử dụng block dạng này không hiệu quả hơn so với toàn các conv layer với filter $3 \\times 3$ (cùng số lượng layer).\nKiến trúc VGG Bằng cách kết hợp nhiều VGG block với nhau, các tác giả đã tạo ra nhiều phiên bản VGG khác nhau, với số layer có trọng số là trong đoạn 11 - 19. Trong paper VGG, ta có 6 kiến trúc với độ sâu tăng dần và tiến hành so sánh với nhau. Điểm chung của các kiến trúc này là phần fully connected đều có 3 layer, và toàn bộ layer đều sử dụng activation ReLU.\nCác phiên bản VGG Ta có một số nhận xét như sau:\nSố lượng conv layer trong các VGG block của các phiên bản là tăng dần. Để ý đến B, C và D thì: C = B + conv layer $1 \\times 1$ trong mỗi VGG block D = C + đổi conv layer $1 \\times 1$ thành $3 \\times 3$ Khi thực nghiệm, ta có B \u0026lt; C \u0026lt; D. Do đó, việc thêm phép biến đổi tuyến tính bằng conv layer $1 \\times 1$ giúp mô hình hoạt động tốt hơn, nhưng nó vẫn không bằng với việc là ta thêm luôn conv layer $3 \\times 3$ 🤔. Độ rộng (số channel) trong từng block được tăng theo bội 2. Ý tưởng này được sử dụng rất rộng rãi trong thời điểm này và cả về sau Để hạn chế overfitting, ta có thể sử dụng thêm dropout cho hai tầng fully connected đầu tiên. Hai kiến trúc phổ biến nhất trong việc áp dụng VGG vào các bài toán khác là D (VGG16) và E (VGG19). Để trực quan hơn, ta có thể thể biểu diễn VGG16 như sau:\nNguồn: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png Cài đặt Các bạn có thể tham khảo phần cài đặt VGG bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"https://htrvu.github.io/post/vgg/","title":"VGG (2014)"}]
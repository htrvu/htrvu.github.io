[{"content":"Giá»›i thiá»‡u Ta biáº¿t ráº±ng, háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh CNN thÆ°á»ng Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« má»™t phiÃªn báº£n ban Ä‘áº§u (cÃ³ thá»ƒ lÃ  dá»±a theo má»™t nguá»“n tÃ i nguyÃªn nÃ o Ä‘Ã³), sau Ä‘Ã³ chÃºng Ä‘Æ°á»£c scale dáº§n lÃªn Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n, vÃ  táº¥t nhiÃªn lÃ  Ä‘á»™ phá»©c táº¡p cÅ©ng tÄƒng theo\nVÃ­ dá»¥: Vá»›i ResNet thÃ¬ ta cÃ³ ResNet18 cho Ä‘áº¿n ResNet152, DenseNet thÃ¬ DenseNet121 cho Ä‘áº¿n 201, MobileNet thÃ¬ ta cÃ³ siÃªu tham sá»‘ width multiplier Ä‘á»ƒ Ä‘iá»u chá»‰nh sá»‘ channel trong tá»«ng layer vÃ  resolutiom multiplier Ä‘á»ƒ Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c táº¡i cÃ¡c layer,â€¦ Nhá»¯ng cÃ¡ch lÃ m Ä‘Ã³ gá»i lÃ  model scaling. Tuy nhiÃªn, ta nháº­n tháº¥y ráº±ng nhá»¯ng thao tÃ¡c model scaling trÆ°á»›c Ä‘Ã³ chá»‰ táº­p trung vÃ o má»™t trong 3 yáº¿u tá»‘: depth - $d$ (sá»‘ layer), width - $w$ (sá»‘ channel) vÃ  resolution - $r$. HÆ¡n ná»¯a, viá»‡c Ä‘iá»u chá»‰nh cÅ©ng khÃ´ng theo má»™t nguyÃªn táº¯c nÃ o mÃ  cÃ²n mang Ä‘áº­m tÃ­nh cháº¥t ngáº«u nhiÃªn, â€œhÃªn xuiâ€, cáº§n pháº£i thá»­ nghiá»‡m ráº¥t nhiá»u láº§n má»›i cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»™t Ä‘á»™ chÃ­nh xÃ¡c mong muá»‘n. Khi Ä‘Ã³ thÃ¬ sá»‘ lÆ°á»£ng tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh cÅ©ng tÄƒng chÃ³ng máº·t!\nCÃ¡c tÃ¡c giáº£ cá»§a paper Ä‘Ã£ cho tháº¥y káº¿t quáº£ thá»±c nghiá»‡m ráº±ng viá»‡c Ä‘iá»u chá»‰nh má»™t trong 3 yáº¿u tá»‘ cÃ³ thá»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c nhÆ°ng chá»‰ tÄƒng Ä‘áº¿n má»™t má»©c nÃ o Ä‘Ã³ thÃ´i, sau Ä‘Ã³ nÃ³ sáº½ bá»‹ bÃ£o hÃ²a. VÃ­ dá»¥ nhÆ° á»Ÿ hÃ¬nh bÃªn dÆ°á»›i:\nTa cÃ³ thá»ƒ Ä‘Æ°a ra cÃ¡c nháº­n xÃ©t nhÆ° sau:\nNáº¿u mÃ´ hÃ¬nh cÃ³ width lá»›n (má»—i layer cÃ³ nhiá»u channel) thÃ¬ nÃ³ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c nhiá»u loáº¡i Ä‘áº·c trÆ°ng khÃ¡c nhau. NhÆ°ng náº¿u mÃ´ hÃ¬nh khÃ´ng Ä‘á»§ sÃ¢u thÃ¬ cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã³ cÅ©ng chÆ°a pháº£i Ä‘áº·c trÆ°ng á»Ÿ má»©c high-level (ná»•i báº­t cho Ä‘á»‘i tÆ°á»£ng) Náº¿u mÃ´ hÃ¬nh cÃ³ depth lá»›n thÃ¬ nÃ³ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng high-level nhÆ°ng náº¿u khÃ´ng cÃ³ width lá»›n thÃ¬ cÅ©ng khÃ´ng há»c Ä‘Æ°á»£c nhiá»u loáº¡i Ä‘áº·c trÆ°ng* Vá» máº·t trá»±c giÃ¡c, náº¿u ta Ä‘Æ°a vÃ o mÃ´ hÃ¬nh má»™t bá»©c áº£nh cÃ³ resolution cao thÃ¬ mÃ´ hÃ¬nh nÃªn cÃ³ depth lá»›n Ä‘á»ƒ cÃ³ thá»ƒ dáº§n há»c cÃ¡c Ä‘áº·c trÆ°ng tá»« cÃ¡c feature maps cÃ³ resolution lá»›n, Ä‘á»“ng thá»i cÅ©ng vÃ¬ sáº½ cÃ³ nhiá»u Ä‘áº·c trÆ°ng hÆ¡n nÃªn ta cáº§n width lá»›n. Do Ä‘Ã³, model scaling nÃªn táº­p trung vÃ o viá»‡c Ä‘iá»u chá»‰nh Ä‘á»“ng thá»i cáº£ 3 yáº¿u tá»‘ $d$, $w$, $r$. Paper cÃ´ng bá»‘ EfficientNet cÃ³ tÃªn lÃ  â€œEfficientNet: Rethinking Model Scaling for Convolutional Neural Networksâ€. CÃ¡c tÃ¡c giáº£ táº­p trung vÃ o viá»‡c Ä‘i tÃ¬m má»™t phÆ°Æ¡ng phÃ¡p model scaling hiá»‡u quáº£, cÃ³ nguyÃªn táº¯c, Ä‘iá»u chá»‰nh Ä‘á»“ng thá»i cáº£ 3 yáº¿u tá»‘ nhÆ° Ä‘Ã£ Ä‘á» cáº­p. PhÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c giá»›i thiá»‡u cÃ³ tÃªn lÃ  compound scaling.\n\u0026ldquo;NguyÃªn táº¯c\u0026rdquo; trong phÆ°Æ¡ng phÃ¡p nÃ y ráº¥t Ä‘Æ¡n giáº£n, ta sáº½ cÃ¹ng Ä‘iá»u chá»‰nh $d$, $w$, $r$ cá»§a toÃ n bá»™ network theo cÃ¹ng má»™t há»‡ sá»‘ gá»i lÃ  compound coefficient (kÃ­ hiá»‡u lÃ  $\\phi$).\nMinh há»a cho cÃ¡c phÆ°Æ¡ng phÃ¡p model scaling. (a) lÃ  mÃ´ hÃ¬nh ban Ä‘áº§u. (b)-(d) thá»±c hiá»‡n Ä‘iá»u chá»‰nh má»™t trong ba yáº¿u tá»‘. (e) lÃ  phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Ä‘á» xuáº¥t, nÃ³ tiáº¿n hÃ nh Ä‘iá»u chá»‰nh cáº£ ba. Táº¥t nhiÃªn lÃ  model scaling chá»‰ phÃ¡t huy tÃ¡c dá»¥ng khi mÃ  mÃ´ hÃ¬nh ban Ä‘áº§u lÃ  Ä‘á»§ tá»‘t. Tá»« phÆ°Æ¡ng phÃ¡p compound scaling, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ Ã¡p dá»¥ng nÃ³ cho ResNet, MobileNet Ä‘á»ƒ chá»©ng tá» Ä‘á»™ hiá»‡u quáº£ cá»§a phÆ°Æ¡ng phÃ¡p. Sau Ä‘Ã³, má»™t há» mÃ´ hÃ¬nh má»›i Ä‘Æ°á»£c Ä‘á» xuáº¥t lÃ  EfficientNet, vá»› 8 phiÃªn báº£n tá»« B0 Ä‘áº¿n B7 vá»›i Ä‘á»™ phá»©c táº¡p vÃ  Ä‘á»™ chÃ­nh xÃ¡c tÄƒng dáº§n trÃªn táº­p ImageNet. EfficientNet-B7 Ä‘Ã£ trá»Ÿ thÃ nh SOTA (state-of-the-art) vá»›i Ä‘á»™ phá»©c táº¡p nhá» hÆ¡n ráº¥t nhiá»u láº§n so vá»›i mÃ´ hÃ¬nh SOTA trÆ°á»›c Ä‘Ã³.\nBÃ i toÃ¡n model scaling Giáº£ sá»­ conv layer thá»© $i$ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  hÃ m sá»‘ $Y_i = F_i(X_i)$, vá»›i input $X_i$ cÃ³ shape lÃ  $\\left (H_i, W_i, C_i \\right)$. Khi Ä‘Ã³, má»™t CNN $N$ cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n lÃ \n$$ N = F_k \\bigodot F_{k-1} \\bigodot \\cdots F_1(X_1) = \\bigodot_{j=1,\u0026hellip;,k} F_j(X_1) $$\nThÃ´ng thÆ°á»ng, cÃ¡c máº¡ng CNN thÆ°á»ng Ä‘Æ°á»£c xÃ¢y dá»±ng theo kiá»ƒu gá»“m nhiá»u giai Ä‘oáº¡n, má»—i giai Ä‘oáº¡n lÃ  sá»± láº·p láº¡i cÃ¡c block cÃ³ cÃ¹ng dáº¡ng cáº¥u trÃºc, chá»‰ khÃ¡c nhau má»™t sá»‘ chi tiáº¿t nhÆ° sá»‘ layer trong block, kÃ­ch thÆ°á»›c cá»§a filter,â€¦ VÃ­ dá»¥, ResNet Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn cÃ¡c residual block, MobileNet thÃ¬ lÃ  cÃ¡c depthwise separable block,â€¦ Do Ä‘Ã³, ta cÃ³ thá»ƒ viáº¿t láº¡i $N$ thÃ nh\n$$ N = \\bigodot_{i=1,\u0026hellip;,s} F_i ^ {L_i}(X_{(H_i, W_i, C_i}) $$\n, vá»›i $F_i$ lÃ  layer Ä‘Æ°á»£c láº·p láº¡i $L_i$ láº§n trong giai Ä‘oáº¡n thá»© $i$, vá»›i input lÃ  $X$ cÃ³ shape $(H_i, W_i, C_i)$.\nBÃ i toÃ¡n model scaling sáº½ cá»‘ Ä‘á»‹nh layer $F_i$ vÃ  Ä‘i Ä‘iá»u chá»‰nh cÃ¡c giÃ¡ trá»‹ $L_i, H_i, W_i, C_i$, sao cho mÃ´ hÃ¬nh thá»a mÃ£n cÃ¡c rÃ ng buá»™c vá» tÃ i nguyÃªn vÃ  Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t cÃ³ thá»ƒ.\nÄiá»u chá»‰nh $L_i$ $\\Leftrightarrow$ Äiá»u chá»‰nh depth Äiá»u chá»‰nh $C_i$ $\\Leftrightarrow$ Äiá»u chá»‰nh width Äá»u chá»‰nh $H_i, W_i$ $\\Leftrightarrow$ Äiá»u chá»‰nh resolution Äá»ƒ giáº£m khÃ´ng gian tÃ¬m kiáº¿m, ta sáº½ Ä‘iá»u chá»‰nh cÃ¡c giÃ¡ trá»‹ trÃªn cá»§a toÃ n bá»™ layer trong mÃ´ hÃ¬nh theo cÃ¹ng má»™t tá»‰ lá»‡. Khi Ä‘Ã³, bÃ i toÃ¡n cá»§a ta lÃ  bÃ i toÃ¡n tá»‘i Æ°u nhÆ° sau:\n, vá»›i $d, w, r$ lÃ  há»‡ sá»‘ Ä‘á»ƒ Ä‘iá»u chá»‰nh depth, width, resolution; $\\hat{F_i}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i}$ lÃ  cÃ¡c giÃ¡ trá»‹ ban Ä‘áº§u cá»§a mÃ´ hÃ¬nh baseline.\nPhÆ°Æ¡ng phÃ¡p compound scaling PhÆ°Æ¡ng phÃ¡p nÃ y sá»­ dá»¥ng compound coefficient $\\phi$ Ä‘á»ƒ Ä‘iá»u chá»‰nh depth, width, resolution theo nguyÃªn táº¯c nhÆ° sau:\nVá»›i mÃ´ hÃ¬nh baseline ban Ä‘áº§u, ta thá»±c hiá»‡n grid search Ä‘á»ƒ tÃ¬m ra bá»™ 3 giÃ¡ trá»‹ tá»‰ lá»‡ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tá»‘t nháº¥t cÃ³ thá»ƒ, sao cho\n$$ \\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\text{ vÃ  } \\alpha \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 $$\nSau Ä‘Ã³, ta sáº½ scale mÃ´ hÃ¬nh lÃªn theo há»‡ sá»‘ $\\phi$ vá»›i\n$$ d = \\alpha ^ \\phi, ; w = \\beta^\\phi, ; r = \\gamma^\\phi $$\nVá» máº·t trá»±c giÃ¡c, ta cÃ³ thá»ƒ xem $\\phi$ nhÆ° lÃ  cÃ¡ch mÃ  chÃºng ta cho biáº¿t lÆ°á»£ng tÃ i nguyÃªn dÃ nh cho model scaling lÃ  bao nhiÃªu, cÃ²n cÃ¡c giÃ¡ trá»‹ $\\alpha, \\beta, \\gamma$ lÃ  cÃ¡ch chÃºng ta phÃ¢n phá»‘i tÃ i nguyÃªn Ä‘Ã³ cho depth, width vÃ  resolution. Giáº£i thÃ­ch cho cÃ¡c rÃ ng buá»™c cho $\\alpha, \\beta, \\gamma$ Ä‘Æ°á»£c trÃ¬nh bÃ y nhÆ° sau:\nTáº¥t nhiÃªn lÃ  Ä‘á»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c viá»‡c scale mÃ´ hÃ¬nh lÃªn thÃ¬ giÃ¡ trá»‹ cá»§a chÃºng pháº£i khÃ´ng nhá» hÆ¡n 1 NgoÃ i ra, má»™t phÃ©p toÃ¡n convolution sáº½ cÃ³ Ä‘á»™ phá»©c táº¡p tá»‰ lá»‡ thuáº­n vá»›i $d, w^2, r^2$. Do Ä‘Ã³, náº¿u ta scale model lÃªn theo há»‡ sá»‘ $\\phi$ thÃ¬ Ä‘á»™ phá»©c táº¡p sáº½ tÄƒng lÃªn má»™t lÆ°á»£ng báº±ng $(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2)^\\phi$. CÃ¡c tÃ¡c giáº£ mong muá»‘n Ä‘á»™ phá»©c táº¡p tÄƒng khoáº£ng $2^\\phi$, do Ä‘Ã³ ta cÃ³ rÃ ng buá»™c $\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2$. Há» mÃ´ hÃ¬nh EfficientNet Neural architecture search EfficientNet lÃ  má»™t há» cÃ¡c mÃ´ hÃ¬nh ráº¥t Ä‘áº·c biá»‡t:\nThá»© nháº¥t, chÃºng Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng â€œmÃ¡yâ€ ğŸ˜€ VÃ o nÄƒm 2017, má»™t ma thuáº­t Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘ trong paper â€œNeural architecture search with reinforcement learningâ€ cá»§a chÃ­nh tÃ¡c giáº£ Quoc V. Le, nÃ³ giÃºp chÃºng ta xÃ¢y má»™t kiáº¿n trÃºc phÃ¹ há»£p nháº¥t cÃ³ thá»ƒ dá»±a theo Ä‘á»™ chÃ­nh xÃ¡c, Ä‘á»™ phá»©c táº¡p mÃ  chÃºng ta yÃªu cáº§u.\nVá»›i há» EfficientNet, cÃ¡c tÃ¡c giáº£ táº­p trung vÃ o viá»‡c giá»›i háº¡n Ä‘á»™ phá»©c táº¡p (cá»¥ thá»ƒ lÃ  FLOPS). Má»¥c tiÃªu tá»‘i Æ°u cá»§a reinforcement learning lÃ \n$$ ACC(m) \\times \\left ( \\frac{FLOPS(m)}{T} \\right )^w $$\n, vá»›i $m$ lÃ  mÃ´ hÃ¬nh, $ACC$ vÃ  $FLOPS$ lÃ  Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ phá»©c táº¡p, $T$ lÃ  FLOPS mong muá»‘n vÃ  nÃ³ báº±ng $400 \\times 10^6$, $w=-0.07$ lÃ  háº±ng sá»‘ Ä‘iá»u chá»‰nh trade-off giá»¯a $ACC$ vÃ  $FLOPS$\nThá»© hai, tá»« má»™t mÃ´ hÃ¬nh ban Ä‘áº§u lÃ  EfficientNet-B0, ta tiáº¿n hÃ nh scale theo 2 bÆ°á»›c:\nBÆ°á»›c 1: Cá»‘ Ä‘á»‹nh $\\phi = 1$, giáº£ sá»­ lÆ°á»£ng tÃ i nguyÃªn mÃ  ta cÃ³ thá»ƒ sá»­ dá»¥ng lÃ  nhiá»u gáº¥p Ä‘Ã´i hiá»‡n táº¡i. Khi Ä‘Ã³, thá»±c hiá»‡n grid search Ä‘á»ƒ tÃ¬m cÃ¡c giÃ¡ trá»‹ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tá»‘t nháº¥t BÆ°á»›c 2: Tá»« cÃ¡c giÃ¡ trá»‹ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tÃ¬m Ä‘Æ°á»£c, tiáº¿n hÃ nh scale theo cÃ¡c giÃ¡ trá»‹ $\\phi$ lá»›n hÆ¡n Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c cÃ¡c phiÃªn báº£n B1-B7. á» Ä‘Ã¢y, ta hoÃ n toÃ n cÃ³ thá»ƒ tÄƒng $\\phi$ lÃªn rá»“i láº¡i grid search nhÆ°ng lÃºc nÃ y chi phÃ­ thá»±c hiá»‡n lÃ  ráº¥t lá»›n. Do Ä‘Ã³, cÃ¡c tÃ¡c giáº£ chá»‰ grid search má»™t láº§n rá»“i sau Ä‘Ã³ chá»‰ cáº§n tÄƒng $\\phi$.\nKiáº¿n trÃºc mÃ´ hÃ¬nh Äáº§u tiÃªn, tá»•ng quan kiáº¿n trÃºc cá»§a EfficientNet-B0 nhÆ° sau:\nTrong Ä‘Ã³: MBConv chÃ­nh lÃ  inverted residual block trong MobileNetV2, cÃ¹ng vá»›i má»™t sá»‘ cáº£i tiáº¿n nhÆ° trong paper â€œSqueeze-and-excitation networksâ€ CÃ¡c mÃ´ hÃ¬nh EfficientNet-B1 cho Ä‘áº¿n B7 chÃ­nh lÃ  káº¿t quáº£ cá»§a viá»‡c Ã¡p dá»¥ng compound scaling lÃªn EfficientNet-B0.\nTÃ i liá»‡u tham kháº£o Paper EfficientNet: https://arxiv.org/abs/1905.11946 ","date":"2023-02-15T11:17:46+07:00","permalink":"https://htrvu.github.io/post/efficientnet/","title":"EfficientNet (2020)"},{"content":"Giá»›i thiá»‡u Tá»« sá»± thÃ nh cÃ´ng cá»§a MobileNet (2017) trong viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh Deep Learning trÃªn cÃ¡c thiáº¿t bá»‹ biÃªn (smartphone, embedded,â€¦) nhá» vÃ o viá»‡c sá»­ dá»¥ng hiá»‡u quáº£ phÃ©p toÃ¡n depthwise separable convolution, nhiá»u nghiÃªn cá»©u dá»±a trÃªn hÆ°á»›ng phÃ¡t triá»ƒn nÃ y Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh.\nDá»±a theo cÃ¡c â€œkinh nghiá»‡mâ€ cÃ³ Ä‘Æ°á»£c cá»§a báº£n thÃ¢n, nhÃ¬n vÃ o MobileNet thÃ¬ ta sáº½ tháº¥y ngay ráº±ng, nÃ³ chÆ°a cÃ³ cÃ¡i skip connection nÃ o cáº£ ğŸ˜€ ÄÃºng z, skip connection Ä‘Ã£ cho tháº¥y Ä‘Æ°á»£c sá»± hiá»‡u quáº£ cá»§a mÃ¬nh trong cÃ¡c mÃ´ hÃ¬nh nhÆ° ResNet, Inception-ResNet, DenseNet,â€¦ táº¡i sao ta khÃ´ng thá»­ thÃªm vÃ o MobileNet? Boom, thÃªm ngay!\nMobileNetV2 Ä‘Æ°á»£c cÃ´ng bá»‘ vá»›i sá»± káº¿ thá»«a tá»« MobileNet vÃ  bá»• sung thÃªm skip connection. Táº¥t nhiÃªn lÃ  khÃ´ng chá»‰ dá»«ng á»Ÿ Ä‘Ã³ ğŸ™‚ CÃ¡c tÃ¡c giáº£ xÃ¢y dá»±ng MobileNetV2 dá»±a trÃªn cÃ¡c inverted residual block, nÆ¡i mÃ  cÃ¡c skip connection dÃ¹ng Ä‘á»ƒ káº¿t ná»‘i cÃ¡c bottleneck layer vá»›i nhau! HÆ¡n ná»¯a, ta cÃ²n cÃ³ má»™t Ä‘iá»ƒm ráº¥t thÃº vá»‹ lÃ  cÃ¡c bottleneck layer nÃ y sá»­ dá»¥ng activation function lÃ  linear!\nMobileNetV2 Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n MobileNet trÃªn táº­p ImageNet, vá»›i sá»‘ tham sá»‘ Ã­t hÆ¡n, lÆ°á»£ng bá»™ nhá»› cáº§n dÃ¹ng táº¡i má»—i layer lÃ  Ã­t hÆ¡n. Tá»« sá»± ra Ä‘á»i cá»§a mÃ´ hÃ¬nh nÃ y, ngÆ°á»i ta cÅ©ng Ä‘Ã£ phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh hiá»‡u quáº£ trong bÃ i toÃ¡n Object Detection nhÆ° SSDLite, hay Semantic Segmentation nhÆ° Mobile DeepLabv3\nBÃ n vá» ReLU Ta biáº¿t ráº±ng tá»« khi paper AlexNet giá»›i thiá»‡u activation function ReLU thÃ¬ nÃ³ Ä‘Ã£ trá»Ÿ thÃ nh má»™t activation function ráº¥t phá»• biáº¿n vÃ  Ä‘Æ°á»£c dÃ¹ng thÆ°á»ng xuyÃªn trong cÃ¡c mÃ´ hÃ¬nh Deep Learning vá»›i Ä‘iá»ƒm máº¡nh quan trá»ng lÃ  Ä‘áº¡o hÃ m cá»§a nÃ³ ráº¥t Ä‘Æ¡n giáº£n. CÃ´ng thá»©c cá»§a ReLU lÃ \n$$ReLU(x) = \\max(x, 0)$$\n, tá»©c lÃ  nÃ³ sáº½ â€œvá»©tâ€ nhá»¯ng giÃ¡ trá»‹ bÃ© hÆ¡n 0 trong input. Äiá»u nÃ y nghÄ©a lÃ  ta sáº½ bá»‹ máº¥t thÃ´ng tin!. Náº¿u input truyá»n vÃ o lÃ  má»™t channel thÃ¬ ta sáº½ bá»‹ máº¥t má»™t lÆ°á»£ng thÃ´ng tin nhá» (hoáº·c cÃ³ thá»ƒ lÃ  lá»›n) trÃªn channel Ä‘Ã³.\nActivation function ReLU Nguá»“n: Research Gate Váº­y táº¡i sao trÆ°á»›c giá» ReLU váº«n luÃ´n Ä‘Æ°á»£c sá»­ dá»¥ng?\nÄiá»u quan trá»ng lÃ  chÃºng ta cÃ³ ráº¥t nhiá»u channel vÃ  giá»¯a cÃ¡c channel nÃ y cÃ³ nhá»¯ng má»‘i liÃªn há»‡ nháº¥t Ä‘á»‹nh. Do Ä‘Ã³, viá»‡c máº¥t thÃ´ng tin á»Ÿ má»™t channel nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c channel khÃ¡c bÃ¹ Ä‘áº¯p. NhÆ° váº­y lÃ  ok. Äá»ƒ minh há»a cho yáº¿u tá»‘ lÃ m máº¥t thÃ´ng tin, cÃ¡c tÃ¡c giáº£ Ä‘Æ°a ra vÃ­ dá»¥ sau:\nBan Ä‘áº§u, input cá»§a ta á»Ÿ khÃ´ng gian 2 chiá»u. Qua phÃ©p biáº¿n Ä‘á»•i báº±ng má»™t ma tráº­n $T$ báº¥t kÃ¬ vÃ  Ã¡p dá»¥ng ReLU, ta cÃ³ cÃ¡c output á»Ÿ cÃ¡c khÃ´ng gian cÃ³ sá»‘ chiá»u khÃ¡c nhau lÃ  2, 3, 5, 15, 30. Äá»ƒ xÃ¡c Ä‘á»‹nh xem thÃ´ng tin cÃ³ bá»‹ máº¥t hay khÃ´ng, ta chiáº¿u cÃ¡c output nÃ y vá» láº¡i khÃ´ng gian 2 chiá»u báº±ng cÃ¡ch dÃ¹ng ma tráº­n nghá»‹ch Ä‘áº£o $T^{-1}$. Khi Ä‘Ã³, káº¿t quáº£ thu Ä‘Æ°á»£c lÃ  cÃ¡c hÃ¬nh tÆ°Æ¡ng á»©ng á»Ÿ trÃªn. RÃµ rÃ ng lÃ  tÃ­nh cháº¥t ban Ä‘áº§u cá»§a input Ä‘Ã£ bá»‹ máº¥t. Gá»‰a sá»­ tá»« má»™t input $D_F \\times D_F \\times M$, qua má»™t sá»‘ layer thÃ¬ ta cÃ³ output $D_F \\times D_F \\times N$ vÃ  ta chuáº©n bá»‹ Ã¡p dá»¥ng ReLU cho output. CÃ¡c tÃ¡c giáº£ chá»©ng minh Ä‘Æ°á»£c ráº±ng ReLU sáº½ khÃ´ng lÃ m máº¥t thÃ´ng tin ban Ä‘áº§u cá»§a input náº¿u nhÆ° $N \u0026lt; M$. Äiá»u nÃ y cÃ³ thá»ƒ phÃ¡t biá»ƒu báº±ng lá»i lÃ  náº¿u má»™t input cÃ³ thá»ƒ Ä‘Æ°á»£c embedded (hay lÃ  nÃ©n) vÃ o má»™t khÃ´ng gian Ã­t chiá»u hÆ¡n (sá»‘ channel Ã­t hÆ¡n) thÃ¬ viá»‡c Ã¡p dá»¥ng ReLU lÃªn káº¿t quáº£ nÃ©n Ä‘Ã³ sáº½ khÃ´ng lÃ m máº¥t thÃ´ng tin.\ná» vÃ­ dá»¥ phÃ­a trÃªn thÃ¬ ta Ä‘Ã£ cÃ³ $N \\geq M$ vÃ  thÃ´ng tin tháº­t sá»± lÃ  Ä‘Ã£ bá»‹ máº¥t. Tá»« nháº­n xÃ©t trÃªn, ta tháº¥y ráº±ng khÃ´ng pháº£i lÃºc nÃ o xÃ i ReLU cÅ©ng tá»‘t. Náº¿u ngáº«m láº¡i, trong cÃ¡c kiáº¿n trÃºc nhÆ° VGG, ResNet, Inception, MobileNet thÃ¬ sá»‘ channel cá»§a chÃºng háº§u nhÆ° luÃ´n tÄƒng qua tá»«ng block (chÃ­nh lÃ  cá»¥m â€œmá»™t sá»‘ layerâ€) nhÆ°ng activation function Ä‘Æ°á»£c sá»­ dá»¥ng luÃ´n lÃ  ReLU. Äiá»u nÃ y lÃ  vÃ¬ chÃºng cÃ³ ráº¥t nhiá»u channel (tÄƒng theo bá»™i 2) nÃªn má»i thá»© váº«n á»•n ğŸ˜€\nNáº¿u báº¡n tháº¯c máº¯c lÃ  vÃ¬ sao sá»‘ channel thÆ°á»ng tÄƒng nhÆ° váº­y thÃ¬ trong CNN, nhá»¯ng conv layer Ä‘áº§u thÆ°á»ng sáº½ há»c nhá»¯ng Ä‘áº·c trÆ°ng Ä‘Æ¡n giáº£n nhÆ° cáº¡nh ngang, dá»c, chÃ©o, vá»‹ trÃ­ cá»§a Ä‘á»‘i tÆ°á»£ng trong áº£nh,â€¦ cÃ ng vá» sau thÃ¬ sáº½ cÃ³ cÃ¡c Ä‘áº·c trÆ°ng cá»¥ thá»ƒ, ná»•i báº­t lÃªn cá»§a Ä‘á»‘i tÆ°á»£ng (vÃ­ dá»¥ nhÆ° tai mÃ¨o, máº¯t mÃ¨o, mÅ©i mÃ¨o,â€¦). Do Ä‘Ã³, cÃ ng vá» sau thÃ¬ ta nÃªn cÃ³ cÃ ng nhiá»u channel Ä‘á»… há»c Ä‘Æ°á»£c nhiá»u Ä‘áº·c trÆ°ng. Linear bottleneck Äáº§u tiÃªn, ta sáº½ nháº¯c láº¡i vá» bottleneck layer. ÄÃ¢y lÃ  dáº¡ng layer thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng vá»›i má»¥c Ä‘Ã­ch lÃ  â€œcÃ´ Ä‘á»ng kiáº¿n thá»©câ€ cá»§a mÃ´ hÃ¬nh, hay nÃ³i rÃµ hÆ¡n lÃ  nÃ©n lÆ°á»£ng thÃ´ng tin láº¡i sao cho vá»«a giá»¯ Ä‘Æ°á»£c thÃ´ng tin vÃ  vá»«a tiáº¿t kiá»‡m tÃ i nguyÃªn (bá»™ nhá»›, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n).\nNhá»¯ng phÃ¡t hiá»‡n vá» ReLU nhÆ° Ä‘Ã£ Ä‘á» cáº­p lÃ  nguá»“n gá»‘c cá»§a cÃ¡c linear bottleneck Ä‘Æ°á»£c sá»­ dá»¥ng trong MobileNetV2.\nTáº¡i sao láº¡i lÃ  linear mÃ  khÃ´ng tiáº¿p tá»¥c dÃ¹ng ReLU rá»“i tÄƒng sá»‘ channel nhÆ° nhá»¯ng block trÆ°á»›c?\nViá»‡c giáº£m sá»‘ channel trong cÃ¡c layer sáº½ giáº£m lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh, tá»« Ä‘Ã³ giáº£m Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n. Do Ä‘Ã³, náº¿u xÃ¢y dá»±ng Ä‘Æ°á»£c má»™t kiáº¿n trÃºc mÃ  sá»‘ lÆ°á»£ng channel trong má»—i layer lÃ  nhá» thÃ¬ nÃ³ sáº½ ráº¥t phÃ¹ há»£p cho cÃ¡c thiáº¿t bá»‹ biÃªn. CÃ¡c tÃ¡c giáº£ hÆ°á»›ng Ä‘áº¿n viá»‡c giá»¯ cho sá»‘ channel cá»§a input vÃ  output cá»§a cÃ¡c block lÃ  nhá», tá»©c lÃ  sá»‘ channel chÆ°a cháº¯c Ä‘á»§ nhiá»u Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng ReLU khÃ´ng lÃ m máº¥t thÃ´ng tin ğŸ˜œ.\nNáº¿u mÃ  toÃ n cÃ¡c output cÃ³ channel nhá» nhÆ° váº­y thÃ¬ lÃ m sao mÃ  mÃ´ hÃ¬nh Ä‘áº¡t hiá»‡u quáº£ Ä‘Æ°á»£c? LÃ­ do lÃ  vÃ¬ nÃ³ lÃ  output cá»§a cÃ¡c bottleneck nÃªn váº«n ok ğŸ˜€ Sá»‘ channel trong cÃ¡c layer cá»§a MobileNetV2\nNguá»“n: Machine Think Inverted residual block vÃ  expansion factor Inverted residual block lÃ  thÃ nh pháº§n chÃ­nh xÃ¢y dá»±ng nÃªn MobileNetV2. Trong block nÃ y, ta sáº½ Ã¡p dá»¥ng cáº£ dethwise separable convolution, linear bottleneck vÃ  skip connection.\nTuy nhiÃªn, lÆ°u Ã½ ráº±ng sá»‘ channel cá»§a input vÃ  output cá»§a cÃ¡c block nÃ y lÃ  ráº¥t nhá». Qua hÃ¬nh á»Ÿ trÃªn thÃ¬ ta tháº¥y chÃºng chá»‰ quanh quáº©n 16, 24, 32. Do Ä‘Ã³, trÆ°á»›c khi Ã¡p dá»¥ng depthwise separable convolution lÃªn input thÃ¬ cÃ¡c tÃ¡c giáº£ thá»±c hiá»‡n giáº£i nÃ©n (expansion) lÆ°á»£ng kiáº¿n thá»©c trong input (input nÃ y lÃ  output cá»§a má»™t block trÆ°á»›c Ä‘Ã³, nÆ¡i mÃ  kiáº¿n thá»©c Ä‘Ã£ Ä‘Æ°á»£c nÃ©n láº¡i bá»Ÿi linear bottleneck).\nVá» máº·t trá»±c giÃ¡c, lÃ­ do cá»§a thao tÃ¡c nÃ y cÃ³ thá»ƒ hiá»ƒu lÃ  ta sáº½ thá»±c hiá»‡n convolution trÃªn thÃ´ng tin Ä‘áº§y Ä‘á»§ hÆ¡n Ä‘á»ƒ cÃ³ thá»ƒ phÃ¡t hiá»‡n Ä‘Æ°á»£c cÃ ng nhiá»u Ä‘áº·c trÆ°ng cÃ ng tá»‘t).\nViá»‡c giáº£i nÃ©n thá»±c cháº¥t lÃ  ta sá»­ dá»¥ng má»™t conv layer $1 \\times 1$, vá»›i sá»‘ lÆ°á»£ng filter sáº½ báº±ng vá»›i sá»‘ lÆ°á»£ng channel cá»§a input nhÃ¢n vá»›i má»™t siÃªu tham sá»‘. SiÃªu tham sá»‘ nÃ y gá»i lÃ  expansion factor vÃ  Ä‘Æ°á»£c kÃ­ hiá»‡u lÃ  $t$.\nTiáº¿p Ä‘áº¿n, theo sau phÃ©p toÃ¡n depthwise separable convolution thÃ¬ ta sáº½ sá»­ dá»¥ng linear bottleneck Ä‘á»ƒ tÃ­nh ra output cá»§a block.\nNhÆ° váº­y, Ä‘iá»ƒm qua cÃ¡c layer sáº½ cÃ³ trong inverted residual block sáº½ bao gá»“m:\nLÆ°u Ã½. Layer â€œlinear 1x1 conv2dâ€ chÃ­nh lÃ  linear bottleneck. TÃ¹y theo giÃ¡ trá»‹ sá»‘ channel $k$ vÃ  $k\u0026rsquo;$ cÃ³ báº±ng nhau hay khÃ´ng mÃ  ta sáº½ Ã¡p dá»¥ng thÃªm skip connection. HÆ¡n ná»¯a, cÃ¡c skip connection trong inverted residual block Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ ná»‘i cÃ¡c bottleneck layer vá»›i nhau!\nVÃŒ sao láº¡i lÃ  ná»‘i bottleneck chá»© khÃ´ng pháº£i ná»‘i cÃ¡c layer khÃ¡c? Output cá»§a bottleneck lÃ  cÃ¡c â€œkiáº¿n thá»©câ€ Ä‘Ã£ Ä‘Æ°á»£c cÃ´ Ä‘á»ng, Ä‘Ã¢y lÃ  nhá»¯ng gÃ¬ mÃ  mÃ´ hÃ¬nh Ä‘Ã£ há»c Ä‘Æ°á»£c vÃ  Ä‘Æ°á»£c biá»ƒu diá»…n trong má»™t khÃ´ng gian Ã­t chiá»u hÆ¡n. Do Ä‘Ã³, ta vá»«a tiáº¿t kiá»‡m Ä‘Æ°á»£c tÃ i nguyÃªn vÃ  vá»«a liÃªn káº¿t Ä‘Æ°á»£c cÃ¡c kiáº¿n thá»©c quan trá»ng vá»›i nhau. Trong cÃ i Ä‘áº·t, tÃ¹y theo giÃ¡ trá»‹ stride cá»§a depthwise conv layer mÃ  ta sáº½ Ã¡p dá»¥ng skip connection hoáº·c lÃ  khÃ´ng. Cá»¥ thá»ƒ nhÆ° sau:\nInverted Residual Block vá»›i stride=1 (cÃ³ skip connection) vÃ  stride=2 (khÃ´ng cÃ³) NgoÃ i ra, ta cÃ³ thá»ƒ tháº¥y trong inverted residual block thÃ¬ activation Ä‘Æ°á»£c dÃ¹ng cho 2 layer Ä‘áº§u tiÃªn lÃ  ReLU6. ÄÃ¢y lÃ  má»™t biáº¿n thá»ƒ cá»§a ReLU, nÃ³ giá»›i háº¡n giÃ¡ trá»‹ output náº±m trong Ä‘oáº¡n $[0, 6]$ nháº±m Ä‘áº£m báº£o sá»± á»•n Ä‘á»‹nh trong tÃ­nh toÃ¡n vá»›i sá»‘ cháº­m Ä‘á»™ng.\nActivation function RELU6\nNguá»“n: Mmuratarat Äá»ƒ dá»… hÃ¬nh dung hÆ¡n vá» inverted residual block, ta cÃ¹ng xem má»™t vÃ­ dá»¥ cho quÃ¡ trÃ¬nh tÃ­nh toÃ¡n vá»›i expansion factor lÃ  6:\nNguá»“n: Machine Think LÆ°u Ã½.\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p thÃªm Ä‘áº¿n luá»“ng truyá»n thÃ´ng tin cá»§a MobileNetV2, yáº¿u tá»‘ má»Ÿ ra nhá»¯ng hÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p theo trong tÆ°Æ¡ng lai. Ta tháº¥y ráº±ng inverted residual block Ä‘Ã£ táº¡o ra Ä‘Æ°á»£c sá»± Ä‘á»™c láº­p giá»¯a sá»‘ channel cá»§a intput/output cá»§a block vÃ  cá»§a cÃ¡c layer náº±m bÃªn trong block:\nPháº§n bÃªn trong Ä‘Æ°á»£c gá»i lÃ  layer transformation vá»›i nhá»¯ng phÃ©p biáº¿n Ä‘á»•i phi tuyáº¿n. Ta hoÃ n toÃ n cÃ³ thá»ƒ nghiÃªn cá»©u thÃªm nhá»¯ng cÃ¡ch xÃ¢y dá»±ng bá»™ pháº­n nÃ y Ä‘á»ƒ tÄƒng Ä‘á»™ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh. Náº¿u expansion factor cá»§a ta \u0026lt; 1 thÃ¬ block nÃ y sáº½ ráº¥t giá»‘ng vá»›i block trong ResNet:\nKiáº¿n trÃºc MobileNetV2 MobileNetV2 Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn viá»‡c sá»­ dá»¥ng nhiá»u inverted residual block. Kiáº¿n trÃºc tá»•ng quan cá»§a nÃ³ nhÆ° sau:\nTrong Ä‘Ã³:\n$t$ lÃ  expansion factor. $c$ lÃ  sá»‘ output channel cá»§a pháº§n bottleneck trong inverted residual block. $n$ lÃ  sá»‘ láº§n sá»­ dá»¥ng block. $s$ lÃ  stride cá»§a block Ä‘áº§u tiÃªn trong dÃ£y $t$ block liÃªn tiáº¿p nhau, cÃ¡c block cÃ²n láº¡i trong dÃ£y cÃ³ stride 1. ToÃ n bá»™ filter Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»u lÃ  $3 \\times 3$. CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t MobileNetV2 báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nTÃ i liá»‡u tham kháº£o Paper MobileNetV2: https://arxiv.org/abs/1801.04381 MachineThink, MobileNet version 2 ","date":"2023-02-13T11:32:55+07:00","permalink":"https://htrvu.github.io/post/mobilenet_v2/","title":"MobileNet V2 (2019)"},{"content":"Skip connection vÃ  concatenate TrÆ°á»›c Ä‘Ã³, kiáº¿n trÃºc ResNet Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ  nÃ³ Ä‘Ã£ cho tháº¥y Ä‘Æ°á»£c sá»©c máº¡nh cá»§a cÃ¡c skip connection khi chÃºng Ä‘Æ°á»£c thÃªm vÃ o cÃ¡c mÃ´ hÃ¬nh tá»« sÃ¢u cho Ä‘áº¿n ráº¥t sÃ¢u (vÃ­ dá»¥ nhÆ° ResNet152). Ta tháº¥y ráº±ng nhá»¯ng kiáº¿n trÃºc Ã¡p dá»¥ng skip connection trÆ°á»›c Ä‘Ã¢y Ä‘á»u cÃ³ má»™t Ä‘iá»ƒm chung lÃ  trong má»™t block thÃ¬ ta sáº½ cÃ³ nhá»¯ng Ä‘iá»ƒm ná»‘i 1 feature map vÃ o lÃ m input cá»§a má»™t layer sau Ä‘Ã³, vÃ  chÃºng Ä‘á»u sá»­ dá»¥ng phÃ©p toÃ¡n cá»™ng.\nResidual block trong ResNet sá»­ dá»¥ng skip connection vá»›i phÃ©p toÃ¡n cá»™ng Nguá»“n: Idiot Developer CÃ´ng thá»©c vá» skip connection trong block trÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t nhÆ° sau:\n$$ x_l = H_l(x_{l - 1}) + x_{l-1} $$\n, vá»›i $H_l$ lÃ  phÃ©p biáº¿n Ä‘á»•i phi tuyáº¿n á»Ÿ layer thá»© $l$, $x_l$ lÃ  output cá»§a layer thá»© $l$.\nPaper DenseNet giá»›i thiá»‡u má»™t kiáº¿n trÃºc vá»›i Ã½ tÆ°á»Ÿng lÃ  feature-map táº¡i layer $l$ sáº½ sá»­ dá»¥ng toÃ n bá»™ feature-maps á»Ÿ phÃ­a trÆ°á»›c (layer $l - 1, l - 2,\u0026hellip;$) Ä‘á»ƒ lÃ m input, vÃ  chÃºng sá»­ dá»¥ng concatenate (thay vÃ¬ phÃ©p toÃ¡n cá»™ng nhÆ° ResNet). Vá»›i tÆ° tÆ°á»Ÿng nhÆ° váº­y, cÃ¡c feature-maps ta cÃ³ Ä‘Æ°á»£c cÃ³ thá»ƒ xem lÃ  má»™t tráº¡ng thÃ¡i cÃ³ pháº¡m vi toÃ n cá»¥c vÃ  báº¥t kÃ¬ layer nÃ o cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng tráº¡ng thÃ¡i nÃ y trong viá»‡c tÃ­nh toÃ¡n ra feature-maps cá»§a nÃ³. Náº¿u viáº¿t theo kiá»ƒu cÃ´ng thá»©c thÃ¬ ta sáº½ cÃ³\n$$ x_l = H_l([x_0, x_1,\u0026hellip;, x_{l-1}]) $$\nLÆ°u Ã½. Äá»ƒ thá»±c hiá»‡n phÃ©p toÃ¡n concatenate thÃ¬ cÃ¡c feature-maps pháº£i cÃ³ cÃ¹ng size, hay lÃ  width vÃ  height.\nMá»™t vÃ­ dá»¥ cho kiáº¿n trÃºc DenseNet nhÆ° sau:\nTrong hÃ¬nh trÃªn, vá»›i $L$ layer, ta cÃ³ $\\dfrac{L(L+1)}{2}$ káº¿t ná»‘i trá»±c tiáº¿p giá»¯a cÃ¡c layer. CÃ¡c káº¿t ná»‘i Ä‘Æ°á»£c táº¡o ra lÃ  ráº¥t dÃ y Ä‘áº·c (dense). Tá»« Ä‘Ã³, tÃªn cá»§a kiáº¿n trÃºc Ä‘Æ°á»£c Ä‘áº·t lÃ  Dense Convolutional Network (DenseNet). BÃ n vá» cÃ¡ch tá»• chá»©c cÃ¡c liÃªn káº¿t nhÆ° váº­y má»™t chÃºt:\nNhÃ³m tÃ¡c giáº£ cho ráº±ng kiáº¿n trÃºc nhÆ° DenseNet sáº½ Ä‘áº£m báº£o lÆ°á»£ng thÃ´ng tin cÅ©ng nhÆ° gradient truyá»n qua cÃ¡c layer lÃ  nhiá»u nháº¥t cÃ³ thá»ƒ , tá»« Ä‘Ã³ mÃ´ hÃ¬nh sáº½ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c tá»« nhiá»u thÃ´ng tin hÆ¡n, vÃ  táº¥t nhiÃªn lÃ  nÃ³ sáº½ táº¡o ra hiá»‡u á»©ng lÃ m dá»‹u bá»›t hiá»‡n tÆ°á»£ng vanishing gradient.\nÄá»“ng thá»i, viá»‡c sá»­ phÃ©p toÃ¡n concatenate cÃ³ mang Ä‘áº¿n cho ta trá»±c giÃ¡c lÃ  cÃ³ sá»± phÃ¢n biá»‡t rÃµ hÆ¡n giá»¯a input trá»±c tiáº¿p tá»« layer á»Ÿ ngay phÃ­a trÆ°á»›c nÃ³ vá»›i cÃ¡c thÃ´ng tin Ä‘Æ°á»£c â€œlÆ°u trá»¯â€ vÃ  truyá»n Ä‘áº¿n tá»« cÃ¡c layer á»Ÿ phÃ­a trÆ°á»›c ná»¯a. Náº¿u sá»­ dá»¥ng phÃ©p toÃ¡n cá»™ng, nhá»¯ng yáº¿u tá»‘ nÃ y Ä‘Ã£ bá»‹ pha láº«n vÃ o nhau.\nCÃ³ má»™t chi tiáº¿t mÃ  ta thÆ°á»ng nghÄ© Ä‘áº¿n á»Ÿ cÃ¡c mÃ´ hÃ¬nh cÃ³ kiáº¿n trÃºc ráº¥t sÃ¢u (nhiá»u layer) lÃ  sá»‘ lÆ°á»£ng tham sá»‘ cá»§a nÃ³ sáº½ ráº¥t lá»›n. Tuy nhiÃªn, vá»›i DenseNet thÃ¬ Ä‘iá»u nÃ y khÃ´ng pháº£i lÃ  váº¥n Ä‘á». Sá»‘ feature-maps cá»§a cÃ¡c layer trong DenseNet sáº½ ráº¥t nhá» (chá»‰ táº§m khÃ´ng quÃ¡ 60), vá»›i lÃ½ do lÃ  Ä‘á»ƒ tÃ­nh toÃ¡n cho layer káº¿ tiáº¿p thÃ¬ ta Ä‘Ã£ dÃ¹ng toÃ n bá»™ feature-maps á»Ÿ cÃ¡c phÃ­a trÆ°á»›c rá»“i chá»© khÃ´ng pháº£i chá»‰ má»—i layer liá»n trÆ°á»›c nÃ³ nhÆ° háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh khÃ¡c, nÃªn táº¡i má»—i layer ta chá»‰ cáº§n táº§m Ä‘Ã³ lÃ  Ä‘á»§ rá»“i ğŸ˜€\nKáº¿t quáº£ so sÃ¡nh giá»¯a DenseNet vÃ  ResNet trÃªn dataste ImageNet Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ cÃ´ng bá»‘ nhÆ° hÃ¬nh bÃªn dÆ°á»›i. Ta tháº¥y ráº±ng DenseNet cÃ³ sá»‘ lÆ°á»£ng tham sá»‘ vÃ  sá»‘ phÃ©p toÃ¡n Ã­t hÆ¡n ResNet, cÃ¹ng vá»›i Ä‘á»™ hiá»‡u quáº£ cao hÆ¡n.\nDense block, transition layer vÃ  growth rate Dense block vÃ  transition layer Ta tháº¥y ráº±ng náº¿u Ã¡p dá»¥ng Ã½ tÆ°á»Ÿng káº¿t ná»‘i dÃ y Ä‘áº·t cá»§a DenseNet cho toÃ n bá»™ layer trong mÃ´ hÃ¬nh thÃ¬ toÃ n bá»™ feature-maps trong táº¥t cáº£ layer nÃ y Ä‘á»u pháº£i cÃ³ cÃ¹ng size (do phÃ©p toÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  concatnerate).\nTuy nhiÃªn, náº¿u toÃ n bá»™ cÃ¡c layer trong kiáº¿n trÃºc Ä‘á»u cÃ³ cÃ¹ng size nhÆ° váº­y thÃ¬ ta khÃ³ mÃ  down-sampling feature-maps vá» cÃ¡c size nhá» hÆ¡n vÃ  rá»“i sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c layer nhÆ° Average Pooling, Dense Ä‘á»ƒ cho ra output nhÆ° cÃ¡c kiáº¿n trÃºc khÃ¡c Ä‘Æ°á»£c. VÃ  viá»‡c â€œcÃ´ Ä‘á»ngâ€ kiáº¿n thá»©c cá»§a mÃ´ hÃ¬nh cÅ©ng sáº½ gáº·p khÃ³ khÄƒn.\nDo Ä‘Ã³, Ã½ tÆ°á»Ÿng káº¿t ná»‘i dÃ y Ä‘áº·t Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ Ã¡p dá»¥ng trong tá»«ng khá»‘i (gá»i lÃ  Dense block), viá»‡c down-sampling sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n trong cÃ¡c khá»›p ná»‘i cÃ¡c Dense block vá»›i nhau (gá»i lÃ  Transition layer).\nCÃ³ tá»•ng cá»™ng 4 dáº¡ng Dense block nhÆ° sau:\nDense block cÆ¡ báº£n:\nHÃ m $H_l$ trong block nÃ y lÃ  sá»± káº¿t há»£p theo thá»© tá»± 3 phÃ©p toÃ¡n: $$ BN \\to ReLU \\to Conv (3 \\times 3) $$\nNgoÃ i ra, ta cÃ³ thá»ƒ thÃªm dropout vÃ o sau Conv Ä‘á»ƒ giáº£m overfitting.\nDense-B block (bottleneck):\nÄá»ƒ tÄƒng hiá»‡u suáº¥t vá» máº·t tÃ­nh toÃ¡n, ta cÃ³ thá»ƒ thÃªm má»™t phÃ©p toÃ¡n Conv $1 \\times 1$ vÃ o $H_l$ Ä‘á»ƒ giáº£m bá»›t sá»‘ lÆ°á»£ng feature-maps input. LÃºc nÃ y, thá»© tá»± cÃ¡c phÃ©p toÃ¡n sáº½ lÃ  $$ BN \\to ReLU \\to Conv (1 \\times 1) \\to BN \\to ReLU \\to Conv (3 \\times 3) $$\nDense-C block (compression):\nTa sáº½ giáº£m sá»‘ lÆ°á»£ng output feature-maps cá»§a cÃ¡c Dense block theo tham sá»‘ $0 \u0026lt; \\theta \\leq 1$: tá»« $m$ feature-maps thÃ nh $\\lfloor \\theta m \\rfloor$ ThÃ´ng thÆ°á»ng, pháº§n cÃ i Ä‘áº·t cá»§a thao tÃ¡c compression Ä‘Æ°á»£c ghÃ©p vÃ o transition layer. Dense-BC block:\nKáº¿t há»£p bottleneck vÃ  compression vÃ o Dense block. Trong kiáº¿n trÃºc tá»•ng thá»ƒ, náº¿u trÆ°á»›c Ä‘Ã³ ta dÃ¹ng Dense-C hoáº·c Dense-BC block thÃ¬ theo sau nÃ³ sáº½ cÃ³ thÃªm layer bottleneck (Conv $1 \\times 1) vÃ  thÃ nh pháº§n nÃ y gá»i lÃ  transition layer. BÃªn cáº¡nh conv layer, thÃ nh pháº§n khÃ´ng thá»ƒ thiáº¿u trong transition layer lÃ  má»™t lá»›p Pooling (cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng Average Pooling) Ä‘á»ƒ thá»±c hiá»‡n down-sampling cÃ¡c feature-maps. Thá»© tá»± cÃ¡c phÃ©p toÃ¡n trong transition layer sáº½ lÃ \n$$ BN \\to ReLU \\to Conv (1 \\times 1) \\to AvgPool (2 \\times 2) $$\nGrowth rate NhÆ° Ä‘Ã£ Ä‘á» cáº­p á»Ÿ pháº§n Ã½ tÆ°á»Ÿng, lÆ°á»£ng tham sá»‘ trong DenseNet Ä‘Æ°á»£c tá»‘i thiá»ƒu hÃ³a lÃ  nhá» vÃ o chi tiáº¿t sá»‘ lÆ°á»£ng feature-maps táº¡i cÃ¡c layer trong DenseNet lÃ  nhá». CÃ¡c tÃ¡c giáº£ xem sá»‘ lÆ°á»£ng feature-maps $k$ táº¡i cÃ¡c layer lÃ  má»™t siÃªu tham sá»‘ cá»§a DenseNet, vÃ  nÃ³ Ä‘Æ°á»£c gá»i lÃ  growth rate.\nThá»±c nghiá»‡m cho tháº¥y ráº±ng cÃ¡c giÃ¡ trá»‹ $k$ mang láº¡i káº¿t quáº£ tá»‘t trÃªn cÃ¡c dataset thÆ°á»ng khÃ´ng quÃ¡ lá»›n. Vá» máº·t trá»±c giÃ¡c, ta cÃ³ thá»ƒ hiá»ƒu $k$ Ä‘iá»u chá»‰nh lÆ°á»£ng thÃ´ng tin má»›i mÃ  má»™t layer cÃ³ thá»ƒ Ä‘Ã³ng gÃ³p vÃ o tráº¡ng thÃ¡i toÃ n cá»¥c (Ä‘Ã³ng gÃ³p má»™t lÆ°á»£ng vá»«a Ä‘á»§ thÃ¬ sáº½ tá»‘t hÆ¡n lÃ  quÃ¡ nhiá»u hay quÃ¡ Ã­t).\nMinh há»a Dense Block vá»›i growth rate lÃ  4 Nguá»“n: https://reliablecho-programming.tistory.com/3 Kiáº¿n trÃºc DenseNet TÃ¹y vÃ o loáº¡i Dense block Ä‘Æ°á»£c sá»­ dá»¥ng, ta cÅ©ng cÃ³ cÃ¡c tÃªn gá»i khÃ¡c nhau cho DenseNet (DenseNet, DenseNet-B, Denset-C, DenseNet-BC). Kiáº¿n trÃºc DenseNet-C (hoáº·c DenseNet-BC) vá»›i 3 Dense block Ä‘Æ°á»£c mÃ´ táº£ trong hÃ¬nh bÃªn dÆ°á»›i: TrÆ°á»›c khi Ä‘áº¿n vá»›i quÃ¡ trÃ¬nh tÃ­nh toÃ¡n qua cÃ¡c Dense block vÃ  Transition layer, ta cÃ³ má»™t layer Conv (vÃ  cÃ³ thá»ƒ cÃ³ thÃªm Pooling, BN) nhÆ° Ä‘a sá»‘ cÃ¡c kiáº¿n trÃºc CNN khÃ¡c. CÃ¡c layer cá»§a cá»§a mÃ´ hÃ¬nh cÅ©ng sá»­ dá»¥ng Global Pooling vÃ  Dense cÃ¹ng activation softmax Ä‘á»ƒ táº¡o ra vector output. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c nhÃ³m tÃ¡c giáº£ thá»­ nghiá»‡m vá»›i dataset ImageNet Ä‘Æ°á»£c tÃ³m táº¯t nhÆ° sau:\nQuan sÃ¡t báº£ng trÃªn, ta cÃ³ nháº­n xÃ©t lÃ  cÃ¡c mÃ´ hÃ¬nh trÃªn Ä‘á»u thuá»™c loáº¡i DenseNet-BC. NgoÃ i ra, cÃ¡c tÃ¡c giáº£ cho biáº¿t giÃ¡ trá»‹ growth rate Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  $k=32$.\nCÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t DenseNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nKhi cÃ i Ä‘áº·t DenseNet, ta thÆ°á»ng sáº½ hÆ¡i phÃ¢n vÃ¢n vá» cÃ¡ch cÃ i Ä‘áº·t cÃ¡c Dense block. LÃ m sao Ä‘á»ƒ cÃ i Ä‘áº·t cÃ¡c káº¿t ná»‘i dÃ y Ä‘áº·c nhÆ° váº­y?\nThá»±c ra cÃ¡ch cÃ i Ä‘áº·t lÃ  ráº¥t Ä‘Æ¡n giáº£n. Ta gá»i khá»‘i gá»“m (Conv $1 \\times 1$, Conv $3 \\times 3$) nhÆ° báº£ng trÃªn lÃ  bottleneck block (bb). Khi Ä‘Ã³ 1 dense block vá»›i 4 bottleneck block sáº½ cÃ³ dáº¡ng nhÆ° sau:\n$$x \\to bb_1 \\to x_1 \\to bb_2 \\to x_2 \\to bb_3 \\to x_3 \\to bb_4 \\to x_4 (output) $$\nTrong Ä‘Ã³:\nbb_1.input = [ x ] bb_2.input = [x1, x] bb_3.input = [x2, x1, x] bb_4.input = [x3, x2, x1, x] MÃ£ giáº£ cho cÃ¡ch cÃ i Ä‘áº·t dense block nÃ y nhÆ° sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def bottleneck_block(input, k): x = BN(input) x = ReLU(x)\tx = conv_1x1(x) x = BN(x) x = ReLU(x) x = conv_3x3(x) return x def dense_block(input, k): c = input x1 = bottleneck_block(c, k) c = concatenate(x1, c) # c = [x1, input] x2 = bottleneck_block(c, k) c = concatenate(x2, c) # c = [x2, x1, input] x3 = bottleneck_block(c, k) c = concatenate(x3, c) # c = [x3, x2, x1, input] x4 = bottleneck_block(c, k) c = concatenate(x4, c) # c = [x4, x3, x2, x1, input] return c # done! NhÆ° váº­y, ta hoÃ n toÃ n cÃ³ thá»ƒ dÃ¹ng má»™t vÃ²ng láº·p Ä‘á»ƒ cÃ i Ä‘áº·t dense block:\n1 2 3 4 5 6 def dense_block(input, k): c = input for i in range(4): x = bottleneck_block(c, k) c = concatenate(x, c) return c TÃ i liá»‡u tham kháº£o Paper DenseNet: https://arxiv.org/abs/1608.06993 ","date":"2023-02-11T18:09:08+07:00","permalink":"https://htrvu.github.io/post/densenet/","title":"DenseNet (2018)"},{"content":"Giá»›i thiá»‡u Class Activation Map (CAM) lÃ  phÆ°Æ¡ng phÃ¡p phá»• biáº¿n trong viá»‡c giáº£i thÃ­ch sá»± hoáº¡t Ä‘á»™ng cá»§a CNN. NÃ³ cho ta biáº¿t ráº±ng CNN sáº½ táº­p trung vÃ o nhá»¯ng pháº§n nÃ o cá»§a áº£nh input Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t áº£nh Ä‘Ã³ thá»¥Ã´c vá» má»™t class nÃ o Ä‘Ã³. ThÃ´ng thÆ°á»ng, CAM cÃ²n Ä‘Æ°á»£c gá»i lÃ  Attention Map.\nÄá»ƒ dá»… hÃ¬nh dung hÆ¡n vá» CAM, ta cÃ³ 2 vÃ­ dá»¥ nhÆ° sau:\nCNN táº­p trung vÃ o pháº§n Ä‘áº§u cá»§a con chÃ³ Ä‘á»ƒ Ä‘Æ°a ra xÃ¡c suáº¥t mÃ  bá»©c áº£nh thuá»™c class â€œdogâ€ CNN táº­p trung vÃ o pháº§n Ä‘áº§u cá»§a con chÃ³ Ä‘á»ƒ Ä‘Æ°a ra xÃ¡c suáº¥t mÃ  bá»©c áº£nh thuá»™c class â€œdogâ€\nCNN táº­p trung vÃ o con mÃ¨o khi Ä‘Æ°a ra xÃ¡c suáº¥t mÃ  bá»©c áº£nh thuá»™c class â€œcatâ€\nNguá»“n: GlassBoxMedicine CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c xáº¿p vÃ o nhÃ³m post-hoc, tá»©c lÃ  ta chá»‰ tiáº¿n hÃ nh sinh ra CAM Ä‘á»ƒ giáº£i thÃ­ch sá»± hoáº¡t Ä‘á»™ng cá»§a CNN sau khi mÃ´ hÃ¬nh nÃ y Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  cÃ³ má»™t bá»™ trá»ng sá»‘ cá»‘ Ä‘á»‹nh.\nViá»‡c giáº£i thÃ­ch CNN báº±ng CAM lÃ  ráº¥t há»£p lÃ½, vÃ¬:\nTa cÃ³ thá»ƒ biáº¿t Ä‘Æ°á»£c mÃ´ hÃ¬nh cá»§a mÃ¬nh cÃ³ Ä‘ang tháº­t sá»± hoáº¡t Ä‘á»™ng tá»‘t hay khÃ´ng (táº­p trung vÃ o Ä‘Ãºng pháº§n quan trá»ng trong áº£nh), tá»©c lÃ  cÃ³ chá»©ng cá»© rÃµ rÃ ng cho cÃ¡c dá»± Ä‘oÃ¡n NÃ³ giÃºp ta ká»‹p thá»i phÃ¡t hiá»‡n nhá»¯ng Ä‘áº·c trÆ°ng mÃ  mÃ´ hÃ¬nh â€œhiá»ƒu láº§mâ€ khi há»c vá» má»™t Ä‘á»‘i tÆ°á»£ng nÃ o Ä‘Ã³. VÃ­ dá»¥, nhá» CAM thÃ¬ ta tháº¥y ráº±ng mÃ´ hÃ¬nh há»c cÃ¡ch nháº­n dáº¡ng tÃ u há»a dá»±a vÃ o cÃ¡c Ä‘Æ°á»ng ray trong áº£nh thÃ¬ rÃµ rÃ ng lÃ  nÃ³ Ä‘Ã£ há»c sai Ä‘áº·c trÆ°ng. TrÆ°á»ng há»£p nÃ y hoÃ n toÃ n cÃ³ thá»ƒ xáº£y ra vÃ¬ pháº§n lá»›n bá»©c áº£nh cÃ³ tÃ u há»a thÃ¬ cÅ©ng cÃ³ Ä‘Æ°á»ng ray, nhÆ°ng ngÆ°á»£c láº¡i thÃ¬ khÃ´ng. Ta cÃ³ thá»ƒ cÃ³ má»™t chiáº¿c xe Ã´ tÃ´ cháº¡y ngang Ä‘Æ°á»ng ray ğŸ˜€ Trong cÃ¡c phÆ°Æ¡ng phÃ¡p sinh ra CAM cho má»™t CNN (theo tá»«ng input) thÃ¬ ta cÃ³ cÃ¡c phÆ°Æ¡ng phÃ¡p ná»•i báº­t nhÆ° Default CAM, Grad-CAM, Score-CAM. Ta sáº½ láº§n lÆ°á»£t Ä‘á» cáº­p Ä‘áº¿n cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã³.\nCÃ¡c phÆ°Æ¡ng phÃ¡p sinh CAM Default CAM (2016) PhÆ°Æ¡ng phÃ¡p nÃ y Ä‘Æ°á»£c Ä‘á» xuáº¥t ngay tá»« khi cÃ¡c Ã½ tÆ°á»Ÿng vá» CAM Ä‘Æ°á»£c cÃ´ng bá»‘. Ta sáº½ dá»±a vÃ o output cá»§a conv layer cuá»‘i cÃ¹ng trong kiáº¿n trÃºc, ngay trÆ°á»›c fully connected layer sinh ra output cá»§a mÃ´ hÃ¬nh vÃ  cÃ¡c trá»ng sá»‘ trong output layer.\nÄáº§u tiÃªn, Ä‘á»ƒ mÃ´ táº£ vá» Ã½ nghÄ©a cá»§a cÃ¡c feature maps trong output cá»§a conv layer cuá»‘i cÃ¹ng thÃ¬ ta xÃ©t vÃ­ dá»¥ sau: Trong hÃ¬nh áº£nh bÃªn dÆ°á»›i, conv layer cuá»‘i cÃ¹ng cá»§a ta lÃ  â€œConv Layer nâ€. Output cá»§a nÃ³ cÃ³ $k$ channel, hay lÃ  $k$ feature maps, má»—i feature map sáº½ liÃªn quan Ä‘áº¿n má»™t Ä‘áº·c trÆ°ng nÃ o Ä‘Ã³ trong áº£nh input. Giáº£ sá»­ nhÆ° feature map $1$ sáº½ phÃ¡t hiá»‡n máº·t ngÆ°á»i trong áº£nh, feature map $2$ sáº½ phÃ¡t phiá»‡n lÃ´ng cá»§a con chÃ³,â€¦, feature map $k$ sáº½ phÃ¡t hiá»‡n tai cá»§a con chÃ³. Minh há»a Ã½ nghÄ©a cÃ¡c feature maps Nguá»“n: Johfischer CÃ¡c trá»ng sá»‘ trong output layer sáº½ mang Ã½ nghÄ©a lÃ  táº§m quan trá»ng cá»§a feature map tÆ°Æ¡ng á»©ng vÆ¡i trá»ng sá»‘ Ä‘Ã³ trong viá»‡c Ä‘Æ°a ra xÃ¡c suáº¥t dá»± Ä‘oÃ¡n áº£nh input thuá»™c má»™t class nÃ o Ä‘Ã³. VÃ­ dá»¥, ta xÃ©t class 2 nhÆ° hÃ¬nh áº£nh bÃªn dÆ°á»›i Khi Ä‘Ã³, vá»›i $k$ feature maps $F_i$, ta cÃ³ $k$ trá»ng sá»‘ $w_i$ vÃ  báº±ng cÃ¡ch tÃ­nh tá»• há»£p tuyáº¿n tÃ­nh cá»§a $F_i$ vÃ  $w_i$ thÃ¬ ta sáº½ cÃ³ CAM cá»§a áº£nh input á»©ng vá»›i class 2. PhÃ©p tá»• há»£p tuyáº¿n tÃ­nh giá»¯a cÃ¡c feature maps Nguá»“n: Johfischer $$CAM_2 = w_1 * F_1 + w_2 * F_2 + \u0026hellip; + w_k * F_k$$\nLÆ°u Ã½: KÃ­ch thÆ°á»›c width vÃ  height cá»§a CAM Ä‘ang báº±ng vá»›i cÃ¡c feature maps vÃ  nÃ³ thÆ°á»ng nhá» hÆ¡n nhiá»u so vá»›i áº£nh input. Äá»ƒ cÃ³ thá»ƒ visualize Ä‘Æ°á»£c nhÆ° trÃªn, ta chá»‰ cáº§n upsample CAM lÃªn báº±ng kÃ­ch thÆ°á»›c cá»§a áº£nh input. Vá»›i má»—i class khÃ¡c nhau thÃ¬ CAM tÃ­nh Ä‘Æ°á»£c lÃ  khÃ¡c nhau Sau khi trÃ¬nh bÃ y Ã½ tÆ°á»Ÿng cá»§a CAM thÃ¬ ta tháº¥y ngay má»™t Ä‘iá»u kiá»‡n mÃ  CNN cáº§n thá»a mÃ£n Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y lÃ  sau cÃ¡c conv layer thÃ¬ nÃ³ chá»‰ cÃ³ duy nháº¥t má»™t fully connected layer Ä‘á»ƒ sinh ra output, vÃ  Ä‘iá»ƒm ná»‘i giá»¯a hai pháº§n nÃ y lÃ  má»™t global average pooling layer (GAP). VÃ­ dá»¥ nhÆ° hÃ¬nh bÃªn dÆ°á»›i:\nNguá»“n: GlassBoxMedicine Trong kiáº¿n trÃºc trÃªn, ta cÃ³ má»™t CNN vá»›i output cá»§a conv layer cuá»‘i cÃ¹ng lÃ  3 feature map lÃ  A1, A2, A3. Qua GAP thÃ¬ ta thu Ä‘Æ°á»£c má»™t layer vá»›i 3 giÃ¡ trá»‹ sá»‘ thá»±c. Theo sau Ä‘Ã³ lÃ  má»™t fully connected layer sinh ra output cá»§a mÃ´ hÃ¬nh. VÃ¬ sao ta cáº§n cÃ³ duy nháº¥t má»™t fully connected layer? Ta Ä‘Ã£ Ä‘á» cáº­p ráº±ng cÃ¡c trá»ng sá»‘ trong output layer sáº½ mang Ã½ nghÄ©a lÃ  táº§m quan trá»ng cá»§a feature map tÆ°Æ¡ng á»©ng. Do Ä‘Ã³, ná»‘i ngay á»Ÿ Ä‘Ã¢y thÃ¬ nÃ³ má»›i â€œÄ‘Ãºng Ã½â€ (vá»›i má»—i neuron trong output layer, ta cÃ³ Ä‘Ãºng $k$ trá»ng sá»‘ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n $k$ feature maps) VÃ¬ sao ta cáº§n GAP? NÃ³ sáº½ táº¡o ra cáº§u ná»‘i giá»¯a cÃ¡c feature map vÃ  output layer vÃ  Ä‘áº£m báº£o ráº±ng sá»‘ channel trong input cá»§a output layer Ä‘Ãºng báº±ng sá»‘ feature maps cá»§a conv layer cuá»‘i cÃ¹ng. Viáº¿t má»™t cÃ¡ch tá»•ng quÃ¡t vÃ  â€œformalâ€ hÆ¡n, ta sáº½ cÃ³ nhÆ° sau:\nXÃ©t má»™t CNN thá»a Ä‘iá»u kiá»‡n Ã¡p dá»¥ng CAM vá»›i conv layer cuá»‘i cÃ¹ng cÃ³ $k$ feature maps $F_i$, output layer cÃ³ $C$ neurons á»©ng vá»›i $C$ class. Ma tráº­n trá»ng sá»‘ táº¡i output layer lÃ  $W_{C \\times k}$. Khi Ä‘Ã³, vá»›i input $X$, CAM Ä‘Æ°á»£c sinh ra cho class $c$ lÃ \n$$CAM_c = W_{c,1} * F_1 + W_{c,2} * F_2 + \u0026hellip; + W_{c,k} * F_k$$\nGrad-CAM (2016) Gradient-weighted Class Activation Mapping (Grad-CAM) lÃ  má»™t phiÃªn báº£n cáº£i tiáº¿n cá»§a Default CAM vá»›i hai yáº¿u tá»‘ sau:\nKhÃ´ng rÃ ng buá»™c Ä‘iá»u kiá»‡n Ä‘á»‘i vá»›i kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh. Pháº§n fully connected layers Ä‘Æ°á»£c phÃ©p á»Ÿ báº¥t kÃ¬ cÃ¡ch tá»• chá»©c nÃ o. LÆ°u Ã½ lÃ  ta váº«n dÃ¹ng GAP (global average pooling). Thay vÃ¬ pháº£i Ã¡p dá»¥ng cho conv layer cuá»‘i cÃ¹ng thÃ¬ ta Ã¡p dá»¥ng cho layer nÃ o cÅ©ng Ä‘Æ°á»£c, nhÆ°ng thÆ°á»ng thÃ¬ ngÆ°á»i ta váº«n hay cÃ¹ng conv layer cuá»‘i hÆ¡n ğŸ˜œ. â€œTáº§m quan trá»ngâ€ cá»§a cÃ¡c feature maps sáº½ Ä‘Æ°á»£c tÃ­nh theo cÃ¡ch khÃ¡c, vÃ  cÃ¡ch tÃ­nh nÃ y sáº½ dá»±a vÃ o gradient! Grad-CAM thÆ°á»ng cho ra káº¿t quáº£ CAM tá»‘t hÆ¡n, táº­p trung vÃ o Ä‘Ãºng vÃ¹ng quan trá»ng hÆ¡n trong áº£nh input khi so sÃ¡nh vá»›i Default CAM. Vi dá»¥:\nSo sÃ¡nh Default CAM vÃ  Grad-CAM Nguá»“n: MDPI XÃ©t má»™t CNN vá»›i conv layer cuá»‘i cÃ¹ng cÃ³ $k$ feature maps $F_i$, output layer cÃ³ $C$ neurons á»©ng vá»›i $C$ class. Gá»i giÃ¡ trá»‹ output cho class $c$ lÃ  $y_c$. Vá»›i áº£nh input $X$, ta Ä‘áº·t:\nTáº§m quan trá»ng cá»§a feature map $F_i$ trong viá»‡c Ä‘Æ°a ra xÃ¡c suáº¥t $X$ thuá»™c class $c$ Ä‘Æ°á»£c tÃ­nh dá»±a vÃ o gradient cá»§a output $Y_c$ theo $F_i$, tá»©c lÃ \n$$ \\alpha_{c,i} = GAP(\\frac{\\partial Y_c}{\\partial F_i}) $$\n, vá»›i GAP lÃ  global average pooling.\nÄá»ƒ Ã½ ráº±ng, $\\dfrac{\\partial Y_c}{\\partial F_i}$ cÃ³ cÃ¹ng shape vá»›i $F_i$, ta tiáº¿n hÃ nh tÃ­nh GAP Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c giÃ¡ trá»‹ sá»‘ thá»±c $\\alpha_{c,i}$\nTá»« Ä‘Ã³, CAM cho class $c$ sáº½ Ä‘Æ°á»£c tÃ­nh báº±ng tá»• há»£p tuyáº¿n tÃ­nh giá»¯a $F_i$ vÃ  $\\alpha_{c,i}$:\n$$CAM_{c} = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nGrad-CAM Ä‘Ã£ hoáº¡t Ä‘á»™ng ráº¥t tá»‘t vÃ  Ä‘Æ°á»£c dÃ¹ng nhiá»u trong viá»‡c giáº£i thÃ­ch cho CNN. Tuy nhiÃªn, Ä‘áº¿n nÄƒm 2021 thÃ¬ cÃ³ 2 tÃ¡c giáº£ Ä‘Ã£ chá»‰ ra ráº±ng cÃ³ nhá»¯ng trÆ°á»ng há»£p Grad-CAM cho ra káº¿t quáº£ khÃ´ng tháº­t sá»± Ä‘Ãºng, khi mÃ  CAM Ä‘Æ°á»£c sinh ra táº­p trung khÃ´ng Ä‘Ãºng vÃ o cÃ¡c pháº§n quan trá»ng. Vi dá»¥ trong y há»c mÃ  cÃ¡c tÃ¡c giáº£ Ä‘Æ°a ra nhÆ° sau:\nLÃ½ do chÃ­nh dáº«n Ä‘áº¿n yáº¿u tá»‘ nÃ y lÃ  á»Ÿ phÃ©p toÃ¡n GAP trong viá»‡c tÃ­nh $\\alpha_{c,i}$. CÃ³ nhá»¯ng tÃ¬nh huá»‘ng mÃ  GAP sáº½ lÃ m máº¥t Ä‘i má»™t sá»‘ Ä‘iá»ƒm ná»•i báº­t á»Ÿ trong má»™t feature map. PhÆ°Æ¡ng phÃ¡p má»›i Ä‘Æ°á»£c giá»›i thiá»‡u lÃ  HiresCam (2021), báº±ng cÃ¡ch thay tháº¿ GAP thÃ nh phÃ©p toÃ¡n tÃ­ch element-wise. CÃ¡c báº¡n cÃ³ thá»ƒ tá»± tÃ¬m hiá»ƒu vá» cÃ¡i nÃ y nhÃ©. Score-CAM (2019) Score-CAM lÃ  má»™t phÆ°Æ¡ng phÃ¡p dá»±a chá»‰ dá»±a vÃ o score cá»§a cÃ¡c class (vector output cá»§a output layer) Ä‘á»ƒ tÃ­nh táº§m quan trá»ng cá»§a cÃ¡c feature maps, tá»« Ä‘Ã³ sinh ra CAM. Káº¿t quáº£ ta cÃ³ Ä‘Æ°á»£c lÃ  cÃ¡c CAM tá»‘t hÆ¡n phÆ°Æ¡ng phÃ¡p Grad-CAM, khi mÃ  vÃ¹ng Ä‘Æ°á»£c chÃº Ã½ trong áº£nh input lÃ  Ä‘Ãºng hÆ¡n, â€œgá»nâ€ hÆ¡n (khÃ´ng lan rá»™ng ra nhá»¯ng thá»© khÃ´ng liÃªn quan láº¯m á»Ÿ cÃ¡c phÃ­a xung quanh). VÃ­ dá»¥:\nSo sÃ¡nh Score-CAM vÃ  Grad-CAM Nguá»“n: Paper Score-CAM - Figure 1 LÆ°u Ã½.\nTa xÃ©t score cá»§a class khi chÆ°a Ã¡p dá»¥ng softmax Ä‘á»ƒ Ä‘Æ°a vá» xÃ¡c suáº¥t. Score-CAM cÅ©ng khÃ´ng cÃ³ rÃ ng buá»™c gÃ¬ vá» kiáº¿n trÃºc cá»§a CNN vÃ  ta cÃ³ thá»ƒ sinh CAM cho cÃ¡c feature map táº¡i báº¥t ká»³ layer nÃ o nhÆ° Grad-CAM. Äá»ƒ trÃ¬nh bÃ y phÆ°Æ¡ng phÃ¡p nÃ y thÃ¬ ta pháº£i dÃ¹ng cÃ¡c kÃ­ hiá»‡u toÃ¡n há»c hÆ¡i nhiá»u má»™t chÃºt ğŸ˜€\nIncrease of Confidence: Giáº£ sá»­ ta cÃ³ hÃ m sá»‘ $y = f(X)$ vá»›i input $X$ lÃ  vector $[x_1, x_2,\u0026hellip;, x_n]^T$ vÃ  $y$ lÃ  sá»‘ thá»±c. Vá»›i má»™t input cÆ¡ sá»Ÿ $X_b$ Ä‘Ã£ biáº¿t nÃ o Ä‘Ã³, táº§m quan trá»ng cá»§a thÃ nh pháº§n $x_i$ Ä‘á»‘i vá»›i viá»‡c tÃ­nh ra giÃ¡ trá»‹ $y$ sáº½ báº±ng vá»›i Ä‘á»™ chÃªnh lá»‡ch cá»§a output khi ta thay Ä‘á»•i pháº§n thá»© $i$ trong $X_b$ báº±ng cÃ¡ch nhÃ¢n thÃªm $x_i$, tá»©c lÃ \n$$ c_i = f(X_b \\circ H_i) - f(X_b) $$\n, vá»›i $\\circ$Â lÃ  element-wise product, $H_i = [1, \u0026hellip;, 1, x_i, 1, \u0026hellip; 1]^T$ (thÃ nh pháº§n thá»© $i$ lÃ  $x_i$)\nBÃ¢y giá», Ã¡p dá»¥ng Increase of Confidence trÃªn vá»›i viá»‡c ta tÃ­nh táº§m quan trá»ng cá»§a cÃ¡c feature maps: Giáº£ sá»­ ta cÃ³ má»™t CNN $Y = f(X)$ vá»›i input $X$ lÃ  áº£nh, $Y$ lÃ  vector cÃ³ $C$ pháº§n tá»­, á»©ng vá»›i score cá»§a $C$ class (chÆ°a Ã¡p dá»¥ng softmax). Ta xÃ©t conv layer thá»© $k$, feature map thá»© $i$ táº¡i layer nÃ y Ä‘Æ°á»£c kÃ­ hiá»‡u lÃ  $F_{k, i}$. Vá»›i má»™t input cÆ¡ sá»Ÿ $X_b$, táº§m quan trá»ng cá»§a $F_{k, i}$ Ä‘á»‘i vá»›i score cá»§a class $c$ $( Y_c = f_c(X))$ lÃ \n$$ C(F_{k, i}) = f_c(X_b \\circ H_{k, i}) - f_c(X_b) $$\n, vá»›i $H_{k, i} = s(Up(F_{k, i})$ lÃ  ta upsample $F_{k, i}$ lÃªn cÃ¹ng shape vá»›i áº£nh input $X$, sau Ä‘Ã³ normalize nÃ³ theo cÃ´ng thá»©c $Z \\leftarrow \\dfrac{Z - \\min_Z}{\\max_Z - \\min_Z}$\nSau khi tÃ­nh cÃ¡c giÃ¡ trá»‹ $C(F_{k, i})$, ta Ä‘Æ°a chÃºng qua softmax Ä‘á»ƒ cÃ³ cÃ¡c giÃ¡ trá»‹ táº§m quan trá»ng vá»›i tá»•ng báº±ng 1:\n$$ \\alpha_{c,i} = \\frac{\\exp(C(F_{k,i}))}{\\sum \\exp(C(F_{k,i})) } $$\nKhi Ä‘Ã³, ta cÃ³ thá»ƒ sinh ra CAM cho class $c$ nhÆ° sau:\n$$CAM_c = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nNhÆ° váº­y, ta Ä‘Ã£ xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c táº§m quan trá»ng cá»§a cÃ¡c feature map theo cÃ¡ch lÃ  chá»‰ dá»±a vÃ o output cá»§a CNN (score cá»§a cÃ¡c class).\nNháº­n xÃ©t Qua cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Ã£ trÃ¬nh bÃ y lÃ  Default CAM, Grad-CAM, Score-CAM, ta cÃ³ thá»ƒ tháº¥y ráº±ng Ä‘iá»ƒm khÃ¡c biá»‡t lá»›n nháº¥t giá»¯a chÃºng lÃ  cÃ¡ch tÃ­nh táº§m quan trá»ng cá»§a cÃ¡c feature maps trong viá»‡c sinh ra output cá»§a CNN.\nCÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o notebook sau: Google Colab\nNá»™i dung cá»§a notebook trÃªn lÃ  sinh ra CAM cho cÃ¡c mÃ´ hÃ¬nh CNN nhÆ° ResNet, DenseNet, EfficientNet vÃ  so sÃ¡nh output cá»§a CAM, Grad-CAM vÃ  Score-CAM. CÃ¡c káº¿t quáº£ cháº¡y Ä‘Æ°á»£c trong notebook trÃªn nhÆ° sau:\nCAM cá»§a cÃ¡c mÃ´ hÃ¬nh CNN khÃ¡c nhau: So sÃ¡nh CAM, Grad-CAM vÃ  Score-CAM cá»§a mÃ´ hÃ¬nh ResNet50: TÃ i liá»‡u tham kháº£o GlassBox, CNN Heat Maps: Class Activation MappingÂ (CAM) GlassBox, Grad-CAM: Visual Explanations from DeepÂ Networks Paper Grad-CAM: https://arxiv.org/abs/1610.02391 Paper Score-CAM: https://arxiv.org/abs/1910.01279 ","date":"2023-02-09T18:30:41+07:00","permalink":"https://htrvu.github.io/post/cam/","title":"CAM, Grad-CAM vÃ  Score-CAM trong CNN"},{"content":"XAI lÃ  gÃ¬? Háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh AI nÃ³i chung hay Deep Learning nÃ³i riÃªng luÃ´n Ä‘Æ°á»£c ngÆ°á»i ta vÃ­ nhÆ° lÃ  má»™t chiáº¿c há»™p Ä‘en (black-box). ChÃºng ta xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh vá»›i ráº¥t nhiá»u layer, tá»« convolution cho Ä‘áº¿n fully connected, sau Ä‘Ã³ sá»­ dá»¥ng cÃ¡c optimizer nhÆ° Adam, RMSprop,â€¦ (hoáº·c nÃ³i chung chung lÃ  gradient descent) Ä‘á»ƒ tá»‘i Æ°u mÃ´ hÃ¬nh, tá»©c lÃ  tÃ¬m ra bá»™ trá»ng sá»‘ sao cho hÃ m máº¥t mÃ¡t cÃ³ giÃ¡ trá»‹ nhá» nháº¥t cÃ³ thá»ƒ. Tuy nhiÃªn, náº¿u ta nhÃ¬n láº¡i mÃ´ hÃ¬nh vÃ  tÃ¬m cÃ¡ch giáº£i thÃ­ch lÃ  vÃ¬ sao cÃ¡c mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c tá»‘t nhÆ° váº­y thÃ¬ Ä‘Ã¢y luÃ´n lÃ  má»™t cÃ¢u há»i khÃ³, Viá»‡c Ä‘Æ°a nhá»¯ng chá»©ng minh cháº·t cháº½, rÃµ rÃ ng lÃ  khÃ´ng Ä‘á» hÆ¡n giáº£n. Tá»« Ä‘Ã³, ta cÃ³ má»™t hÆ°á»›ng nghiÃªn cá»©u vá» cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£i thÃ­ch sá»± hoáº¡t Ä‘á»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh AI vÃ  lÄ©nh vá»±c nÃ y Ä‘Æ°á»£c gá»i lÃ  Explainable AI (XAI).\nNguá»“n: https://impact.nuigalway.ie/wp-content/uploads/2022/01/blackboxpng.png VÃ¬ sao chÃºng ta cáº§n pháº£i tÃ¬m cÃ¡ch giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh AI?\nTa láº¥y má»™t vÃ­ dá»¥ vá» mÃ´ hÃ¬nh cháº©n Ä‘oÃ¡n ung thÆ° dáº¡ dÃ y dá»±a vÃ o hÃ¬nh áº£nh chá»¥p ná»™i soi Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ cÃ¡c bá»‡nh viá»‡n. LÃºc nÃ y, tÃ­nh chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh sáº½ trá»Ÿ nÃªn Ä‘áº·c biá»‡t nghiÃªm trá»ng, nÃ³ cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n sá»©c khá»e vÃ  cáº£ tÃ­nh máº¡ng cá»§a bá»‡nh nhÃ¢n. Náº¿u mÃ´ hÃ¬nh cháº©n Ä‘oÃ¡n lÃ  ung thÆ° thÃ¬ ta cÅ©ng cáº§n nÃ³ Ä‘Æ°a ra nhá»¯ng â€œchá»©ng cá»©â€ cho cháº©n Ä‘oÃ¡n Ä‘Ã³, táº¥t nhiÃªn lÃ  chá»©ng cá»© pháº£i Ä‘Ãºng, mang tÃ­nh thuyáº¿t phá»¥c cao thÃ¬ má»›i cháº¥p nhÃ¢n Ä‘Æ°á»£c. NgoÃ i lÄ©nh vá»±c y táº¿ thÃ¬ ta cÃ²n cÃ³ cÃ¡c vÃ­ dá»¥ khÃ¡c nhÆ° trong há»‡ thá»‘ng báº£o máº­t cá»§a ngÃ¢n hÃ ng,â€¦\nNguá»“n: Webflow\nNguá»“n: MicroAI\nKhi AI cÃ ng Ä‘Æ°á»£c á»©ng dá»¥ng nhiá»u vÃ o cuá»™c sá»‘ng thÃ¬ nhu cáº§u giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh AI cÅ©ng sáº½ dáº§n nhiá»u lÃªn. Äiá»u Ä‘Ã³ dáº«n Ä‘áº¿n sá»± phÃ¡t triá»ƒn máº¡nh cá»§a XAI trong thá»i gian gáº§n Ä‘Ã¢y.\nDiá»…n giáº£i má»™t mÃ´ hÃ¬nh AI Kháº£ nÄƒng diá»…n giáº£i mÃ´ hÃ¬nh (interpretability) lÃ  má»©c Ä‘á»™ hiá»ƒu biáº¿t cá»§a chÃºng ta vá» cÃ¡ch mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng, mÃ  cá»¥ thá»ƒ hÆ¡n lÃ  vá» quÃ¡ trÃ¬nh Ä‘Æ°a ra dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh. Ta cÃ³ hai hÆ°á»›ng tiáº¿p cáº­n chÃ­nh Ä‘á»‘i vá»›i viá»‡c diá»…n giáº£i mÃ´ hÃ¬nh lÃ  intrinsic vÃ  post-hoc.\nNguá»“n: Kemal Erdem Intrinsic (dá»±a vÃ o báº£n cháº¥t cá»§a mÃ´ hÃ¬nh): CÃ¡ch tiáº¿p cáº­n nÃ y thÆ°á»ng dÃ¹ng cho nhá»¯ng mÃ´ hÃ¬nh thuá»™c nhÃ³m white-box, Ä‘áº·c biá»‡t lÃ  nhá»¯ng mÃ´ hÃ¬nh Machine Learning nhÆ° Linear Regresion, Decision Tree, SVM,â€¦ Äáº±ng sau nhá»¯ng mÃ´ hÃ¬nh Ä‘Ã³ lÃ  cÃ¡c lÃ½ thuyáº¿t toÃ¡n cháº·t cháº½, ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c ngay cÃ´ng thá»©c tÃ­nh ra trá»ng sá»‘ tá»‘i Æ°u cá»§a bÃ i. NÃ³i cÃ¡ch khÃ¡c, khi chÆ°a cáº§n huáº¥n luyá»‡n thÃ¬ ta cÅ©ng cÃ³ thá»ƒ giáº£i thÃ­ch ráº±ng mÃ´ hÃ¬nh sáº½ hoáº¡t Ä‘á»™ng theo cÃ¡ch nhÆ° tháº¿ nÃ y, nhÆ° tháº¿ kia. Decision Tree Nguá»“n: javatpoint\nSVM Nguá»“n: Wikipedia\nPost-hoc: ÄÃ¢y lÃ  cÃ¡ch tiáº¿p cáº­n chÃºng ta thÆ°á»ng dÃ¹ng khi diá»…n giáº£i cÃ¡c mÃ´ hÃ¬nh Deep Learning, vÃ  Ä‘áº·c biá»‡t lÃ  nÃ³ Ä‘Æ°á»£c tiáº¿n hÃ nh sau khi mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i má»™t bá»™ trá»ng sá»‘ Ä‘á»§ tá»‘t. VÃ¬ viá»‡c giáº£i thÃ­ch, chá»©ng minh cháº·t cháº½, chÃ­nh xÃ¡c vá» quÃ¡ trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a cÃ¡c mÃ´ hÃ¬nh Deep Learning lÃ  ráº¥t khÃ³ khÄƒn nÃªn post-hoc lÃ  hÆ°á»›ng tiáº¿p cáº­n Ä‘Æ°á»£c Æ°u tiÃªn hÆ¡n. Trong post-hoc, ta cÃ³ 2 cÃ¡ch diá»…n giáº£i lÃ  model-agnostic vÃ  model-specific.\nModel-agnostic: CÃ¡ch nÃ y nghÄ©a lÃ  chÃºng ta cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¹ng má»™t phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ diá»…n giáº£i cho toÃ n bá»™ cÃ¡c mÃ´ hÃ¬nh mÃ  khÃ´ng cáº§n quan tÃ¢m Ä‘áº¿n kiáº¿n trÃºc cá»§a chÃºng. NhÆ° váº­y, ta chá»‰ dá»±a vÃ o input vÃ  output cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘Æ°a ra cÃ¡ch diá»…n giáº£i. Model-specific: Vá»›i cÃ¡ch nÃ y thÃ¬ tÃ¹y theo nhá»¯ng mÃ´ hÃ¬nh, hay lÃ  há» cÃ¡c mÃ´ hÃ¬nh, mÃ  ta sáº½ Ä‘Æ°a ra cÃ¡ch diá»…n giáº£i tÆ°Æ¡ng á»©ng. Ta cÃ³ thá»ƒ tháº¥y ráº±ng model-specific cÃ³ thá»ƒ dá»… tiáº¿n hÃ nh hÆ¡n model-agnostic ráº¥t nhiá»u.\nNhá»¯ng phÆ°Æ¡ng phÃ¡p trong XAI mÃ  mÃ¬nh trÃ¬nh bÃ y trong tÆ°Æ¡ng lai sáº½ chá»§ yáº¿u thuá»™c vá» hÆ°á»›ng post-hoc.\nVÃ¬ sao chÃºng ta bÃ n nhiá»u vá» kháº£ nÄƒng diá»…n giáº£i mÃ´ hÃ¬nh (interpretability) nhÆ°ng lÄ©nh vá»±c nÃ y láº¡i gá»i lÃ  Explainable AI (thiÃªn vá» kháº£ nÄƒng giáº£i thÃ­ch mÃ´ hÃ¬nh)?\n2 thuáº­t ngá»¯ diá»…n giáº£i vÃ  giáº£i thÃ­ch cÃ³ thá»ƒ xem lÃ  mang Ã½ nghÄ©a tÆ°Æ¡ng tá»± vÃ  cÃ³ thá»ƒ dÃ¹ng thay tháº¿ cho nhau. Tuy nhiÃªn, cÃ³ má»™t vÃ i quan Ä‘iá»ƒm cho ráº±ng kháº£ nÄƒng diá»…n giáº£i lÃ  nÃ³i Ä‘áº¿n má»™t tÃ­nh cháº¥t bá»‹ Ä‘á»™ng cá»§a mÃ´ hÃ¬nh vÃ  nÃ³ cáº§n con ngÆ°á»i chÃºng ta can thiá»‡p vÃ o, cÃ²n kháº£ nÄƒng giáº£i thÃ­ch lÃ  thiÃªn vá» chá»§ Ä‘á»™ng, tá»©c lÃ  mÃ´ hÃ¬nh cÃ³ thá»ƒ tá»± giáº£i thÃ­ch cho chÃ­nh nÃ³. á» Ä‘Ã¢y, con ngÆ°á»i chÃºng ta Ä‘ang tÃ¬m cÃ¡ch giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh, do Ä‘Ã³ ta Æ°u tiÃªn gá»i lÃ  diá»…n giáº£i. ÄÃ¡nh giÃ¡ phÆ°Æ¡ng phÃ¡p XAI Má»™t váº¥n Ä‘á» khÃ¡c mÃ  ngÆ°á»i ta thÆ°á»ng quan tÃ¢m Ä‘áº¿n lÃ  cÃ¡ch Ä‘Ã¡nh giÃ¡ má»™t phÆ°Æ¡ng phÃ¡p XAI, tá»©c lÃ  xÃ©t xem cÃ¡ch diá»…n giáº£i mÃ´ hÃ¬nh A Ä‘Ã£ thuyáº¿t phá»¥c, Ä‘Ã£ Ä‘Ãºng hay chÆ°a. Hiá»‡n táº¡i, ta chÆ°a cÃ³ má»™t Ä‘á»™ Ä‘o nÃ o Ä‘á»ƒ cÃ³ thá»ƒ so sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p vá»›i nhau. Pháº§n lá»›n thÃ¬ nÃ³ náº±m á»Ÿ cÃ¡c nháº­n xÃ©t cá»§a con ngÆ°á»i thÃ´ng qua viá»‡c quan sÃ¡t ğŸ˜€\nTÃ i liá»‡u tham kháº£o Mobiquity, An introduction to Explainable Artificial Intelligence (XAI) Erdem, XAI Methods - The Introduction ","date":"2023-02-09T15:30:31+07:00","permalink":"https://htrvu.github.io/post/intro-xai/","title":"Giá»›i thiá»‡u vá» XAI"},{"content":"Giá»›i thiá»‡u Qua cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c giá»›i thiá»‡u nhÆ° VGG, GoogLeNet hay ResNet thÃ¬ ta tháº¥y ráº±ng chÃºng Ä‘á»u Ä‘Æ°á»£c phÃ¡t triá»ƒn theo hÆ°á»›ng tÄƒng dáº§n Ä‘á»™ sÃ¢u vÃ  Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n, ká»ƒ tá»« khi AlexNet Ä‘Æ°á»£c cÃ´ng bá»‘. Sá»‘ lÆ°á»£ng tham sá»‘ cá»§a chÃºng lÃ  ráº¥t lá»›n.\nTuy nhiÃªn, cÃ¡c á»©ng dá»¥ng AI trong thá»±c táº¿ nhÆ° robotics, xe tá»± hÃ nh thÃ¬ cÃ¡c phÃ©p tÃ­nh toÃ¡n cá»§a mÃ´ hÃ¬nh cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n trong má»™t khoáº£ng thá»i gian giá»›i háº¡n, cÃ¹ng vá»›i tÃ i nguyÃªn pháº§n cá»©ng háº¡n cháº¿. Do Ä‘Ã³, ta pháº£i Ä‘á»‘i máº·t vá»›i má»™t trade-off giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»…, kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh.\nVÃ o thá»i Ä‘iá»ƒm nÃ y, cÃ³ 2 hÆ°á»›ng giáº£i phÃ¡p chÃ­nh Ä‘á»ƒ cÃ³ thá»ƒ Ä‘Æ°a cÃ¡c mÃ´ hÃ¬nh vÃ o á»©ng dá»¥ng thá»±c táº¿ nhÆ° sau:\nNÃ©n cÃ¡c mÃ´ hÃ¬nh phá»©c táº¡p láº¡i thÃ´ng qua cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ° lÆ°á»£ng tá»­ hÃ³a (quantization), hashing, cáº¯t tá»‰a mÃ´ hÃ¬nh XÃ¢y dá»±ng vÃ  huáº¥n luyÃªn cÃ¡c mÃ´ hÃ¬nh nhá», Ä‘á»™ phá»©c táº¡p tháº¥p ngay tá»« Ä‘áº§u. MobileNet Ä‘Æ°á»£c phÃ¡t triá»ƒn theo hÆ°á»›ng thá»© 2, trong Ä‘Ã³, nÃ³ táº­p trung vÃ o cÃ¡c yáº¿u tá»‘:\nVá»«a Ä‘áº£m báº£o kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh Ä‘á»§ nhá», tá»‘c Ä‘á»™ suy diá»…n Ä‘á»§ nhanh (Ä‘á»™ trá»… tháº¥p) vÃ  vá»›i Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»§ cao. Cung cáº¥p hai siÃªu tham sá»‘ cho phÃ©p ta Ä‘iá»u chá»‰nh trade-off giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»…, kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh: width multiplier (liÃªn quan Ä‘áº¿n sá»‘ channel trong tá»«ng layer) vÃ  resolution multiplier (width vÃ  height trong tá»«ng layer) Depthwise separable convolutions MobileNet Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« cÃ¡c layer convolution khÃ¡ Ä‘áº·c biá»‡t, chÃºng Ä‘Æ°á»£c gá»i lÃ  depthwise separable convolutions. Depthwise separable convolution Ä‘Æ°á»£c táº¡o ra tá»« hai phÃ©p toÃ¡n:\nDepthwise convolution: Ãp dá»¥ng tá»«ng filter cho tá»«ng channel cá»§a input. Náº¿u input cÃ³ bao nhiÃªu channel thÃ¬ ta sáº½ cÃ³ báº¥y nhiÃªu filter. Pointwise convolution: ÄÃ¢y thá»±c cháº¥t lÃ  convolution layer thÃ´ng thÆ°á»ng vá»›i filter 1 x 1. NÃ³ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ tá»•ng há»£p cÃ¡c káº¿t quáº£ tá»« phÃ©p toÃ¡n depthwise convolution vÃ  tÃ­nh ra output, thÃ´ng qua cÃ¡c phÃ©p toÃ¡n tá»• há»£p tuyáº¿n tÃ­nh. Nguá»“n: Research Gate Ta cÃ³ thá»ƒ tháº¥y ngay sá»± khÃ¡c biá»‡t giá»¯a depthwise separable convolution vÃ  convolution thÃ´ng thÆ°á»ng nhÆ° sau:\nConvolution thÃ´ng thÆ°á»ng: Má»—i filter sáº½ tÆ°Æ¡ng tÃ¡c vá»›i toÃ n bá»™ channel cá»§a input. Giáº£ sá»­ input cá»§a ta lÃ  $D_F \\times D_F \\times M$, má»™t filter $3 \\times 3$ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ¬ filter nÃ y sáº½ trá»Ÿ thÃ nh má»™t tensor vá»›i shape $3 \\times 3 \\times M$, ta thá»±c hiá»‡n convolution trÃªn tá»«ng channel vÃ  sau Ä‘Ã³ cá»™ng $M$ ma tráº­n láº¡i vá»›i nhau, thu Ä‘Æ°á»£c káº¿t quáº£ $D_F \\times D_F$. Náº¿u sá»­ dá»¥ng $N$ filter Ä‘á»ƒ tÃ­nh thÃ¬ ta sáº½ cÃ³ káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  $D_F \\times D_F \\times N$. Depthwise separable convolution: Ban Ä‘áº§u, cÃ¡c channel Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘á»™c láº­p vá»›i tá»«ng filter riÃªng, sau Ä‘Ã³ má»›i káº¿t há»£p láº¡i sau nhá» vÃ o pointwise convolution. Vá»›i input $D_F \\times D_F \\times M$ thÃ¬ khi Ä‘Æ°a qua depthwise convotution, ta sáº½ cÃ³ káº¿t quáº£ lÃ  $D_F \\times D_F \\times M$. Náº¿u pointwise convolution sá»­ dá»¥ng $N$ filter $1 \\times 1$ thÃ¬ ta cÃ³ káº¿t quáº£ cuá»‘i cÃ¹ng lÃ  $D_F \\times D_F \\times N$ Váº¥n Ä‘á» Ä‘áº·t ra lÃ  táº¡i sao sá»­ dá»¥ng depthwise separable convolution láº¡i cÃ³ thá»ƒ giÃºp cho MobileNet gá»n nháº¹ hÆ¡n, tÃ­nh toÃ¡n nhanh hÆ¡n vÃ  cÃ³ Ä‘á»™ chÃ­nh xÃ¡c Ä‘á»§ tá»‘t, khÃ´ng há» kÃ©m cáº¡nh cÃ¡c mÃ´ hÃ¬nh to lá»›n khÃ¡c. Ta sáº½ Ä‘áº·t tÃ­nh má»™t chÃºt:\nGiáº£ sá»­ input cá»§a ta lÃ  feature maps $\\bold{F}: D_F \\times D_F \\times M$, output cuá»‘i cÃ¹ng lÃ  $\\bold{G}: D_F \\times D_F \\times N$.\nVá»›i convolution thÃ´ng thÆ°á»ng: Giáº£ sá»­ ta dÃ¹ng $N$ filter $\\bold{K}: D_K \\times D_K$, stride lÃ  1, padding sao cho kÃ­ch thÆ°á»›c width vÃ  height khÃ´ng Ä‘á»•i. Khi Ä‘Ã³, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n sáº½ lÃ \n$$ O_1 = D_K \\times D_K \\times D_F \\times D_F \\times M \\times N $$\n, vÃ¬ vá»›i má»—i filter thÃ¬: má»—i láº§n tÃ­nh toÃ¡n ta pháº£i thá»±c hiá»‡n $D_K \\times D_K$ phÃ©p toÃ¡n nhÃ¢n, sau Ä‘Ã³ cá»™ng chÃºng láº¡i; ta tÃ­nh táº¡i $D_F \\times D_F$ vá»‹ trÃ­ trÃªn $M$ channel cá»§a input, vÃ  ta sá»­ dá»¥ng $N$ filter.\nVá»›i depthwise separable convolution: á» bÆ°á»›c depthwise convolution thÃ¬ ta dÃ¹ng $M$ filter $\\bold{K}: D_K \\times D_K$, stride lÃ  1, padding phÃ¹ há»£p. Khi Ä‘Ã³, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n sáº½ lÃ \n$$ D_K \\times D_K \\times D_F \\times D_F \\times M $$\n, vÃ¬ ta chá»‰ Ä‘Æ¡n giáº£n lÃ  Ã¡p dá»¥ng Ä‘Æ¡n láº» tá»«ng filter cho tá»«ng channels\nVá»›i pointwise convolution thÃ¬ ta dÃ¹ng $N$ filter $\\bold{K}: 1 \\times 1$, stride lÃ  1, padding 0. Khi Ä‘Ã³, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n sáº½ lÃ \n$$ D_F \\times D_F \\times M \\times N $$\nDo Ä‘Ã³, ta cÃ³ Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n lÃ \n$$ O_2 = D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N $$\nLÃºc nÃ y, Ä‘em chia cho nhau thÃ¬ ta cÃ³ tá»‰ lá»‡\n$$ \\frac{O_2}{O_1}=\\frac{D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N}{D_K \\times D_K \\times D_F \\times D_F \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_K^2} $$\nNhÆ° váº­y, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n khi sá»­ dá»¥ng depthwise separable convolution Ä‘Ã£ giáº£m khoáº£ng $D_K^2$ láº§n so vá»›i convolution thÃ´ng thÆ°á»ng. MobileNet sá»­ dá»¥ng cÃ¡c filter $3 \\times 3$, tá»« Ä‘Ã³ giáº£m Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n Ä‘i khoáº£ng 8 Ä‘áº¿n 9 láº§n, trong khi Ä‘á»™ chÃ­nh xÃ¡c chá»‰ giáº£m Ä‘i má»™t pháº§n nhá».\nSiÃªu tham sá»‘ Ä‘iá»u chá»‰nh trade-off Äá»ƒ cÃ³ thá»ƒ há»— trá»£ tá»‘t hÆ¡n viá»‡c Ã¡p dá»¥ng MobileNet vÃ o cÃ¡c thiáº¿t bá»‹ biÃªn trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿, cÃ¡c tÃ¡c giáº£ cÃ²n cung cáº¥p thÃªm cho ta hai siÃªu tham sá»‘ Ä‘á»ƒ Ä‘iá»u chá»‰nh trade-off giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ trá»…, kÃ­ch thÆ°á»›c cá»§a mÃ´ hÃ¬nh\nWidth multiplier Tham sá»‘ width multiplier (kÃ­ hiá»‡u lÃ  $\\alpha$) sáº½ tÃ¡c Ä‘á»™ng lÃªn giÃ¡ trá»‹ sá»‘ channel cá»§a cÃ¡c layer. Vá»›i nhá»¯ng cÃ´ng thá»©c á»Ÿ trÃªn thÃ¬ sá»‘ channel chÃ­nh lÃ  $M$ vÃ  $N$. GiÃ¡ trá»‹ $\\alpha \\in (0, 1]$ vÃ  ta thÆ°á»ng Ä‘áº·t lÃ  $1, 0.75, 0.5, 0.25.$ Khi Ä‘Ã³, thá»© tháº­t sá»± Ä‘Æ°á»£c thay Ä‘á»•i chÃ­nh lÃ  sá»‘ lÆ°á»£ng filter mÃ  ta dÃ¹ng trong cÃ¡c phÃ©p toÃ¡n pointwise convolution.\nÄá»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a depthwise separable convolution khi ta cÃ³ sá»­ dá»¥ng width multiplier $\\alpha$ lÃ \n$$ D_K \\times D_K \\times D_F \\times D_F \\times \\alpha M + D_F \\times D_F \\times \\alpha M \\times \\alpha N $$\nResolution multiplier Tham sá»‘ resolution muiltiplier (kÃ­ hiá»‡u lÃ  $\\rho$ ) liÃªn quan Ä‘áº¿n kÃ­ch thÆ°á»›c width vÃ  height (chÃ­nh lÃ  $D_F$ trong cÃ¡c cÃ´ng thá»©c trÃªn). Miá»n giÃ¡ trá»‹ cá»§a nÃ³ cÅ©ng sáº½ tÆ°Æ¡ng tá»± nhÆ° $\\alpha$. Thá»±c cháº¥t thÃ¬ ta sáº½ chá»‰ Ã¡p dá»¥ng nÃ³ vÃ o input ban Ä‘áº§u cá»§a mÃ´ hÃ¬nh (áº£nh). CÃ¡c kÃ­ch thÆ°á»›c input mÃ  ta thÆ°á»ng sá»­ dá»¥ng vá»›i mÃ´ hÃ¬nh MobileNet lÃ  224, 192, 160 hoáº·c 128.\nÄá»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a depthwise separable convolution khi ta cÃ³ sá»­ dá»¥ng thÃªm resolution multiplier $\\rho$ lÃ \n$$ D_K \\times D_K \\times \\rho D_F \\times \\rho D_F \\times \\alpha M + \\rho D_F \\times \\rho D_F \\times \\alpha M \\times \\alpha N $$\nKiáº¿n trÃºc mÃ´ hÃ¬nh Depthwise Separable block MÃ´ hÃ¬nh MobileNet V1 Ä‘Æ°á»£c táº¡o thÃ nh bá»Ÿi cÃ¡c thÃ nh pháº§n chÃ­nh lÃ  depthwise separable block. ChÃºng bao gá»“m hai phÃ©p toÃ¡n nhÆ° ta Ä‘Ã£ Ä‘á» cáº­p lÃ  depthwise convolution vÃ  pointwise convolution. Äi kÃ¨m vá»›i cÃ¡c layer Ä‘Ã³ lÃ  batch norm vÃ  activation ReLU.\nNguá»“n: Research Gate Kiáº¿n trÃºc MobileNet MobileNet sá»­ dá»¥ng táº¥t cáº£ gá»“m 13 depthwise separable block. Tá»•ng thá»ƒ kiáº¿n trÃºc cá»§a MobileNet Ä‘Æ°á»£c thá»ƒ hiá»‡n á»Ÿ báº£ng sau:\nTrong Ä‘Ã³:\nConv dw lÃ  depthwise convolution. Ta cÃ³ thá»ƒ tháº¥y cÃ¡c filter shape cá»§a chÃºng luÃ´n cÃ³ cÃ¹ng sá»‘ channel trong input size. CÃ¡c conv layer vá»›i filter shape $1 \\times 1$ chÃ­nh lÃ  pointwise convolution. s1 tá»©c lÃ  stride = 1, tÆ°Æ¡ng tá»± vá»›i s2. ToÃ n bá»™ cÃ¡c conv layer trong mÃ´ hÃ¬nh Ä‘á»u cÃ³ padding sao cho kÃ­ch thÆ°á»›c width vÃ  height cá»§a input vÃ  output cá»§a layer Ä‘Ã³ lÃ  nhÆ° nhau. Note:\nTrong báº£ng trÃªn cÃ³ váº» cÃ³ má»™t chá»— gÃµ nháº§m. Äá»ƒ Ã½ Ä‘áº¿n layer â€œConv dw / s2â€ Ä‘áº§u tiÃªn tá»« phÃ­a dÆ°á»›i lÃªn, náº¿u Ä‘Ã¢y lÃ  s2 thÃ¬ input size cá»§a layer â€œConv / s1â€ tiáº¿p theo pháº£i bá»‹ giáº£m size chá»© khÃ´ng pháº£i $7 \\times 7$. Do Ä‘Ã³, trong cÃ i Ä‘áº·t mÃ´ hÃ¬nh á»Ÿ bÃªn dÆ°á»›i thÃ¬ mÃ¬nh Ä‘Ã£ Ä‘á»•i nÃ³ thÃ nh â€œConv dw / s1â€. CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t MobileNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nTÃ i liá»‡u tham kháº£o Paper MobileNet: https://arxiv.org/abs/1704.04861 ","date":"2023-02-08T18:13:00+07:00","permalink":"https://htrvu.github.io/post/mobilenet/","title":"MobileNet (2017)"},{"content":"Giá»›i thiá»‡u CÃ¡c mÃ´ hÃ¬nh thuá»™c há» Inception-ResNet Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn Ã½ tÆ°á»Ÿng lÃ  káº¿t há»£p skip connection vÃ o cÃ¡c Inception block (cÃ¡c Ã½ tÆ°á»Ÿng tá»« ResNet vÃ  GoogLeNet). VÃ¬ paper nÃ y chá»‰ mang tÃ­nh thá»±c nghiá»‡m lÃ  chÃ­nh nÃªn mÃ¬nh sáº½ khÃ´ng trÃ¬nh bÃ y chi tiáº¿t ğŸ‘€.\nKiáº¿n trÃºc mÃ´ hÃ¬nh Inception-ResNet V1 Vá» máº·t tá»•ng quan, Inception-ResNet V1 cÃ³ kiáº¿n trÃºc nhÆ° sau:\nKiáº¿n trÃºc Inception-ResNet V1\nStem cá»§a Inception-ResNet V1\nTa sáº½ Ä‘á» cáº­p Ä‘áº¿n cÃ¡c loáº¡i Inception-ResNet block vÃ  Reduction:\nInception-ResNet block: Ta tháº¥y ráº±ng pháº§n â€œInceptionâ€ trong cÃ¡c block nÃ y lÃ  Ä‘Æ¡n giáº£n hÆ¡n khÃ¡ nhiá»u so vá»›i cÃ¡c Inception block nguyÃªn máº«u. Inception-ResNet-A V1\nInception-ResNet-B V1\nInception-ResNet-C V1\nReduction: ChÃºng thá»±c hiá»‡n nhiá»‡m vá»¥ giáº£m kÃ­ch thÆ°á»›c (width, height) cá»§a cÃ¡c tensor Ä‘i má»™t ná»­a. Kiáº¿n trÃºc cá»§a chÃºng ráº¥t giá»‘ng vá»›i cÃ¡c Inception block Reduction-A\nReduction-B\nInception-ResNet V2 PhiÃªn báº£n thá»© hai cá»§a Inception-ResNet cÃ³ kiáº¿n trÃºc tá»•ng thá»ƒ giá»‘ng há»‡t vá»›i phiÃªn báº£n Ä‘áº§u tiÃªn, ta chá»‰ cÃ³ má»™t sá»‘ thay Ä‘á»•i á»Ÿ cÃ¡c block Inception-ResNet vÃ  Reduction\nTÃ i liá»‡u tham kháº£o Paper Inception-ResNet: https://arxiv.org/abs/1602.07261 ","date":"2023-02-08T18:03:46+07:00","permalink":"https://htrvu.github.io/post/inception-resnet/","title":"Inception-Reset (2016)"},{"content":"Giá»›i thiá»‡u Ta biáº¿t ráº±ng, viá»‡c táº¡o ra cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n (nhiá»u layer) chÆ°a cháº¯c Ä‘Ã£ mang láº¡i hiá»‡u quáº£ tá»‘t hÆ¡n nhá»¯ng mÃ´ hÃ¬nh â€œcáº¡nâ€ hÆ¡n. VÃ­ dá»¥, vá»›i táº­p CIFAR10 thÃ¬ ta cÃ³ má»™t káº¿t quáº£ thá»­ nghiá»‡m cho tháº¥y ráº±ng mÃ´ hÃ¬nh sÃ¢u hÆ¡n láº¡i cÃ³ Ä‘á»™ hiá»‡u quáº£ kÃ©m hÆ¡n:\nÄá»‘i vá»›i viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n thÃ¬ ta cÃ³ thá»ƒ sáº½ bá»‹ gáº·p pháº£i cÃ¡c váº¥n Ä‘á» sau:\nOverfitting: MÃ´ hÃ¬nh cÃ ng sÃ¢u thÆ°á»ng sáº½ cÃ ng phá»©c táº¡p nÃªn nÃ³ ráº¥t dá»… bá»‹ overfitting. Vanishing/exploding gradient. Váº¥n Ä‘á» nÃ y thÃ¬ ta Ä‘Ã£ cÃ³ má»™t sá»‘ cÃ¡ch giáº£i quyáº¿t phá»• biáº¿n nhÆ° thay Ä‘á»•i activation function, cÃ¡c phÆ°Æ¡ng phÃ¡p khá»Ÿi táº¡o trá»ng sá»‘ nhÆ° Xavier, He Initialization. NgoÃ i 2 váº¥n Ä‘á» trÃªn, ta cÃ³ má»™t váº¥n Ä‘á» Ä‘áº·c biá»‡t hÆ¡n lÃ  degradation: Accuracy tÄƒng dáº§n cho Ä‘áº¿n má»™t Ä‘á»™ sÃ¢u nháº¥t Ä‘á»‹nh thÃ¬ ngá»«ng tÄƒng (bÃ£o hÃ²a) rá»“i sau Ä‘Ã³ sáº½ giáº£m dáº§n.\nLÆ°u Ã½. Nhiá»u trÆ°á»ng há»£p degradation khÃ´ng pháº£i do overfitting gÃ¢y ra. Degradation cho chÃºng ta tháº¥y ráº±ng viá»‡c tá»‘i Æ°u cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n lÃ  khÃ´ng há» dá»… dÃ ng. Trong quÃ¡ trÃ¬nh xÃ¢y dá»±ng mÃ´ hÃ¬nh, tá»« má»™t mÃ´ hÃ¬nh ban Ä‘áº§u, sau khi thÃªm má»™t sá»‘ layer vÃ o thÃ¬ táº¥t nhiÃªn lÃ  ta mong ráº±ng mÃ´ hÃ¬nh má»›i pháº£i cÃ³ Ä‘á»™ hiá»‡u quáº£ Ã­t nháº¥t lÃ  ngang mÃ´ hÃ¬nh gá»‘c. Tuy nhiÃªn, khi xáº£y ra degradation thÃ¬ mong muá»‘n Ä‘Ã³ Ä‘Ã£ khÃ´ng thá»ƒ thÃ nh sá»± tháº­t Ä‘Æ°á»£c. ğŸ˜€\nResNet Ä‘Æ°á»£c cÃ´ng bá»‘ nháº±m giáº£i quyáº¿t váº¥n Ä‘á» degradation Ä‘á»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n. Khi nháº¯c Ä‘áº¿n ResNet thÃ¬ ta sáº½ láº­p tá»©c nghÄ© Ä‘áº¿n nhá»¯ng mÃ´ hÃ¬nh vá»›i Ä‘á»™ sÃ¢u ráº¥t khá»§ng, tháº­m chá»‰ lÃ  lÃªn Ä‘áº¿n 100, 200 layer.\nHÃ m pháº§n dÆ° vÃ  skip connection Nháº¯c láº¡i vá» cÃ¡i mong muá»‘n á»Ÿ trÃªn, ráº±ng mÃ´ hÃ¬nh má»›i pháº£i cÃ³ Ä‘á»™ hiá»‡u quáº£ Ã­t nháº¥t lÃ  ngang mÃ´ hÃ¬nh gá»‘c, ta cÃ³ thá»ƒ nghÄ© ngay Ä‘áº¿n má»™t phÆ°Æ¡ng phÃ¡p cá»±c kÃ¬ Ä‘Æ¡n giáº£n: cÃ¡c layer phÃ­a sau sáº½ lÃ  identity mapping, tá»©c lÃ  input vÃ  output cá»§a nÃ³ sáº½ giá»‘ng nhau. Vá»›i cÃ¡ch lÃ m nÃ y thÃ¬ hiá»ƒn nhiÃªn lÃ  ta Ä‘áº¡t Ä‘Æ°á»£c mong muá»‘n rá»“i, vÃ¬ Ä‘á»™ hiá»‡u quáº£ cá»§a mÃ´ hÃ¬nh má»›i vÃ  mÃ´ hÃ¬nh gá»‘c rÃµ rÃ ng sáº½ y há»‡t nhau.\nTuy nhiÃªn, náº¿u chá»‰ dá»«ng láº¡i á»Ÿ Ä‘Ã³ thÃ´i thÃ¬ thÃªm layer vÃ o lÃ m gÃ¬ :v Ta muá»‘n Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n! CÃ¡c tÃ¡c giáº£ cá»§a paper ResNet giá»›i thiá»‡u má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  deep residual learning (há»c pháº§n dÆ°).\nGiáº£ sá»­ ta cÃ³ má»™t block B cÃ¡c layer, input cá»§a nÃ³ lÃ  $x$. Vá»›i má»™t mÃ´ hÃ¬nh thÃ´ng thÆ°á»ng, ta sáº½ â€œhá»câ€ má»™t hÃ m sá»‘ Ä‘áº§u ra mong muá»‘n lÃ  $f(x)$. LÃºc nÃ y, ban Ä‘áº§u thÃ¬ ta hoÃ n toÃ n chÆ°a cÃ³ má»™t thÃ´ng tin gÃ¬ vá» $f(x)$ cáº£, viá»‡c â€œhá»câ€ sáº½ xuáº¥t phÃ¡t tá»« má»™t Ä‘áº¡i lÆ°á»£ng ngáº«u nhiÃªn. Äá»‘i vá»›i phÆ°Æ¡ng phÃ¡p deep residual learning, output cá»§a block B sáº½ cÃ³ dáº¡ng $h(x) = x + f(x)$, vÃ  ta sáº½ Ä‘i há»c $f(x)$. LÃºc nÃ y, $f(x)$ Ä‘Æ°á»£c gá»i lÃ  residual function (hÃ m pháº§n dÆ°) Ta cÃ³ cÃ¡c nháº­n xÃ©t sau: Trong deep residual learning, ta Ä‘Ã£ cÃ³ sá»± â€œgá»£i Ã½â€ cho hÃ m mong muá»‘n thÃ´ng qua giÃ¡ trá»‹ input $x$. Äá»‘i vá»›i thÃ´ng thÆ°á»ng thÃ¬ khÃ´ng cÃ³ sá»± gá»£i Ã½ nÃ o Ä‘Æ°á»£c Ä‘Æ°a ra cáº£. Viá»‡c há»c $f(x)$ nhÆ° lÃ  má»™t hÃ m pháº§n dÆ° lÃ  dá»… hÆ¡n so vá»›i viá»‡c há»c hÃ m mong muá»‘n. Náº¿u identity mapping lÃ  káº¿t quáº£ tá»‘i Æ°u thÃ¬ ta cÃ³ luÃ´n $f(x)=0$ Vá» máº·t báº£n cháº¥t, vá»›i phÆ°Æ¡ng phÃ¡p deep residual learning, ta Ä‘Ã£ tÃ¡c Ä‘á»™ng vÃ o lá»›p hÃ m chá»©a hÃ m mong muá»‘n, sao cho lá»›p má»›i lÃ  lá»›n hÆ¡n (bao gá»“m) lá»›p cÅ©. Nguá»“n: Dive into AI Äá»ƒ minh há»a cho yáº¿u tá»‘ \u0026ldquo;giÃºp viá»‡c há»c trá»Ÿ nÃªn dá»… hÆ¡n\u0026rdquo;, ta cÃ³ vÃ­ dá»¥ nhÆ° sau:\nSá»‘ Ä‘iá»ƒm cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng sáº½ Ã­t hÆ¡n háº³n, Ä‘á»“ thá»‹ cÅ©ng sáº½ \"trÆ¡n\" hÆ¡n khi cÃ³ sá»­ dá»¥ng skip connection Nguá»“n: Jeremy Jordan NhÆ° váº­y, Ä‘iá»ƒm nháº¥n cá»§a ResNet lÃ  ta Ä‘i há»c cÃ¡c hÃ m pháº§n dÆ°, thÃ´ng qua viá»‡c â€œgá»£i Ã½â€ cho hÃ m sá»‘ mong muá»‘n má»™t giÃ¡ trá»‹ báº±ng vá»›i giÃ¡ trá»‹ input. Trong cÃ i Ä‘áº·t, thao tÃ¡c nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua má»™t káº¿t ná»‘i gá»i lÃ  skip connection. Ta sáº½ cá»™ng input $x$ vÃ o output cá»§a block thÃ´ng thÆ°á»ng.\nÄá»‘i vá»›i viá»‡c cá»™ng nhÆ° váº­y thÃ¬ thá»±c cháº¥t lÃ  ta Ä‘ang Ä‘i cá»™ng hai ma tráº­n. Khi Ä‘Ã³, má»™t váº¥n Ä‘á» cÃ³ thá»ƒ náº£y sinh lÃ  vá» shape cá»§a chÃºng, mÃ  thÆ°á»ng lÃ  sá»‘ channel. Náº¿u sá»‘ channel khÃ´ng khá»›p thÃ¬ ta cáº§n pháº£i tiáº¿n hÃ nh â€œÄ‘iá»u chá»‰nhâ€. VÃ¬ cÃ¡c block Ä‘Æ°á»£c sá»­ dá»¥ng thÆ°á»ng gá»“m cÃ¡c conv layer vá»›i stride vÃ  padding phÃ¹ há»£p Ä‘á»ƒ giá»¯ nguyÃªn width vÃ  height nÃªn ta sáº½ chá»‰ xÃ©t Ä‘áº¿n sá»‘ channel. Náº¿u sá»‘ channel cá»§a $x$ vÃ  output ban Ä‘áº§u lÃ  nhÆ° nhau thÃ¬ ta gá»i skip connection nÃ y lÃ  identity skip connection NgÆ°á»£c láº¡i, ta sáº½ dÃ¹ng conv layer $1 \\times 1$ Ä‘á»ƒ Ä‘iá»u chá»‰nh sá»‘ channel cá»§a $x$. LÃºc nÃ y, skip connection Ä‘Æ°á»£c gá»i lÃ  projection skip connection. Sá»­ dá»¥ng skip connection (identity) Nguá»“n: Dive into AI NgoÃ i ra, ta cÃ²n cÃ³ má»™t Ä‘iá»ƒm máº¡nh quan trá»ng cá»§a skip connection lÃ  nÃ³ giÃºp cho gradient Ä‘Æ°á»£c lan truyá»n tá»‘t hÆ¡n trong quÃ¡ trÃ¬nh backpropagation, tá»« Ä‘Ã³ gÃ³p pháº§n lÃ m giáº£m hiá»‡n tÆ°á»£ng vanishing gradient\nQuan sÃ¡t hÃ¬nh phÃ­a trÃªn, ta tháº¥y ráº±ng khi backpropagation thÃ¬ layer á»Ÿ ngay trÆ°á»›c block nháº­n Ä‘Æ°á»£c gradient tá»« hai layer phÃ­a sau nÃ³ (má»™t layer liá»n trÆ°á»›c nÃ³ vÃ  má»™t layer Ä‘Æ°á»£c káº¿t ná»‘i thÃ´ng qua skip connection). Residual block Residual block (khá»‘i pháº§n dÆ°) Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch thÃªm má»™t skip connection vÃ o má»™t block thÃ´ng thÆ°á»ng trong cÃ¡c mÃ´ hÃ¬nh CNN nhÆ° VGG block. Ta cÃ³ hai loáº¡i residual block nhÆ° sau:\nBasic: CÃ¡c conv layer trong block nÃ y cÃ³ filter $3 \\times 3$. Block dáº¡ng nÃ y thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng cho cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u vá»«a pháº£i. Bottleneck: Nháº±m giáº£m bá»›t sá»‘ lÆ°á»£ng tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n, cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng dáº¡ng block nÃ y vá»›i hai conv layer $1 \\times 1$ vá»›i vai trÃ² lÃ  giáº£m/tÄƒng sá»‘ channel, táº¡o ra má»™t hÃ¬nh dÃ¡ng giá»‘ng nhÆ° nÃºt tháº¯t cá»• chai. Hai layer conv $1 \\times 1$ nhÆ° váº­y Ä‘Æ°á»£c gá»i lÃ  bottleneck layer. Äá»ƒ cho dá»… hÃ¬nh dÃ¹ng thÃ¬ ta cÃ³ thá»ƒ xem Ä‘Ã¢y nhÆ° lÃ  thao tÃ¡c â€œcÃ´ Ä‘á»ng kiáº¿n thá»©câ€ cá»§a mÃ´ hÃ¬nh, hay nÃ³i rÃµ hÆ¡n lÃ  nÃ©n lÆ°á»£ng thÃ´ng tin láº¡i sao cho vá»«a giá»¯ Ä‘Æ°á»£c thÃ´ng tin vÃ  vá»«a tiáº¿t kiá»‡m tÃ i nguyÃªn (bá»™ nhá»›, Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n). Basic Residual Block vÃ  Bottleneck Residual Block LÆ°u Ã½. CÃ³ má»™t chi tiáº¿t nÃ y khÃ¡ quan trá»ng trong cÃ i Ä‘áº·t lÃ  ta sáº½ tiáº¿n hÃ nh cá»™ng ma tráº­n trÆ°á»›c rá»“i má»›i Ä‘Æ°a káº¿t quáº£ qua activation function.\nNgoÃ i ra, ta cÃ²n cÃ³ 2 loáº¡i skip connection lÃ  identity vÃ  projection. LÃºc nÃ y, tÃ¹y vÃ o sá»‘ channels cá»§a input vÃ  output ban Ä‘áº§u cÃ³ khá»›p hay khÃ´ng mÃ  block tÆ°Æ¡ng á»©ng sáº½ chá»©a loáº¡i skip connection phÃ¹ há»£p.\nBasic residual block vá»›i identity vÃ  projection skip connection Nguá»“n: Dive into AI Kiáº¿n trÃºc ResNet ResNet Ä‘Æ°á»£c táº¡o nÃªn báº±ng cÃ¡ch sá»­ dá»¥ng nhiá»u residual block liÃªn tiáº¿p nhau, tÆ°Æ¡ng tá»± nhÆ° nhá»¯ng gÃ¬ mÃ  GoogLeNet hay VGG Ä‘Ã£ thá»±c hiá»‡n. CÃ¡c tÃ¡c giáº£ cá»§a paper ResNet táº¡o ra nhiá»u phiÃªn báº£n ResNet khÃ¡c nhau vá»›i Ä‘á»™ sÃ¢u tÄƒng dáº§n.\nDá»±a vÃ o sá»‘ lÆ°á»£ng layer cÃ³ trá»ng sá»‘ thÃ¬ ta sáº½ cÃ³ tÃªn cÃ¡c mÃ´ hÃ¬nh nhÆ° ResNet18 (18 layer cÃ³ trá»ng sá»‘), ResNet34,â€¦ CÃ¡c phiÃªn báº£n ResNet Ta cÃ³ má»™t sá»‘ nháº­n xÃ©t nhÆ° sau:\nCÃ¡c phiÃªn báº£n cÃ³ Ä‘á»™ sÃ¢u lá»›n nhÆ° 50, 101 vÃ  152 sá»­ dá»¥ng Bottleneck Residual Block. Vá»›i 18 vÃ  34 thÃ¬ chÃºng dÃ¹ng Basic block Trong cÃ¡c kiáº¿n trÃºc trÃªn thÃ¬ ta sá»­ dá»¥ng cáº£ 2 loáº¡i skip connection: identity vÃ  projection VÃ­ dá»¥, vá»›i ResNet18 thÃ¬ trong cá»¥m Basic block Ä‘áº§u tiÃªn cá»§a cá»¥m conv3_x sáº½ cÃ³ sá»‘ channel lÃ  64, cÃ²n output ban Ä‘áº§u cá»§a block nÃ y thÃ¬ lÃ  128 nÃªn ta pháº£i Ã¡p dá»¥ng projection vÃ o input Äá»‘i vá»›i height vÃ  width thÃ¬ cÃ¡c tÃ¡c giáº£ cho biáº¿t viá»‡c downsampling input sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n táº¡i conv layer Ä‘áº§u tiÃªn cá»§a cÃ¡c cá»¥m conv3_x, conv4_x, conv5_x (vá»›i stride lÃ  2). CÃ¡c conv layer khÃ¡c thÃ¬ Ä‘á»u cÃ³ stride 1. Ta sáº½ cáº§n chÃº Ã½ Ä‘áº¿n chi tiáº¿t nÃ y khi tiáº¿n hÃ nh cÃ i Ä‘áº·t ResNet. CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t ResNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau\nTÃ i liá»‡u tham kháº£o Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into AI, ResNet ","date":"2023-02-08T17:41:09+07:00","permalink":"https://htrvu.github.io/post/resnet/","title":"Resnet (2015)"},{"content":" CÃ¡ nhÃ¢n mÃ¬nh tháº¥y GoogLeNet lÃ  má»™t paper khÃ³ Ä‘á»c. Khi viáº¿t ra bÃ i nÃ y thÃ¬ mÃ¬nh váº«n Ä‘ang cáº£m tháº¥y hÆ¡i lÃº vá» ná»™i dung cá»§a nÃ³ ğŸ˜€\nGiá»›i thiá»‡u Tá»« khi AlexNet Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o nÄƒm 2012 vÃ  Ä‘áº·t ná»n táº£ng cho cÃ¡c máº¡ng Deep CNN, GoogLeNet, hay Inception V1 (2014), lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc cÃ³ cÃ¡ch thiáº¿t káº¿ ráº¥t thÃº vá»‹ khi nÃ³ táº­n dá»¥ng hiá»‡u quáº£ cÃ¡c conv layer, Ä‘áº·t ná»n mÃ³ng cho nhiá»u mÃ´ hÃ¬nh sau nÃ y.\nâ€œInceptionâ€ cÃ³ thá»ƒ dá»‹ch lÃ  â€œsá»± khá»Ÿi Ä‘áº§uâ€, nghe cÃ³ váº» ráº¥t há»£p lÃ½ ğŸ˜€. NgoÃ i ra, á»Ÿ trong bÃ i viáº¿t vá» VGG, mÃ¬nh cÃ³ Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» thiáº¿t káº¿ kiáº¿n trÃºc mÃ´ hÃ¬nh theo hÆ°á»›ng cÃ³ sá»± láº·p láº¡i cÃ¡c khuÃ´n máº«u. GoogLeNet cÅ©ng sáº½ Ä‘Æ°á»£c thiáº¿t káº¿ nhÆ° váº­y.\nLÆ°u Ã½. TÃªn cá»§a mÃ´ hÃ¬nh nÃ y lÃ  GoogLeNet, khÃ´ng pháº£i GoogleNet =)) TÃ¡c giáº£ cho biáº¿t Ã½ nghÄ©a cá»§a cÃ¡i tÃªn nÃ y lÃ  â€œThis name is a homage to Yann LeCuns pioneering LeNet 5 network.â€\nGoogLeNet Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« nhá»¯ng má»¥c tiÃªu cá»§a nghiÃªn cá»©u nhÆ° sau:\nNÃ¢ng cao kháº£ nÄƒng táº­n dá»¥ng tÃ i nguyÃªn tÃ­nh toÃ¡n Cho phÃ©p tÄƒng chiá»u rá»™ng vÃ  chiá»u sÃ¢u cá»§a kiáº¿n trÃºc mÃ´ hÃ¬nh mÃ  váº«n Ä‘áº£m báº£o Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n lÃ  á»Ÿ má»©c cháº¥p nháº­n Ä‘Æ°á»£c. GoogLeNet tháº­t sá»± Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng Ä‘iá»u Ä‘Ã³, vÃ  nÃ³ Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn nguyÃªn lÃ½ Hebbian: â€œneurons that fire together, wire togetherâ€. Paper GoogLeNet Ä‘Ã£ Ä‘Æ°a ra cÃ¡c ná»n táº£ng lÃ½ thuyáº¿t ráº¥t â€œcÄƒng tháº³ngâ€ Ä‘á»ƒ cho tháº¥y ráº±ng mÃ´ hÃ¬nh CNN cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng â€œÄ‘á»§ tá»‘tâ€, Ä‘iá»u mÃ  ta chÆ°a Ä‘Æ°á»£c biáº¿t Ä‘áº¿n á»Ÿ trong cÃ¡c paper trÆ°á»›c Ä‘Ã³ ğŸ˜€\nNgoÃ i ra, cÃ³ má»™t quan Ä‘iá»ƒm khÃ¡ thÃº vá»‹ khi mÃ´ táº£ vá» kiáº¿n trÃºc cá»§a GoogLeNet nhÆ° sau: Khi xÃ¢y dá»±ng kiáº¿n trÃºc CNN, thay vÃ¬ pháº£i suy nghÄ© xem trong cÃ¡c máº¡ng CNN ta nÃªn Ã¡p dá»¥ng filter vá»›i kÃ­ch thÆ°á»›c bao nhiÃªu, hÃ£y Ã¡p dá»¥ng luÃ´n nhiá»u filter vá»›i kÃ­ch thÆ°á»›c khÃ¡c nhau vÃ  tá»•ng há»£p káº¿t quáº£ láº¡i ğŸ˜œ\nMÃ¬nh cÅ©ng cÃ³ Ä‘á»“ng tÃ¬nh vá»›i quan Ä‘iá»ƒm nÃ y. Tuy nhiÃªn, nguá»“n gá»‘c Ä‘áº±ng sau nÃ³ cÃ³ váº» khÃ´ng chá»‰ Ä‘Æ¡n giáº£n lÃ  nhÆ° váº­y. Trong paper, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ tiáº¿n hÃ nh phÃ¢n tÃ­ch vÃ  thá»­ nghiá»‡m nhiá»u láº¯m cho ra Ä‘Æ°á»£c cÃ¡i kiáº¿n trÃºc cá»§a GoogLeNet. Káº¿t ná»‘i thÆ°a vÃ  CNN Äáº§u tiÃªn, táº¡i thá»i Ä‘iá»ƒm trÆ°á»›c khi Inception Ä‘Æ°á»£c cÃ´ng bá»‘, chÃºng ta cÃ³ thá»ƒ cáº£i thiá»‡n má»™t mÃ´ hÃ¬nh DNN báº±ng nhá»¯ng cÃ¡ch nhÆ° sau:\nTÄƒng chiá»u sÃ¢u cá»§a mÃ´ hÃ¬nh (tá»©c lÃ  sá»‘ layer) TÄƒng chiá»u rá»™ng (sá»‘ channel trong má»—i layer) Tuy nhiÃªn, vá»›i hai cÃ¡ch trÃªn thÃ¬ sáº½ cÃ³ nhá»¯ng váº¥n Ä‘á» mÃ  ta cáº§n lÆ°u tÃ¢m lÃ  hiá»‡n tÆ°á»£ng overfitting vÃ  sá»± gia tÄƒng Ä‘á»™ phá»©c táº¡p cá»§a mÃ´ hÃ¬nh.\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n má»™t hÆ°á»›ng Ä‘i cÃ³ thá»ƒ giáº£m bá»›t hai váº¥n Ä‘á» trÃªn lÃ  sá»­ dá»¥ng kiáº¿n trÃºc káº¿t ná»‘i thÆ°a (sparsely connected architectures). Ta cÃ³ thá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n nhÆ° sau:\nVá»›i hai fully connected layer liÃªn tiáº¿p nhau, má»—i neuron trong layer sau sáº½ káº¿t ná»‘i vá»›i táº¥t cáº£ cÃ¡c unit trong layer trÆ°á»›c. Náº¿u nhÆ° ta thay Ä‘á»•i Ä‘i má»™t chÃºt, má»—i neuron trong layer sau sáº½ chá»‰ káº¿t ná»‘i Ä‘áº¿n má»™t vÃ i unit trong layer trÆ°á»›c, kiáº¿n trÃºc cÃ³ Ä‘Æ°á»£c sáº½ trá»Ÿ nÃªn â€œthÆ°aâ€ hÆ¡n. Káº¿t ná»‘i thÆ°a trong fully connected layer Äá»‘i vá»›i conv layer thÃ¬ ta Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c tÃ­nh cháº¥t thÆ°a nhÆ° váº­y. Ta biáº¿t ráº±ng, vá»›i má»—i pháº§n tá»­ trÃªn feature map cá»§a layer hiá»‡n táº¡i thÃ¬ ta tÃ­nh nÃ³ dá»±a vÃ o má»™t vÃ¹ng nhá» trÃªn feature map output cá»§a layer trÆ°á»›c Ä‘Ã³. Giáº£ sá»­ hai layer nÃ y Ä‘á»u chá»‰ cÃ³ 1 channel thÃ¬ ta cÃ³ thá»ƒ biá»ƒu diá»…n nÃ³ nhÆ° hÃ¬nh bÃªn dÆ°á»›i: Káº¿t ná»‘i thÆ°a trong conv layer Kiáº¿n trÃºc thÆ°a Ä‘Æ°á»£c cÃ¡c tÃ¡c giáº£ mÃ´ táº£ lÃ  mÃ´ phá»ng láº¡i há»‡ thá»‘ng sinh há»c (vÃ­ dá»¥ nhÆ° khi ta nhÃ¬n vÃ o má»™t Ä‘á»‘i tÆ°á»£ng thÃ¬ ta thÆ°á»ng chá»‰ chÃº Ã½ má»™t sá»‘ Ä‘iá»ƒm trÃªn Ä‘á»‘i tÆ°á»£ng Ä‘Ã³ thÃ´i, khÃ³ mÃ  chÃº Ã½ táº¥t cáº£ Ä‘Æ°á»£c).\nNgoÃ i ra, cÃ³ má»™t ná»n tÃ ng lÃ½ thuyáº¿t ráº¥t trÃ¢u bÃ² vá» kiáº¿n trÃºc thÆ°a cá»§a Arora nhÆ° sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Tháº­t sá»± mÃ¬nh cÅ©ng chÆ°a hiá»ƒu háº¿t Ã½ cá»§a cÃ¢u trÃªnâ€¦ NhÆ°ng Ä‘áº¡i Ã½ cá»§a nÃ³ cÃ³ váº» lÃ  náº¿u ta biá»ƒu diá»…n Ä‘Æ°á»£c phÃ¢n phá»‘i cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cá»§a má»™t dataset báº±ng má»™t kiáº¿n trÃºc DNN lá»›n vÃ  thÆ°a thÃ¬ tá»« Ä‘Ã³ ta cÃ³ thá»ƒ xÃ¢y nÃªn Ä‘Æ°á»£c má»™t kiáº¿n trÃºc tá»‘i Æ°u (vá» cáº£ Ä‘á»™ chÃ­nh xÃ¡c vÃ  Ä‘á»™ phá»©c táº¡p), báº±ng cÃ¡ch xÃ¢y tá»«ng layer má»™t ğŸ˜€\nTuy nhiÃªn, vá»›i cÃ¡c tÃ i nguyÃªn pháº§n cá»©ng trong thá»i gian nÃ y thÃ¬ ráº¥t khÃ³ Ä‘á»ƒ ta cÃ³ thá»ƒ tÃ­nh toÃ¡n trÃªn cÃ¡c máº¡ng DNN thÆ°a. Do Ä‘Ã³, cÃ¡c tÃ¡c giáº£ Ä‘i theo hÆ°á»›ng tÃ¬m má»™t mÃ´ hÃ¬nh lÃ  cÃ³ táº­n dá»¥ng má»™t sá»‘ thÃ´ng tin vá» tÃ­nh cháº¥t thÆ°a nhÆ°ng váº«n thá»±c hiá»‡n tÃ­nh toÃ¡n trÃªn cÃ¡c ma tráº­n Ä‘áº§y Ä‘á»§. Äáº¥y chÃ­nh lÃ  hÆ°á»›ng sá»­ dá»¥ng cÃ¡c phÃ©p toÃ¡n convolution!.\nCÅ©ng vÃ¬ lÃ½ do nÃ y, á»Ÿ pháº§n kiáº¿n trÃºc cá»§a GoogLeNet thÃ¬ ta sáº½ tháº¥y nÃ³ chá»‰ cÃ³ duy nháº¥t má»™t fully connected layer Ä‘á»ƒ sinh ra output, cÃ²n láº¡i chá»‰ toÃ n conv layer thÃ´i ğŸ˜— NÃ³i Ä‘áº¿n viá»‡c Ã¡p dá»¥ng cÃ¡c filter trong conv layer, cÃ¡c tÃ¡c giáº£ cho ráº±ng:\nMá»—i pháº§n tá»­ trong feature map cá»§a má»™t layer sáº½ cÃ³ má»‘i tÆ°Æ¡ng quan vá»›i má»™t vÃ¹ng nÃ o Ä‘Ã³ trÃªn áº£nh input (receptive field). Ta sáº½ gom cÃ¡c pháº§n tá»­ cÃ¹ng tÆ°Æ¡ng quan vá»›i má»™t vÃ¹ng vÃ o cÃ¹ng má»™t cá»¥m. Äá»ƒ Ã½ ráº±ng, trong nhá»¯ng layer á»Ÿ gáº§n áº£nh input thÃ¬ ta sáº½ cÃ³ nhiá»u cá»¥m vÃ  kÃ­ch thÆ°á»›c má»—i cá»¥m lÃ  nhá». CÃ ng Ä‘i qua cÃ¡c layer CNN thÃ¬ sá»‘ lÆ°á»£ng cá»¥m sáº½ Ã­t láº¡i vÃ  kÃ­ch thÆ°á»›c cá»¥m sáº½ lan rá»™ng ra. Táº¥t nhiÃªn lÃ  váº«n cÃ³ thá»ƒ cÃ³ nhá»¯ng cá»¥m cÃ³ kÃ­ch thÆ°á»›c nhá» táº¡i nhá»¯ng layer Ä‘Ã³. Do váº­y, ta nÃªn cÃ³ cÃ¡c filter vá»›i kÃ­ch thÆ°á»›c lá»›n hÆ¡n Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng táº¡i cÃ¡c cá»¥m lá»›n, Ä‘á»“ng thá»i cÅ©ng cáº§n cÃ³ filter kÃ­ch thÆ°á»›c nhá» Ä‘á»‘i vá»›i cÃ¡c cá»¥m nhá» hÆ¡n. Qua má»™t loáº¡t cÃ¡c thá»­ nghiá»‡m, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ chá»n 3 filter lÃ  $1 \\times 1$, $3 \\times 3$ vÃ  $5 \\times 5$. Má»™t khá»‘i chá»©a 3 filter trÃªn Ä‘Æ°á»£c Ä‘áº·t tÃªn lÃ  Inception module.\nBÃ¢y giá» quay láº¡i vá»›i lÃ½ thuyáº¿t cá»§a Arora, á»Ÿ Ã½ xÃ¢y dá»±ng máº¡ng tá»‘i Æ°u qua tá»«ng layer má»™t. GoogLeNet Ä‘Æ°á»£c táº¡o nÃªn báº±ng Ä‘Ãºng Ã½ tÆ°á»Ÿng nhÆ° váº­y, ta ná»‘i Inception module - by - Inception module ğŸ˜€\nP/s: CÃ¡c báº¡n cÃ³ thá»ƒ tÃ¬m Ä‘á»c pháº§n Motivation trong paper gá»‘c vÃ  tá»± cáº£m nháº­n nÃ³ nhÃ©.\nInception module Inception module Qua cÃ¡c mÃ´ táº£ á»Ÿ pháº§n trÃªn, ta cÃ³ thá»ƒ liÃªn tÆ°á»Ÿng Ä‘áº¿n kiáº¿n trÃºc cá»§a Inception module lÃ  má»™t thá»© gÃ¬ Ä‘Ã³ giá»‘ng vá»›i cÃ¡i á»Ÿ hÃ¬nh (a), vá»›i Ä‘á»§ 3 loáº¡i filter lÃ  $1 \\times 1$, $3 \\times 3$ vÃ  $5 \\times 5$. CÃ¡c tÃ¡c giáº£ dÃ¹ng thÃªm cáº£ má»™t layer max pooling trong Ä‘Ã³ ná»¯a, vá»›i lÃ­ do ráº¥t Ä‘Æ¡n giáº£n lÃ  vÃ¬ á»Ÿ thá»i Ä‘iá»ƒm Ä‘Ã³ thÃ¬ max pooling thÆ°á»ng mang láº¡i hiá»‡u quáº£ tá»‘t trong cÃ¡c kiáº¿n trÃºc máº¡ng CNN =))\nTruy nhiÃªn, cÃ¡ch cÃ i Ä‘áº·t nhÆ° hÃ¬nh (a) sáº½ dáº«n Ä‘áº¿n sá»‘ lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh lÃ  ráº¥t lá»›n. Thay vÃ o Ä‘Ã³, ta cÃ³ thá»ƒ táº¡o ra kiáº¿n trÃºc dáº¡ng â€œbottleneckâ€ báº±ng cÃ¡ch sá»­ dá»¥ng thÃªm cÃ¡c layer convolution $1 \\times 1$ nhÆ° hÃ¬nh (b), nháº±m má»¥c Ä‘Ã­ch chÃ­nh lÃ  giáº£m sá»‘ channel. Sá»‘ lÆ°á»£ng phÃ©p tÃ­nh lÃºc nÃ y sáº½ Ä‘Æ°á»£c giáº£m má»™t cÃ¡ch Ä‘Ã¡ng ká»ƒ.\nLÆ°u Ã½ ráº±ng, á»Ÿ cuá»‘i module ta cÃ³ phÃ©p toÃ¡n concatenate, tá»©c lÃ  cÃ¡c feature-maps cá»§a toÃ n bá»™ layer convolution Ä‘á»u pháº£i cÃ³ cÃ¹ng size (tá»©c lÃ  width vÃ  height)\nKiáº¿n trÃºc GoogLeNet GoogLeNet sá»­ dá»¥ng tá»•ng cá»™ng 9 Inception module, gá»“m cÃ³ 22 layer cÃ³ trá»ng sá»‘ (tÃ­nh cáº£ pooling lÃ  27), Ä‘Æ°á»£c tÃ³m táº¯t qua báº£ng sau:\nMá»™t Ä‘iá»ƒm Ä‘áº·c biá»‡t khi train GoogLeNet lÃ  cÃ¡c tÃ¡c giáº£ sá»­ dá»¥ng cÃ¡c auxiliary classifiers (xem hÃ¬nh bÃªn dÆ°á»›i). CÃ¡c thÃ nh pháº§n nÃ y sáº½ khÃ¡ giá»‘ng nhÆ° giá»‘ng há»‡t vá»›i pháº§n cuá»‘i cá»§a mÃ´ hÃ¬nh (bá»™ classifier). NhÆ° váº­y, ta cÃ³ thá»ƒ xem GoogLeNet cÃ³ 3 bá»™ classifier. Auxiliary classifiers Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i cÃ¡c Ã½ nghÄ©a nhÆ° sau:\nHáº¡n cháº¿ vanishing gradient TÄƒng regularization LÆ°u Ã½:\nLoss khi huáº¥n luyá»‡n sáº½ cá»™ng loss cá»§a cáº£ ba láº¡i vá»›i nhau. Khi test thÃ¬ ta thÆ°á»ng chá»‰ quan tÃ¢m Ä‘áº¿n bá»™ classifer cuá»‘i cÃ¹ng. Má»™t cÃ¡ch lÃ m khÃ¡c lÃ  ta xÃ i cáº£ 3, sau Ä‘Ã³ láº¥y káº¿t quáº£ trung bÃ¬nh. Nguá»“n: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t GoogLeNet báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau. Trong cÃ¡ch cÃ i Ä‘áº·t nÃ y, mÃ¬nh sáº½ bá» qua auxiliary classifiers.\nTÃ i liá»‡u tham kháº£o Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"https://htrvu.github.io/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giá»›i thiá»‡u Dá»±a trÃªn sá»± thÃ nh cÃ´ng cá»§a AlexNet vÃ o nÄƒm 2012, nhiá»u nghiÃªn cá»©u Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh nháº±m tÃ¬m ra cÃ¡c phÆ°Æ¡ng phÃ¡p hay kiáº¿n trÃºc má»›i Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n, vÃ­ dá»¥ nhÆ°:\nThay Ä‘á»•i (tÄƒng, giáº£m) kÃ­ch thÆ°á»›c cá»§a conv filter Thay Ä‘á»•i stride, padding cá»§a conv layer Train vÃ  test trÃªn cÃ¡c input vá»›i nhiá»u Ä‘á»™ phÃ¢n giáº£i (resolution) áº£nh khÃ¡c nhau Trong nÄƒm 2014, VGG lÃ  má»™t trong nhá»¯ng káº¿t quáº£ nghiÃªn cá»©u ná»•i báº­t nháº¥t, vÃ  nÃ³ táº­p trung vÃ o má»™t váº¥n Ä‘á» khÃ¡c vá»›i cÃ¡c hÆ°á»›ng trÃªn lÃ  Ä‘á»™ sÃ¢u (depth, hay lÃ  sá»‘ layer) cá»§a mÃ´ hÃ¬nh. VGG Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c káº¿t quáº£ tá»‘t nháº¥t vÃ o thá»i Ä‘iá»ƒm nÃ³ ra má»›i trÃªn dataset ImageNet vÃ  cÃ¡c dataset khÃ¡c, trong cÃ¡c task nhÆ° classification, localization,â€¦\nNgoÃ i ra, ta cÃ³ thá»ƒ Ä‘Æ°a ra nháº­n xÃ©t nhÆ° sau vá» AlexNet:\nDÃ¹ AlexNet Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c CNN cÃ³ thá»ƒ Ä‘áº¡t Ä‘á»™ hiá»‡u quáº£ tá»‘t, nÃ³ láº¡i khÃ´ng cung cáº¥p má»™t khuÃ´n máº«u nÃ o cho viá»‡c nghiÃªn cá»©u, thiáº¿t káº¿ cÃ¡c máº¡ng má»›i. Theo thá»i gian, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ thay Ä‘á»•i suy nghÄ© tá»« quy mÃ´ nhá»¯ng neuron riÃªng láº» sang cÃ¡c táº§ng, rá»“i sau Ä‘Ã³ lÃ  cÃ¡c khá»‘i (block) gá»“m cÃ¡c táº§ng láº·p láº¡i theo khuÃ´n máº«u. Kiáº¿n trÃºc cá»§a VGG lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc phá»• biáº¿n Ä‘áº§u tiÃªn Ä‘Æ°á»£c xÃ¢y dá»±ng theo Ã½ tÆ°á»Ÿng nhÆ° váº­y.\nVGG block Äiá»ƒm ná»•i báº­t cá»§a VGG lÃ  ta chá»‰ dÃ¹ng duy nháº¥t má»™t kÃ­ch thÆ°á»›c filter trong má»i conv layer lÃ  $3 \\times 3$, vÃ  ta dáº§n tÄƒng Ä‘á»™ sÃ¢u cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡c conv layer. HÆ¡n ná»¯a, ta cÃ²n Ã¡p dá»¥ng nhiá»u conv layer liá»n nhau rá»“i má»›i dÃ¹ng Ä‘áº¿n max pooling. Ta cÃ³ thá»ƒ gá»i má»™t block gá»“m nhá»¯ng layer nhÆ° tháº¿ lÃ  VGG block.\nCÃ¡c tÃ¡c giáº£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n má»™t váº¥n Ä‘á» cho cÃ¡ch Ã¡p dá»¥ng nÃ y nhÆ° sau: Viá»‡c dÃ¹ng nhiá»u conv layer 3 x 3 liá»n nhau nhÆ° váº­y so vá»›i dÃ¹ng má»™t conv layer vá»›i filter lá»›n hÆ¡n (vÃ­ dá»¥ 7 x 7) nhÆ° háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o thá»i Ä‘iá»ƒm trÆ°á»›c Ä‘Ã³ thÃ¬ cÃ³ gÃ¬ â€œtá»‘tâ€ hÆ¡n, khi mÃ  features map sau cÃ¹ng ta thu Ä‘Æ°á»£c cÃ³ thá»ƒ cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c? Äá»ƒ tráº£ lá»i, ta cÃ³ 2 Ã½ chÃ­nh nhÆ° sau: Giáº£m sá»‘ lÆ°á»£ng tham sá»‘ cá»§a mÃ´ hÃ¬nh (Ä‘áº·t tÃ­nh lÃ  biáº¿t nhaa ğŸ˜œ) DÃ¹ng nhiá»u conv layer thÃ¬ ta cÃ³ kháº£ nÄƒng sáº½ phÃ¡t hiá»‡n Ä‘Æ°á»£c nhiá»u feature cÃ³ Ã­ch hÆ¡n (hai conv layer sáº½ táº¡o thÃ nh má»™t \u0026ldquo;hÃ m há»£p\u0026rdquo;), tá»« Ä‘Ã³ decision function sáº½ ok hÆ¡n. NgoÃ i ra, VGG block sá»­ dá»¥ng padding 1 (giá»¯ nguyÃªn kÃ­ch thÆ°á»›c input), theo sau Ä‘Ã³ lÃ  max pooling vá»›i pool size $2 \\times 2$ vÃ  stride 2 (giáº£m kÃ­ch thÆ°á»›c input Ä‘i má»™t ná»­a). Kiáº¿n trÃºc cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ nhÆ° hÃ¬nh bÃªn dÆ°á»›i:\nVGG block Nguá»“n: Dive intro AI Trong cÃ¡c bÃ i toÃ¡n Ã¡p dá»¥ng VGG, Ä‘Ã´i khi ta cÃ³ thá»ƒ gáº·p VGG block vá»›i má»™t conv layer $1 \\times 1$ á»Ÿ trÆ°á»›c max pooling. Block dáº¡ng nÃ y thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i má»¥c Ä‘Ã­ch chÃ­nh lÃ  bá»• sung thÃªm má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh ná»¯a. Tuy nhiÃªn, trong thá»±c nghiá»‡m thÃ¬ cÃ¡c tÃ¡c giáº£ Ä‘Ã£ cho tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng block dáº¡ng nÃ y khÃ´ng hiá»‡u quáº£ hÆ¡n so vá»›i toÃ n cÃ¡c conv layer vá»›i filter $3 \\times 3$ (cÃ¹ng sá»‘ lÆ°á»£ng layer).\nKiáº¿n trÃºc VGG Báº±ng cÃ¡ch káº¿t há»£p nhiá»u VGG block vá»›i nhau, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ táº¡o ra nhiá»u phiÃªn báº£n VGG khÃ¡c nhau, vá»›i sá»‘ layer cÃ³ trá»ng sá»‘ lÃ  trong Ä‘oáº¡n 11 - 19. Trong paper VGG, ta cÃ³ 6 kiáº¿n trÃºc vá»›i Ä‘á»™ sÃ¢u tÄƒng dáº§n vÃ  tiáº¿n hÃ nh so sÃ¡nh vá»›i nhau. Äiá»ƒm chung cá»§a cÃ¡c kiáº¿n trÃºc nÃ y lÃ  pháº§n fully connected Ä‘á»u cÃ³ 3 layer, vÃ  toÃ n bá»™ layer Ä‘á»u sá»­ dá»¥ng activation ReLU.\nCÃ¡c phiÃªn báº£n VGG Ta cÃ³ má»™t sá»‘ nháº­n xÃ©t nhÆ° sau:\nSá»‘ lÆ°á»£ng conv layer trong cÃ¡c VGG block cá»§a cÃ¡c phiÃªn báº£n lÃ  tÄƒng dáº§n. Äá»ƒ Ã½ Ä‘áº¿n B, C vÃ  D thÃ¬: C = B + conv layer $1 \\times 1$ trong má»—i VGG block D = C + Ä‘á»•i conv layer $1 \\times 1$ thÃ nh $3 \\times 3$ Khi thá»±c nghiá»‡m, ta cÃ³ B \u0026lt; C \u0026lt; D. Do Ä‘Ã³, viá»‡c thÃªm phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh báº±ng conv layer $1 \\times 1$ giÃºp mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n, nhÆ°ng nÃ³ váº«n khÃ´ng báº±ng vá»›i viá»‡c lÃ  ta thÃªm luÃ´n conv layer $3 \\times 3$ ğŸ¤”. Äá»™ rá»™ng (sá»‘ channel) trong tá»«ng block Ä‘Æ°á»£c tÄƒng theo bá»™i 2. Ã tÆ°á»Ÿng nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t rá»™ng rÃ£i trong thá»i Ä‘iá»ƒm nÃ y vÃ  cáº£ vá» sau Äá»ƒ háº¡n cháº¿ overfitting, ta cÃ³ thá»ƒ sá»­ dá»¥ng thÃªm dropout cho hai táº§ng fully connected Ä‘áº§u tiÃªn. Hai kiáº¿n trÃºc phá»• biáº¿n nháº¥t trong viá»‡c Ã¡p dá»¥ng VGG vÃ o cÃ¡c bÃ i toÃ¡n khÃ¡c lÃ  D (VGG16) vÃ  E (VGG19). Äá»ƒ trá»±c quan hÆ¡n, ta cÃ³ thá»ƒ thá»ƒ biá»ƒu diá»…n VGG16 nhÆ° sau:\nNguá»“n: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png CÃ i Ä‘áº·t CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o pháº§n cÃ i Ä‘áº·t VGG báº±ng Tensorflow vÃ  Pytorch táº¡i repo sau.\nTÃ i liá»‡u tham kháº£o Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"https://htrvu.github.io/post/vgg/","title":"VGG (2014)"}]
[{"content":"Trong bài viết về RNN, mình đã đề cập khá kỹ về mô hình này nhưng để ứng dụng được nó vào các bài toán thì ta cần phải làm thêm bước “số hóa” dữ liệu từ văn bản sao cho máy tính có thể hiểu được.\nNếu máy tính hiểu được càng nhiều về các từ thì nghĩa là cách số hóa càng có hiệu quả. Do đó, ta cần quan tâm đến vấn đề “hiểu”. Hiểu như thế nào là đủ tốt? 😀 Đối với NLP, ta có những phương pháp (hay có thể nói là kỹ thuật) biểu diễn từ phổ biến là one-hot encoding, TF-IDF và Word Embedding. Nội dung của bài viết này sẽ tập trung vào one-hot encoding và Word Embedding.\nOne-hot encoding Ý tưởng Từ điển (vocabulary) là một thành phần không thể thiếu của mọi hệ thống ngôn ngữ. Những từ ta dùng thường ngày hầu như là sẽ nằm ở một vị trí nào đó trong từ điển (có thể các từ địa phương thì sẽ không có). One-hot encoding là phương pháp biểu diễn từ bằng chính thông tin vị trí này.\nVới những từ không có trong từ điển thì ta thường sử dụng một giá trị vị trí đặc biệt để cho biết từ đó là unknown. Giả sử tập từ điển của chúng ta có $S$ từ và không có từ trên các văn bản là không có trong từ điển. Khi đó, mỗi từ sẽ được biểu diễn bằng một vector nhị phân có $S$ chiều, với duy nhất một phần tử bằng 1 tại chiều ứng với vị trí của từ đó trong từ điển và các phần tử còn lại là 0. Ví dụ:\nMinh họa phương pháp one-hot encoding với kích thước từ điển là 9\nNguồn: Shane Lynn Khi kết hợp phương pháp one-hot encoding vào mô hình RNN để giải quyết các bài toán thì ở trong mỗi giai đoạn ta sẽ có:\nInput và label sẽ là các vector nhị phân tương ứng với các từ Output là một vector thể hiện một phân bố xác suất, với phần tử thứ $i$ là xác suất mà từ output là từ ở vị trí thứ $i$ trong từ điển (do đó activation function thường dùng ở đây chính là softmax) Ví dụ áp dụng one-hot encoding vào RNN trong bài toán sinh văn bản theo từng kí tự\nNguồn: Stanford - Natural Language Processing Vì sao ta lại sử dụng vector nhị phân để biểu diễn các từ mà không dùng luôn giá trị số thực là vị trí của từ trong từ điển?\nCâu hỏi này cũng giống như hỏi rằng trong bài toán image classification thì vì sao ta không cài đặt output là một số thực và sau đó làm tròn để có kết quả mà lại là một vector phân bố xác suất. Tất nhiên là nếu làm theo cách đó thì mọi thứ vẫn CÓ THỂ ổn, quá trình huấn luyện cũng có thể được thành công. Tuy nhiên, ta có những điều cần lưu tâm như sau: Với output là số thực như vậy thì cost function hầu như chắc chắn là MSE (Mean Square Error). Khi đó, quá trình huấn luyện sẽ rất dễ rơi vào vị trí tối ưu cục bộ. Nếu mà số từ trong từ điển là rất nhiều thì kết quả của các phép tính trong cách biểu diễn dùng số thực là rất lớn. Để ý rằng, trong các biểu diễn one-hot encoding thì khoảng cách giữa một từ với các từ khác nó sẽ bằng hằng số là $\\sqrt{2}$. Trong khi đó, với cách biểu diễn dùng duy nhất số thực thì lại không, có những cặp từ rất gần nhau và có những cặp từ cực kì xa nhau, trong khi ta chưa có bất cứ điều gì thể hiện được rằng từ này nên gần với một từ hơn so với từ kia. Hạn chế one-hot encoding Trong cách biểu diễn one-hot encoding, ta thấy rằng máy tính đã có thể phân biệt được các từ với nhau, có thể biết được từ được dùng trong câu input là từ gì và có thể cho biết từ mà nó tính ra được ở output là từ gì. Nói chung là máy tính đã hiểu được “mặt trước” của các từ.\nTuy nhiên, ta vẫn chưa thể biểu diễn được mối quan hệ giữa các từ với nhau. Như đã đề cập ở phần trước, khoảng cách giữa hai cặp từ phân biệt bất kỳ đều bằng $\\sqrt{2}$, trong khi những từ có nghĩa gần gần nhau như “good” và “nice” thì nên có khoảng cách gần nhau, còn những từ trái nghĩa nhau như “good” và “bad” thì cũng nên cách nhau rất xa. Chính vì yếu tố này mà thường thì việc áp dụng one-hot encoding vào RNN khó có thể mang lại kết quả như mong muốn.\nBên cạnh đó, cách biểu diễn one-hot encoding thật sự là rất tốn kém về mặt bộ nhớ 😀 Nếu mà kích thước từ điển rất lớn thì cứ mỗi từ như vậy ta lại cần một vector có số chiều khổng lồ để biểu diễn. Một cách khắc phục vấn đề này là sử dụng ma trận thưa (sparse matrix), nhưng mà việc cài đặt thì cũng không phải đơn giản.\nTừ các hạn chế của one-hot encoding, ta có một phương pháp tốt hơn, vừa có thể biểu diễn được mối quan hệ giữa các từ và vừa tiết kiệm được bộ nhớ, đó là Word Embedding!\nWord Embedding Ý tưởng Đầu tiên, embedding nói chung là phương pháp đưa một vector có số chiều lớn (thường ở dạng thưa, tức là hầu hết các phần tử đều bằng 0), về một vector có số chiều nhỏ hơn (và không thưa).\nTa thấy ngay rằng one-hot vector để biểu diễn các từ trong một tập từ điển lớn chính là vector có số chiều lớn và ở dạng thưa 😀 Embedding có thể được áp dụng ở nhiều mảng khác nhau chứ không phải mỗi xử lý ngôn ngữ, ví dụ như hình ảnh cũng có. Word Embedding là một phương pháp biểu diễn các từ bằng một vector đặc trưng. Ví dụ, với các từ {man, woman, king, queen, apple, orange} và tập các đặc trưng {gender, age, food} thì ta có thể biểu diễn mỗi từ bằng một vector 3 chiều như sau:\nman woman king queen apple orange gender -1 1 -0.9 0.97 0.0 0.01 age 0.3 0.25 0.7 0.69 0.02 0.0 food 0.01 0.0 0.005 0.015 0.97 0.96 Trong bảng trên, mỗi từ trong từ điển ban đầu đã được ánh xạ thành một vector 3 chiều (còn one-hot vector để biểu diễn chúng thì có 6 chiều). Trong đó, giá trị vector ứng với mỗi từ sẽ chứa những nét đặc trưng về mặt ngữ nghĩa của từ đó. Kí hiệu $e_{word}$ là embedding vector của từ $word$. Ta có một số nhận xét sau: $e_{apple}$ và $e_{orange}$ có giá trị tại đặc trưng food rất cao và hai đặc trưng còn lại thì không. $e_{man}$ có đặc trưng gender là -1 còn $e_{woman}$ là 1, hàm ý rằng giới tính “man” và “woman” là trái ngược nhau. $e_{man}$ với $e_{king}$ có giá trị tại đặc trưng gender rất giống nhau, đối với age thì có sự khác biệt, hàm ý rằng “king” thì thường lớn tuổi hơn “man”. Ta có nhân xét tương tự với “woman” và “king”. Nếu ta tính thử độ tương đồng (similarity) giữa các vector (thường là khoảng cách Cosine hoặc khoảng cách Euclid), thì kết quả sẽ có ý nghĩa như sau: Hai vector $e_{man}$ và $e_{king}$ rất gần nhau. Tương tự với $e_{woman}$ và $e_{queen}$, $e_{apple}$ và $e_{orange}$. Điều này thể hiện rằng các từ trong mỗi cặp có quan hệ gần gũi với nhau về ngữ nghĩa. Hai vector $e_{man}$ và $e_{woman}$ có hướng gần như là ngược nhau, thể hiện rằng hai từ này có quan hệ trái ngược nhau. Thông thường, người ta thường sử dụng phương pháp t-SNE để giảm chiều các embedding vector xuống 2 chiều và trực quan hóa chúng để có góc nhìn rõ hơn về Word Embedding. Ví dụ như hình bên dưới, với các từ có nghĩa tương tự nhau thì ta sẽ thấy chúng có xu hướng cùng thuộc về một cụm:\nSử dụng t-SNE để trực quan hóa các embedding vector\nNguồn: Neptune AI Đối với trực quan hóa trong không gian 3 chiều thì các bạn có thể truy cập vào trang này của Tensorflow. Trong trang web đó, nếu tìm kiếm từ “soccer” thì ta sẽ thấy các vector được highlight lên là vector ứng với các từ có nghĩa rất tương tự, và hầu hết là liên quan đến thể thao.\nNhư vậy, phương pháp Word Embedding đã có thể khắc phục được hạn chế của one-hot encoding trong việc thể hiện mối quan hệ giữa các từ.\nTính chất của Word Embedding Trong khả năng biểu diễn các từ bằng vector đặc trưng và thể hiện được mối quan hệ giữa từ đó với những từ khác, ta có một tính chất thú vị liên quan đến Analogy Reasoning (suy diễn tương tự).\nVí dụ: Cho trước 3 từ “man”, “woman” và “king”. Trong đó, “man” đã có một quan hệ nhất định với “woman”. Ta cần tìm một từ sao cho quan hệ giữa “king” với từ này cũng tương tự như quan hệ giữa “man” và “woman”. Nếu một hệ thống Word Embedding đủ tốt thì ta sẽ có tính chất rằng những cặp từ $(w_{i1}, w_{i2})$ mà có quan hệ giữa hai từ trong một cặp là rất tương tự nhau thì các vector $x_i = e_{w_{i1}} - e_{w_{i2}}$ sẽ có hướng cũng rất tương tự. Ví dụ:\nMinh họa Analogy Reasoning\nNguồn: Polakowo Dựa vào tính chất này, ta có thể giải quyết câu hỏi đặt ra ở phía trên rằng từ cần tìm sẽ là “queen”. Để kiểm chứng, hãy xét lại bảng ở phần 2.1, ta có:\n$e_{man} -e_{woman} = \\begin{bmatrix}-2 \u0026amp; 0.05 \u0026amp; 0.01\\end{bmatrix}^\\top$ $e_{king} - e_{queen} = \\begin{bmatrix}-1.87 \u0026amp; 0.01 \u0026amp; -0.01 \\end{bmatrix}^\\top$ Để biểu diễn bài toán analogy reasoning như ví dụ bên trên một cách hình thức hơn, ta có thể phát biểu như sau:\nTìm từ $w$ sao cho\n$$w = \\argmax_w \\left ( \\text{sim} ( e_w, e_{man} - e_{woman} + e_{king} )\\right )$$\nvà kết quả là $w = queen$.\nSử dụng Word Embedding trong RNN Trong phương pháp one-hot encoding, ta sử dụng các one-hot vector ở input và output của RNN. Đối với word-embedding thì ta sẽ thay đổi một chút ở input, còn output thì vẫn dùng one-hot encoding để biết được mô hình dự đoán từ nào. 😜\nTại sao lại như thế?\nĐối với input, đưa vào RNN embedding vector thì chắc chắn mô hình có thể học tốt hơn so với one-hot vector rồi. Trong output, ta thấy rằng việc mô hình tính ra một vector dạng phân bố xác suất và sau đó xác định từ tương ứng bằng cách softmax thì sẽ dễ hơn nhiều so với việc output ra một embedding vector rồi từ vector này đi tìm từ gốc. Thật ra thì mình chưa thấy ai thực hiện tìm từ dựa vào embedding vector cả. 😀 Như vậy, để sử dụng Word Embedding trong RNN thì ta sẽ dùng embedding vector của các từ để làm input cho RNN.\nVí dụ: mượn tạm ảnh của các pháp sư Trung Hoa z =)) word-embedding-with-rnn.png Minh họa sử dụng Word Embedding trong RNN\nNguồn: Pháp sư nào đó Lưu ý.\nKhi sử dụng Word Embedding trong RNN thì thường ta sẽ dùng theo hướng transfer learning hoặc fine-tuning. Điều này có nghĩa là các embedding vector của mỗi từ có thể đã được cung cấp sẵn, ta chỉ việc đem vào dùng trong mô hình là đủ và nếu cần thiết thì cũng sẽ tiếp tục huấn luyện trên nền tảng đã có. Embedding matrix Ở các phần trên thì ta chỉ mới nêu sơ lược về phương pháp Word Embedding chứ chưa đề cập đến việc làm thế nào để xây dựng được các vector biểu diễn từ như vậy. Đầu tiên, thứ chúng ta cần xây dựng trong Word Embedding được gọi là embedding matrix (kí hiệu là $E$), với số dòng là số đặc trưng được dùng để mô tả cho mỗi từ và số cột bằng với số từ trong từ điển.\nĐể minh họa, ta sẽ dùng lại ví dụ ở phần 2.1. embedding matrix $E$ sẽ là\n$$ E = \\begin{bmatrix}-1 \u0026amp; 1 \u0026amp; -0.9 \u0026amp; 0.97 \u0026amp; 0.0 \u0026amp; 0.01 \\\\ 0.3 \u0026amp; 0.25 \u0026amp; 0.7 \u0026amp; 0.69 \u0026amp; 0.02 \u0026amp; 0.0 \\\\ 0.01 \u0026amp; 0.0 \u0026amp; 0.005 \u0026amp; 0.015 \u0026amp; 0.97 \u0026amp; 0.96\\end{bmatrix} $$\nVới từ $man$, one-hot vector của từ này là\n$$o_{man} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\end{bmatrix}^\\top$$\nEmbedding vector của $man$ sẽ được tính bằng công thức\n$$e_{man} = E \\cdot o_{man} = \\begin{bmatrix} -1 \u0026amp; 0.3 \u0026amp; 0.01 \\end{bmatrix}^\\top$$\nĐể xây dựng các embedding matrix, ta có hai hướng phổ biến như sau:\nWord2vec: Huấn luyện một mô hình MLP. embedding matrix sẽ là một ma trận trọng số trong mô hình MLP sau khi đã huấn luyện xong. GloVe (Global Vector for word representations): Mạnh hơn Word2vec, có sử dụng thêm những kỹ thuật liên quan đến xác suất thống kê. Trong bài viết này, mình sẽ tập trung vào word2vec.\nWord2vec Word2vec là một mô hình rất đơn giản và nổi tiếng trong việc tạo embedding matrix. Ý tưởng của word2vec xuất phát từ hai nhận xét sau:\nHai từ thường xuất hiện trong các ngữ cảnh tương tự nhau thì có thể có quan hệ gần gũi nhau về mặt ngữ nghĩa. Khi cho biết trước các từ xung quanh từ bị thiếu trong câu, ta có thể dự đoán ra được từ đó. Ví dụ, với câu “Husky là một … chó rất ngáo” thì từ trong dấu ba chấm có khả năng cao là “loài”. Đây là ví dụ với câu ngắn, còn nếu câu dài thì đôi khi ta chỉ cần xét tầm 10 từ xung quanh từ cần dự đoán là đã đủ để đoán ra được. Đầu tiên, ta đề cập đến khái niệm từ mục tiêu (target word) và từ ngữ cảnh (context word). Có thể nói từ mục tiêu là từ ta đang xem xét và từ ngữ cảnh là các từ xuất hiện xung quanh từ mục tiêu ở trong các đoạn văn bản của kho dữ liệu, với phạm vi là cách từ mục tiêu không quá $\\dfrac{C}{2}$ từ. Vùng phạm vi này còn lại là cửa sổ trượt (sliding window).\nVới câu ví dụ ở trên thì, từ “loài” là target word. Nếu xét cửa sổ trượt có kích thước $C = 4$ thì các context word sẽ bao gồm “là”, “một”, “chó”, “rất”. Để có cái nhìn rõ hơn về các khái niệm này, ta xét ví dụ bên dưới với kích thước sliding window là 4. Từ màu xanh là target word, các từ trong ô màu trắng là context word\nNguồn: Machine Learning cho dữ liệu dạng bảng Lưu ý.\nTừ phương pháp word2vec, ta sẽ có hai embedding vector cho mỗi từ, ứng với hai trường hợp là từ đó đóng vai trò target word và context word. Lý do là vì trong mỗi tình huống thì ngữ nghĩa của nó có thể sẽ khác nhau. Trong word2vec, ta sẽ đi xây dựng một mô hình MLP (Multi-layer Perceptron, hay nói cách khác là Neural Network) chỉ gồm 1 hidden layer, với mục đích có thể là:\nDựa vào target word để dự đoán context word Dựa vào các context word để dự đoán target word Tùy vào mục đích mà ta sẽ có một kiến trúc MLP khác nhau. Với mục đích (1) thì ta có Skip-gram, mục đích (2) là CBoW (Continuous Bag of Word)\nQuay lại với hai nhận xét đã mở ra ý tưởng cho word2vec thì nhận xét thứ nhất được thể hiện rõ hơn ở trong Skip-gram và nhận xét thứ hai thì ở trong CBoW 😀 Để có cái nhìn tổng quan về sự khác biệt giữa Skip-gram và CBoW, ta có hình ảnh so sánh như bên dưới:\nSự khác biệt giữa CBoW và skip-gram trong một câu với target word là W(t), context word là W(t-2), W(t-1), W(t+1), W(t+2)\nNguồn: https://i.stack.imgur.com/ShJJX.png Skip-gram Skip-gram là cách xây dựng mô hình MLP theo hướng dự đoán context word dựa vào target word. Về mặt toán học thì ta sẽ đi tìm xác suất xảy ra các context word khi biết trước target word.\nVí dụ, kho dữ liệu ta có hai câu là {“em ấy học toán tốt”, “em ấy học toán giỏi”}. Với từ mục tiêu “học” và kích thước sliding window là $C=4$, ta sẽ tìm xác suất\n$$ P_0 = P(\\text{\u0026ldquo;em\u0026rdquo;}, \\text{\u0026ldquo;ấy\u0026rdquo;}, \\text{\u0026ldquo;toán\u0026rdquo;}, \\text{\u0026ldquo;tốt\u0026rdquo;}, \\text{\u0026ldquo;giỏi\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) $$\nGiả sử các từ trên là độc lập với nhau, khi đó\n$$ P_0 = P(\\text{\u0026ldquo;em\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;ấy\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;toán\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;tốt\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;giỏi\u0026rdquo;} | \\text{\u0026ldquo;học\u0026rdquo;}) $$\nTập từ điển sẽ có 6 từ nên input của mô hình MLP sẽ là vector 6 chiều. Ta sử dụng hidden layer với 300 neuron. Xét hai cặp (context, target) lần lượt là (học, tốt) và (học, giỏi). Khi đó, mô hình skip-gram sẽ có dạng như hình bên dưới, với $\\bold{U}$ và $\\bold{V}$ lần lượt là ma trận trọng số giữa layer input-hidden và hidden-output.\nTham khảo: ProtonX - Word2vec Ta thấy rằng, hai từ “tốt” và “giỏi” cùng xuất hiện trong một ngữ cảnh, do đó chúng nên có quan hệ nào đó về mặt ngữ nghĩa 😀 Lưu ý.\nShape của $\\bold{U}$ có thể là (embedding_dim x vocab_size), tức là $(300 \\times 6)$, hoặc là (vocab_size x embedding_dim), tức là $(6 \\times 300)$. Với bài viết này thì mình sử dụng (embedding_dim x vocab_size). Tương tự như với $\\bold{V}$. Khi lần đầu tìm hiểu về Skip-gram, mình có một thắc mắc mà mình nghĩ là cũng rất nhiều người có cùng thắc mắc như thế 😜 Trong hình trên, ta thấy rằng mô hình cùng nhận vào một input là one-hot vector của từ học, sử dụng cùng hai ma trận trọng số $\\bold{U}$ và $\\bold{V}$, vì sao label lại có những giá trị khác nhau?\nThực chất, trong quá trình huấn luyện skip-gram, ta sẽ sử dụng optimizer SGD (stochastic gradient descent), tại mỗi thời điểm thì ta sẽ chọn ngẫu nhiên một cặp (target, context) rồi tiến hành cập nhật các ma trận trọng số một chút dựa theo cặp được chọn. Trong đó, phép chọn này không nên tuân theo phân phối đều mà nên có heuristic một chút, ví dụ như cặp nào xuất hiện càng nhiều thì có xác suất được chọn càng cao. Do đó, cặp (target, context) nào xuất hiện càng nhiều thì mô hình càng “học” được nhiều thứ về nó. Kết quả là sau quá trình huấn luyện, từ dự đoán của một context word sẽ là một phân bố xác suất “đủ gần” với tất cả các target word của nó trong kho dữ liệu. Ngoài ra, trong quá trình huấn luyện skip-gram thì chúng ta thường sử dụng loss function là cross-entropy (có dùng đến $\\text{softmax}$ activation function).\nCBoW (Continuous Bag of Words) Có thể nói CBoW là một phiên bản ngược lại của Skip-gram. Trong CBoW, ta sẽ sử dụng các context word để dự đoán target word. Về mặt toán học thì ta sẽ đi tìm xác suất xảy ra target word khi biết trước các context word.\nVới cùng ví dụ như phần về Skip-gram, ta sẽ tìm xác suất $$P_0 = P(\\text{\u0026ldquo;học\u0026rdquo;} | \\text{\u0026ldquo;em\u0026rdquo;}, \\text{\u0026ldquo;ấy\u0026rdquo;}, \\text{\u0026ldquo;toán\u0026rdquo;}, \\text{\u0026ldquo;tốt\u0026rdquo;}, \\text{\u0026ldquo;giỏi\u0026rdquo;})$$\nLúc này, ta thường tính một “từ trung bình” của các context word (embedding vector trung bình), sau đó thay vào biểu thức trên $$P_0 = P(\\text{\u0026ldquo;học\u0026rdquo;} | \\overline{word})$$\nĐể dễ minh họa, ta xét hai context words “em”, “toán” của target word “học”. Khi đó, mô hình CBoW sẽ có dạng như hình bên dưới, với $\\bold{V}$ và $\\bold{U}$ là ma trận trọng số giữa layer input-hidden và hidden-output. Lưu ý rằng, sau khi tính ra output tại hidden layer của các context word thì ta sẽ thực hiện thao tác tính trung bình để có một vector trung bình, sau đó mới tính ra predicted word. Tham khảo: ProtonX - Word2vec Trong huấn luyện mô hình, ta cũng loss function là cross-entropy và optimizer SGD giống như Skip-gram. Trong đó, ở mỗi bước của SGD thì thứ ta chọn ngẫu nhiên là một câu ngắn trong kho dữ liệu.\nTrích xuất embedding matrix Sau khi huấn luyện xong các mô hình như Skip-gram và CBoW thì ta thu được các ma trận trọng số $\\bold{U}$ và $\\bold{V}$. Nếu các bạn để ý thì trong mỗi mô hình, thứ tự mình sử dụng kí hiệu $\\bold{U}$ và $\\bold{V}$ là khác nhau.\nTrong Skip-gram, $\\bold{U}$ là ma trận trọng số nối giữa input-hidden, liên quan đến target word và $\\bold{V}$ thì nối giữa hidden-output và nó liên quan đến context word. Cũng vì sự “liên quan” giữa ma trận trọng số là các từ, trong CBoW thì $\\bold{V}$ được đưa lên thành ma trận trọng số giữa input-hidden, tương tự cho $\\bold{U}$. Ta biết rằng ma trận trọng số nối giữa layer input-hidden có nhiệm vụ chính là “học” các đặc trưng của từ input, còn ma trận trọng số nối giữa hidden-output thì có nhiệm vụ chính là dự đoán từ. Như vậy, rõ ràng là ta nên dùng ma trận trọng số đầu tiên để làm embedding matrix. Tuy nhiên, có sự khác biệt nào giữa các ma trận thu được từ Skip-gram và CBoW?\nĐối với Skip-gram, $\\bold{U}$ liên quan trực tiếp đến target word. Khi đó, embedding vector của mỗi từ tính được dựa vào $\\bold{U}$ sẽ mang nhiều thông tin về mặt ngữ nghĩa hơn. Ngược lại, trong CBoW, $\\bold{V}$ liên quan trực tiếp đến các context word nên embedding vector của mỗi từ tính được sẽ nghiêng về phía ngữ pháp. Ví dụ, với từ “cat”, ta tính embedding vector của nó theo cả hai ma trận $\\bold{U}$ trong Skip-gram và $\\bold{V}$ trong CBoW. Tiếp đến thì ta sẽ tìm từ tương đồng với “cat” nhất . Khi đó, sử dụng $\\bold{U}$ thì kết quả có thể là “dog”, còn dùng $\\bold{V}$ thì rất có thể sẽ là “cats” 😀.\nNhận xét Hai hướng tiếp cận Skip-gram và CBoW đều có những điểm mạnh và yếu của riêng nó (ví dụ như về mặt ngữ nghĩa và ngữ pháp) nhưng nhìn chung thì chúng đều cho ta những embedding vector đủ tốt để sử dụng trong các bài toán khác.\nTuy nhiên, ta có một điểm yếu khá quan trọng trong việc huấn luyện Skip-gram và CBoW. Về hàm loss function thì mình đã đề cập là chúng đều sử dụng cross-entropy và cần đến $\\text{softmax}$ activation function. Trong trường hợp từ điển có rất nhiều từ thì thao tác tính $\\text{softmax}$ này sẽ rất rất lâu 😀\nLĐể khắc phục, ta có một số cách như là sử dụng Hierarchy Softmax hoặc là Negative Sampling. Trong bài viết này thì mình sẽ không đề cập đến chúng 😜\nVấn đề thiên vị trong Word Embdding Nghe rất là ảo, nhưng mà nó có tồn tại 😅 Điều này xảy ra phần lớn là do kho dữ liệu văn bản mà chúng ta sử dụng để xây dựng embedding matrix.\nĐể lấy ví dụ, mình sẽ xét trường hợp liên quan đến giới tính. Cùng quay lại bài toán Analogy Reasoning trong phần 2.2:\nVới 3 từ “man”, “woman” và “king” thì ta có thể tìm được từ “queen” sao cho quan hệ giữa “king” và “queen” sẽ tương tự như giữa “man\u0026quot; và “woman”. Bây giờ giả sử ta có “man”, “doctor”, “woman” và cần tìm từ X sao cho quan hệ giữa “woman” và X tương tự như giữa “man\u0026quot; và “doctor”. Nếu kho dữ liệu liên quan phần lớn đến việc người đàn ông là trụ cột trong gia đình (xã hội thời xa xưa) thì kết quả X rất có thể là “babysitter” (người trông trẻ). Như vậy, đã có vấn đề thiên vị cho nam giới. Để hạn chế vấn đề này, ta có một số phương pháp như Hard Debiasing, Soft Debiasing. Mình sẽ không đề cập đến những phương pháp này ở đây, các bạn có thể tự tìm đọc nhé 😀 Nó phần lớn là liên quan đến các phép biến đổi toán học.\nDưới đây là minh họa cho vấn đề thiên vị mà mình đã lấy ví dụ ở trên để cho các bạn dễ hình dung:\nĐầu tiên, hướng thay đổi giới tính là từ trái qua phải. Vì vấn đề thiên vị đang xảy ra liên quan đến giới tính nên ta gọi đây là bias direction. Trước khi điều chỉnh, ta thấy embedding vector của doctor nghiêng về phía bên nam giới hơn, tương tự như babysitter. Sau khi hạn chế vấn đề thiên vị điều chỉnh, các embedding vector của doctor và baby sitter nên nghiêng về phía “công bằng” hơn đối với hai giới tính, tức là hướng trực giao với hướng bias direction tại vị trí trung bình. Trước khi điều chỉnh (embedding vector gốc)\nSau khi thực hiện debiasing\nNguồn: Vagdevik\nTài liệu tham khảo Vũ Hữu Tiệp, Machine Learning cho dữ liệu dạng bảng - Word2vec Kavita Gane, Word2Vec: A Comparison Between CBOW, SkipGram \u0026amp; SkipGramSI Dive into Deep Learning, Word2vec ","date":"2023-02-19T10:51:57+07:00","permalink":"https://htrvu.github.io/post/word-embedding/","title":"Word Embedding"},{"content":"Đây là bài viết đầu tiên của mình trong lĩnh vực Natural Language Processing (Xử lý ngôn ngữ tự nhiên). Nó sẽ khá dài vì mình đã trình bày kỹ quá trình back-propagation. Mong các bạn đọc hết nhé! 😀\nSơ lược về Natural Language Processing Bên cạnh Computer Vision (CV - Thị giác máy tính) thì Natural Language Processing (NLP - Xử lý ngôn ngữ tự nhiên) cũng là một mảng rất quan trọng và được nghiên cứu rộng rãi trong Deep Learning. Các sản phẩm nổi tiếng trên thế giới liên quan đến chữ viết, giọng nói, âm thanh như Google Dịch, Google Assistant, Siri, Alexa, ChatGPT… đều là các thành quả của việc áp dụng NLP vào thực tế.\nNguồn: Data Science Dojo Tuy NLP và CV là hai lĩnh vực nghiên cứu khác nhau nhưng chúng có những mối quan hệ rất đặc biệt và thú vị. Ta có thể đem ý tưởng của CV qua NLP, ví dụ như sử dụng phép toán convolution trong xử lý tính toán với văn bản, và ngược lại là đem ý tưởng của NLP qua CV, mà đặc biệt nổi tiếng gần đây là sử dụng Attention, Transformer trong CV. Việc kết hợp NLP và CV đã tạo ra nhiều kết quả rất nổi bật trong các ứng dụng như Image Captioning, Text-to-Image.\nNếu mà kể tên ra thì ta có thể nhắc đến ngay mô hình rất ảo diệu là Stable Diffusion 😀 Nền móng của NLP bắt nguồn từ những mô hình toán học và xác suất, đặc biệt là mô hình Markov. Khi Deep Learning bắt đầu phát triển mạnh mẽ, ta đã có thêm những mô hình khác với độ hiệu quả rất tuyệt vời như Recurrent Neural Network, Sequence to Sequence, Attention Mechanism và Transformer. Trong đó, Recurrent Neural Network, hay là RNN, là sự khởi đầu thú vị của Deep Learning trong NLP. Mô hình RNN cũng là sẽ chủ đề của bài viết này.\nSequence data, sequence models Sequence data Ta có thể hiểu sequence data (dữ liệu dạng chuỗi) là dạng dữ liệu mà các giá trị trong đó được sắp xếp theo một trình tự không gian/thời gian nào đó và chúng có những mối liên hệ với nhau. Ví dụ:\nVăn bản: Đây là dạng sequence data rất phổ biến. Các từ trong câu tất nhiên là được sắp xếp theo một trình tự nhất định để tạo ra được một câu có nghĩa Âm thanh: Một đoạn ghi âm giọng nói, một bản nhạc Video: Các frame (ảnh) của video theo các thời điểm liên tiếp nhau Sinh học: Trình tự của một đoạn gen, dãy protein,… Time series: Các dữ liệu thu thập theo thời gian như thị trường chứng khoán Văn bản\nÂm thanh\nVideo\nTime series\nLưu ý:\nTùy theo dạng dữ liệu mà ta sẽ có cách “số hóa\u0026quot; chúng sao cho các mô hình có thể tiến hành “học” được. Vì sao cần phải làm vậy? Vì máy tính chỉ hiểu được những con số, mà cụ thể hơn là chỉ hiểu 0 và 1 😀\nTrong bài viết này, mình sẽ tạm thời chưa đề cập đến điều này. Đối với dữ liệu dạng văn bản, các bạn có thể xem bài viết tiếp theo về Word Embeddings nhé.\nSequence models Khác với các dữ liệu dạng hình ảnh mà ta thường thấy trong Computer Vision, Natural Language Processing sẽ tập trung vào việc xử lý các dữ liệu dạng chuỗi. Do đó, các mô hình trong NLP thường được gọi là sequence model.\nĐể có sự phân biệt rõ hơn giữa sequence model và các model trong CV, ta thường xét đến input và các thức tính toán của chúng.\nVới model trong CV, input của ta sẽ là một ảnh xám hoặc là RGB (ma trận nhiều chiều). Trong quá trình tính toán, ta có thể thực hiện tính toán song song trên các giá trị đầu vào. Trong khi đó, sequence model sẽ nhận vào input là dữ liệu ở các giai đoạn khác nhau, mỗi giai đoạn thì ta sẽ có một vector hay một ma trận nhiều chiều. Khi tính toán, ta sẽ tính toán tuần tự từng giai đoạn một. Ví dụ, với input là một câu văn bản thì từng giai đoạn sẽ ứng với từng từ, mỗi từ có thể được biểu diễn bởi một số hoặc một vector. Minh họa sequence model Nguồn: Jeddy92 Sequence models được sử dụng cho nhiều bài toán phổ biến trong thực tế như sau:\nNguồn: deeplearning.ai Recurrent Neural Network (RNN) Để cho dễ diễn đạt, ta xét một bài toán trong NLP với input là một câu có độ dài $T_x$, từ ứng với vị trí thứ $i$ được biểu diễn bằng vector $x^{\u0026lt; i \u0026gt;}$ có $D$ chiều, output là một câu có độ dài $T_y$ và những từ tương ứng được biểu diễn là $y^{\u0026lt; i \u0026gt;}$ (tương tự như $x^{\u0026lt; i \u0026gt;}$).\nThông thường thì ta sẽ giả sử luôn $T_x = T_y$ để bài toán đơn giản hơn một chút. Để thực hiện được thì chỉ đơn giản là padding/truncate để chúng bằng nhau thôi 😜 Nếu bạn đang thắc mắc là có bài toán nào dạng như này thì hãy nghĩ đến Machine Translation. Hạn chế của mô hình Multi-layers Percentron Một cách tự nhiên, ta hoàn toàn có thể xây dựng một mô hình MLP (Multi-layers Perceptron) với input layer có $T_x \\times D$ neurons, output layer cũng có $T_y \\times D$ neurons và ở giữa là các hidden layer.\nNhằm mục đích minh họa, mình sẽ chỉ biểu diễn mỗi vector $x^{\u0026lt; i \u0026gt;}$ và $y^{\u0026lt; i \u0026gt;}$ là một neuron. Khi đó, kiến trúc của mô hình MLP sẽ có dạng như sau:\nMinh họa sử dụng MLP cho bài toán đặt ra\nNguồn: deeplearning.ai Với MLP thì ta đã thực hiện tính toán song song, tức là tính luôn trên toàn bộ input và cho ra output. Tuy nhiên, cách tiếp này có những hạn chế khá nghiêm trọng:\nKhông phải câu input nào cũng có độ dài $T_x$ như nhau, output cũng vậy. Nếu các câu này có nhiều từ và mỗi từ được biểu diễn bởi vector có số chiều lớn thì mô hình sẽ có rất rất nhiều trọng số. Mô hình không học được sự “chia sẻ đặc trưng” giữa các vị trí khác nhau trong câu. Ví dụ, cụm từ “tôi đi học” xuất hiện trong câu input thì cho dù nó bắt đầu ở vị trí nào đi nữa, ta vẫn nên học được các đặc trưng rất tương tự nhau. Lưu ý. Đây cũng là một trong những vấn đề dẫn đến mô hình CNN được áp dụng nhiều hơn trong lĩnh vực Computer Vision chứ không phải là MLP. Ý tưởng của RNN Vì sequence data có một đặc điểm là thứ tự của các giá trị trong input là rất quan trọng nên ta thường thiên về hướng lần lượt xử lý trên từng vị trí một. Đồng thời, khi đi đến các vị trí sau thì ta cũng nên có những thông tin đã trích xuất được từ các vị trí trước. Ý tưởng của RNN chính là như vậy.\nSơ lược về cách hoạt động của RNN được mô tả như sau:\nVới điều kiện giả sử $T_x=T_y$, ta sẽ thực hiện tính toán $T_x$ lần, tại thời điểm (hay là vị trí) thứ $i$ thì từ $x^{\u0026lt; i \u0026gt;}$ (input) ta tính ra $y^{\u0026lt; i \u0026gt;}$ (output). Hơn nữa, để thực hiện thao tác lưu giữ các thông tin cho đến thời điểm hiện tại và đưa nó qua các thời điểm sau, ta cũng cần tính thêm một giá trị là $h^{\u0026lt; i \u0026gt;}$ (giả sử luôn $h^{\u0026lt;0\u0026gt;} = 0$). Lúc này, ta gọi $h^{\u0026lt; i \u0026gt;}$ là trạng thái ẩn (hidden state) Như vậy, tại mỗi thời điểm thì ta cần tính ra $h^{\u0026lt; i \u0026gt;}$ và $y^{\u0026lt; i \u0026gt;}$, dựa vào 2 input là $x^{\u0026lt; i \u0026gt;}$ và thông tin từ những thời điểm trước được tổng hợp tại $h^{\u0026lt;i - 1\u0026gt;}$. Minh họa ban đầu cho RNN\nNguồn: deeplearning.ai Nhìn vào hình trên, ta có thể suy ra rằng những trọng số mà RNN cần học là ma trận trọng số của hidden layer ở các thời điểm.\nĐể thuận tiện cho các phần sau, ta sẽ quy ước kí hiệu như sau:\nVector input, output và hidden state tại thời điểm $t$ lần lượt là $\\bold{X}_t \\in \\mathbb{R}^d$, $\\bold{O}_t \\in \\mathbb{R}^o$ và $\\bold{H}_t \\in \\mathbb{R}^h$. Ma trận trọng số cho phép tính liên quan giữa $\\bold{X}_t$ và $\\bold{H}_t$ là $\\bold{W}^{t}_{ xh } \\in \\mathbb{R}^{ h \\times d }$. Tương tự như trên, ta có $\\bold{W}^{t}_{hh} \\in \\mathbb{R}^{h \\times h}$ và $\\bold{W}^{t}_{ho} \\in \\mathbb{R}^{o \\times h}$. Khi đó, ta có thể biểu diễn RNN như sau:\nNguồn: Dive into DL Sự chia sẻ trọng số giữa các thời điểm Qua mô tả về cách hoạt động của RNN ở phần trước, ta thấy rằng nếu ở mỗi thời điểm mà ta cần dùng một bộ trọng số khác nhau thì lượng tham số của mô hình RNN sẽ lớn không kém gì MLP ở phần 3.1 😀\nTrong RNN, giữa các thời điểm sẽ có sự chia sẻ trọng số, tức là mọi thời điểm đều dùng cùng một bộ trọng số $(\\bold{W}_{xh}, \\bold{W}_{hh}, \\bold{W}_{ho})$ để tính toán $\\bold{O}_t$ và $\\bold{H}_t$. Lợi ích của việc chia sẻ trọng số bao gồm:\nSố lượng trọng số trong RNN sẽ giảm đi rất nhiều lần so với MLP Ta cũng có thể khắc phục được nhược điểm của MLP trong việc “chia sẻ đặc trưng” giữa các vị trí khác nhau trong câu. Khi đó, từ hình ở phần 3.2, sau khi chú thích vị trí các ma trận trọng số được sử dụng thì ta có hình sau:\nThay vì phải biểu diễn đủ các thời điểm, ta có thể viết gọn lại RNN như hình bên dưới:\nBiểu diễn gọn hơn của Recurrent Neural Network Quá trình feed-forward Đầu tiên, ta sẽ xem hidden state ban đầu là $\\bold{H}_0 = \\bold{0}$. Để đơn giản, ta sẽ bỏ qua các giá trị bias ứng với $\\bold{H}_t$ và $\\bold{O}_t$. Khi đó, quá trình feed-forward tại thời điểm $t \u0026gt; 0$ diễn ra như sau:\n$$\\begin{equation} \\bold{H}_t = \\phi_h (\\bold{W}_{xh} \\bold{X}_t + \\bold{W}_{hh} \\bold{H}_{t-1}) \\end{equation}$$ $$\\begin{equation} \\bold{O}_t = \\phi_o (\\bold{W}_{ho} \\bold{H}_t) \\end{equation}$$\n, với $\\phi_h$ và $\\phi_o$ là các activation function. Thông thường, $\\phi_h$ là $ReLU$ hoặc $\\tanh$ và $\\phi_o$ thường là $\\text{softmax}$.\nNhư vậy, từ $\\bold{X}_{t}$ và $\\bold{H}_{t-1}$ ta sẽ tính được $\\bold{H}_{t}$, và từ $\\bold{H}_{t}$ thì ta sẽ tính được $\\bold{O}_{t}$.\nNhận xét.\nTa có thể nhận thấy rất rõ sự khác biệt giữa feed-forward trong RNN so với MLP. Với MLP thì nó chỉ cần hai phép toán (xem như phần hidden layers chỉ có 1 layer) là có luôn kết quả cuối cùng:\n$$\\begin{equation*} \\bold{H} = \\phi_h (\\bold{X} \\bold{W}_{xh}^\\top ) \\end{equation*}$$ $$\\begin{equation*} \\bold{O} = \\phi_o (\\bold{H} \\bold{W}^\\top _{ho} ) \\end{equation*}$$\n, với $\\bold{X} \\in \\mathbb{R}^{n \\times d}$ và $\\bold{H} \\in \\mathbb{R}^{n \\times h}$ là các ma trận ứng với toàn bộ giá trị input và hidden state (trong RNN thì $\\bold{X}_t$ và $\\bold{H}_t$ là các vector).\nBack-propagation Through Time (BPTT) Khi mà feed-forward trong RNN diễn ra khác với MLP thì tất nhiên là back-propagation cũng khác 😀 Thuật toán back-propagation trong sequence model được gọi là Back-propagation Through Time (BPTT).\nĐầu tiên, ta sẽ đề cập đến cost function. Giả sử output của RNN tại thời điểm $t$ là $\\bold{O}_t$ và label là $\\bold{Y}_t$. Kí hiệu $l(\\bold{O}_t, \\bold{Y}_t)$ là loss tại thời điểm $t$. Khi đó cost function của ta là\n$$\\begin{equation} L= \\sum_{i=1}^{T} l(\\bold{O}_t, \\bold{Y}_t) \\end{equation}$$\nThực ra là ta có chia $L$ cho $T$ nữa nhưng để cho gọn thì thôi bỏ qua 😜 Những gì ta cần thực hiện trong quá trình BPTT là tính đạo hàm của $L$ theo các ma trận trọng số $\\bold{W}_{xh}$, $\\bold{W}_{hh}$ và $\\bold{W}_{ho}$. Điều đặc biệt ở đây là trong quá trình feed-forward thì $\\bold{H}_{t-1}$ sẽ lại được dùng để tính $\\bold{H}_t$ chứ nó không đi một “mạch\u0026quot; từ $\\bold{X}$ đến $\\bold{H}$ rồi từ $\\bold{H}$ đến $\\bold{O}$ như trong feed-forward của MLP thông thường.\nĐể công thức được gọn nhẹ hơn, ta sẽ giả sử luôn các activation function $\\phi_h$ và $\\phi_o$ là hàm đồng nhất, tức là\n$$\\phi_h(\\bold{x}) = \\phi_o(\\bold{x}) = \\bold{x}$$\nQuá trình BPPT diễn ra như sau:\nĐầu tiên, dễ nhất là tính đạo hàm $L$ theo $\\bold{W}_{ho}$ 😀 Từ biểu thức $(3)$ thì ta có ngay\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{O}_{t}} = \\frac{\\partial l(\\bold{O}_t, \\bold{Y}_t)}{\\partial \\bold{O}_{t}} \\end{equation}$$\nDo đó, kết hợp $(2)$ và $(4)$ thì\n$$ \\frac{\\partial L}{\\partial \\bold{W}_{ho}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{O}_{t}} \\frac{\\partial \\bold{O}_t}{\\partial \\bold{W}_{ho}} \\right ) = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{O}_{t}}\\bold{H}_t^\\top \\right ) $$\nLưu ý. Cách tính giá trị của $\\dfrac{\\partial l(\\bold{O}_t, \\bold{Y}_t)}{\\partial \\bold{O}_{t}}$ trong biểu thức $(4)$ sẽ phụ thuộc vào hàm $l$ và thường thì nó rất dễ tính 😜\nTiếp theo, ta có nhận xét sau:\n$\\bold{H}_T$ chỉ tham gia vào một biểu thức trong quá trình feed-forward (để tính ra $\\bold{O}_T$) $\\bold{H}_{t}$ với $t \u0026lt; T$ thì tham gia vào hai biểu thức (tính $\\bold{H}_{t +1}$ và $\\bold{O}_t$) Do đó, cách tính $\\dfrac{\\partial L}{\\partial \\bold{H}_{t}}$ sẽ có sự khác biệt tùy theo giá trị $t$.\nVới $t = T$: Từ $(2)$ và $(4)$ ta có\n$$ \\frac{\\partial L}{\\partial \\bold{H}_{T}} = \\frac{\\partial L}{\\partial \\bold{O}_{T}} \\frac{\\partial \\bold{O}_T}{\\partial \\bold{H}_{T}} = \\bold{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\bold{O}_{T}} $$\nVới $t \u0026lt; T$: Từ $(1), (2)$ và $(4)$ thì\n$$ \\frac{\\partial L}{\\partial \\bold{H}_{t}} = \\frac{\\partial L}{\\partial \\bold{O}_{t}} \\frac{\\partial \\bold{O}_t}{\\partial \\bold{H}_{t}} + \\frac{\\partial L}{\\partial \\bold{H}_{t+1}} \\frac{\\partial \\bold{H}_{t+1}}{\\partial \\bold{H}_{t}} = \\bold{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\bold{O}_{t}} + \\bold{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\bold{H}_{t+1}} $$\nCứ tiếp tục biến đổi tiếp với $\\dfrac{\\partial L}{\\partial \\bold{H}_{t+1}}$ và cứ như thế cho đến $T$, ta sẽ có\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\mathbf{H}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_{hh}^\\top\\right)}^{T-i} \\mathbf{W}_{ho}^\\top \\frac{\\partial L}{\\partial \\mathbf{O}_{T+t-i}} \\end{equation}$$\nĐể ý rằng biểu thức $(5)$ cũng đúng với $t = T$.\nVậy từ $(1)$ và $(5)$ thì\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{W}_{xh}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\frac{\\partial \\bold{H}_t}{\\partial \\bold{W}_{xh}} \\right ) = = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\bold{X}_t^\\top \\right ) \\end{equation}$$\n$$\\begin{equation} \\frac{\\partial L}{\\partial \\bold{W}_{hh}} = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\frac{\\partial \\bold{H}_t}{\\partial \\bold{W}_{hh}} \\right ) = = \\sum_{t=1}^{T} \\left ( \\frac{\\partial L}{\\partial \\bold{H}_{t}} \\bold{H}_{t-1}^\\top \\right ) \\end{equation}$$\nVấn đề vanishing và exploding gradients trong RNN Qua các biểu thức $(6)$ và $(7)$, ta thấy rằng nếu giá trị $T$ lớn (tức là câu input gồm rất nhiều từ) thì sẽ có hai trường hợp xảy ra đối với các giá trị gradient $\\dfrac{\\partial L}{\\partial \\bold{W}_{xh}}$ và $\\dfrac{\\partial L}{\\partial \\bold{W}_{ho}}$:\nVanishing: Trong quá trình tính $\\left ( \\bold{W}_{hh}^\\top \\right ) ^ {T-i}$ có nhiều giá trị nhỏ hơn 1 được nhân với nhau. Exploding: Ngược lại (lớn hơn 1). Ta có một giải pháp để hạn chế hiện tượng này là Truncated BPTT, tức là ta chỉ lan truyền gradients đến một trạng thái cách trạng thái hiện tại một khoảng nào đó thôi chứ không lan truyền toàn bộ.\nĐối với hiện tượng vanishing trong RNN, ta có thể diễn đạt nó một cách văn vở hơn là mô hình đã bị “quên” những thông tin tích lũy từ phía trước.\nVí dụ, ta có hai câu sau:\nThe cat, which always … (very long descriptions) …, was very cute The cats, which always … (very long descriptions) …, were very cute Thông thường, động từ to-be ở trước “very cute” sẽ phụ thuộc vào danh từ ở đầu câu (”cat” hay “cats”). Tuy nhiên, nếu RNN bị vanishing gradient thì nó sẽ không thể nhớ được trước đó là một con mèo hay nhiều con mèo để mà chọn động từ to-be cho đúng. 😀\nYếu tố này đã mở ra một hướng phát triển cho RNN là ta sẽ cố gắng tính toán thêm những phép toán khác để duy trì được các thông tin từ trước, từ phía xa mà RNN hiện tại không thể ghi nhớ được. Từ đó, ta có sự ra đời của Gated Recurrent Unit (GRU) và Long Short-Term Memory (LSTM).\nQuá trình huấn luyện và sử dụng mô hình RNN trong thực tế Đối với các mô hình ML hay CNN thông thường, những thao tác diễn ra trong bước huấn luyện (training) và sử dụng trong thực tế (testing) là rất giống nhau: Từ một intput cho ra một output và chỉ như vậy là xong (ta tạm bỏ qua back-propagation).\nNguồn: Towards Data Science Tuy nhiên, trong RNN thì training và testing sẽ có sự khác biệt khá rõ rệt. Đối với training, ở mỗi thời điểm thì ta sẽ luôn có input và label ứng với thời điểm đó. Tuy nhiên, trong testing thì ta chỉ có duy nhất input cho thời điểm đầu tiên và input của những thời điểm sau chính là output của thời điểm trước đó.\nTa xét ví dụ với bài toán xây dựng mô hình RNN sinh ra đoạn văn bản như sau:\nNguồn: ML Lectures Trong training, từ một câu có $T_x$ từ là $x_1, x_2,\u0026hellip;, x_{T_x}$ thì ta sẽ tạo ra được một training sample với $(T_x + 1)$ thời điểm, ở thời điểm $t$ thì input và label lần lượt là $x_t$ và $x_{t+1}$ (giả sử $x_0$ và $x_{T_x + 1}$ lần lượt là các từ đặc biệt nhằm báo hiệu bắt đầu và kết thúc đoạn).\nTrong testing, từ một từ ban đầu là $x_1$ (thường là từ bắt đầu đoạn văn bản), ta sẽ có testing sample với 1 thời điểm. Sau khi qua RNN, ta có thêm từ mới là $y_1$. Sau đó, $y_1$ được sử dụng như là input của thời điểm thứ hai, đưa qua RNN và có tiếp từ $y_2$. Quá trình cứ lặp lại cho đến khi đã sinh ra đủ số từ chúng ta cần hoặc là từ được sinh ra chính là từ kết thúc đoạn.\nCác dạng mô hình RNN Chúng ta để ý rằng dạng mô hình RNN mình đã trình bày ở phần trước đang nhận input là một câu và output của nó cũng là một câu. Dạng mô hình này còn gọi là many-to-many. Tùy vào bài toán cần giải quyết mà ta có các dạng như sau:\nCác dạng của mô hình RNN\nNguồn: Javatpoint One-to-one: Cái này thì rất thường thấy, ví dụ như Image Classification. One-to-many: Có thể lấy ví dụ như bài toán sinh ra văn bản hoặc âm nhạc. Ta cung cấp input là một từ bất kì và mô hình sẽ tạo ra từ kế tiếp, ta lại đem từ này vào làm input để có từ tiếp theo. Many-to-one: Các bài toán như Sentiment Analysis, Mail fitlering,… Trong dạng mô hình này, chỉ có thời điểm cuối cùng là có tính ra output, các thời điểm trước thì chỉ có tính hidden state. Many-to-many: Chủ yếu là bài toán Machine Translation. Tài liệu tham khảo Robin M. Schmidtm, Recurrent Neural Networks (RNNs): A gentle Introduction and Overview Dive into DL, Recurrent Neural Network DeepLearning.AI, Deep Learning Specialization, 5. Sequence Models ","date":"2023-02-15T21:42:44+07:00","permalink":"https://htrvu.github.io/post/rnn/","title":"Recurrent Neural Network"},{"content":"Giới thiệu Ta biết rằng, hầu hết các mô hình CNN thường được xây dựng từ một phiên bản ban đầu (có thể là dựa theo một nguồn tài nguyên nào đó), sau đó chúng được scale dần lên để đạt được độ chính xác tốt hơn, và tất nhiên là độ phức tạp cũng tăng theo\nVí dụ: Với ResNet thì ta có ResNet18 cho đến ResNet152, DenseNet thì DenseNet121 cho đến 201, MobileNet thì ta có siêu tham số width multiplier để điều chỉnh số channel trong từng layer và resolutiom multiplier để điều chỉnh kích thước tại các layer,… Những cách làm đó gọi là model scaling. Tuy nhiên, ta nhận thấy rằng những thao tác model scaling trước đó chỉ tập trung vào một trong 3 yếu tố: depth - $d$ (số layer), width - $w$ (số channel) và resolution - $r$. Hơn nữa, việc điều chỉnh cũng không theo một nguyên tắc nào mà còn mang đậm tính chất ngẫu nhiên, “hên xui”, cần phải thử nghiệm rất nhiều lần mới có thể đạt được một độ chính xác mong muốn. Khi đó thì số lượng tham số của các mô hình cũng tăng chóng mặt!\nCác tác giả của paper đã cho thấy kết quả thực nghiệm rằng việc điều chỉnh một trong 3 yếu tố có thể tăng độ chính xác nhưng chỉ tăng đến một mức nào đó thôi, sau đó nó sẽ bị bão hòa. Ví dụ như ở hình bên dưới:\nTa có thể đưa ra các nhận xét như sau:\nNếu mô hình có width lớn (mỗi layer có nhiều channel) thì nó có thể học được nhiều loại đặc trưng khác nhau. Nhưng nếu mô hình không đủ sâu thì các đặc trưng đó cũng chưa phải đặc trưng ở mức high-level (nổi bật cho đối tượng) Nếu mô hình có depth lớn thì nó có thể học được các đặc trưng high-level nhưng nếu không có width lớn thì cũng không học được nhiều loại đặc trưng* Về mặt trực giác, nếu ta đưa vào mô hình một bức ảnh có resolution cao thì mô hình nên có depth lớn để có thể dần học các đặc trưng từ các feature maps có resolution lớn, đồng thời cũng vì sẽ có nhiều đặc trưng hơn nên ta cần width lớn. Do đó, model scaling nên tập trung vào việc điều chỉnh đồng thời cả 3 yếu tố $d$, $w$, $r$. Paper công bố EfficientNet có tên là “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”. Các tác giả tập trung vào việc đi tìm một phương pháp model scaling hiệu quả, có nguyên tắc, điều chỉnh đồng thời cả 3 yếu tố như đã đề cập. Phương pháp được giới thiệu có tên là compound scaling.\n\u0026ldquo;Nguyên tắc\u0026rdquo; trong phương pháp này rất đơn giản, ta sẽ cùng điều chỉnh $d$, $w$, $r$ của toàn bộ network theo cùng một hệ số gọi là compound coefficient (kí hiệu là $\\phi$).\nMinh họa cho các phương pháp model scaling. (a) là mô hình ban đầu. (b)-(d) thực hiện điều chỉnh một trong ba yếu tố. (e) là phương pháp được đề xuất, nó tiến hành điều chỉnh cả ba. Tất nhiên là model scaling chỉ phát huy tác dụng khi mà mô hình ban đầu là đủ tốt. Từ phương pháp compound scaling, các tác giả đã áp dụng nó cho ResNet, MobileNet để chứng tỏ độ hiệu quả của phương pháp. Sau đó, một họ mô hình mới được đề xuất là EfficientNet, vớ 8 phiên bản từ B0 đến B7 với độ phức tạp và độ chính xác tăng dần trên tập ImageNet. EfficientNet-B7 đã trở thành SOTA (state-of-the-art) với độ phức tạp nhỏ hơn rất nhiều lần so với mô hình SOTA trước đó.\nBài toán model scaling Giả sử conv layer thứ $i$ được định nghĩa là hàm số $Y_i = F_i(X_i)$, với input $X_i$ có shape là $\\left (H_i, W_i, C_i \\right)$. Khi đó, một CNN $N$ có thể được biểu diễn là\n$$ N = F_k \\bigodot F_{k-1} \\bigodot \\cdots F_1(X_1) = \\bigodot_{j=1,\u0026hellip;,k} F_j(X_1) $$\nThông thường, các mạng CNN thường được xây dựng theo kiểu gồm nhiều giai đoạn, mỗi giai đoạn là sự lặp lại các block có cùng dạng cấu trúc, chỉ khác nhau một số chi tiết như số layer trong block, kích thước của filter,… Ví dụ, ResNet được xây dựng dựa trên các residual block, MobileNet thì là các depthwise separable block,… Do đó, ta có thể viết lại $N$ thành\n$$ N = \\bigodot_{i=1,\u0026hellip;,s} F_i ^ {L_i}(X_{(H_i, W_i, C_i}) $$\n, với $F_i$ là layer được lặp lại $L_i$ lần trong giai đoạn thứ $i$, với input là $X$ có shape $(H_i, W_i, C_i)$.\nBài toán model scaling sẽ cố định layer $F_i$ và đi điều chỉnh các giá trị $L_i, H_i, W_i, C_i$, sao cho mô hình thỏa mãn các ràng buộc về tài nguyên và đạt độ chính xác cao nhất có thể.\nĐiều chỉnh $L_i$ $\\Leftrightarrow$ Điều chỉnh depth Điều chỉnh $C_i$ $\\Leftrightarrow$ Điều chỉnh width Đều chỉnh $H_i, W_i$ $\\Leftrightarrow$ Điều chỉnh resolution Để giảm không gian tìm kiếm, ta sẽ điều chỉnh các giá trị trên của toàn bộ layer trong mô hình theo cùng một tỉ lệ. Khi đó, bài toán của ta là bài toán tối ưu như sau:\n, với $d, w, r$ là hệ số để điều chỉnh depth, width, resolution; $\\hat{F_i}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i}$ là các giá trị ban đầu của mô hình baseline.\nPhương pháp compound scaling Phương pháp này sử dụng compound coefficient $\\phi$ để điều chỉnh depth, width, resolution theo nguyên tắc như sau:\nVới mô hình baseline ban đầu, ta thực hiện grid search để tìm ra bộ 3 giá trị tỉ lệ $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tốt nhất có thể, sao cho\n$$ \\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2 \\text{ và } \\alpha \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 $$\nSau đó, ta sẽ scale mô hình lên theo hệ số $\\phi$ với\n$$ d = \\alpha ^ \\phi, ; w = \\beta^\\phi, ; r = \\gamma^\\phi $$\nVề mặt trực giác, ta có thể xem $\\phi$ như là cách mà chúng ta cho biết lượng tài nguyên dành cho model scaling là bao nhiêu, còn các giá trị $\\alpha, \\beta, \\gamma$ là cách chúng ta phân phối tài nguyên đó cho depth, width và resolution. Giải thích cho các ràng buộc cho $\\alpha, \\beta, \\gamma$ được trình bày như sau:\nTất nhiên là để thực hiện được việc scale mô hình lên thì giá trị của chúng phải không nhỏ hơn 1 Ngoài ra, một phép toán convolution sẽ có độ phức tạp tỉ lệ thuận với $d, w^2, r^2$. Do đó, nếu ta scale model lên theo hệ số $\\phi$ thì độ phức tạp sẽ tăng lên một lượng bằng $(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2)^\\phi$. Các tác giả mong muốn độ phức tạp tăng khoảng $2^\\phi$, do đó ta có ràng buộc $\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2$. Họ mô hình EfficientNet Neural architecture search EfficientNet là một họ các mô hình rất đặc biệt:\nThứ nhất, chúng được xây dựng bằng “máy” 😀 Vào năm 2017, một ma thuật đã được công bố trong paper “Neural architecture search with reinforcement learning” của chính tác giả Quoc V. Le, nó giúp chúng ta xây một kiến trúc phù hợp nhất có thể dựa theo độ chính xác, độ phức tạp mà chúng ta yêu cầu.\nVới họ EfficientNet, các tác giả tập trung vào việc giới hạn độ phức tạp (cụ thể là FLOPS). Mục tiêu tối ưu của reinforcement learning là\n$$ ACC(m) \\times \\left ( \\frac{FLOPS(m)}{T} \\right )^w $$\n, với $m$ là mô hình, $ACC$ và $FLOPS$ là độ chính xác và độ phức tạp, $T$ là FLOPS mong muốn và nó bằng $400 \\times 10^6$, $w=-0.07$ là hằng số điều chỉnh trade-off giữa $ACC$ và $FLOPS$\nThứ hai, từ một mô hình ban đầu là EfficientNet-B0, ta tiến hành scale theo 2 bước:\nBước 1: Cố định $\\phi = 1$, giả sử lượng tài nguyên mà ta có thể sử dụng là nhiều gấp đôi hiện tại. Khi đó, thực hiện grid search để tìm các giá trị $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tốt nhất Bước 2: Từ các giá trị $(d, w, r) = (\\alpha, \\beta, \\gamma)$ tìm được, tiến hành scale theo các giá trị $\\phi$ lớn hơn để có được các phiên bản B1-B7. Ở đây, ta hoàn toàn có thể tăng $\\phi$ lên rồi lại grid search nhưng lúc này chi phí thực hiện là rất lớn. Do đó, các tác giả chỉ grid search một lần rồi sau đó chỉ cần tăng $\\phi$.\nKiến trúc mô hình Đầu tiên, tổng quan kiến trúc của EfficientNet-B0 như sau:\nTrong đó: MBConv chính là inverted residual block trong MobileNetV2, cùng với một số cải tiến như trong paper “Squeeze-and-excitation networks” Các mô hình EfficientNet-B1 cho đến B7 chính là kết quả của việc áp dụng compound scaling lên EfficientNet-B0.\nTài liệu tham khảo Paper EfficientNet: https://arxiv.org/abs/1905.11946 ","date":"2023-02-15T11:17:46+07:00","permalink":"https://htrvu.github.io/post/efficientnet/","title":"EfficientNet (2020)"},{"content":"Giới thiệu Từ sự thành công của MobileNet (2017) trong việc triển khai các mô hình Deep Learning trên các thiết bị biên (smartphone, embedded,…) nhờ vào việc sử dụng hiệu quả phép toán depthwise separable convolution, nhiều nghiên cứu dựa trên hướng phát triển này đã được tiến hành.\nDựa theo các “kinh nghiệm” có được của bản thân, nhìn vào MobileNet thì ta sẽ thấy ngay rằng, nó chưa có cái skip connection nào cả 😀 Đúng z, skip connection đã cho thấy được sự hiệu quả của mình trong các mô hình như ResNet, Inception-ResNet, DenseNet,… tại sao ta không thử thêm vào MobileNet? Boom, thêm ngay!\nMobileNetV2 được công bố với sự kế thừa từ MobileNet và bổ sung thêm skip connection. Tất nhiên là không chỉ dừng ở đó 🙂 Các tác giả xây dựng MobileNetV2 dựa trên các inverted residual block, nơi mà các skip connection dùng để kết nối các bottleneck layer với nhau! Hơn nữa, ta còn có một điểm rất thú vị là các bottleneck layer này sử dụng activation function là linear!\nMobileNetV2 đạt được độ chính xác cao hơn MobileNet trên tập ImageNet, với số tham số ít hơn, lượng bộ nhớ cần dùng tại mỗi layer là ít hơn. Từ sự ra đời của mô hình này, người ta cũng đã phát triển các mô hình hiệu quả trong bài toán Object Detection như SSDLite, hay Semantic Segmentation như Mobile DeepLabv3\nBàn về ReLU Ta biết rằng từ khi paper AlexNet giới thiệu activation function ReLU thì nó đã trở thành một activation function rất phổ biến và được dùng thường xuyên trong các mô hình Deep Learning với điểm mạnh quan trọng là đạo hàm của nó rất đơn giản. Công thức của ReLU là\n$$ReLU(x) = \\max(x, 0)$$\n, tức là nó sẽ “vứt” những giá trị bé hơn 0 trong input. Điều này nghĩa là ta sẽ bị mất thông tin!. Nếu input truyền vào là một channel thì ta sẽ bị mất một lượng thông tin nhỏ (hoặc có thể là lớn) trên channel đó.\nActivation function ReLU Nguồn: Research Gate Vậy tại sao trước giờ ReLU vẫn luôn được sử dụng?\nĐiều quan trọng là chúng ta có rất nhiều channel và giữa các channel này có những mối liên hệ nhất định. Do đó, việc mất thông tin ở một channel này có thể được channel khác bù đắp. Như vậy là ok. Để minh họa cho yếu tố làm mất thông tin, các tác giả đưa ra ví dụ sau:\nBan đầu, input của ta ở không gian 2 chiều. Qua phép biến đổi bằng một ma trận $T$ bất kì và áp dụng ReLU, ta có các output ở các không gian có số chiều khác nhau là 2, 3, 5, 15, 30. Để xác định xem thông tin có bị mất hay không, ta chiếu các output này về lại không gian 2 chiều bằng cách dùng ma trận nghịch đảo $T^{-1}$. Khi đó, kết quả thu được là các hình tương ứng ở trên. Rõ ràng là tính chất ban đầu của input đã bị mất. Gỉa sử từ một input $D_F \\times D_F \\times M$, qua một số layer thì ta có output $D_F \\times D_F \\times N$ và ta chuẩn bị áp dụng ReLU cho output. Các tác giả chứng minh được rằng ReLU sẽ không làm mất thông tin ban đầu của input nếu như $N \u0026lt; M$. Điều này có thể phát biểu bằng lời là nếu một input có thể được embedded (hay là nén) vào một không gian ít chiều hơn (số channel ít hơn) thì việc áp dụng ReLU lên kết quả nén đó sẽ không làm mất thông tin.\nỞ ví dụ phía trên thì ta đã có $N \\geq M$ và thông tin thật sự là đã bị mất. Từ nhận xét trên, ta thấy rằng không phải lúc nào xài ReLU cũng tốt. Nếu ngẫm lại, trong các kiến trúc như VGG, ResNet, Inception, MobileNet thì số channel của chúng hầu như luôn tăng qua từng block (chính là cụm “một số layer”) nhưng activation function được sử dụng luôn là ReLU. Điều này là vì chúng có rất nhiều channel (tăng theo bội 2) nên mọi thứ vẫn ổn 😀\nNếu bạn thắc mắc là vì sao số channel thường tăng như vậy thì trong CNN, những conv layer đầu thường sẽ học những đặc trưng đơn giản như cạnh ngang, dọc, chéo, vị trí của đối tượng trong ảnh,… càng về sau thì sẽ có các đặc trưng cụ thể, nổi bật lên của đối tượng (ví dụ như tai mèo, mắt mèo, mũi mèo,…). Do đó, càng về sau thì ta nên có càng nhiều channel đễ học được nhiều đặc trưng. Linear bottleneck Đầu tiên, ta sẽ nhắc lại về bottleneck layer. Đây là dạng layer thường được dùng với mục đích là “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán).\nNhững phát hiện về ReLU như đã đề cập là nguồn gốc của các linear bottleneck được sử dụng trong MobileNetV2.\nTại sao lại là linear mà không tiếp tục dùng ReLU rồi tăng số channel như những block trước?\nViệc giảm số channel trong các layer sẽ giảm lượng tham số của mô hình, từ đó giảm được độ phức tạp tính toán. Do đó, nếu xây dựng được một kiến trúc mà số lượng channel trong mỗi layer là nhỏ thì nó sẽ rất phù hợp cho các thiết bị biên. Các tác giả hướng đến việc giữ cho số channel của input và output của các block là nhỏ, tức là số channel chưa chắc đủ nhiều để đảm bảo rằng ReLU không làm mất thông tin 😜.\nNếu mà toàn các output có channel nhỏ như vậy thì làm sao mà mô hình đạt hiệu quả được? Lí do là vì nó là output của các bottleneck nên vẫn ok 😀 Số channel trong các layer của MobileNetV2\nNguồn: Machine Think Inverted residual block và expansion factor Inverted residual block là thành phần chính xây dựng nên MobileNetV2. Trong block này, ta sẽ áp dụng cả dethwise separable convolution, linear bottleneck và skip connection.\nTuy nhiên, lưu ý rằng số channel của input và output của các block này là rất nhỏ. Qua hình ở trên thì ta thấy chúng chỉ quanh quẩn 16, 24, 32. Do đó, trước khi áp dụng depthwise separable convolution lên input thì các tác giả thực hiện giải nén (expansion) lượng kiến thức trong input (input này là output của một block trước đó, nơi mà kiến thức đã được nén lại bởi linear bottleneck).\nVề mặt trực giác, lí do của thao tác này có thể hiểu là ta sẽ thực hiện convolution trên thông tin đầy đủ hơn để có thể phát hiện được càng nhiều đặc trưng càng tốt).\nViệc giải nén thực chất là ta sử dụng một conv layer $1 \\times 1$, với số lượng filter sẽ bằng với số lượng channel của input nhân với một siêu tham số. Siêu tham số này gọi là expansion factor và được kí hiệu là $t$.\nTiếp đến, theo sau phép toán depthwise separable convolution thì ta sẽ sử dụng linear bottleneck để tính ra output của block.\nNhư vậy, điểm qua các layer sẽ có trong inverted residual block sẽ bao gồm:\nLưu ý. Layer “linear 1x1 conv2d” chính là linear bottleneck. Tùy theo giá trị số channel $k$ và $k\u0026rsquo;$ có bằng nhau hay không mà ta sẽ áp dụng thêm skip connection. Hơn nữa, các skip connection trong inverted residual block được dùng để nối các bottleneck layer với nhau!\nVÌ sao lại là nối bottleneck chứ không phải nối các layer khác? Output của bottleneck là các “kiến thức” đã được cô đọng, đây là những gì mà mô hình đã học được và được biểu diễn trong một không gian ít chiều hơn. Do đó, ta vừa tiết kiệm được tài nguyên và vừa liên kết được các kiến thức quan trọng với nhau. Trong cài đặt, tùy theo giá trị stride của depthwise conv layer mà ta sẽ áp dụng skip connection hoặc là không. Cụ thể như sau:\nInverted Residual Block với stride=1 (có skip connection) và stride=2 (không có) Ngoài ra, ta có thể thấy trong inverted residual block thì activation được dùng cho 2 layer đầu tiên là ReLU6. Đây là một biến thể của ReLU, nó giới hạn giá trị output nằm trong đoạn $[0, 6]$ nhằm đảm bảo sự ổn định trong tính toán với số chậm động.\nActivation function RELU6\nNguồn: Mmuratarat Để dễ hình dung hơn về inverted residual block, ta cùng xem một ví dụ cho quá trình tính toán với expansion factor là 6:\nNguồn: Machine Think Lưu ý.\nCác tác giả có đề cập thêm đến luồng truyền thông tin của MobileNetV2, yếu tố mở ra những hướng phát triển tiếp theo trong tương lai. Ta thấy rằng inverted residual block đã tạo ra được sự độc lập giữa số channel của intput/output của block và của các layer nằm bên trong block:\nPhần bên trong được gọi là layer transformation với những phép biến đổi phi tuyến. Ta hoàn toàn có thể nghiên cứu thêm những cách xây dựng bộ phận này để tăng độ hiệu quả của mô hình. Nếu expansion factor của ta \u0026lt; 1 thì block này sẽ rất giống với block trong ResNet:\nKiến trúc MobileNetV2 MobileNetV2 được xây dựng dựa trên việc sử dụng nhiều inverted residual block. Kiến trúc tổng quan của nó như sau:\nTrong đó:\n$t$ là expansion factor. $c$ là số output channel của phần bottleneck trong inverted residual block. $n$ là số lần sử dụng block. $s$ là stride của block đầu tiên trong dãy $t$ block liên tiếp nhau, các block còn lại trong dãy có stride 1. Toàn bộ filter được sử dụng đều là $3 \\times 3$. Cài đặt Các bạn có thể tham khảo phần cài đặt MobileNetV2 bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper MobileNetV2: https://arxiv.org/abs/1801.04381 MachineThink, MobileNet version 2 ","date":"2023-02-13T11:32:55+07:00","permalink":"https://htrvu.github.io/post/mobilenet_v2/","title":"MobileNet V2 (2019)"},{"content":"Skip connection và concatenate Trước đó, kiến trúc ResNet được công bố và nó đã cho thấy được sức mạnh của các skip connection khi chúng được thêm vào các mô hình từ sâu cho đến rất sâu (ví dụ như ResNet152). Ta thấy rằng những kiến trúc áp dụng skip connection trước đây đều có một điểm chung là trong một block thì ta sẽ có những điểm nối 1 feature map vào làm input của một layer sau đó, và chúng đều sử dụng phép toán cộng.\nResidual block trong ResNet sử dụng skip connection với phép toán cộng Nguồn: Idiot Developer Công thức về skip connection trong block trên có thể được viết như sau:\n$$ x_l = H_l(x_{l - 1}) + x_{l-1} $$\n, với $H_l$ là phép biến đổi phi tuyến ở layer thứ $l$, $x_l$ là output của layer thứ $l$.\nPaper DenseNet giới thiệu một kiến trúc với ý tưởng là feature-map tại layer $l$ sẽ sử dụng toàn bộ feature-maps ở phía trước (layer $l - 1, l - 2,\u0026hellip;$) để làm input, và chúng sử dụng concatenate (thay vì phép toán cộng như ResNet). Với tư tưởng như vậy, các feature-maps ta có được có thể xem là một trạng thái có phạm vi toàn cục và bất kì layer nào cũng có thể sử dụng trạng thái này trong việc tính toán ra feature-maps của nó. Nếu viết theo kiểu công thức thì ta sẽ có\n$$ x_l = H_l([x_0, x_1,\u0026hellip;, x_{l-1}]) $$\nLưu ý. Để thực hiện phép toán concatenate thì các feature-maps phải có cùng size, hay là width và height.\nMột ví dụ cho kiến trúc DenseNet như sau:\nTrong hình trên, với $L$ layer, ta có $\\dfrac{L(L+1)}{2}$ kết nối trực tiếp giữa các layer. Các kết nối được tạo ra là rất dày đặc (dense). Từ đó, tên của kiến trúc được đặt là Dense Convolutional Network (DenseNet). Bàn về cách tổ chức các liên kết như vậy một chút:\nNhóm tác giả cho rằng kiến trúc như DenseNet sẽ đảm bảo lượng thông tin cũng như gradient truyền qua các layer là nhiều nhất có thể , từ đó mô hình sẽ có thể học được từ nhiều thông tin hơn, và tất nhiên là nó sẽ tạo ra hiệu ứng làm dịu bớt hiện tượng vanishing gradient.\nĐồng thời, việc sử phép toán concatenate có mang đến cho ta trực giác là có sự phân biệt rõ hơn giữa input trực tiếp từ layer ở ngay phía trước nó với các thông tin được “lưu trữ” và truyền đến từ các layer ở phía trước nữa. Nếu sử dụng phép toán cộng, những yếu tố này đã bị pha lẫn vào nhau.\nCó một chi tiết mà ta thường nghĩ đến ở các mô hình có kiến trúc rất sâu (nhiều layer) là số lượng tham số của nó sẽ rất lớn. Tuy nhiên, với DenseNet thì điều này không phải là vấn đề. Số feature-maps của các layer trong DenseNet sẽ rất nhỏ (chỉ tầm không quá 60), với lý do là để tính toán cho layer kế tiếp thì ta đã dùng toàn bộ feature-maps ở các phía trước rồi chứ không phải chỉ mỗi layer liền trước nó như hầu hết các mô hình khác, nên tại mỗi layer ta chỉ cần tầm đó là đủ rồi 😀\nKết quả so sánh giữa DenseNet và ResNet trên dataste ImageNet được các tác giả công bố như hình bên dưới. Ta thấy rằng DenseNet có số lượng tham số và số phép toán ít hơn ResNet, cùng với độ hiệu quả cao hơn.\nDense block, transition layer và growth rate Dense block và transition layer Ta thấy rằng nếu áp dụng ý tưởng kết nối dày đặt của DenseNet cho toàn bộ layer trong mô hình thì toàn bộ feature-maps trong tất cả layer này đều phải có cùng size (do phép toán được sử dụng là concatnerate).\nTuy nhiên, nếu toàn bộ các layer trong kiến trúc đều có cùng size như vậy thì ta khó mà down-sampling feature-maps về các size nhỏ hơn và rồi sau đó sử dụng các layer như Average Pooling, Dense để cho ra output như các kiến trúc khác được. Và việc “cô đọng” kiến thức của mô hình cũng sẽ gặp khó khăn.\nDo đó, ý tưởng kết nối dày đặt được các tác giả áp dụng trong từng khối (gọi là Dense block), việc down-sampling sẽ được thực hiện trong các khớp nối các Dense block với nhau (gọi là Transition layer).\nCó tổng cộng 4 dạng Dense block như sau:\nDense block cơ bản:\nHàm $H_l$ trong block này là sự kết hợp theo thứ tự 3 phép toán: $$ BN \\to ReLU \\to Conv (3 \\times 3) $$\nNgoài ra, ta có thể thêm dropout vào sau Conv để giảm overfitting.\nDense-B block (bottleneck):\nĐể tăng hiệu suất về mặt tính toán, ta có thể thêm một phép toán Conv $1 \\times 1$ vào $H_l$ để giảm bớt số lượng feature-maps input. Lúc này, thứ tự các phép toán sẽ là $$ BN \\to ReLU \\to Conv (1 \\times 1) \\to BN \\to ReLU \\to Conv (3 \\times 3) $$\nDense-C block (compression):\nTa sẽ giảm số lượng output feature-maps của các Dense block theo tham số $0 \u0026lt; \\theta \\leq 1$: từ $m$ feature-maps thành $\\lfloor \\theta m \\rfloor$ Thông thường, phần cài đặt của thao tác compression được ghép vào transition layer. Dense-BC block:\nKết hợp bottleneck và compression vào Dense block. Trong kiến trúc tổng thể, nếu trước đó ta dùng Dense-C hoặc Dense-BC block thì theo sau nó sẽ có thêm layer bottleneck (Conv $1 \\times 1) và thành phần này gọi là transition layer. Bên cạnh conv layer, thành phần không thể thiếu trong transition layer là một lớp Pooling (các tác giả sử dụng Average Pooling) để thực hiện down-sampling các feature-maps. Thứ tự các phép toán trong transition layer sẽ là\n$$ BN \\to ReLU \\to Conv (1 \\times 1) \\to AvgPool (2 \\times 2) $$\nGrowth rate Như đã đề cập ở phần ý tưởng, lượng tham số trong DenseNet được tối thiểu hóa là nhờ vào chi tiết số lượng feature-maps tại các layer trong DenseNet là nhỏ. Các tác giả xem số lượng feature-maps $k$ tại các layer là một siêu tham số của DenseNet, và nó được gọi là growth rate.\nThực nghiệm cho thấy rằng các giá trị $k$ mang lại kết quả tốt trên các dataset thường không quá lớn. Về mặt trực giác, ta có thể hiểu $k$ điều chỉnh lượng thông tin mới mà một layer có thể đóng góp vào trạng thái toàn cục (đóng góp một lượng vừa đủ thì sẽ tốt hơn là quá nhiều hay quá ít).\nMinh họa Dense Block với growth rate là 4 Nguồn: https://reliablecho-programming.tistory.com/3 Kiến trúc DenseNet Tùy vào loại Dense block được sử dụng, ta cũng có các tên gọi khác nhau cho DenseNet (DenseNet, DenseNet-B, Denset-C, DenseNet-BC). Kiến trúc DenseNet-C (hoặc DenseNet-BC) với 3 Dense block được mô tả trong hình bên dưới: Trước khi đến với quá trình tính toán qua các Dense block và Transition layer, ta có một layer Conv (và có thể có thêm Pooling, BN) như đa số các kiến trúc CNN khác. Các layer của của mô hình cũng sử dụng Global Pooling và Dense cùng activation softmax để tạo ra vector output. Các mô hình được nhóm tác giả thử nghiệm với dataset ImageNet được tóm tắt như sau:\nQuan sát bảng trên, ta có nhận xét là các mô hình trên đều thuộc loại DenseNet-BC. Ngoài ra, các tác giả cho biết giá trị growth rate được sử dụng là $k=32$.\nCài đặt Các bạn có thể tham khảo phần cài đặt DenseNet bằng Tensorflow và Pytorch tại repo sau.\nKhi cài đặt DenseNet, ta thường sẽ hơi phân vân về cách cài đặt các Dense block. Làm sao để cài đặt các kết nối dày đặc như vậy?\nThực ra cách cài đặt là rất đơn giản. Ta gọi khối gồm (Conv $1 \\times 1$, Conv $3 \\times 3$) như bảng trên là bottleneck block (bb). Khi đó 1 dense block với 4 bottleneck block sẽ có dạng như sau:\n$$x \\to bb_1 \\to x_1 \\to bb_2 \\to x_2 \\to bb_3 \\to x_3 \\to bb_4 \\to x_4 (output) $$\nTrong đó:\nbb_1.input = [ x ] bb_2.input = [x1, x] bb_3.input = [x2, x1, x] bb_4.input = [x3, x2, x1, x] Mã giả cho cách cài đặt dense block này như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def bottleneck_block(input, k): x = BN(input) x = ReLU(x)\tx = conv_1x1(x) x = BN(x) x = ReLU(x) x = conv_3x3(x) return x def dense_block(input, k): c = input x1 = bottleneck_block(c, k) c = concatenate(x1, c) # c = [x1, input] x2 = bottleneck_block(c, k) c = concatenate(x2, c) # c = [x2, x1, input] x3 = bottleneck_block(c, k) c = concatenate(x3, c) # c = [x3, x2, x1, input] x4 = bottleneck_block(c, k) c = concatenate(x4, c) # c = [x4, x3, x2, x1, input] return c # done! Như vậy, ta hoàn toàn có thể dùng một vòng lặp để cài đặt dense block:\n1 2 3 4 5 6 def dense_block(input, k): c = input for i in range(4): x = bottleneck_block(c, k) c = concatenate(x, c) return c Tài liệu tham khảo Paper DenseNet: https://arxiv.org/abs/1608.06993 ","date":"2023-02-11T18:09:08+07:00","permalink":"https://htrvu.github.io/post/densenet/","title":"DenseNet (2018)"},{"content":"Giới thiệu Class Activation Map (CAM) là phương pháp phổ biến trong việc giải thích sự hoạt động của CNN. Nó cho ta biết rằng CNN sẽ tập trung vào những phần nào của ảnh input để dự đoán xác suất ảnh đó thụôc về một class nào đó. Thông thường, CAM còn được gọi là Attention Map.\nĐể dễ hình dung hơn về CAM, ta có 2 ví dụ như sau:\nCNN tập trung vào phần đầu của con chó để đưa ra xác suất mà bức ảnh thuộc class “dog” CNN tập trung vào phần đầu của con chó để đưa ra xác suất mà bức ảnh thuộc class “dog”\nCNN tập trung vào con mèo khi đưa ra xác suất mà bức ảnh thuộc class “cat”\nNguồn: GlassBoxMedicine Các phương pháp này được xếp vào nhóm post-hoc, tức là ta chỉ tiến hành sinh ra CAM để giải thích sự hoạt động của CNN sau khi mô hình này đã được huấn luyện và có một bộ trọng số cố định.\nViệc giải thích CNN bằng CAM là rất hợp lý, vì:\nTa có thể biết được mô hình của mình có đang thật sự hoạt động tốt hay không (tập trung vào đúng phần quan trọng trong ảnh), tức là có chứng cứ rõ ràng cho các dự đoán Nó giúp ta kịp thời phát hiện những đặc trưng mà mô hình “hiểu lầm” khi học về một đối tượng nào đó. Ví dụ, nhờ CAM thì ta thấy rằng mô hình học cách nhận dạng tàu hỏa dựa vào các đường ray trong ảnh thì rõ ràng là nó đã học sai đặc trưng. Trường hợp này hoàn toàn có thể xảy ra vì phần lớn bức ảnh có tàu hỏa thì cũng có đường ray, nhưng ngược lại thì không. Ta có thể có một chiếc xe ô tô chạy ngang đường ray 😀 Trong các phương pháp sinh ra CAM cho một CNN (theo từng input) thì ta có các phương pháp nổi bật như Default CAM, Grad-CAM, Score-CAM. Ta sẽ lần lượt đề cập đến các phương pháp đó.\nCác phương pháp sinh CAM Default CAM (2016) Phương pháp này được đề xuất ngay từ khi các ý tưởng về CAM được công bố. Ta sẽ dựa vào output của conv layer cuối cùng trong kiến trúc, ngay trước fully connected layer sinh ra output của mô hình và các trọng số trong output layer.\nĐầu tiên, để mô tả về ý nghĩa của các feature maps trong output của conv layer cuối cùng thì ta xét ví dụ sau: Trong hình ảnh bên dưới, conv layer cuối cùng của ta là “Conv Layer n”. Output của nó có $k$ channel, hay là $k$ feature maps, mỗi feature map sẽ liên quan đến một đặc trưng nào đó trong ảnh input. Giả sử như feature map $1$ sẽ phát hiện mặt người trong ảnh, feature map $2$ sẽ phát phiện lông của con chó,…, feature map $k$ sẽ phát hiện tai của con chó. Minh họa ý nghĩa các feature maps Nguồn: Johfischer Các trọng số trong output layer sẽ mang ý nghĩa là tầm quan trọng của feature map tương ứng vơi trọng số đó trong việc đưa ra xác suất dự đoán ảnh input thuộc một class nào đó. Ví dụ, ta xét class 2 như hình ảnh bên dưới Khi đó, với $k$ feature maps $F_i$, ta có $k$ trọng số $w_i$ và bằng cách tính tổ hợp tuyến tính của $F_i$ và $w_i$ thì ta sẽ có CAM của ảnh input ứng với class 2. Phép tổ hợp tuyến tính giữa các feature maps Nguồn: Johfischer $$CAM_2 = w_1 * F_1 + w_2 * F_2 + \u0026hellip; + w_k * F_k$$\nLưu ý: Kích thước width và height của CAM đang bằng với các feature maps và nó thường nhỏ hơn nhiều so với ảnh input. Để có thể visualize được như trên, ta chỉ cần upsample CAM lên bằng kích thước của ảnh input. Với mỗi class khác nhau thì CAM tính được là khác nhau Sau khi trình bày ý tưởng của CAM thì ta thấy ngay một điều kiện mà CNN cần thỏa mãn để có thể áp dụng phương pháp này là sau các conv layer thì nó chỉ có duy nhất một fully connected layer để sinh ra output, và điểm nối giữa hai phần này là một global average pooling layer (GAP). Ví dụ như hình bên dưới:\nNguồn: GlassBoxMedicine Trong kiến trúc trên, ta có một CNN với output của conv layer cuối cùng là 3 feature map là A1, A2, A3. Qua GAP thì ta thu được một layer với 3 giá trị số thực. Theo sau đó là một fully connected layer sinh ra output của mô hình. Vì sao ta cần có duy nhất một fully connected layer? Ta đã đề cập rằng các trọng số trong output layer sẽ mang ý nghĩa là tầm quan trọng của feature map tương ứng. Do đó, nối ngay ở đây thì nó mới “đúng ý” (với mỗi neuron trong output layer, ta có đúng $k$ trọng số liên quan trực tiếp đến $k$ feature maps) Vì sao ta cần GAP? Nó sẽ tạo ra cầu nối giữa các feature map và output layer và đảm bảo rằng số channel trong input của output layer đúng bằng số feature maps của conv layer cuối cùng. Viết một cách tổng quát và “formal” hơn, ta sẽ có như sau:\nXét một CNN thỏa điều kiện áp dụng CAM với conv layer cuối cùng có $k$ feature maps $F_i$, output layer có $C$ neurons ứng với $C$ class. Ma trận trọng số tại output layer là $W_{C \\times k}$. Khi đó, với input $X$, CAM được sinh ra cho class $c$ là\n$$CAM_c = W_{c,1} * F_1 + W_{c,2} * F_2 + \u0026hellip; + W_{c,k} * F_k$$\nGrad-CAM (2016) Gradient-weighted Class Activation Mapping (Grad-CAM) là một phiên bản cải tiến của Default CAM với hai yếu tố sau:\nKhông ràng buộc điều kiện đối với kiến trúc của mô hình. Phần fully connected layers được phép ở bất kì cách tổ chức nào. Lưu ý là ta vẫn dùng GAP (global average pooling). Thay vì phải áp dụng cho conv layer cuối cùng thì ta áp dụng cho layer nào cũng được, nhưng thường thì người ta vẫn hay cùng conv layer cuối hơn 😜. “Tầm quan trọng” của các feature maps sẽ được tính theo cách khác, và cách tính này sẽ dựa vào gradient! Grad-CAM thường cho ra kết quả CAM tốt hơn, tập trung vào đúng vùng quan trọng hơn trong ảnh input khi so sánh với Default CAM. Vi dụ:\nSo sánh Default CAM và Grad-CAM Nguồn: MDPI Xét một CNN với conv layer cuối cùng có $k$ feature maps $F_i$, output layer có $C$ neurons ứng với $C$ class. Gọi giá trị output cho class $c$ là $y_c$. Với ảnh input $X$, ta đặt:\nTầm quan trọng của feature map $F_i$ trong việc đưa ra xác suất $X$ thuộc class $c$ được tính dựa vào gradient của output $Y_c$ theo $F_i$, tức là\n$$ \\alpha_{c,i} = GAP(\\frac{\\partial Y_c}{\\partial F_i}) $$\n, với GAP là global average pooling.\nĐể ý rằng, $\\dfrac{\\partial Y_c}{\\partial F_i}$ có cùng shape với $F_i$, ta tiến hành tính GAP để có được giá trị số thực $\\alpha_{c,i}$\nTừ đó, CAM cho class $c$ sẽ được tính bằng tổ hợp tuyến tính giữa $F_i$ và $\\alpha_{c,i}$:\n$$CAM_{c} = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nGrad-CAM đã hoạt động rất tốt và được dùng nhiều trong việc giải thích cho CNN. Tuy nhiên, đến năm 2021 thì có 2 tác giả đã chỉ ra rằng có những trường hợp Grad-CAM cho ra kết quả không thật sự đúng, khi mà CAM được sinh ra tập trung không đúng vào các phần quan trọng. Vi dụ trong y học mà các tác giả đưa ra như sau:\nLý do chính dẫn đến yếu tố này là ở phép toán GAP trong việc tính $\\alpha_{c,i}$. Có những tình huống mà GAP sẽ làm mất đi một số điểm nổi bật ở trong một feature map. Phương pháp mới được giới thiệu là HiresCam (2021), bằng cách thay thế GAP thành phép toán tích element-wise. Các bạn có thể tự tìm hiểu về cái này nhé. Score-CAM (2019) Score-CAM là một phương pháp dựa chỉ dựa vào score của các class (vector output của output layer) để tính tầm quan trọng của các feature maps, từ đó sinh ra CAM. Kết quả ta có được là các CAM tốt hơn phương pháp Grad-CAM, khi mà vùng được chú ý trong ảnh input là đúng hơn, “gọn” hơn (không lan rộng ra những thứ không liên quan lắm ở các phía xung quanh). Ví dụ:\nSo sánh Score-CAM và Grad-CAM Nguồn: Paper Score-CAM - Figure 1 Lưu ý.\nTa xét score của class khi chưa áp dụng softmax để đưa về xác suất. Score-CAM cũng không có ràng buộc gì về kiến trúc của CNN và ta có thể sinh CAM cho các feature map tại bất kỳ layer nào như Grad-CAM. Để trình bày phương pháp này thì ta phải dùng các kí hiệu toán học hơi nhiều một chút 😀\nIncrease of Confidence: Giả sử ta có hàm số $y = f(X)$ với input $X$ là vector $[x_1, x_2,\u0026hellip;, x_n]^T$ và $y$ là số thực. Với một input cơ sở $X_b$ đã biết nào đó, tầm quan trọng của thành phần $x_i$ đối với việc tính ra giá trị $y$ sẽ bằng với độ chênh lệch của output khi ta thay đổi phần thứ $i$ trong $X_b$ bằng cách nhân thêm $x_i$, tức là\n$$ c_i = f(X_b \\circ H_i) - f(X_b) $$\n, với $\\circ$ là element-wise product, $H_i = [1, \u0026hellip;, 1, x_i, 1, \u0026hellip; 1]^T$ (thành phần thứ $i$ là $x_i$)\nBây giờ, áp dụng Increase of Confidence trên với việc ta tính tầm quan trọng của các feature maps: Giả sử ta có một CNN $Y = f(X)$ với input $X$ là ảnh, $Y$ là vector có $C$ phần tử, ứng với score của $C$ class (chưa áp dụng softmax). Ta xét conv layer thứ $k$, feature map thứ $i$ tại layer này được kí hiệu là $F_{k, i}$. Với một input cơ sở $X_b$, tầm quan trọng của $F_{k, i}$ đối với score của class $c$ $( Y_c = f_c(X))$ là\n$$ C(F_{k, i}) = f_c(X_b \\circ H_{k, i}) - f_c(X_b) $$\n, với $H_{k, i} = s(Up(F_{k, i})$ là ta upsample $F_{k, i}$ lên cùng shape với ảnh input $X$, sau đó normalize nó theo công thức $Z \\leftarrow \\dfrac{Z - \\min_Z}{\\max_Z - \\min_Z}$\nSau khi tính các giá trị $C(F_{k, i})$, ta đưa chúng qua softmax để có các giá trị tầm quan trọng với tổng bằng 1:\n$$ \\alpha_{c,i} = \\frac{\\exp(C(F_{k,i}))}{\\sum \\exp(C(F_{k,i})) } $$\nKhi đó, ta có thể sinh ra CAM cho class $c$ như sau:\n$$CAM_c = \\alpha_{c,1} * F_1 + \\alpha_{c,2} * F_2 + \u0026hellip; + \\alpha_{c,k} * F_k$$\nNhư vậy, ta đã xác định được tầm quan trọng của các feature map theo cách là chỉ dựa vào output của CNN (score của các class).\nNhận xét Qua các phương pháp đã trình bày là Default CAM, Grad-CAM, Score-CAM, ta có thể thấy rằng điểm khác biệt lớn nhất giữa chúng là cách tính tầm quan trọng của các feature maps trong việc sinh ra output của CNN.\nCài đặt Các bạn có thể tham khảo notebook sau: Google Colab\nNội dung của notebook trên là sinh ra CAM cho các mô hình CNN như ResNet, DenseNet, EfficientNet và so sánh output của CAM, Grad-CAM và Score-CAM. Các kết quả chạy được trong notebook trên như sau:\nCAM của các mô hình CNN khác nhau: So sánh CAM, Grad-CAM và Score-CAM của mô hình ResNet50: Tài liệu tham khảo GlassBox, CNN Heat Maps: Class Activation Mapping (CAM) GlassBox, Grad-CAM: Visual Explanations from Deep Networks Paper Grad-CAM: https://arxiv.org/abs/1610.02391 Paper Score-CAM: https://arxiv.org/abs/1910.01279 ","date":"2023-02-09T18:30:41+07:00","permalink":"https://htrvu.github.io/post/cam/","title":"CAM, Grad-CAM và Score-CAM trong CNN"},{"content":"XAI là gì? Hầu hết các mô hình AI nói chung hay Deep Learning nói riêng luôn được người ta ví như là một chiếc hộp đen (black-box). Chúng ta xây dựng các mô hình với rất nhiều layer, từ convolution cho đến fully connected, sau đó sử dụng các optimizer như Adam, RMSprop,… (hoặc nói chung chung là gradient descent) để tối ưu mô hình, tức là tìm ra bộ trọng số sao cho hàm mất mát có giá trị nhỏ nhất có thể. Tuy nhiên, nếu ta nhìn lại mô hình và tìm cách giải thích là vì sao các mô hình hoạt động được tốt như vậy thì đây luôn là một câu hỏi khó, Việc đưa những chứng minh chặt chẽ, rõ ràng là không đề hơn giản. Từ đó, ta có một hướng nghiên cứu về các phương pháp giải thích sự hoạt động của các mô hình AI và lĩnh vực này được gọi là Explainable AI (XAI).\nNguồn: https://impact.nuigalway.ie/wp-content/uploads/2022/01/blackboxpng.png Vì sao chúng ta cần phải tìm cách giải thích các mô hình AI?\nTa lấy một ví dụ về mô hình chẩn đoán ung thư dạ dày dựa vào hình ảnh chụp nội soi được sử dụng ở các bệnh viện. Lúc này, tính chính xác của mô hình sẽ trở nên đặc biệt nghiêm trọng, nó có thể ảnh hưởng đến sức khỏe và cả tính mạng của bệnh nhân. Nếu mô hình chẩn đoán là ung thư thì ta cũng cần nó đưa ra những “chứng cứ” cho chẩn đoán đó, tất nhiên là chứng cứ phải đúng, mang tính thuyết phục cao thì mới chấp nhân được. Ngoài lĩnh vực y tế thì ta còn có các ví dụ khác như trong hệ thống bảo mật của ngân hàng,…\nNguồn: Webflow\nNguồn: MicroAI\nKhi AI càng được ứng dụng nhiều vào cuộc sống thì nhu cầu giải thích các mô hình AI cũng sẽ dần nhiều lên. Điều đó dẫn đến sự phát triển mạnh của XAI trong thời gian gần đây.\nDiễn giải một mô hình AI Khả năng diễn giải mô hình (interpretability) là mức độ hiểu biết của chúng ta về cách mô hình hoạt động, mà cụ thể hơn là về quá trình đưa ra dự đoán của mô hình. Ta có hai hướng tiếp cận chính đối với việc diễn giải mô hình là intrinsic và post-hoc.\nNguồn: Kemal Erdem Intrinsic (dựa vào bản chất của mô hình): Cách tiếp cận này thường dùng cho những mô hình thuộc nhóm white-box, đặc biệt là những mô hình Machine Learning như Linear Regresion, Decision Tree, SVM,… Đằng sau những mô hình đó là các lý thuyết toán chặt chẽ, ta có thể tìm được ngay công thức tính ra trọng số tối ưu của bài. Nói cách khác, khi chưa cần huấn luyện thì ta cũng có thể giải thích rằng mô hình sẽ hoạt động theo cách như thế này, như thế kia. Decision Tree Nguồn: javatpoint\nSVM Nguồn: Wikipedia\nPost-hoc: Đây là cách tiếp cận chúng ta thường dùng khi diễn giải các mô hình Deep Learning, và đặc biệt là nó được tiến hành sau khi mô hình đã được huấn luyện với một bộ trọng số đủ tốt. Vì việc giải thích, chứng minh chặt chẽ, chính xác về quá trình hoạt động của các mô hình Deep Learning là rất khó khăn nên post-hoc là hướng tiếp cận được ưu tiên hơn. Trong post-hoc, ta có 2 cách diễn giải là model-agnostic và model-specific.\nModel-agnostic: Cách này nghĩa là chúng ta có thể áp dụng cùng một phương pháp để diễn giải cho toàn bộ các mô hình mà không cần quan tâm đến kiến trúc của chúng. Như vậy, ta chỉ dựa vào input và output của mô hình để đưa ra cách diễn giải. Model-specific: Với cách này thì tùy theo những mô hình, hay là họ các mô hình, mà ta sẽ đưa ra cách diễn giải tương ứng. Ta có thể thấy rằng model-specific có thể dễ tiến hành hơn model-agnostic rất nhiều.\nNhững phương pháp trong XAI mà mình trình bày trong tương lai sẽ chủ yếu thuộc về hướng post-hoc.\nVì sao chúng ta bàn nhiều về khả năng diễn giải mô hình (interpretability) nhưng lĩnh vực này lại gọi là Explainable AI (thiên về khả năng giải thích mô hình)?\n2 thuật ngữ diễn giải và giải thích có thể xem là mang ý nghĩa tương tự và có thể dùng thay thế cho nhau. Tuy nhiên, có một vài quan điểm cho rằng khả năng diễn giải là nói đến một tính chất bị động của mô hình và nó cần con người chúng ta can thiệp vào, còn khả năng giải thích là thiên về chủ động, tức là mô hình có thể tự giải thích cho chính nó. Ở đây, con người chúng ta đang tìm cách giải thích các mô hình, do đó ta ưu tiên gọi là diễn giải. Đánh giá phương pháp XAI Một vấn đề khác mà người ta thường quan tâm đến là cách đánh giá một phương pháp XAI, tức là xét xem cách diễn giải mô hình A đã thuyết phục, đã đúng hay chưa. Hiện tại, ta chưa có một độ đo nào để có thể so sánh các phương pháp với nhau. Phần lớn thì nó nằm ở các nhận xét của con người thông qua việc quan sát 😀\nTài liệu tham khảo Mobiquity, An introduction to Explainable Artificial Intelligence (XAI) Erdem, XAI Methods - The Introduction ","date":"2023-02-09T15:30:31+07:00","permalink":"https://htrvu.github.io/post/intro-xai/","title":"Giới thiệu về XAI"},{"content":"Giới thiệu Qua các mô hình đã được giới thiệu như VGG, GoogLeNet hay ResNet thì ta thấy rằng chúng đều được phát triển theo hướng tăng dần độ sâu và độ phức tạp tính toán của mô hình để đạt được độ chính xác cao hơn, kể từ khi AlexNet được công bố. Số lượng tham số của chúng là rất lớn.\nTuy nhiên, các ứng dụng AI trong thực tế như robotics, xe tự hành thì các phép tính toán của mô hình cần được thực hiện trong một khoảng thời gian giới hạn, cùng với tài nguyên phần cứng hạn chế. Do đó, ta phải đối mặt với một trade-off giữa độ chính xác và độ trễ, kích thước mô hình.\nVào thời điểm này, có 2 hướng giải pháp chính để có thể đưa các mô hình vào ứng dụng thực tế như sau:\nNén các mô hình phức tạp lại thông qua các phương pháp như lượng tử hóa (quantization), hashing, cắt tỉa mô hình Xây dựng và huấn luyên các mô hình nhỏ, độ phức tạp thấp ngay từ đầu. MobileNet được phát triển theo hướng thứ 2, trong đó, nó tập trung vào các yếu tố:\nVừa đảm bảo kích thước mô hình đủ nhỏ, tốc độ suy diễn đủ nhanh (độ trễ thấp) và với độ chính xác đủ cao. Cung cấp hai siêu tham số cho phép ta điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước mô hình: width multiplier (liên quan đến số channel trong từng layer) và resolution multiplier (width và height trong từng layer) Depthwise separable convolutions MobileNet được xây dựng từ các layer convolution khá đặc biệt, chúng được gọi là depthwise separable convolutions. Depthwise separable convolution được tạo ra từ hai phép toán:\nDepthwise convolution: Áp dụng từng filter cho từng channel của input. Nếu input có bao nhiêu channel thì ta sẽ có bấy nhiêu filter. Pointwise convolution: Đây thực chất là convolution layer thông thường với filter 1 x 1. Nó được dùng để tổng hợp các kết quả từ phép toán depthwise convolution và tính ra output, thông qua các phép toán tổ hợp tuyến tính. Nguồn: Research Gate Ta có thể thấy ngay sự khác biệt giữa depthwise separable convolution và convolution thông thường như sau:\nConvolution thông thường: Mỗi filter sẽ tương tác với toàn bộ channel của input. Giả sử input của ta là $D_F \\times D_F \\times M$, một filter $3 \\times 3$ được áp dụng thì filter này sẽ trở thành một tensor với shape $3 \\times 3 \\times M$, ta thực hiện convolution trên từng channel và sau đó cộng $M$ ma trận lại với nhau, thu được kết quả $D_F \\times D_F$. Nếu sử dụng $N$ filter để tính thì ta sẽ có kết quả cuối cùng là $D_F \\times D_F \\times N$. Depthwise separable convolution: Ban đầu, các channel được tính toán độc lập với từng filter riêng, sau đó mới kết hợp lại sau nhờ vào pointwise convolution. Với input $D_F \\times D_F \\times M$ thì khi đưa qua depthwise convotution, ta sẽ có kết quả là $D_F \\times D_F \\times M$. Nếu pointwise convolution sử dụng $N$ filter $1 \\times 1$ thì ta có kết quả cuối cùng là $D_F \\times D_F \\times N$ Vấn đề đặt ra là tại sao sử dụng depthwise separable convolution lại có thể giúp cho MobileNet gọn nhẹ hơn, tính toán nhanh hơn và có độ chính xác đủ tốt, không hề kém cạnh các mô hình to lớn khác. Ta sẽ đặt tính một chút:\nGiả sử input của ta là feature maps $\\bold{F}: D_F \\times D_F \\times M$, output cuối cùng là $\\bold{G}: D_F \\times D_F \\times N$.\nVới convolution thông thường: Giả sử ta dùng $N$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding sao cho kích thước width và height không đổi. Khi đó, độ phức tạp tính toán sẽ là\n$$ O_1 = D_K \\times D_K \\times D_F \\times D_F \\times M \\times N $$\n, vì với mỗi filter thì: mỗi lần tính toán ta phải thực hiện $D_K \\times D_K$ phép toán nhân, sau đó cộng chúng lại; ta tính tại $D_F \\times D_F$ vị trí trên $M$ channel của input, và ta sử dụng $N$ filter.\nVới depthwise separable convolution: Ở bước depthwise convolution thì ta dùng $M$ filter $\\bold{K}: D_K \\times D_K$, stride là 1, padding phù hợp. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times M $$\n, vì ta chỉ đơn giản là áp dụng đơn lẻ từng filter cho từng channels\nVới pointwise convolution thì ta dùng $N$ filter $\\bold{K}: 1 \\times 1$, stride là 1, padding 0. Khi đó, độ phức tạp tính toán sẽ là\n$$ D_F \\times D_F \\times M \\times N $$\nDo đó, ta có độ phức tạp tính toán là\n$$ O_2 = D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N $$\nLúc này, đem chia cho nhau thì ta có tỉ lệ\n$$ \\frac{O_2}{O_1}=\\frac{D_K \\times D_K \\times D_F \\times D_F \\times M + D_F \\times D_F \\times M \\times N}{D_K \\times D_K \\times D_F \\times D_F \\times M \\times N} = \\frac{1}{N} + \\frac{1}{D_K^2} $$\nNhư vậy, độ phức tạp tính toán khi sử dụng depthwise separable convolution đã giảm khoảng $D_K^2$ lần so với convolution thông thường. MobileNet sử dụng các filter $3 \\times 3$, từ đó giảm được độ phức tạp tính toán đi khoảng 8 đến 9 lần, trong khi độ chính xác chỉ giảm đi một phần nhỏ.\nSiêu tham số điều chỉnh trade-off Để có thể hỗ trợ tốt hơn việc áp dụng MobileNet vào các thiết bị biên trong các ứng dụng thực tế, các tác giả còn cung cấp thêm cho ta hai siêu tham số để điều chỉnh trade-off giữa độ chính xác và độ trễ, kích thước của mô hình\nWidth multiplier Tham số width multiplier (kí hiệu là $\\alpha$) sẽ tác động lên giá trị số channel của các layer. Với những công thức ở trên thì số channel chính là $M$ và $N$. Giá trị $\\alpha \\in (0, 1]$ và ta thường đặt là $1, 0.75, 0.5, 0.25.$ Khi đó, thứ thật sự được thay đổi chính là số lượng filter mà ta dùng trong các phép toán pointwise convolution.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng width multiplier $\\alpha$ là\n$$ D_K \\times D_K \\times D_F \\times D_F \\times \\alpha M + D_F \\times D_F \\times \\alpha M \\times \\alpha N $$\nResolution multiplier Tham số resolution muiltiplier (kí hiệu là $\\rho$ ) liên quan đến kích thước width và height (chính là $D_F$ trong các công thức trên). Miền giá trị của nó cũng sẽ tương tự như $\\alpha$. Thực chất thì ta sẽ chỉ áp dụng nó vào input ban đầu của mô hình (ảnh). Các kích thước input mà ta thường sử dụng với mô hình MobileNet là 224, 192, 160 hoặc 128.\nĐộ phức tạp tính toán của depthwise separable convolution khi ta có sử dụng thêm resolution multiplier $\\rho$ là\n$$ D_K \\times D_K \\times \\rho D_F \\times \\rho D_F \\times \\alpha M + \\rho D_F \\times \\rho D_F \\times \\alpha M \\times \\alpha N $$\nKiến trúc mô hình Depthwise Separable block Mô hình MobileNet V1 được tạo thành bởi các thành phần chính là depthwise separable block. Chúng bao gồm hai phép toán như ta đã đề cập là depthwise convolution và pointwise convolution. Đi kèm với các layer đó là batch norm và activation ReLU.\nNguồn: Research Gate Kiến trúc MobileNet MobileNet sử dụng tất cả gồm 13 depthwise separable block. Tổng thể kiến trúc của MobileNet được thể hiện ở bảng sau:\nTrong đó:\nConv dw là depthwise convolution. Ta có thể thấy các filter shape của chúng luôn có cùng số channel trong input size. Các conv layer với filter shape $1 \\times 1$ chính là pointwise convolution. s1 tức là stride = 1, tương tự với s2. Toàn bộ các conv layer trong mô hình đều có padding sao cho kích thước width và height của input và output của layer đó là như nhau. Note:\nTrong bảng trên có vẻ có một chỗ gõ nhầm. Để ý đến layer “Conv dw / s2” đầu tiên từ phía dưới lên, nếu đây là s2 thì input size của layer “Conv / s1” tiếp theo phải bị giảm size chứ không phải $7 \\times 7$. Do đó, trong cài đặt mô hình ở bên dưới thì mình đã đổi nó thành “Conv dw / s1”. Cài đặt Các bạn có thể tham khảo phần cài đặt MobileNet bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper MobileNet: https://arxiv.org/abs/1704.04861 ","date":"2023-02-08T18:13:00+07:00","permalink":"https://htrvu.github.io/post/mobilenet/","title":"MobileNet (2017)"},{"content":"Giới thiệu Các mô hình thuộc họ Inception-ResNet được phát triển dựa trên ý tưởng là kết hợp skip connection vào các Inception block (các ý tưởng từ ResNet và GoogLeNet). Vì paper này chỉ mang tính thực nghiệm là chính nên mình sẽ không trình bày chi tiết 👀.\nKiến trúc mô hình Inception-ResNet V1 Về mặt tổng quan, Inception-ResNet V1 có kiến trúc như sau:\nKiến trúc Inception-ResNet V1\nStem của Inception-ResNet V1\nTa sẽ đề cập đến các loại Inception-ResNet block và Reduction:\nInception-ResNet block: Ta thấy rằng phần “Inception” trong các block này là đơn giản hơn khá nhiều so với các Inception block nguyên mẫu. Inception-ResNet-A V1\nInception-ResNet-B V1\nInception-ResNet-C V1\nReduction: Chúng thực hiện nhiệm vụ giảm kích thước (width, height) của các tensor đi một nửa. Kiến trúc của chúng rất giống với các Inception block Reduction-A\nReduction-B\nInception-ResNet V2 Phiên bản thứ hai của Inception-ResNet có kiến trúc tổng thể giống hệt với phiên bản đầu tiên, ta chỉ có một số thay đổi ở các block Inception-ResNet và Reduction\nTài liệu tham khảo Paper Inception-ResNet: https://arxiv.org/abs/1602.07261 ","date":"2023-02-08T18:03:46+07:00","permalink":"https://htrvu.github.io/post/inception-resnet/","title":"Inception-Reset (2016)"},{"content":"Khó khăn trong huấn luyện mô hình lớn Ta biết rằng, việc tạo ra các mô hình có độ sâu lớn (nhiều layer) chưa chắc đã mang lại hiệu quả tốt hơn những mô hình “cạn” hơn. Ví dụ, với tập CIFAR10 thì ta có một kết quả thử nghiệm cho thấy rằng mô hình sâu hơn lại có độ hiệu quả kém hơn:\nĐối với việc huấn luyện các mô hình có độ sâu lớn thì ta có thể sẽ bị gặp phải các vấn đề sau:\nOverfitting: Mô hình càng sâu thường sẽ càng phức tạp nên nó rất dễ bị overfitting. Vanishing/exploding gradient: Vấn đề này thì ta đã có một số cách giải quyết phổ biến như thay đổi activation function, các phương pháp khởi tạo trọng số như He Initialization. Trong He Initialization, tại mỗi layer thì ta khởi tạo các giá trị bias với gái trị 0, các trọng số sẽ tuân theo phân phối chuẩn với kỳ vọng 0, phương sai $\\dfrac{2}{D_h}$, trong đó $D_h$ là số units của layer liền trước layer hiện tại. Ngoài 2 vấn đề trên, ta có một vấn đề đặc biệt hơn là degradation: Accuracy tăng dần cho đến một độ sâu nhất định thì ngừng tăng (bão hòa) rồi sau đó sẽ giảm dần.\nLưu ý rằng, nhiều trường hợp degradation không phải do overfitting gây ra. Khi mô hình có độ phức tạp đủ lớn, những sự thay đổi dù là rất nhỏ trong trọng số cũng sẽ gây ra sự biến thiên lớn trong giá trị của gradient, điều này dẫn đến các bước cập nhật trọng số qua những lần chạy gradient descent sẽ không mang lại lợi ích gì nhiều mà còn có thể khiến quá trình huấn luyện \u0026ldquo;đi lạc\u0026rdquo;.\nSự biến thiên của gradient khi trọng số thay đổi trong các mô hình\n(a) Mô hình chỉ có 1 hidden layer; (b) Mô hình có độ sâu lớn với 24 hidden layers\nNguồn: Understanding Deep Learning - Simon J.D. Prince Degradation cho chúng ta thấy rằng việc tối ưu các mô hình có độ sâu lớn là không hề dễ dàng. Trong quá trình xây dựng mô hình, từ một mô hình ban đầu, sau khi thêm một số layer vào thì tất nhiên là ta mong rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc. Tuy nhiên, khi xảy ra degradation thì mong muốn đó đã không thể thành sự thật được. 😀\nResNet được công bố nhằm giải quyết vấn đề degradation đối với các mô hình có độ sâu lớn. Khi nhắc đến ResNet thì ta sẽ lập tức nghĩ đến những mô hình với độ sâu rất khủng, thậm chỉ là lên đến 100, 200 layer.\nHàm phần dư và skip connection Ý tưởng về hàm phần dư Nhắc lại về cái mong muốn ở phần trước, rằng mô hình mới phải có độ hiệu quả ít nhất là ngang mô hình gốc, ta có thể nghĩ ngay đến một phương pháp cực kì đơn giản: các layer phía sau sẽ là identity mapping, tức là input và output của nó sẽ giống nhau. Với cách làm này thì hiển nhiên là ta đạt được mong muốn rồi, vì độ hiệu quả của mô hình mới và mô hình gốc rõ ràng sẽ y hệt nhau.\nTuy nhiên, nếu chỉ dừng lại ở đó thôi thì thêm layer vào làm gì :v Ta muốn đạt được kết quả tốt hơn! Các tác giả của paper ResNet giới thiệu một phương pháp gọi là deep residual learning (học phần dư).\nGiả sử ta có một block $B$ các layer, input của nó là $\\bold{x}$. Với một mô hình thông thường, ta sẽ “học” một hàm số đầu ra mong muốn là $f(\\bold{x})$. Lúc này, ban đầu thì ta hoàn toàn chưa có một thông tin gì về $f(\\bold{x})$ cả, việc “học” sẽ xuất phát từ một đại lượng ngẫu nhiên.\nĐối với phương pháp deep residual learning, output của block $B$ sẽ có dạng\n$$h(\\bold{x}) = \\bold{x} + f(\\bold{x})$$\nvà ta sẽ đi học $f(\\bold{x})$. Lúc này, $f(\\bold{x})$ được gọi là residual function (hàm phần dư)\nTa có các nhận xét sau:\nTrong deep residual learning, ta đã có sự “gợi ý” cho hàm mong muốn thông qua giá trị input $\\bold{x}$. Đối với thông thường thì không có sự gợi ý nào được đưa ra cả. Việc học $f(\\bold{x})$ như là một hàm phần dư là dễ hơn so với việc học hàm mong muốn. Nếu identity mapping là kết quả tối ưu thì ta có luôn $f(\\bold{x})=0$ Về mặt bản chất, với phương pháp deep residual learning, ta đã tác động vào lớp hàm chứa hàm mong muốn, sao cho lớp mới là lớn hơn (bao gồm) lớp cũ. Nguồn: Dive into DL Để minh họa cho yếu tố \u0026ldquo;giúp việc học trở nên dễ hơn\u0026rdquo;, ta có ví dụ như sau:\nĐồ thị của loss function cũng \"trơn\" hơn rất nhiều khi có sử dụng skip connection, từ đó ta sẽ dễ huấn luyện mô hình hơn Nguồn: Jeremy Jordan Skip connection Như vậy, điểm nhấn của ResNet là ta đi học các hàm phần dư, thông qua việc “gợi ý” cho hàm số mong muốn một giá trị bằng với chính giá trị input ban đầu. Trong cài đặt, thao tác này được thực hiện thông qua một kết nối gọi là skip connection. Ta sẽ cộng input $\\bold{x}$ vào output của block thông thường.\nĐối với việc cộng như vậy thì thực chất là ta đang đi cộng hai ma trận. Khi đó, một vấn đề có thể nảy sinh là về shape của chúng, mà thường là số channel. Nếu số channel không khớp thì ta cần phải tiến hành “điều chỉnh”. Vì các block được sử dụng thường gồm các conv layer với stride và padding phù hợp để giữ nguyên width và height nên ta sẽ chỉ xét đến số channel. Nếu số channel của $\\bold{x}$ và output ban đầu là như nhau thì ta gọi skip connection này là identity skip connection. Ngược lại, ta sẽ dùng conv layer $1 \\times 1$ để điều chỉnh số channel của $\\bold{x}$. Lúc này, skip connection được gọi là projection skip connection. Sử dụng skip connection (identity) Nguồn: Dive into DL Ngoài ra, ta còn có một điểm mạnh quan trọng của skip connection là nó giúp cho gradient được lan truyền tốt hơn trong quá trình backpropagation, từ đó góp phần làm giảm hiện tượng vanishing gradient.\nQuan sát hình phía trên, ta thấy rằng khi backpropagation thì layer ở ngay trước block nhận được gradient từ hai layer phía sau nó (một layer liền trước nó và một layer được kết nối thông qua skip connection). Vấn đề exploding gradient Như mình đã đề cập ở đầu bài viết, với các mô hình có độ sâu lớn thì hai vấn đề đối với gradient mà ta thường gặp là vanishing và exploding. Đầu tiên, ta có những cách khởi tạo trọng số để hạn chế các vấn đề này như He Initialization. Ngoài ra, với vanishing thì cũng đã được phần nào hạn chế đi bằng skip connection. Tuy nhiên, nếu dùng He Initialization cùng với skip connection thì skip connection lại có khả năng gây ra exploding gradient 😃\nLý do của điều này nằm ở thao tác cộng giá trị $\\bold{x}$ với giá trị tính được của hàm $f(\\bold{x})$. Ta hoàn toàn có thể xem $\\bold{x}$ và $f(\\bold{x})$ là hai biến ngẫu nhiên độc lập, khi đó\n$$\\text{var}(\\bold{x} + f(\\bold{x})) = \\text{var} (\\bold{x}) + \\text{var} (f(\\bold{x}))$$\nHơn nữa, khi ta dùng ReLU và He Initialization thì $\\text{var}(f(\\bold{x})) = \\text{var}(\\bold{x})$. Do đó, có thể nói là ta đã làm cho phương sai của input $\\bold{x}$ tăng gấp đôi sau khi đi qua một block có dùng skip connection. Như vậy, khi dần qua nhiều block thì phương sai sẽ cứ tăng lên theo số mũ 2 và có thể dẫn đến hiện tượng exploding gradient.\nĐể hạn chế vấn đề exploding như trên, ta có hai cách phổ biến:\nSau khi tính xong $h(\\bold{x}) = \\bold{x} + f(\\bold{x})$ thì ta sẽ scale $h(\\bold{x})$ bằng cách nhân nó với $\\dfrac{1}{\\sqrt{2}}$. Sử dụng Batch Normalization với hai trọng số $(\\gamma, \\delta) = (1, 0)$ để chuẩn hóa $\\bold{x}$ trước khi tính $f(\\bold{x})$ (không thay đổi giá trị $\\bold{x}$ gốc vì giá trị gốc sẽ được sử dụng cho phép cộng ở sau. Bằng cách này, kết hợp với He Initialization thì $f(\\bold{x})$ cũng sẽ có phương sai là 1. Từ đó, phương sai của tổng $h(\\bold{x})$ sẽ chỉ tăng tuyến tính theo số lượng block được sử dụng. Minh họa hai cách hạn chế exploding trong skip connection\nNguồn: Understanding Deep Learning - Simon J.D. Prince Cách giải quyết số 2 được các tác giả sử dụng trong kiến trúc của ResNet. Đây là lý do ta thấy các layer Batch Normalization được sử dụng trong residual block ở phần tiếp theo.\nResidual block Các loại residual block Residual block (khối phần dư) được tạo ra bằng cách thêm một skip connection vào một block thông thường trong các mô hình CNN như VGG block. Ta có hai loại residual block như sau:\nBasic: Các conv layer trong block này có filter $3 \\times 3$. Block dạng này thường được dùng cho các mô hình có độ sâu vừa phải. Bottleneck: Nhằm giảm bớt số lượng tham số của các mô hình có độ sâu lớn, các tác giả sử dụng dạng block này với hai conv layer $1 \\times 1$ với vai trò là giảm/tăng số channel, tạo ra một hình dáng giống như nút thắt cổ chai. Hai layer conv $1 \\times 1$ như vậy được gọi là bottleneck layer. Để cho dễ hình dùng thì ta có thể xem đây như là thao tác “cô đọng kiến thức” của mô hình, hay nói rõ hơn là nén lượng thông tin lại sao cho vừa giữ được thông tin và vừa tiết kiệm tài nguyên (bộ nhớ, độ phức tạp tính toán). Basic Residual Block và Bottleneck Residual Block Ngoài ra, ta còn có 2 loại skip connection là identity và projection. Lúc này, tùy vào số channels của input và output ban đầu có khớp hay không mà block tương ứng sẽ chứa loại skip connection phù hợp.\nBasic residual block với identity và projection skip connection Nguồn: Dive into DL Thứ tự thực hiện các phép toán Quan sát kỹ hình ảnh mô tả residual block ở phía trên, ta sẽ thấy rằng thao tác cộng input $\\bold{x}$ và output của layer Batch Norm thứ hai (chính là $f(\\bold{x})$) được thực hiện trước khi đi qua ReLU. Vì sao lại như thế?\nLý do rất đơn giản. Nếu ta đưa $f(\\bold{x})$ qua ReLU trước rồi mới cộng vào $\\bold{x}$ thì $\\bold{x}$ đang được cộng với một lượng không âm. Cứ nhiều lần như thế thì giá trị model tích lũy được sẽ chỉ có tăng dần lên chứ không có giảm 😃 Điều này chắc chắn là không ổn và có thể khiến mô hình không thể học được đặc trưng gì có ích cả.\nKiến trúc ResNet ResNet được tạo nên bằng cách sử dụng nhiều residual block liên tiếp nhau, tương tự như những gì mà GoogLeNet hay VGG đã thực hiện. Các tác giả của paper ResNet tạo ra nhiều phiên bản ResNet khác nhau với độ sâu tăng dần.\nDựa vào số lượng layer có trọng số thì ta sẽ có tên các mô hình như ResNet18 (18 layer có trọng số), ResNet34,… Các phiên bản ResNet Ta có một số nhận xét như sau:\nCác phiên bản có độ sâu lớn như 50, 101 và 152 sử dụng Bottleneck Residual Block. Với 18 và 34 thì chúng dùng Basic block Trong các kiến trúc trên thì ta sử dụng cả 2 loại skip connection: identity và projection Ví dụ, với ResNet18 thì trong cụm Basic block đầu tiên của cụm conv3_x sẽ có số channel là 64, còn output ban đầu của block này thì là 128 nên ta phải áp dụng projection vào input Đối với height và width thì các tác giả cho biết việc downsampling input sẽ được thực hiện tại conv layer đầu tiên của các cụm conv3_x, conv4_x, conv5_x (với stride là 2). Các conv layer khác thì đều có stride 1. Ta sẽ cần chú ý đến chi tiết này khi tiến hành cài đặt ResNet. Cài đặt Các bạn có thể tham khảo phần cài đặt ResNet bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper ResNet: https://arxiv.org/abs/1512.03385 Dive into DL, ResNet Simon J.D. Prince, Understanding Deep Learning ","date":"2023-02-08T17:41:09+07:00","permalink":"https://htrvu.github.io/post/resnet/","title":"Resnet (2015)"},{"content":" Cá nhân mình thấy GoogLeNet là một paper khó đọc. Khi viết ra bài này thì mình vẫn đang cảm thấy hơi lú về nội dung của nó 😀\nGiới thiệu Từ khi AlexNet được công bố vào năm 2012 và đặt nền tảng cho các mạng Deep CNN, GoogLeNet, hay Inception V1 (2014), là một trong những kiến trúc có cách thiết kế rất thú vị khi nó tận dụng hiệu quả các conv layer, đặt nền móng cho nhiều mô hình sau này.\n“Inception” có thể dịch là “sự khởi đầu”, nghe có vẻ rất hợp lý 😀. Ngoài ra, ở trong bài viết về VGG, mình có đề cập đến vấn đề thiết kế kiến trúc mô hình theo hướng có sự lặp lại các khuôn mẫu. GoogLeNet cũng sẽ được thiết kế như vậy.\nLưu ý. Tên của mô hình này là GoogLeNet, không phải GoogleNet =)) Tác giả cho biết ý nghĩa của cái tên này là “This name is a homage to Yann LeCuns pioneering LeNet 5 network.”\nGoogLeNet được xây dựng từ những mục tiêu của nghiên cứu như sau:\nNâng cao khả năng tận dụng tài nguyên tính toán Cho phép tăng chiều rộng và chiều sâu của kiến trúc mô hình mà vẫn đảm bảo được độ phức tạp tính toán là ở mức chấp nhận được. GoogLeNet thật sự đã đạt được những điều đó, và nó được xây dựng dựa trên nguyên lý Hebbian: “neurons that fire together, wire together”. Paper GoogLeNet đã đưa ra các nền tảng lý thuyết rất “căng thẳng” để cho thấy rằng mô hình CNN có thể hoạt động “đủ tốt”, điều mà ta chưa được biết đến ở trong các paper trước đó 😀\nNgoài ra, có một quan điểm khá thú vị khi mô tả về kiến trúc của GoogLeNet như sau: Khi xây dựng kiến trúc CNN, thay vì phải suy nghĩ xem trong các mạng CNN ta nên áp dụng filter với kích thước bao nhiêu, hãy áp dụng luôn nhiều filter với kích thước khác nhau và tổng hợp kết quả lại 😜\nMình cũng có đồng tình với quan điểm này. Tuy nhiên, nguồn gốc đằng sau nó có vẻ không chỉ đơn giản là như vậy. Trong paper, các tác giả đã tiến hành phân tích và thử nghiệm nhiều lắm cho ra được cái kiến trúc của GoogLeNet. Kết nối thưa và CNN Đầu tiên, tại thời điểm trước khi Inception được công bố, chúng ta có thể cải thiện một mô hình DNN bằng những cách như sau:\nTăng chiều sâu của mô hình (tức là số layer) Tăng chiều rộng (số channel trong mỗi layer) Tuy nhiên, với hai cách trên thì sẽ có những vấn đề mà ta cần lưu tâm là hiện tượng overfitting và sự gia tăng độ phức tạp của mô hình.\nCác tác giả có đề cập đến một hướng đi có thể giảm bớt hai vấn đề trên là sử dụng kiến trúc kết nối thưa (sparsely connected architectures). Ta có thể hiểu đơn giản như sau:\nVới hai fully connected layer liên tiếp nhau, mỗi neuron trong layer sau sẽ kết nối với tất cả các unit trong layer trước. Nếu như ta thay đổi đi một chút, mỗi neuron trong layer sau sẽ chỉ kết nối đến một vài unit trong layer trước, kiến trúc có được sẽ trở nên “thưa” hơn. Kết nối thưa trong fully connected layer Đối với conv layer thì ta đã đạt được tính chất thưa như vậy. Ta biết rằng, với mỗi phần tử trên feature map của layer hiện tại thì ta tính nó dựa vào một vùng nhỏ trên feature map output của layer trước đó. Giả sử hai layer này đều chỉ có 1 channel thì ta có thể biểu diễn nó như hình bên dưới: Kết nối thưa trong conv layer Kiến trúc thưa được các tác giả mô tả là mô phỏng lại hệ thống sinh học (ví dụ như khi ta nhìn vào một đối tượng thì ta thường chỉ chú ý một số điểm trên đối tượng đó thôi, khó mà chú ý tất cả được).\nNgoài ra, có một nền tàng lý thuyết rất trâu bò về kiến trúc thưa của Arora như sau:\nIf the probability distribution of the data-set is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer by layer by analyzing the correlation statistics of the activations of the last layer and clustering neurons with highly correlated outputs. Thật sự mình cũng chưa hiểu hết ý của câu trên… Nhưng đại ý của nó có vẻ là nếu ta biểu diễn được phân phối các điểm dữ liệu của một dataset bằng một kiến trúc DNN lớn và thưa thì từ đó ta có thể xây nên được một kiến trúc tối ưu (về cả độ chính xác và độ phức tạp), bằng cách xây từng layer một 😀\nTuy nhiên, với các tài nguyên phần cứng trong thời gian này thì rất khó để ta có thể tính toán trên các mạng DNN thưa. Do đó, các tác giả đi theo hướng tìm một mô hình là có tận dụng một số thông tin về tính chất thưa nhưng vẫn thực hiện tính toán trên các ma trận đầy đủ. Đấy chính là hướng sử dụng các phép toán convolution!.\nCũng vì lý do này, ở phần kiến trúc của GoogLeNet thì ta sẽ thấy nó chỉ có duy nhất một fully connected layer để sinh ra output, còn lại chỉ toàn conv layer thôi 😗 Nói đến việc áp dụng các filter trong conv layer, các tác giả cho rằng:\nMỗi phần tử trong feature map của một layer sẽ có mối tương quan với một vùng nào đó trên ảnh input (receptive field). Ta sẽ gom các phần tử cùng tương quan với một vùng vào cùng một cụm. Để ý rằng, trong những layer ở gần ảnh input thì ta sẽ có nhiều cụm và kích thước mỗi cụm là nhỏ. Càng đi qua các layer CNN thì số lượng cụm sẽ ít lại và kích thước cụm sẽ lan rộng ra. Tất nhiên là vẫn có thể có những cụm có kích thước nhỏ tại những layer đó. Do vậy, ta nên có các filter với kích thước lớn hơn để học các đặc trưng tại các cụm lớn, đồng thời cũng cần có filter kích thước nhỏ đối với các cụm nhỏ hơn. Qua một loạt các thử nghiệm, các tác giả đã chọn 3 filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Một khối chứa 3 filter trên được đặt tên là Inception module.\nBây giờ quay lại với lý thuyết của Arora, ở ý xây dựng mạng tối ưu qua từng layer một. GoogLeNet được tạo nên bằng đúng ý tưởng như vậy, ta nối Inception module - by - Inception module 😀\nP/s: Các bạn có thể tìm đọc phần Motivation trong paper gốc và tự cảm nhận nó nhé.\nInception module Inception module Qua các mô tả ở phần trên, ta có thể liên tưởng đến kiến trúc của Inception module là một thứ gì đó giống với cái ở hình (a), với đủ 3 loại filter là $1 \\times 1$, $3 \\times 3$ và $5 \\times 5$. Các tác giả dùng thêm cả một layer max pooling trong đó nữa, với lí do rất đơn giản là vì ở thời điểm đó thì max pooling thường mang lại hiệu quả tốt trong các kiến trúc mạng CNN =))\nTruy nhiên, cách cài đặt như hình (a) sẽ dẫn đến số lượng tham số của mô hình là rất lớn. Thay vào đó, ta có thể tạo ra kiến trúc dạng “bottleneck” bằng cách sử dụng thêm các layer convolution $1 \\times 1$ như hình (b), nhằm mục đích chính là giảm số channel. Số lượng phép tính lúc này sẽ được giảm một cách đáng kể.\nLưu ý rằng, ở cuối module ta có phép toán concatenate, tức là các feature-maps của toàn bộ layer convolution đều phải có cùng size (tức là width và height)\nKiến trúc GoogLeNet GoogLeNet sử dụng tổng cộng 9 Inception module, gồm có 22 layer có trọng số (tính cả pooling là 27), được tóm tắt qua bảng sau:\nMột điểm đặc biệt khi train GoogLeNet là các tác giả sử dụng các auxiliary classifiers (xem hình bên dưới). Các thành phần này sẽ khá giống như giống hệt với phần cuối của mô hình (bộ classifier). Như vậy, ta có thể xem GoogLeNet có 3 bộ classifier. Auxiliary classifiers được sử dụng với các ý nghĩa như sau:\nHạn chế vanishing gradient Tăng regularization Lưu ý:\nLoss khi huấn luyện sẽ cộng loss của cả ba lại với nhau. Khi test thì ta thường chỉ quan tâm đến bộ classifer cuối cùng. Một cách làm khác là ta xài cả 3, sau đó lấy kết quả trung bình. Nguồn: https://img2018.cnblogs.com/blog/1603578/201906/1603578-20190626151101604-1002238110.png Cài đặt Các bạn có thể tham khảo phần cài đặt GoogLeNet bằng Tensorflow và Pytorch tại repo sau. Trong cách cài đặt này, mình sẽ bỏ qua auxiliary classifiers.\nTài liệu tham khảo Paper GoogLeNet: https://arxiv.org/abs/1409.4842 ","date":"2023-02-08T01:59:53+07:00","permalink":"https://htrvu.github.io/post/googlenet/","title":"GoogLeNet - Inception V1 (2014)"},{"content":"Giới thiệu Dựa trên sự thành công của AlexNet vào năm 2012, nhiều nghiên cứu đã được tiến hành nhằm tìm ra các phương pháp hay kiến trúc mới để đạt được kết quả tốt hơn, ví dụ như:\nThay đổi (tăng, giảm) kích thước của conv filter Thay đổi stride, padding của conv layer Train và test trên các input với nhiều độ phân giải (resolution) ảnh khác nhau Trong năm 2014, VGG là một trong những kết quả nghiên cứu nổi bật nhất, và nó tập trung vào một vấn đề khác với các hướng trên là độ sâu (depth, hay là số layer) của mô hình. VGG đã đạt được các kết quả tốt nhất vào thời điểm nó ra mới trên dataset ImageNet và các dataset khác, trong các task như classification, localization,…\nNgoài ra, ta có thể đưa ra nhận xét như sau về AlexNet:\nDù AlexNet đã chứng minh được CNN có thể đạt độ hiệu quả tốt, nó lại không cung cấp một khuôn mẫu nào cho việc nghiên cứu, thiết kế các mạng mới. Theo thời gian, các nhà nghiên cứu đã thay đổi suy nghĩ từ quy mô những neuron riêng lẻ sang các tầng, rồi sau đó là các khối (block) gồm các tầng lặp lại theo khuôn mẫu. Kiến trúc của VGG là một trong những kiến trúc phổ biến đầu tiên được xây dựng theo ý tưởng như vậy.\nVGG block Điểm nổi bật của VGG là ta chỉ dùng duy nhất một kích thước filter trong mọi conv layer là $3 \\times 3$, và ta dần tăng độ sâu của mô hình bằng các conv layer. Hơn nữa, ta còn áp dụng nhiều conv layer liền nhau rồi mới dùng đến max pooling. Ta có thể gọi một block gồm những layer như thế là VGG block.\nCác tác giả có đề cập đến một vấn đề cho cách áp dụng này như sau: Việc dùng nhiều conv layer 3 x 3 liền nhau như vậy so với dùng một conv layer với filter lớn hơn (ví dụ 7 x 7) như hầu hết các mô hình đã được công bố vào thời điểm trước đó thì có gì “tốt” hơn, khi mà features map sau cùng ta thu được có thể có cùng kích thước? Để trả lời, ta có 2 ý chính như sau: Giảm số lượng tham số của mô hình (đặt tính là biết nhaa 😜) Dùng nhiều conv layer thì ta có khả năng sẽ phát hiện được nhiều feature có ích hơn (hai conv layer sẽ tạo thành một \u0026ldquo;hàm hợp\u0026rdquo;), từ đó decision function sẽ ok hơn. Ngoài ra, VGG block sử dụng padding 1 (giữ nguyên kích thước input), theo sau đó là max pooling với pool size $2 \\times 2$ và stride 2 (giảm kích thước input đi một nửa). Kiến trúc của nó có thể được mô tả như hình bên dưới:\nVGG block Nguồn: Dive intro AI Trong các bài toán áp dụng VGG, đôi khi ta có thể gặp VGG block với một conv layer $1 \\times 1$ ở trước max pooling. Block dạng này thường được sử dụng với mục đích chính là bổ sung thêm một phép biến đổi tuyến tính nữa. Tuy nhiên, trong thực nghiệm thì các tác giả đã cho thấy rằng việc sử dụng block dạng này không hiệu quả hơn so với toàn các conv layer với filter $3 \\times 3$ (cùng số lượng layer).\nKiến trúc VGG Bằng cách kết hợp nhiều VGG block với nhau, các tác giả đã tạo ra nhiều phiên bản VGG khác nhau, với số layer có trọng số là trong đoạn 11 - 19. Trong paper VGG, ta có 6 kiến trúc với độ sâu tăng dần và tiến hành so sánh với nhau. Điểm chung của các kiến trúc này là phần fully connected đều có 3 layer, và toàn bộ layer đều sử dụng activation ReLU.\nCác phiên bản VGG Ta có một số nhận xét như sau:\nSố lượng conv layer trong các VGG block của các phiên bản là tăng dần. Để ý đến B, C và D thì: C = B + conv layer $1 \\times 1$ trong mỗi VGG block D = C + đổi conv layer $1 \\times 1$ thành $3 \\times 3$ Khi thực nghiệm, ta có B \u0026lt; C \u0026lt; D. Do đó, việc thêm phép biến đổi tuyến tính bằng conv layer $1 \\times 1$ giúp mô hình hoạt động tốt hơn, nhưng nó vẫn không bằng với việc là ta thêm luôn conv layer $3 \\times 3$ 🤔. Độ rộng (số channel) trong từng block được tăng theo bội 2. Ý tưởng này được sử dụng rất rộng rãi trong thời điểm này và cả về sau Để hạn chế overfitting, ta có thể sử dụng thêm dropout cho hai tầng fully connected đầu tiên. Hai kiến trúc phổ biến nhất trong việc áp dụng VGG vào các bài toán khác là D (VGG16) và E (VGG19). Để trực quan hơn, ta có thể thể biểu diễn VGG16 như sau:\nNguồn: https://miro.medium.com/max/827/1*UeAhoKM0kJfCPA03wt5H0A.png Cài đặt Các bạn có thể tham khảo phần cài đặt VGG bằng Tensorflow và Pytorch tại repo sau.\nTài liệu tham khảo Paper VGG: https://arxiv.org/abs/1409.1556 Dive intro AI - VGG ","date":"2023-02-08T01:52:02+07:00","permalink":"https://htrvu.github.io/post/vgg/","title":"VGG (2014)"}]
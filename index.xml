<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hoang Trong Vu</title>
        <link>https://htrvu.github.io/</link>
        <description>Recent content on Hoang Trong Vu</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 19 Mar 2023 00:25:41 +0700</lastBuildDate><atom:link href="https://htrvu.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Bài toán Machine Translation, mô hình Sequence to Sequence và độ đo BLEU</title>
        <link>https://htrvu.github.io/post/mt_seq2seq/</link>
        <pubDate>Sun, 19 Mar 2023 00:25:41 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/mt_seq2seq/</guid>
        <description>Bài toán Machine Translation Giới thiệu Machine Translation (dịch máy) là bài toán rất phổ biến trong lĩnh vực NLP. Sản phẩm Google Dịch mà chúng ta vẫn dùng hằng ngày chính là một mô hình dịch máy khá tốt và nó được huấn luyện bởi Google 😀
Thật ra bài toán Machine Translation đã ra đời từ rất lâu. Tất nhiên rồi, vì nó đóng vai trò rất quan trọng trong giao tiếp.</description>
        </item>
        <item>
        <title>Deep RNN và Bidirectional RNN</title>
        <link>https://htrvu.github.io/post/deep-rnn_birnn/</link>
        <pubDate>Fri, 24 Feb 2023 13:48:53 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/deep-rnn_birnn/</guid>
        <description>Giới thiệu Qua các bài viết về RNN truyền thống, LSTM và GRU thì mình đều trình bày về các mô hình với duy nhất một cell trong kiến trúc (recurrent cell, LSTM cell hoặc là GRU cell). Ngoài ra, ta thấy các hidden state cũng được truyền theo một hướng cố định là từ trái sang phải (thời điểm $t$ đến thời điểm $t + 1$). Nếu bỏ qua chi tiết về các &amp;ldquo;thời điểm&amp;rdquo; thì nhìn chúng sẽ không khác gì một mô hình MLP cơ bản trong Machine Learning.</description>
        </item>
        <item>
        <title>Long Short-Term Memory (LSTM) và Gated Recurrent Unit (GRU)</title>
        <link>https://htrvu.github.io/post/lstm-gru/</link>
        <pubDate>Fri, 24 Feb 2023 00:51:43 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/lstm-gru/</guid>
        <description>Vanishing gradient và long-term, short-term dependency Trong bài viết về mô hình RNN truyền thống, mình đã có đề cập đến vấn đề vanishing gradient của nó dựa vào công thức của quá trình BPPT (Back-propagation Through Time). Hệ quả của vấn đề này là RNN gặp khó khăn trong việc ghi nhớ thông tin trong những câu có nhiều từ.
Để minh họa rõ hơn về hệ quả của vanishing gradient, ta xét ví dụ với mô hình RNN dùng để sinh ra văn bản.</description>
        </item>
        <item>
        <title>Word Embedding</title>
        <link>https://htrvu.github.io/post/word-embedding/</link>
        <pubDate>Sun, 19 Feb 2023 10:51:57 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/word-embedding/</guid>
        <description>Trong bài viết về Recurrent Neural Network, mình đã đề cập khá kỹ về mô hình này nhưng để ứng dụng được nó vào các bài toán thì ta cần phải làm thêm bước “số hóa” dữ liệu từ văn bản sao cho máy tính có thể hiểu được.
Nếu máy tính hiểu được càng nhiều về các từ thì nghĩa là cách số hóa càng có hiệu quả. Do đó, ta cần quan tâm đến vấn đề “hiểu”.</description>
        </item>
        <item>
        <title>Recurrent Neural Network (RNN)</title>
        <link>https://htrvu.github.io/post/rnn/</link>
        <pubDate>Wed, 15 Feb 2023 21:42:44 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/rnn/</guid>
        <description>Đây là bài viết đầu tiên của mình trong lĩnh vực Natural Language Processing (NLP - Xử lý ngôn ngữ tự nhiên). Nó sẽ khá dài một chút, mong các bạn đọc hết nhé! 😀
Sơ lược về Natural Language Processing Bên cạnh Computer Vision (CV - Thị giác máy tính) thì Natural Language Processing (NLP - Xử lý ngôn ngữ tự nhiên) cũng là một mảng rất quan trọng và được nghiên cứu rộng rãi trong Deep Learning.</description>
        </item>
        <item>
        <title>EfficientNet (2020)</title>
        <link>https://htrvu.github.io/post/efficientnet/</link>
        <pubDate>Wed, 15 Feb 2023 11:17:46 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/efficientnet/</guid>
        <description>Giới thiệu Ta biết rằng, hầu hết các mô hình CNN thường được xây dựng từ một phiên bản ban đầu (có thể là dựa theo một nguồn tài nguyên nào đó), sau đó chúng được scale dần lên để đạt được độ chính xác tốt hơn, và tất nhiên là độ phức tạp cũng tăng theo
Ví dụ: Với ResNet thì ta có ResNet18 cho đến ResNet152, DenseNet thì DenseNet121 cho đến 201, MobileNet thì ta có siêu tham số width multiplier để điều chỉnh số channel trong từng layer và resolutiom multiplier để điều chỉnh kích thước tại các layer,… Những cách làm đó gọi là model scaling.</description>
        </item>
        <item>
        <title>MobileNet V2 (2019)</title>
        <link>https://htrvu.github.io/post/mobilenet_v2/</link>
        <pubDate>Mon, 13 Feb 2023 11:32:55 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/mobilenet_v2/</guid>
        <description>Giới thiệu Từ sự thành công của MobileNet (2017) trong việc triển khai các mô hình Deep Learning trên các thiết bị biên (smartphone, embedded,…) nhờ vào việc sử dụng hiệu quả phép toán depthwise separable convolution, nhiều nghiên cứu dựa trên hướng phát triển này đã được tiến hành.
Dựa theo các “kinh nghiệm” có được của bản thân, nhìn vào MobileNet thì ta sẽ thấy ngay rằng, nó chưa có cái skip connection nào cả 😀 Đúng z, skip connection đã cho thấy được sự hiệu quả của mình trong các mô hình như ResNet, Inception-ResNet, DenseNet,… tại sao ta không thử thêm vào MobileNet?</description>
        </item>
        <item>
        <title>DenseNet (2018)</title>
        <link>https://htrvu.github.io/post/densenet/</link>
        <pubDate>Sat, 11 Feb 2023 18:09:08 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/densenet/</guid>
        <description>Skip connection và concatenate Trước đó, kiến trúc ResNet được công bố và nó đã cho thấy được sức mạnh của các skip connection khi chúng được thêm vào các mô hình từ sâu cho đến rất sâu (ví dụ như ResNet152). Ta thấy rằng những kiến trúc áp dụng skip connection trước đây đều có một điểm chung là trong một block thì ta sẽ có những điểm nối 1 feature map vào làm input của một layer sau đó, và chúng đều sử dụng phép toán cộng.</description>
        </item>
        <item>
        <title>CAM, Grad-CAM và Score-CAM trong CNN</title>
        <link>https://htrvu.github.io/post/cam/</link>
        <pubDate>Thu, 09 Feb 2023 18:30:41 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/cam/</guid>
        <description>Giới thiệu Class Activation Map (CAM) là phương pháp phổ biến trong việc giải thích sự hoạt động của CNN. Nó cho ta biết rằng CNN sẽ tập trung vào những phần nào của ảnh input để dự đoán xác suất ảnh đó thụôc về một class nào đó. Thông thường, CAM còn được gọi là Attention Map.
Để dễ hình dung hơn về CAM, ta có 2 ví dụ như sau:</description>
        </item>
        <item>
        <title>Giới thiệu về XAI</title>
        <link>https://htrvu.github.io/post/intro-xai/</link>
        <pubDate>Thu, 09 Feb 2023 15:30:31 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/intro-xai/</guid>
        <description>XAI là gì? Hầu hết các mô hình AI nói chung hay Deep Learning nói riêng luôn được người ta ví như là một chiếc hộp đen (black-box). Chúng ta xây dựng các mô hình với rất nhiều layer, từ convolution cho đến fully connected, sau đó sử dụng các optimizer như Adam, RMSprop,… (hoặc nói chung chung là gradient descent) để tối ưu mô hình, tức là tìm ra bộ trọng số sao cho hàm mất mát có giá trị nhỏ nhất có thể.</description>
        </item>
        <item>
        <title>MobileNet (2017)</title>
        <link>https://htrvu.github.io/post/mobilenet/</link>
        <pubDate>Wed, 08 Feb 2023 18:13:00 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/mobilenet/</guid>
        <description>Giới thiệu Qua các mô hình đã được giới thiệu như VGG, GoogLeNet hay ResNet thì ta thấy rằng chúng đều được phát triển theo hướng tăng dần độ sâu và độ phức tạp tính toán của mô hình để đạt được độ chính xác cao hơn, kể từ khi AlexNet được công bố. Số lượng tham số của chúng là rất lớn.
Tuy nhiên, các ứng dụng AI trong thực tế như robotics, xe tự hành thì các phép tính toán của mô hình cần được thực hiện trong một khoảng thời gian giới hạn, cùng với tài nguyên phần cứng hạn chế.</description>
        </item>
        <item>
        <title>Inception-Reset (2016)</title>
        <link>https://htrvu.github.io/post/inception-resnet/</link>
        <pubDate>Wed, 08 Feb 2023 18:03:46 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/inception-resnet/</guid>
        <description>Giới thiệu Các mô hình thuộc họ Inception-ResNet được phát triển dựa trên ý tưởng là kết hợp skip connection vào các Inception block (các ý tưởng từ ResNet và GoogLeNet). Vì paper này chỉ mang tính thực nghiệm là chính nên mình sẽ không trình bày chi tiết 👀.
Kiến trúc mô hình Inception-ResNet V1 Về mặt tổng quan, Inception-ResNet V1 có kiến trúc như sau:
Kiến trúc Inception-ResNet V1</description>
        </item>
        <item>
        <title>Resnet (2015)</title>
        <link>https://htrvu.github.io/post/resnet/</link>
        <pubDate>Wed, 08 Feb 2023 17:41:09 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/resnet/</guid>
        <description>Khó khăn trong huấn luyện mô hình lớn Ta biết rằng, việc tạo ra các mô hình có độ sâu lớn (nhiều layer) chưa chắc đã mang lại hiệu quả tốt hơn những mô hình “cạn” hơn. Ví dụ, với tập CIFAR10 thì ta có một kết quả thử nghiệm cho thấy rằng mô hình sâu hơn lại có độ hiệu quả kém hơn:
Đối với việc huấn luyện các mô hình có độ sâu lớn thì ta có thể sẽ bị gặp phải các vấn đề sau:</description>
        </item>
        <item>
        <title>GoogLeNet - Inception V1 (2014)</title>
        <link>https://htrvu.github.io/post/googlenet/</link>
        <pubDate>Wed, 08 Feb 2023 01:59:53 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/googlenet/</guid>
        <description>Cá nhân mình thấy GoogLeNet là một paper khó đọc. Khi viết ra bài này thì mình vẫn đang cảm thấy hơi lú về nội dung của nó 😀
Giới thiệu Từ khi AlexNet được công bố vào năm 2012 và đặt nền tảng cho các mạng Deep CNN, GoogLeNet, hay Inception V1 (2014), là một trong những kiến trúc có cách thiết kế rất thú vị khi nó tận dụng hiệu quả các conv layer, đặt nền móng cho nhiều mô hình sau này.</description>
        </item>
        <item>
        <title>VGG (2014)</title>
        <link>https://htrvu.github.io/post/vgg/</link>
        <pubDate>Wed, 08 Feb 2023 01:52:02 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/vgg/</guid>
        <description>Giới thiệu Dựa trên sự thành công của AlexNet vào năm 2012, nhiều nghiên cứu đã được tiến hành nhằm tìm ra các phương pháp hay kiến trúc mới để đạt được kết quả tốt hơn, ví dụ như:
Thay đổi (tăng, giảm) kích thước của conv filter Thay đổi stride, padding của conv layer Train và test trên các input với nhiều độ phân giải (resolution) ảnh khác nhau Trong năm 2014, VGG là một trong những kết quả nghiên cứu nổi bật nhất, và nó tập trung vào một vấn đề khác với các hướng trên là độ sâu (depth, hay là số layer) của mô hình.</description>
        </item>
        <item>
        <title>About</title>
        <link>https://htrvu.github.io/page/about/</link>
        <pubDate>Tue, 07 Feb 2023 17:10:36 +0700</pubDate>
        
        <guid>https://htrvu.github.io/page/about/</guid>
        <description>Hi there 👋 Welcome to my blog!
My name is Vu and I&amp;rsquo;m currently a third-year student at VNUHCM - University of Science. My major is Computer Science and I&amp;rsquo;m interested in Mathematics, Machine Learning and Deep Learning 🤖.
This blog is created with my willing to share the knowledge to others about many wonderful things in Artificial Intelligence, specially in Deep Learning. Also, writing the posts is an great opportunity for me to review what I have learned.</description>
        </item>
        <item>
        <title>Archives</title>
        <link>https://htrvu.github.io/page/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://htrvu.github.io/page/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>Links</title>
        <link>https://htrvu.github.io/page/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://htrvu.github.io/page/links/</guid>
        <description>To use this feature, add links section to frontmatter.
This page&amp;rsquo;s frontmatter:
1 2 3 4 5 6 7 8 9 links: - title: GitHub description: GitHub is the world&amp;#39;s largest software development platform. website: https://github.com image: https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png - title: TypeScript description: TypeScript is a typed superset of JavaScript that compiles to plain JavaScript. website: https://www.typescriptlang.org image: ts-logo-128.jpg image field accepts both local and external images.</description>
        </item>
        <item>
        <title>Search</title>
        <link>https://htrvu.github.io/page/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://htrvu.github.io/page/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>

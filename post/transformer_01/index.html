<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='Giá»›i thiá»‡u cÆ¡ cháº¿ Attention vÃ  cÃ¡c biáº¿n thá»ƒ, mÃ´ hÃ¬nh Transformer trong bÃ i toÃ¡n Machine Translation'>
<title>CÆ¡ cháº¿ Attention vÃ  mÃ´ hÃ¬nh Transformer</title>

<link rel='canonical' href='https://htrvu.github.io/post/transformer_01/'>

<link rel="stylesheet" href="/scss/style.min.5eda91a11f055cbfcec5ec0fd36a1f7cba21a6a47d4778547d7d8f3099d2ebe2.css"><meta property='og:title' content='CÆ¡ cháº¿ Attention vÃ  mÃ´ hÃ¬nh Transformer'>
<meta property='og:description' content='Giá»›i thiá»‡u cÆ¡ cháº¿ Attention vÃ  cÃ¡c biáº¿n thá»ƒ, mÃ´ hÃ¬nh Transformer trong bÃ i toÃ¡n Machine Translation'>
<meta property='og:url' content='https://htrvu.github.io/post/transformer_01/'>
<meta property='og:site_name' content='Trong-Vu Hoang'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='nlp' /><meta property='article:tag' content='attention' /><meta property='article:tag' content='transformer' /><meta property='article:tag' content='self-attention' /><meta property='article:tag' content='cross-attention' /><meta property='article:published_time' content='2023-07-17T21:58:29&#43;07:00'/><meta property='article:modified_time' content='2023-07-17T21:58:29&#43;07:00'/>
<meta name="twitter:title" content="CÆ¡ cháº¿ Attention vÃ  mÃ´ hÃ¬nh Transformer">
<meta name="twitter:description" content="Giá»›i thiá»‡u cÆ¡ cháº¿ Attention vÃ  cÃ¡c biáº¿n thá»ƒ, mÃ´ hÃ¬nh Transformer trong bÃ i toÃ¡n Machine Translation">
    <link rel="shortcut icon" href="/neural.png" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu4ef57fa6d8ae56d86d9663ef18f7ace5_124758_300x0_resize_q75_box.jpg" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Trong-Vu Hoang</a></h1>
            <h2 class="site-description">On my way!</h2>
        </div>
    </header><ol class="social-menu">
            
                <li>
                    <a 
                        href='https://github.com/htrvu'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
  <path d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5" />
</svg>



                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://www.linkedin.com/in/htrvu/'
                        target="_blank"
                        title="Linkedin"
                        rel="me"
                    >
                        
                        
                            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"> <path stroke="none" d="M0 0h24v24H0z" fill="none"/> <rect x="4" y="4" width="16" height="16" rx="2" /> <line x1="8" y1="11" x2="8" y2="16" /> <line x1="8" y1="8" x2="8" y2="8.01" /> <line x1="12" y1="16" x2="12" y2="11" /> <path d="M16 16v-3a2 2 0 0 0 -4 0" /> </svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        

        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>Home</span>
            </a>
        </li>
        
        

        <li >
            <a href='/page/about/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About</span>
            </a>
        </li>
        
        

        <li >
            <a href='/page/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        

        <li >
            <a href='/page/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        

        <div class="menu-bottom-section">
            
            
                <li id="dark-mode-toggle">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                    <span>Dark Mode</span>
                </li>
            
        </div>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        
        <h2 class="widget-title section-title">Table of contents</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#tá»•ng-quan-vá»-kiáº¿n-trÃºc-transformer">Tá»•ng quan vá» kiáº¿n trÃºc Transformer</a></li>
    <li><a href="#word-embedding-vÃ -positional-encoding">Word Embedding vÃ  Positional Encoding</a></li>
    <li><a href="#cÆ¡-cháº¿-attention-vÃ -sá»±-truy-xuáº¥t-thÃ´ng-tin">CÆ¡ cháº¿ Attention vÃ  sá»± truy xuáº¥t thÃ´ng tin</a></li>
    <li><a href="#scale-dot-product-attention">Scale Dot-Product Attention</a></li>
    <li><a href="#self-attention-vÃ -cross-attention">Self-Attention vÃ  Cross-Attention</a>
      <ol>
        <li><a href="#self-attention">Self-Attention</a></li>
        <li><a href="#cross-attention">Cross-Attention</a></li>
      </ol>
    </li>
    <li><a href="#multi-head-attention-vÃ -masked-multi-head-attention">Multi-Head Attention vÃ  Masked Multi-Head Attention</a>
      <ol>
        <li><a href="#mutli-head-attention">Mutli-Head Attention</a></li>
        <li><a href="#masked-multi-head-attention">Masked-Multi Head Attention</a></li>
      </ol>
    </li>
    <li><a href="#layer-normalization">Layer Normalization</a></li>
    <li><a href="#feed-forward-network-vÃ -skip-connection">Feed Forward Network vÃ  skip connection</a></li>
    <li><a href="#encoder">Encoder</a></li>
    <li><a href="#decoder">Decoder</a>
      <ol>
        <li><a href="#tá»•ng-quan-vá»-kiáº¿n-trÃºc">Tá»•ng quan vá» kiáº¿n trÃºc</a></li>
        <li><a href="#decoder-trong-quÃ¡-trÃ¬nh-huáº¥n-luyá»‡n">Decoder trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n</a></li>
        <li><a href="#decoder-trong-quÃ¡-trÃ¬nh-dá»±-Ä‘oÃ¡n">Decoder trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n</a></li>
      </ol>
    </li>
    <li><a href="#káº¿t-luáº­n">Káº¿t luáº­n</a></li>
    <li><a href="#tÃ i-liá»‡u-tham-kháº£o">TÃ i liá»‡u tham kháº£o</a></li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/nlp/" style="background-color: #2a9d8f; color: #fff;">
                Natual Language Processing
            </a>
        
            <a href="/categories/dl/" style="background-color: #2a9d8f; color: #fff;">
                Deep Learning
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/post/transformer_01/">CÆ¡ cháº¿ Attention vÃ  mÃ´ hÃ¬nh Transformer</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            Giá»›i thiá»‡u cÆ¡ cháº¿ Attention vÃ  cÃ¡c biáº¿n thá»ƒ, mÃ´ hÃ¬nh Transformer trong bÃ i toÃ¡n Machine Translation
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Jul 17, 2023</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    31 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <p><strong>Note.</strong> Ná»™i dung bÃ i viáº¿t nÃ y pháº§n lá»›n Ä‘Æ°á»£c trÃ­ch tá»« bÃ¡o cÃ¡o Ä‘á»“ Ã¡n mÃ´n há»c Há»c thá»‘ng kÃª cá»§a nhÃ³m mÃ¬nh vÃ  báº¡n <a class="link" href="https://github.com/huuthientran"  target="_blank" rel="noopener"
    >Tráº§n Há»¯u ThiÃªn</a> táº¡i HCMUS, vá»›i chá»§ Ä‘á» lÃ  English-Vietnamese Machine Translation. CÃ¡c báº¡n cÃ³ thá»ƒ xem mÃ£ nguá»“n cá»§a Ä‘á»“ Ã¡n nÃ y táº¡i <a class="link" href="https://github.com/htrvu/Transformer-MT"  target="_blank" rel="noopener"
    >Ä‘Ã¢y</a> ğŸš€</p>
<p>Trong bÃ i viáº¿t <a class="link" href="https://htrvu.github.io/post/mt_seq2seq/" >trÆ°á»›c</a>, mÃ¬nh Ä‘Ã£ giá»›i thiá»‡u vá» bÃ i toÃ¡n Machine Translation cÃ¹ng má»™t mÃ´ hÃ¬nh khÃ¡ quen thuá»™c trong bÃ i toÃ¡n Ä‘Ã³ lÃ  Seq2seq. Trong giai Ä‘oáº¡n trÆ°á»›c nÄƒm 2017, cÃ¡c mÃ´ hÃ¬nh Deep Learning Ä‘Æ°á»£c xÃ¢y dá»±ng cho bÃ i toÃ¡n Machine Translation thÆ°á»ng Ä‘Æ°á»£c tá»• chá»©c theo kiáº¿n trÃºc <strong>Encoder-Decoder</strong> nhÆ° Seq2seq vá»›i pháº§n lÃµi lÃ  cÃ¡c biáº¿n thá»ƒ cáº£i tiáº¿n hÆ¡n cá»§a Recurrent Neural Network (RNN). Tuy nhiÃªn, nhá»¯ng mÃ´ hÃ¬nh NTM Ä‘Ã³ Ä‘á»u cÃ³ má»™t sá»‘ háº¡n cháº¿ nháº¥t Ä‘á»‹nh khi thá»±c hiá»‡n dá»‹ch nhá»¯ng cÃ¢u cÃ³ cÃ¢u trÃºc phá»©c táº¡p hoáº·c cÃ³ Ä‘á»™ dÃ i lá»›n, mÃ  thÆ°á»ng tháº¥y nháº¥t lÃ  háº¡n cháº¿ vá» Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n vÃ  viá»‡c ghi nhá»› quan há»‡ phá»¥ thuá»™c giá»¯a cÃ¡c tá»« trong cÃ¢u vá» máº·t ngá»¯ nghÄ©a. LÃ½ do cá»§a nhá»¯ng háº¡n cháº¿ nÃ y pháº§n lá»›n Ä‘áº¿n tá»« thao tÃ¡c tÃ­nh toÃ¡n, xá»­ lÃ½ tuáº§n tá»± cÃ¡c tá»« (hay lÃ  token) cá»§a RNN.</p>
<p><strong>Transformer</strong>, má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o nÄƒm 2017 Ä‘Ã£ táº¡o ra sá»± Ä‘á»™t phÃ¡ lá»›n trong lÄ©nh vá»±c NLP vá»›i Ä‘á»™ chÃ­nh xÃ¡c cÅ©ng nhÆ° tá»‘c Ä‘á»™ xá»­ lÃ½ Ä‘á»u vÆ°á»£t xa nhá»¯ng mÃ´ hÃ¬nh Deep Learning trÆ°á»›c Ä‘Ã³. DÃ¹ váº«n tá»• chá»©c mÃ´ hÃ¬nh theo kiáº¿n trÃºc Encoder-Decoder thÃ´ng dá»¥ng nhÆ°ng Transformer Ä‘Ã£ kháº¯c phá»¥c Ä‘Æ°á»£c nhá»¯ng nhÆ°á»£c Ä‘iá»ƒm cá»§a cÃ¡c mÃ´ hÃ¬nh trÆ°á»›c nhá» vÃ o viá»‡c táº­n dá»¥ng tá»‘i Ä‘a sá»©c máº¡nh cá»§a <strong>cÆ¡ cháº¿ Attention</strong> vá»›i hai phÃ©p toÃ¡n <strong>Self-Attention</strong> vÃ  <strong>Cross-Attention</strong>. Ká»ƒ tá»« Ä‘Ã³, mÃ´ hÃ¬nh Transformer Ä‘Ã£ trá»Ÿ thÃ nh má»™t tiÃªu chuáº©n má»›i trong bÃ i toÃ¡n Machine Translation nÃ³i riÃªng vÃ  trong nhiá»u bÃ i toÃ¡n thuá»™c lÄ©nh vá»±c NLP nÃ³i chung. KhÃ´ng dá»«ng láº¡i á»Ÿ Ä‘Ã³, Ã½ tÆ°á»Ÿng tá»« kiáº¿n trÃºc cá»§a Transformer cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng má»™t cÃ¡ch ráº¥t thÃ nh cÃ´ng á»Ÿ cÃ¡c bÃ i toÃ¡n thuá»™c lÄ©nh vá»±c khÃ¡c nhÆ° Computer Vision.</p>
<p>Trong bÃ i viáº¿t nÃ y, mÃ¬nh sáº½ trÃ¬nh bÃ y vá» <strong>Transformer</strong> cÃ¹ng <strong>cÆ¡ cháº¿ Attention</strong> (tá»•ng quan vÃ  sau Ä‘Ã³ Ä‘i vÃ o cá»¥ thá»ƒ vá»›i Self-Attention vÃ  Cross-Attention). Ná»™i dung cá»§a bÃ i nÃ y sáº½ lÃ  <strong>ráº¥t dÃ i</strong>.</p>
<h2 id="tá»•ng-quan-vá»-kiáº¿n-trÃºc-transformer">Tá»•ng quan vá» kiáº¿n trÃºc Transformer</h2>
<p><span id="overall"></span></p>
<p>MÃ´ hÃ¬nh Transformer cÃ³ kiáº¿n trÃºc Ä‘Æ°á»£c tá»• chá»©c dáº¡ng Encoder-Decoder vá»›i pháº§n lÃµi lÃ  cÆ¡ cháº¿ Attention. Tá»•ng quan kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i, trong Ä‘Ã³:</p>
<ul>
<li><strong>Encoder</strong> cÃ³ vai trÃ² chÃ­nh lÃ  há»c má»‘i tÆ°Æ¡ng quan giá»¯a cÃ¡c tá»« vá»›i nhau trong cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n (cá»¥ thá»ƒ hÆ¡n lÃ  giá»¯a tá»«ng tá»« vá»›i cÃ¡c tá»« cÃ²n láº¡i trong cÃ¢u). Ta cÃ³ thá»ƒ hiá»ƒu Ä‘Ã¢y nhÆ° lÃ  viá»‡c ta Ä‘á»c má»™t cÃ¢u Tiáº¿ng Anh vÃ  cá»‘ gáº¯ng hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¢u Ä‘Ã³.</li>
<li><strong>Decoder</strong> sáº½ láº§n lÆ°á»£t sinh ra cÃ¡c tá»« cho cÃ¢u thuá»™c ngÃ´n ngá»¯ Ä‘Ã­ch. Trong quÃ¡ trÃ¬nh sinh tá»« nÃ y thÃ¬ Decoder sáº½ vá»«a Ä‘á»ƒ Ã½ Ä‘áº¿n cÃ¡c tá»« nÃ³ Ä‘Ã£ sinh ra trÆ°á»›c Ä‘Ã³ vÃ  vá»«a Ä‘á»ƒ Ã½ Ä‘áº¿n má»™t sá»‘ tá»« liÃªn quan trong cÃ¢u ngÃ´n ngá»¯ nguá»“n Ä‘á»ƒ dá»‹ch cho Ä‘Ãºng. ÄÃ¢y cÅ©ng cÃ³ thá»ƒ xem lÃ  sau khi Ä‘Ã£ hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¢u Tiáº¿ng Anh rá»“i thÃ¬ ta tá»«ng bÆ°á»›c dá»‹ch cÃ¢u Ä‘Ã³ ra Tiáº¿ng Viá»‡t sao cho trÃ´i cháº£y, máº¡ch láº¡c.</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 450px;" src='./images/transformer.png'>
<p align="center" style="margin: 0; color: #888;">Tá»•ng quan kiáº¿n trÃºc cá»§a mÃ´ hÃ¬nh Transformer</p>
</div>
<p>Ta sáº½ láº§n lÆ°á»£t Ä‘á» cáº­p Ä‘áº¿n nhá»¯ng thÃ nh pháº§n quan trá»ng trong kiáº¿n trÃºc cá»§a Transformer, mÃ  Ä‘áº·c biá»‡t lÃ  <strong>Scale Dot-Product Attention</strong> vÃ  <strong>Multi-Head Attention</strong>, thÃ nh pháº§n Ä‘Ã³ng vai trÃ² ráº¥t lá»›n trong viá»‡c táº¡o nÃªn sá»©c máº¡nh cá»§a mÃ´ hÃ¬nh nÃ y.</p>
<h2 id="word-embedding-vÃ -positional-encoding">Word Embedding vÃ  Positional Encoding</h2>
<p>Pháº§n Ä‘áº§u tiÃªn trong kiáº¿n trÃºc cá»§a Transformer lÃ  <strong>embedding</strong>, vá»›i hai thÃ nh pháº§n lÃ  Word Embedding vÃ  Positional Encoding.</p>
<p><strong>Word Embedding</strong> lÃ  má»™t phÆ°Æ¡ng phÃ¡p biá»ƒu diá»…n cÃ¡c tá»« trong cÃ¢u thÃ nh cÃ¡c vector Ä‘áº·c trÆ°ng má»™t cÃ¡ch há»£p lÃ½, sao cho cÃ¡c vector nÃ y thá»ƒ hiá»‡n Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau. MÃ¬nh Ä‘Ã£ cÃ³ Ä‘á» cáº­p Ä‘áº¿n nÃ³ á»Ÿ trong bÃ i viáº¿t <a class="link" href="https://htrvu.github.io/post/word-embedding/" >nÃ y</a>.</p>
<ul>
<li>Äá»‘i vá»›i bÃ i toÃ¡n Machine Translation, ta cáº§n lÆ°u Ã½ ráº±ng táº­p tá»« Ä‘iá»ƒn cá»§a ngÃ´n ngá»¯ nguá»“n vÃ  Ä‘Ã­ch cÃ³ thá»ƒ cÃ³ <strong>sá»‘ tá»« khÃ¡c nhau</strong> nÃªn vector one-hot biá»ƒu diá»…n cÃ¡c tá»« trong cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n vÃ  Ä‘Ã­ch cÅ©ng cÃ³ thá»ƒ cÃ³ sá»‘ chiá»u khÃ¡c nhau. Tuy nhiÃªn, ta sáº½ Ä‘á»u Ä‘Æ°a chÃºng vá»  cÃ¡c word embedding vectors vá»›i cÃ¹ng sá»‘ chiá»u vÃ  giÃ¡ trá»‹ nÃ y Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  lÃ  $d_{model}$.</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 400px;" src='./images/word_embed.png'>
<p align="center" style="margin: 0; color: #888;">Káº¿t quáº£ khi sá»­ dá»¥ng thuáº­t toÃ¡n t-SNE Ä‘á»ƒ trá»±c quan hÃ³a cÃ¡c word embedding vector trÃªn khÃ´ng gian 2 chiá»u cá»§a táº­p tá»« vá»±ng Tiáº¿ng Anh</p>
</div>
<p>VÃ¬ mÃ´ hÃ¬nh Transformer khÃ´ng tÃ­nh toÃ¡n tuáº§n tá»± theo thá»© tá»± cá»§a cÃ¡c tá»« trong cÃ¢u nhÆ° nhá»¯ng mÃ´ hÃ¬nh dá»±a trÃªn pháº§n lÃµi lÃ  RNN (RNN-based) nÃªn Ä‘á»ƒ cÃ³ thá»ƒ cung cáº¥p <strong>thÃ´ng tin vá» vá»‹ trÃ­ cá»§a cÃ¡c tá»« trong cÃ¢u</strong> cho mÃ´ hÃ¬nh, cÃ¡c tÃ¡c giáº£ cá»§a Transformer Ä‘Ã£ Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  <strong>Positional Encoding</strong>. Pháº§n thÃ´ng tin cÃ³ Ä‘Æ°á»£c tá»« Positional Encoding sáº½ Ä‘Æ°á»£c <strong>cá»™ng</strong> vÃ o word embedding vectors cá»§a cÃ¡c tá»« trong cÃ¢u ngÃ´n ngá»¯ nguá»“n vÃ  ngÃ´n ngá»¯ Ä‘Ã­ch. Thao tÃ¡c cá»™ng nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n á»Ÿ pháº§n ngay sau &ldquo;Input Embedding&rdquo; vÃ  &ldquo;Output Embedding&rdquo; cá»§a hÃ¬nh mÃ´ táº£ kiáº¿n trÃºc á»Ÿ <a class="link" href="#overall" >pháº§n 1</a>.</p>
<p>CÃ´ng thá»©c cá»§a Positional Encoding Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh thÃ´ng qua <strong>hÃ m Sinusoid</strong>. Táº¥t nhiÃªn lÃ  Ä‘á»ƒ cá»™ng Ä‘Æ°á»£c positional encoding vector (PE) vá»›i word embedding vector (WE) thÃ¬ hai vector nÃ y pháº£i cÃ³ cÃ¹ng sá»‘ chiá»u lÃ  $d_{model}$. Vá»›i tá»« á»Ÿ vá»‹ trÃ­ thá»© $pos$ trong cÃ¢u, giÃ¡ trá»‹ cá»§a pháº§n tá»­ táº¡i vÃ­ trÃ­ $2i$ vÃ  $(2i+1)$ ($0 \leq 2i, 2i+1 \leq d_{model} - 1)$ trong PE Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi:
$$
\begin{align*}
PE(pos, 2i) &amp;= \sin \left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right) \\
PE(pos, 2i+1) &amp;= \cos \left(  \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right)
\end{align*}
$$</p>
<p>Minh há»a cho káº¿t quáº£ cá»§a PE vá»›i cÃ¡c tá»« trong cÃ¢u Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i vá»›i má»™t cÃ¢u gá»“m 20 tá»«, má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t WE 128 chiá»u. Ta tháº¥y ráº±ng PE cá»§a cÃ¡c tá»« trong cÃ¢u Ä‘á»u Ä‘Ã´i má»™t khÃ¡c nhau, tá»©c lÃ  nÃ³ Ä‘Ã£ giÃºp ta mÃ£ hÃ³a Ä‘Æ°á»£c thÃ´ng tin vá»‹ trÃ­ cá»§a cÃ¡c tá»«.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 300px;" src='./images/pe_heatmap.png'>
<p align="center" style="margin: 0; color: #888;">Minh há»a káº¿t quáº£ cá»§a cÃ¡c positional encoding vectors</p>
</div>
<p>NgoÃ i ra, cÃ¡c tÃ¡c giáº£ cá»§a Transformer cÃ²n cho biáº¿t ráº±ng viá»‡c sá»­ dá»¥ng hÃ m Sinusoid Ä‘á»ƒ tÃ­nh PE cÃ³ thá»ƒ giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c <strong>vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i</strong> cá»§a cÃ¡c tá»«, khi mÃ  $PE(pos)$ vÃ  $PE(pos + k)$ cÃ³ thá»ƒ Ä‘Æ°á»£c biáº¿n Ä‘á»•i qua láº¡i thÃ´ng qua má»™t phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh, tá»©c lÃ  tá»“n táº¡i ma tráº­n $M$ sao cho
$$
\begin{equation*}
M \cdot \begin{bmatrix}
\sin(pos \cdot \omega_i) \\
\cos(pos \cdot \omega_i)
\end{bmatrix} = \begin{bmatrix}
\sin((pos + k) \cdot \omega_i) \\
\cos((pos + k) \cdot \omega_i)
\end{bmatrix}
\end{equation*}
$$
, vá»›i $\omega_i = \left (10000^{\frac{2i}{d_{model}}} \right)^{-1}$.</p>
<p>NhÆ° váº­y, vá»›i $x_{pos}$ lÃ  vector one-hot cá»§a tá»« thá»© $pos$ trong cÃ¢u, ta cÃ³ thá»ƒ biá»ƒu diá»…n káº¿t quáº£ sau viá»‡c káº¿t há»£p word embedding vector vÃ  positional encoding vector nhÆ° sau:
$$
\begin{equation*}
z_{pos} = WE(x_{pos}) * \sqrt{d_{model}} + PE(pos)
\end{equation*}
$$
, trong Ä‘Ã³ $*$ lÃ  phÃ©p nhÃ¢n element-wise.</p>
<p><strong>LÆ°u Ã½ 1.</strong> Má»™t cÃ¡ch giáº£i thÃ­ch cho viá»‡c nhÃ¢n WE vá»›i $\sqrt{d_{model}}$ trÆ°á»›c khi cá»™ng vá»›i PE lÃ  ta muá»‘n lÆ°á»£ng thÃ´ng tin nháº­n Ä‘Æ°á»£c tá»« PE sáº½ khÃ´ng tÃ¡c Ä‘á»™ng quÃ¡ nhiá»u Ä‘áº¿n nhá»¯ng thÃ´ng tin mÃ  WE Ä‘Ã£ cung cáº¥p vá» cÃ¡c tá»«.</p>
<p><strong>LÆ°u Ã½ 2.</strong> Ta cÃ²n cÃ³ thá»ƒ hÃ¬nh dung Ã½ nghÄ©a positional encoding vector thÃ´ng qua vÃ­ dá»¥ vá» <strong>cÃ¡ch biá»ƒu diá»…n nhá»‹ phÃ¢n</strong> cá»§a cÃ¡c sá»‘ nguyÃªn. XÃ©t hÃ¬nh minh há»a cho 16 sá»‘ nguyÃªn Ä‘áº§u tiÃªn biÃªn dÆ°á»›i thÃ¬:</p>
<ul>
<li>Má»—i sá»‘ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng duy nháº¥t má»™t chuá»—i nhá»‹ phÃ¢n vÃ  cÃ¡c chuá»—i nÃ y Ä‘Ã´i má»™t phÃ¢n biá»‡t.</li>
<li>Äi tá»« bit tháº¥p Ä‘áº¿n bit cap thÃ¬ &ldquo;táº§n suáº¥t thay Ä‘á»•i&rdquo; cá»§a cÃ¡c bit tÄƒng dáº§n. VÃ­ dá»¥, tá»« 0 tá»›i 7 thÃ¬ bit 0 giá»¯ nguyÃªn, bit 1 thay Ä‘á»•i 1 láº§n, bit 2 thay Ä‘á»•i 3 láº§n, v.v. Náº¿u ta quan sÃ¡t láº¡i hÃ¬nh minh há»a káº¿t quáº£ cÃ¡c positional encoding vectors á»Ÿ phÃ­a trÃªn thÃ¬ cÅ©ng tháº¥y tÃ­nh cháº¥t tÆ°Æ¡ng tá»±.</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 300px;" src='./images/pe_intuition.jpg'>
<p align="center" style="margin: 0; color: #888;">LiÃªn há»‡ giá»¯a Positional Encoding vÃ  cÃ¡ch biá»ƒu diá»…n nhá»‹ phÃ¢n cá»§a cÃ¡c sá»‘ nguyÃªn</p>
</div>
<h2 id="cÆ¡-cháº¿-attention-vÃ -sá»±-truy-xuáº¥t-thÃ´ng-tin">CÆ¡ cháº¿ Attention vÃ  sá»± truy xuáº¥t thÃ´ng tin</h2>
<span id='sec:attention'>
<p><strong>Attention</strong> lÃ  má»™t Ã½ tÆ°á»Ÿng ráº¥t thÃº vá»‹ trong Deep Learning khi mÃ  nÃ³ Ä‘Ã£ mÃ´ phá»ng láº¡i cÃ¡ch bá»™ nÃ£o cá»§a con ngÆ°á»i hoáº¡t Ä‘á»™ng khi chÃºng ta phÃ¢n tÃ­ch, nhÃ¬n nháº­n má»™t Ä‘á»‘i tÆ°á»£ng. VÃ­ dá»¥, máº¯t chÃºng ta cÃ³ táº§m nhÃ¬n ráº¥t rá»™ng nhÆ°ng táº¡i má»—i thá»i Ä‘iá»ƒm thÃ¬ ta chá»‰ táº­p trung vÃ o má»™t vÃ¹ng nháº¥t Ä‘á»‹nh trong táº§m nhÃ¬n Ä‘á»ƒ láº¥y thÃ´ng tin. Attention Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ nh cÃ´ng vÃ o nhiá»u lÄ©nh vá»±c khÃ¡c nhau, nhiá»u bÃ i toÃ¡n khÃ¡c nhau trong Deep Learning.</p>
<p>Äá»ƒ mÃ´ táº£ sÆ¡ lÆ°á»£c vá» quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a cÆ¡ cháº¿ Attention, ta xÃ©t cÃ¡ch Attention Ä‘Æ°á»£c Ã¡p dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh trong bÃ i toÃ¡n Machine Translation, á»Ÿ giai Ä‘oáº¡n trÆ°á»›c khi Transformer Ä‘Æ°á»£c cÃ´ng bá»‘ (hÃ¬nh bÃªn dÆ°á»›i). Giáº£ sá»­ input cá»§a Encoder lÃ  cÃ¡c vector $x_i$ vÃ  output vector tÆ°Æ¡ng á»©ng lÃ  $h_i$, ta Ä‘ang tÃ­nh toÃ¡n cho tá»« Ä‘áº§u tiÃªn trong output cá»§a Decoder vá»›i input cá»§a Decoder lÃ  vector $y_1$. LÃºc nÃ y, ta thá»±c hiá»‡n Attention tá»« $y_1$ Ä‘áº¿n cÃ¡c vector $x_i$, vá»›i Ã½ nghÄ©a lÃ  khi ta dá»‹ch tá»« Ä‘áº§u tiÃªn nÃ y thÃ¬ ta nÃªn chÃº Ã½ vÃ o cÃ¡c tá»« nÃ o á»Ÿ trong cÃ¢u nguá»“n.</p>
<div style="display: flex; flex-direction: column; align-items: center;" id="attention_ex">
<img style="max-height: 400px;" src='./images/attention_ex.png'>
<p align="center" style="margin: 0; color: #888;">MÃ´ táº£ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a cÆ¡ cháº¿ Attention</p>
</div>
<p>Trong Attention, ta cÃ³ cÃ¡c khÃ¡i niá»‡m vá» <strong>context vector</strong> $c_i$ vÃ  <strong>attention weight</strong> $\alpha_{ij}$. CÃ¡c giÃ¡ trá»‹ attention weight $\alpha_{ij}$ sáº½ náº±m trong Ä‘oáº¡n [0, 1] vÃ  cho biáº¿t má»©c Ä‘á»™ chÃº Ã½ cá»§a vector $y_i$ vÃ o vector $x_j$ vÃ  context vector $c_i$ lÃ  káº¿t quáº£ thu Ä‘Æ°á»£c khi thá»±c hiá»‡n Attention tá»« vector $y_i$.</p>
<p>Ta tÃ­nh context vector $c_i$ theo cÃ´ng thá»©c
$$
\begin{equation*}
c_i = \sum_{j=1}^N \left( \alpha_{ij} \cdot h_j \right )
\end{equation*}
$$
, trong Ä‘Ã³ $N$ lÃ  Ä‘á»™ dÃ i cá»§a cÃ¢u nguá»“n vÃ  attention weight $\alpha_{ij}$ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh nhÆ° sau:
$$
\begin{equation}
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{t=1}^N \exp(e_{it})}
% \label{eq:attn_weights_1}
\end{equation}
$$
, vá»›i $e_{ij} = f(y_i, h_j)$ vÃ  $f$ lÃ  má»™t <strong>mÃ´ hÃ¬nh há»c</strong>, nÃ³ cÃ³ thá»ƒ Ä‘Æ¡n giáº£n chá»‰ lÃ  Neural Network vá»›i má»™t layer, input vector sáº½ lÃ  vector Ä‘Æ°á»£c ná»‘i giá»¯a $\alpha_{ij}$ vÃ  $h_j$, output lÃ  vector 1 chiá»u.</p>
<p>Tá»« cÃ´ng thá»©c $(1)$, ta cÃ³ thá»ƒ viáº¿t gá»n báº±ng cÃ¡ch biá»ƒu diá»…n báº±ng cÃ¡c vector $N$ chiá»u $\alpha_i$ vÃ  $e_i$ nhÆ° sau:
$$
\begin{equation*}
\alpha_i = \text{softmax}(e_i)
\end{equation*}
$$</p>
<p>NhÆ° váº­y, $\alpha_{ij}$ cÃ ng lá»›n thÃ¬ $y_i$ cÃ ng chÃº Ã½ vÃ o $x_j$ vÃ  rÃµ rÃ ng ta cÃ³ $\sum_{j=1}^N \alpha_{ij} = 1$. Vá» máº·t trá»±c quan, ngÆ°á»i ta thÆ°á»ng biá»ƒu diá»…n attention weights báº±ng má»™t ma tráº­n nhÆ° hÃ¬nh bÃªn dÆ°á»›i, trong Ä‘Ã³ Ã´ cÃ³ mÃ u cÃ ng sÃ¡ng thÃ¬ sáº½ á»©ng vá»›i attention weights cÃ ng lá»›n.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 250px;" src='./images/attention_ex2.png'>
<p align="center" style="margin: 0; color: #888;">Minh há»a trá»±c quan vá» attention weights</p>
</div>
<p>Trong Transformer, Attention cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° lÃ  má»™t cÆ¡ cháº¿ <strong>truy xuáº¥t thÃ´ng tin</strong>, khi mÃ  attention weights $\alpha_{ij}$ cÃ ng lá»›n thÃ¬ cÃ ng thá»ƒ hiá»‡n sá»± &ldquo;liÃªn quan Ä‘áº¿n nhau&rdquo; giá»¯a cá»§a $y_i$ vÃ  $x_i$. Khi nÃ³i Ä‘áº¿n sá»± truy xuáº¥t thÃ´ng tin, ta cÃ³ 3 khÃ¡i niá»‡m Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  <strong>query</strong>, <strong>key</strong> vÃ  <strong>value</strong>. Má»™t cÃ¡ch mÃ´ táº£ ráº¥t dá»… hiá»ƒu vá» 3 khÃ¡i niá»‡m nÃ y vá»›i vÃ­ dá»¥ Google Search trong <a class="link" href="https://pbcquoc.github.io/transformer/"  target="_blank" rel="noopener"
    >bÃ i viáº¿t cá»§a anh Quá»‘c</a> nhÆ° sau:</p>
<ul>
<li>Query: Vector dÃ¹ng Ä‘á»ƒ chá»©a thÃ´ng tin cá»§a tá»« Ä‘Æ°á»£c tÃ¬m kiáº¿m, so sÃ¡nh (hoáº·c lÃ  ma tráº­n vá»›i má»—i dÃ²ng lÃ  vector á»©ng vá»›i má»™t tá»«). VÃ­ dá»¥, tá»« khÃ³a mÃ  ta nháº­p vÃ o Ã´ tÃ¬m kiáº¿m cá»§a Google Search lÃ  query.</li>
<li>Key: Ma tráº­n dÃ¹ng Ä‘á»ƒ biá»ƒu diá»…n thÃ´ng tin chÃ­nh cá»§a cÃ¡c tá»« Ä‘Æ°á»£c so sÃ¡nh vá»›i tá»« cáº§n tÃ¬m kiáº¿m á»Ÿ trÃªn, má»—i dÃ²ng lÃ  vector á»©ng vá»›i má»™t tá»«. VÃ­ dá»¥, tiÃªu Ä‘á» cÃ¡c trang web mÃ  Google sáº½ so sÃ¡nh vá»›i tá»« khÃ³a ta Ä‘Ã£ nháº­p lÃ  cÃ¡c key.</li>
<li>Value: Ma tráº­n biá»ƒu diá»…n Ä‘áº§y Ä‘á»§ ná»™i dung, Ã½ nghÄ©a cá»§a cÃ¡c tá»« cÃ³ trong key, má»—i dÃ²ng lÃ  vector á»©ng vá»›i má»™t tá»«. NÃ³ nhÆ° lÃ  ná»™i dung cÃ¡c trang web Ä‘Æ°á»£c hiá»ƒn thá»‹ cho ngÆ°á»i dÃ¹ng sau khi tÃ¬m kiáº¿m.
\end{itemize}</li>
</ul>
<p>Trong vÃ­ dá»¥ vá» attention á»Ÿ hÃ¬nh <a class="link" href="#attention_ex" >trÃªn</a>, query sáº½ lÃ  vector $y_1$, keys vÃ  values sáº½ Ä‘á»u lÃ  ma tráº­n $H$ táº¡o bá»Ÿi cÃ¡c vector $h_i$. QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n giá»¯a query, keys vÃ  values trong vÃ­ dá»¥ Ä‘Ã³ Ä‘Æ°á»£c mÃ´ táº£ láº¡i trong hÃ¬nh bÃªn dÆ°á»›i, vá»›i:</p>
<ul>
<li>Attention weights Ä‘Æ°á»£c tÃ­nh tá»« query vÃ  keys.</li>
<li>Context vector Ä‘Æ°á»£c tÃ­nh tá»« attention weights vÃ  values.</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 180px;" src='./images/query_key_value.png'>
<p align="center" style="margin: 0; color: #888;">Attention vá»›i query, key vÃ  value. Trong Ä‘Ã³, vector key vÃ  value cá»§a cÃ¡c tá»« trong háº§u háº¿t cÃ¡c trÆ°á»ng há»£p lÃ  giá»‘ng nhau.</p>
</div>
<p><strong>LÆ°u Ã½</strong>. Vá»›i Ã½ nghÄ©a cá»§a tá»«ng ma tráº­n, trong má»™t sá»‘ tÃ¬nh huá»‘ng thÃ¬ keys vÃ  values cÃ³ thá»ƒ cÃ³ giÃ¡ trá»‹ khÃ¡c nhau chá»© khÃ´ng nháº¥t thiáº¿t lÃ  luÃ´n giá»‘ng nhau.</p>
<h2 id="scale-dot-product-attention">Scale Dot-Product Attention</h2>
<p>Trong mÃ´ hÃ¬nh Transformer, Attention Ä‘Æ°á»£c tÃ­nh toÃ¡n Ä‘Æ¡n giáº£n vÃ  nhanh hÆ¡n so vá»›i háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng Attention trong Machine Translation trÆ°á»›c Ä‘Ã³. LÃ½ do lÃ  vÃ¬ mÃ´ hÃ¬nh há»c $f$ dÃ¹ng Ä‘á»ƒ tÃ­nh attention weights $\alpha_{ij}$ sáº½ chá»‰ Ä‘Æ¡n giáº£n lÃ  phÃ©p toÃ¡n <strong>tÃ­ch vÃ´ hÆ°á»›ng</strong> (Dot-Product). NgoÃ i ra, ta cÃ²n cÃ³ thÃªm thao tÃ¡c scale cÃ¡c giÃ¡ trá»‹ trong ma tráº­n káº¿t quáº£ vá»›i má»™t giÃ¡ trá»‹ háº±ng sá»‘. Do Ä‘Ã³, quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Attention trong Transformer Ä‘Æ°á»£c gá»i lÃ  <strong>Scale Dot-Product Attention</strong>.</p>
<p><strong>Táº¡i sao láº¡i chá»‰ Ä‘Æ¡n giáº£n lÃ  dÃ¹ng phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng?</strong></p>
<ul>
<li>Äá»ƒ Ã½ ráº±ng, káº¿t quáº£ tÃ­ch vÃ´ hÆ°á»›ng cá»§a hai vector cÃ ng lá»›n thÃ¬ hai vector Ä‘Ã³ cÃ ng &ldquo;liÃªn quan Ä‘áº¿n nhau&rdquo;. Náº¿u xÃ©t hai vector cÃ³ chuáº©n báº±ng 1 thÃ¬ Ä‘iá»u nÃ y Ä‘ang cho biáº¿t ráº±ng gÃ³c giá»¯a hai vector Ä‘Ã³ Ä‘ang ráº¥t nhá». NhÆ° váº­y, phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng cÃ³ thá»ƒ lÃ m Ä‘Æ°á»£c nhiá»‡m vá»¥ cá»§a mÃ´ hÃ¬nh há»c $f$ má»™t cÃ¡ch ráº¥t nhanh gá»n.</li>
</ul>
<p>TrÆ°á»›c háº¿t, kÃ½ hiá»‡u cÃ¡c ma tráº­n queries, keys vÃ  values láº§n lÆ°á»£t lÃ  $Q$, $K$ vÃ  $V$, vá»›i $Q \in \mathbb{R}^{length_q \times d_q}$, $K \in \mathbb{R}^{length_k \times d_k}$ vÃ  $V \in \mathbb{R}^{length_v \times d_v}$. Gá»i cÃ¡c vector query trong ma tráº­n $Q$ lÃ  $q_i$ (á»©ng vá»›i tá»«ng dÃ²ng cá»§a ma tráº­n), tÆ°Æ¡ng tá»± vá»›i $k_i$ trong $K$ vÃ  $v_i$ trong $V$. Ta cÃ³ má»™t sá»‘ lÆ°u Ã½ sau:</p>
<ul>
<li>Äá»ƒ dá»… hÃ¬nh dung, vá»›i vÃ­ dá»¥ vá» Attention á»Ÿ hÃ¬nh <a class="link" href="#attention_ex" >nÃ y</a> thÃ¬ $length_k = length_v = 5$ vÃ  $length_q$ sáº½ báº±ng vá»›i sá»‘ tá»« á»Ÿ phÃ­a Decoder (vÃ  trong hÃ¬nh Ä‘Ã³ thÃ¬ lÃ  1).</li>
<li>Ta xÃ©t ma tráº­n $Q$ vÃ¬ tá»« pháº§n vá» <a class="link" href="#sec:attention" >Attention</a> thÃ¬ ta tháº¥y ráº±ng vá»›i má»—i vector $y_i$, quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Attention tá»« nÃ³ Ä‘áº¿n cÃ¡c vector $x_j$ lÃ  hoÃ n toÃ n Ä‘á»™c láº­p vá»›i cÃ¡c vector $y_t$ khÃ¡c, tá»©c lÃ  ta cÃ³ thá»ƒ thá»±c hiá»‡n <strong>tÃ­nh toÃ¡n Attention song song</strong> vá»›i táº¥t cáº£ cÃ¡c vector query.</li>
<li>VÃ¬ $f$ lÃ  phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng nÃªn sá»‘ chiá»u cá»§a vector query vÃ  vector key pháº£i giá»‘ng nhau, tá»©c lÃ  $d_q = d_k$. NgoÃ i ra, ta thÆ°á»ng cÃ³ $length_q = length_k = length_v$.</li>
</ul>
<p>QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Scale Dot-Product Attention Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. Vá» máº·t cÃ´ng thá»©c, ma tráº­n output sáº½ thuá»™c $\mathbb{R}^{length_q \times d_v}$ vÃ  nÃ³ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh báº±ng
$$
\begin{equation*}
\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^\top}{\sqrt{d_k}} \right) V
\end{equation*}
$$
, trong Ä‘Ã³ phÃ©p toÃ¡n $\text{softmax}$ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn tá»«ng dÃ²ng cá»§a ma tráº­n $QK^\top$.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 350px;" src='./images/scale_dot_product_attn.png'>
<p align="center" style="margin: 0; color: #888;">CÃ¡c bÆ°á»›c tÃ­nh toÃ¡n trong Scale Dot-Product Attention.</p>
</div>
<p><strong>LÆ°u Ã½</strong>. Äá»ƒ giáº£i thÃ­ch cho sá»± cáº§n thiáº¿t cá»§a viá»‡c scale cÃ¡c pháº§n tá»­ trong ma tráº­n $QK^\top$ báº±ng cÃ¡ch chia cho $\sqrt{d_k}$ trÆ°á»›c khi tÃ­nh softmax thÃ¬ ta cáº§n quan tÃ¢m Ä‘áº¿n phÆ°Æ¡ng sai cá»§a cÃ¡c giÃ¡ trá»‹ nÃ y. Náº¿u ban Ä‘áº§u cÃ¡c vector $q_i$ vÃ  $k_j$ cÃ³ phÆ°Æ¡ng sai lÃ  1 thÃ¬ cÃ¡c pháº§n tá»­ trong $QK^\top$ sáº½ cÃ³ phÆ°Æ¡ng sai lÃ  $d_k$ (hoáº·c lÃ  $d_q$ vÃ¬ ta cÃ³ $d_q = d_k$). ÄÃ³ lÃ  má»™t giÃ¡ trá»‹ ráº¥t lá»›n vÃ  nÃ³ sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n quÃ¡ trÃ¬nh tÃ­nh attention weights (phÃ©p toÃ¡n softmax): giáº£m Ä‘á»™ chÃ­nh xÃ¡c vÃ  thá»i gian tÃ­nh tÄƒng lÃªn khÃ¡ nhiá»u.</p>
<h2 id="self-attention-vÃ -cross-attention">Self-Attention vÃ  Cross-Attention</h2>
<p>KhÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh RNN-based, Transformer Ä‘Ã£ thay tháº¿ toÃ n bá»™ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n há»“i quy báº±ng cÃ¡c phÃ©p toÃ¡n Attention vÃ  má»™t sá»‘ fully connected layer. NÃ³i cÃ¡ch khÃ¡c, Transformer Ä‘Ã£ sá»­ dá»¥ng Attention Ä‘á»ƒ há»c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong cÃ¢u nguá»“n vÃ  trong cÃ¢u Ä‘Ã­ch.</p>
<ul>
<li>NgoÃ i ra, quÃ¡ trÃ­nh tÃ­nh toÃ¡n nÃ y hoÃ n toÃ n cÃ³ thá»ƒ diá»…n ra song song chá»© khÃ´ng cáº§n pháº£i tuáº§n tá»± tá»«ng vá»‹ trÃ­ nhÆ° RNN-based.</li>
<li>Äá»ƒ ngáº¯n gá»n hÆ¡n, tá»« pháº§n nÃ y trá»Ÿ Ä‘i, khi nÃ³i Ä‘áº¿n Attention trong Transformer thÃ¬ ta hiá»ƒu Ä‘Ã³ lÃ  Scale Dot-Product Attention.</li>
</ul>
<p>Trong Transformer, Attention Ä‘Æ°á»£c sá»­ dá»¥ng theo hai dáº¡ng lÃ  <strong>Self-Attention</strong> vÃ  <strong>Cross-Attention</strong>. Sá»± khÃ¡c nhau giá»¯a Self-Attention vÃ  Cross-Attention náº±m á»Ÿ cÃ¡c cÃ¡ch xÃ¡c Ä‘á»‹nh giÃ¡ trá»‹ ma tráº­n $Q$, $K$ vÃ  $V$ Ä‘á»ƒ tÃ­nh Scale Dot-Product Attention, trong Ä‘Ã³:</p>
<ul>
<li><strong>Self-Attention:</strong> Ba ma tráº­n $Q$, $K$ vÃ  $V$ Ä‘á»u Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« input cá»§a Encoder hoáº·c cá»§a Decoder, tá»©c lÃ  cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n hoáº·c ngÃ´n ngá»¯ Ä‘Ã­ch. NhÆ° váº­y, Self-Attention sáº½ diá»…n ra trong ná»™i bá»™ cÃ¢u nguá»“n vÃ  ná»™i bá»™ cÃ¢u Ä‘Ã­ch.</li>
<li><strong>Cross-Attention:</strong> Ma tráº­n $Q$ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« input cá»§a Decoder, tá»©c lÃ  cÃ¢u ngÃ´n ngá»¯ Ä‘Ã­ch. Trong khi Ä‘Ã³, $K$ vÃ  $V$ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« cÃ¢u nguá»“n á»Ÿ phÃ­a Encoder. Äiá»u nÃ y nghÄ©a lÃ  Cross-Attention thá»±c hiá»‡n Attention tá»« cÃ¢u Ä‘Ã­ch vÃ o cÃ¢u nguá»“n (Ä‘Ã¢y tháº­t lÃ  Ã½ tÆ°á»Ÿng sá»­ dá»¥ng Attention trong nhá»¯ng mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³).</li>
</ul>
<h3 id="self-attention">Self-Attention</h3>
<p><span id='sec:self-attn'></span></p>
<p>TrÆ°á»›c tiÃªn, ta xÃ©t Ä‘áº¿n Self-Attention. Self-Attention sáº½ thá»±c hiá»‡n nhiá»‡m vá»¥ <strong>há»c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau trong cÃ¹ng má»™t cÃ¢u</strong> (thay tháº¿ quÃ¡ trÃ¬nh tÃ­nh toÃ¡n há»“i quy trong cÃ¡c mÃ´ hÃ¬nh RNN-based). Do Ä‘Ã³ mÃ  cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$ Ä‘Æ°á»£c tÃ­nh tá»« cÃ¡c tá»« trong cÃ¹ng má»™t cÃ¢u. HÆ¡n ná»¯a, ta cÅ©ng cÃ³ $length_q = length_k = length_v = n$ vá»›i $n$ lÃ  sá»‘ tá»« cá»§a cÃ¢u Ä‘Ã³.</p>
<p>Giáº£ sá»­ ráº±ng, cÃ¢u input sau khi qua cÃ¡c pháº§n Embedding (Word Embedding vÃ  Positional Encoding) thÃ¬ ta thu Ä‘Æ°á»£c ma tráº­n $X \in \mathbb{R}^{n \times d_{model}}$. Khi Ä‘Ã³:</p>
<ul>
<li>Ta sá»­ dá»¥ng 3 fully connected layer Ä‘á»ƒ biáº¿n Ä‘á»•i $X$ thÃ nh cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$. Gá»i 3 ma tráº­n trá»ng sá»‘ á»©ng vá»›i cÃ¡c layer Ä‘Ã³ lÃ  $W_Q$, $W_K$ vÃ  $W_V$. ÄÃ¢y chÃ­nh lÃ  cÃ¡c ma tráº­n tham sá»‘ mÃ  Transformer cáº§n há»c cho quÃ¡ trÃ¬nh Self-Attention.</li>
<li>Tá»« $Q$, $K$ vÃ  $V$, qua phÃ©p toÃ¡n Scale Dot-Product Attention, ta thu Ä‘Æ°á»£c ma tráº­n káº¿t quáº£ $Z \in \mathbb{R}^{n \times d_v}$.</li>
</ul>
<p>QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n nÃ y cá»§a Self-Attention Ä‘Æ°á»£c mÃ´ táº£ trong hÃ¬nh bÃªn dÆ°á»›i:</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 450px;" src='./images/self-attn.png'>
<p align="center" style="margin: 0; color: #888;">QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Self-Attention vá»›i d = d_model</p>
</div>
<p>Minh há»a cho káº¿t quáº£ cá»§a phÃ©p toÃ¡n Self-Attention trong má»™t cÃ¢u Tiáº¿ng Anh Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. Trong Ä‘Ã³, mÃ u cÃ ng Ä‘áº­m thá»ƒ hiá»‡n cho attention weights tá»« tá»« &ldquo;it&rdquo; vÃ o tá»« tÆ°Æ¡ng á»©ng lÃ  cÃ ng lá»›n.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 400px;" src='./images/self-attn_ex.png'>
<p align="center" style="margin: 0; color: #888;">Minh há»a káº¿t quáº£ cá»§a Self-Attention táº¡i tá»« "it" trong cÃ¢u Ä‘áº§u vÃ o</p>
</div>
<p>NhÆ° váº­y, Self-Attention Ä‘Ã£ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong cÃ¹ng má»™t cÃ¢u vá» máº·t ngá»¯ nghÄ©a má»™t cÃ¡ch Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£ hÆ¡n so vá»›i quÃ¡ trÃ¬nh tÃ­nh toÃ¡n há»“i quy trong cÃ¡c mÃ´ hÃ¬nh RNN-based.</p>
<h3 id="cross-attention">Cross-Attention</h3>
<p>Cross-Attention thá»±c ra chÃ­nh lÃ  cÃ¡ch sá»­ dá»¥ng Attention trong nhiá»u mÃ´ hÃ¬nh trÆ°á»›c Ä‘Ã³ vá»›i bÃ i toÃ¡n Machine Translation. Nhiá»‡m vá»¥ cá»§a nÃ³ trong bÃ i toÃ¡n Machine Translation lÃ  thá»±c hiá»‡n <strong>Attention tá»« cÃ¢u Ä‘Ã­ch vÃ o cÃ¢u nguá»“n</strong>. Äiá»u nÃ y cÃ³ thá»ƒ xem nhÆ° lÃ  viá»‡c ta sá»­ dá»¥ng nhá»¯ng gÃ¬ Ä‘Ã£ hiá»ƒu Ä‘Æ°á»£c á»Ÿ cÃ¢u thuá»™c ngÃ´n ngá»¯ nguá»“n Ä‘á»ƒ dá»‹ch dáº§n cÃ¢u Ä‘Ã³ ra cÃ¢u ngÃ´n ngá»¯ Ä‘Ã­ch.</p>
<p>Cá»¥ thá»ƒ hÆ¡n, ta cÃ³:</p>
<ul>
<li>GiÃ¡ trá»‹ cá»§a ma tráº­n $K$ vÃ  $V$ sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« cÃ¢u nguá»“n theo <strong>má»™t cÃ¡ch nÃ o Ä‘Ã³</strong> (cÃ¡c pháº§n vá» <a class="link" href="#sec:encoder" >Encoder</a> vÃ  <a class="link" href="#sec:decoder" >Decoder</a> sáº½ Ä‘á» cáº­p kÄ© hÆ¡n vá» chi tiáº¿t nÃ y).</li>
<li>Äá»‘i vá»›i ma tráº­n $Q$ thÃ¬ nÃ³ cÅ©ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tá»« cÃ¡c tá»« hiá»‡n cÃ³ trong cÃ¢u ngÃ´n ngá»¯ Ä‘Ã­ch tÃ­nh Ä‘áº¿n vá»‹ trÃ­ Ä‘ang xÃ©t. Ta váº«n sáº½ Ä‘áº£m báº£o ráº±ng $d_q = d_k$ vÃ  quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Cross-Attention hoÃ n toÃ n tÆ°Æ¡ng tá»± nhÆ° Self-Attention.</li>
<li>MÃ´ hÃ¬nh Transformer cÅ©ng cáº§n há»c ba ma tráº­n tham sá»‘ tÆ°Æ¡ng á»©ng Ä‘á»ƒ tÃ­nh ra $Q$, $K$ vÃ  $V$ trÆ°á»›c khi thá»±c hiá»‡n Scale Dot-Product Attention).</li>
</ul>
<p>Äá»ƒ dá»… hÃ¬nh dung hÆ¡n vá» sá»± khÃ¡c nhau giá»¯a $Q$, $K$ vÃ  $V$ trong Self-Attention vÃ  Cross-Attention, ta cÃ³ minh há»a trong hÃ¬nh bÃªn dÆ°á»›i, vá»›i cÃ¡c hÃ m $f$, $g$ vÃ  $h$ Ä‘áº¡i diá»‡n cho quÃ¡ trÃ¬nh tÃ­nh toÃ¡n ra ba ma tráº­n Ä‘Ã³.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 350px;" src='./images/self_vs_cross_attn.png'>
<p align="center" style="margin: 0; color: #888;">Sá»± khÃ¡c nhau vá» cÃ¡ch tÃ­nh toÃ¡n $Q$, $K$ vÃ  $V$ trong Self-Attention vÃ  Cross-Attention</p>
</div>
<p><strong>LÆ°u Ã½.</strong> Cross-Attention lÃ  má»™t thÃ nh pháº§n ráº¥t thÃº vá»‹. Nhá» cÃ³ Cross-Attention mÃ  kiáº¿n trÃºc Transformer cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng hiá»‡u quáº£ vÃ o nhiá»u bÃ i toÃ¡n khÃ¡c nhau. VÃ­ dá»¥, vá»›i bÃ i toÃ¡n <strong>Image Captioning</strong>, ta cÃ³ thá»ƒ sá»­ dá»¥ng Encoder Ä‘á»ƒ &ldquo;hiá»ƒu&rdquo; Ã½ nghÄ©a cá»§a áº£nh Ä‘áº§u vÃ o, sau Ä‘Ã³, trong quÃ¡ trÃ¬nh sinh cÃ¢u mÃ´ táº£ cho áº£nh, ta cÃ³ thá»ƒ sá»­ dá»¥ng Cross-Attention Ä‘áº¿n cÃ¡c Ä‘áº·c trÆ°ng rÃºt ra tá»« áº£nh Ä‘Ã³ Ä‘á»ƒ thu Ä‘Æ°á»£c cÃ¡c cÃ¢u mÃ´ táº£ tá»‘t hÆ¡n.</p>
<h2 id="multi-head-attention-vÃ -masked-multi-head-attention">Multi-Head Attention vÃ  Masked Multi-Head Attention</h2>
<h3 id="mutli-head-attention">Mutli-Head Attention</h3>
<!-- \label{sec:multi-head} -->
<p>Äá»ƒ Ã½ ráº±ng, <a class="link" href="#sec:self-attn" >Self-Attention</a> Ä‘ang thá»±c hiá»‡n Attention tá»« má»™t tá»« trong cÃ¢u Ä‘áº¿n toÃ n bá»™ cÃ¡c tá»« cÃ²n láº¡i trong cÃ¢u (ká»ƒ cáº£ chÃ­nh tá»« Ä‘Ã³). TÆ°Æ¡ng tá»± nhÆ° vá»›i Cross-Attention. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  ta Ä‘ang thá»±c hiá»‡n <strong>Global Attention</strong>. Tuy nhiÃªn, trong ngÃ´n ngá»¯, Ä‘Ã´i khi tá»« X cÃ³ thá»ƒ cÃ³ quan há»‡ &ldquo;máº¡nh&rdquo; vá»›i má»™t tá»« Y theo má»™t phÆ°Æ¡ng diá»‡n nÃ o Ä‘Ã³, vÃ  nÃ³ cÅ©ng cÃ³ thá»ƒ cÃ³ quan há»‡ máº¡nh vá»›i tá»« Z theo má»™t phÆ°Æ¡ng diá»‡n khÃ¡c ná»¯a. Do Ä‘Ã³, náº¿u thá»±c hiá»‡n Global Attention thÃ¬ cÃ¡c quan há»‡ nÃ y cÃ³ thá»ƒ bá»‹ trung hÃ²a láº«n nhau vÃ  khiáº¿n ta máº¥t Ä‘i nhá»¯ng Ä‘áº·c trÆ°ng cÃ³ Ã­ch.</p>
<p>Ta xÃ©t má»™t vÃ­ dá»¥ Ä‘Æ°á»£c Ä‘á» cáº­p trong bÃ i giáº£ng cá»§a <a class="link" href="https://drive.google.com/file/d/1y8YxaJwjjnhdpYeLWOTq27PIEInoOLHj/view"  target="_blank" rel="noopener"
    >ProtonX</a>. Vá»›i cÃ¢u &ldquo;TÃ´i Ä‘i há»c á»Ÿ HÃ  Ná»™i&rdquo; vÃ  ta Ä‘ang xÃ©t tá»« &ldquo;TÃ´i&rdquo; nhÆ° lÃ  query vector. Náº¿u xÃ©t theo 3 phÆ°Æ¡ng diá»‡n, hay lÃ  3 cÃ¢u há»i,  sau Ä‘Ã¢y: &ldquo;Ai?&rdquo;, &ldquo;LÃ m gÃ¬?&rdquo;, &ldquo;á» Ä‘Ã¢u?&rdquo;, thÃ¬ vá»›i má»—i phÆ°Æ¡ng diá»‡n, má»‘i quan há»‡ giá»¯a tá»« &ldquo;TÃ´i&rdquo; vá»›i cÃ¡c tá»« con láº¡i Ä‘Æ°á»£c thá»ƒ hiá»‡n máº¡nh nháº¥t á»Ÿ láº§n lÆ°á»£t cÃ¡c tá»« &ldquo;TÃ´i&rdquo;, &ldquo;Ä‘i&rdquo; vÃ  &ldquo;há»c&rdquo;, &ldquo;á»Ÿ&rdquo; vÃ  &ldquo;HÃ &rdquo; vÃ  &ldquo;Ná»™i&rdquo; (minh há»a á»Ÿ hÃ¬nh bÃªn dÆ°á»›i).</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 200px;" src='./images/multi-head-motivation.png'>
<p align="center" style="margin: 0; color: #888;">XÃ©t tá»« "TÃ´i" trong ba phÆ°Æ¡ng diá»‡n khÃ¡c nhau khi thá»±c hiá»‡n Self-Attention</p>
</div>
<p>Chi tiáº¿t nÃ y lÃ  xuáº¥t phÃ¡t cho Ã½ tÆ°á»Ÿng cá»§a <strong>Multi-Head Attention</strong>. NÃ³ sáº½ háº¡n cháº¿ áº£nh hÆ°á»Ÿng cá»§a Global Attention trong viá»‡c trung hÃ²a má»‘i quan há»‡ giá»¯a cÃ¡c tá»« vá»›i nhau trong nhiá»u phÆ°Æ¡ng diá»‡n.</p>
<p>Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, Multi-Head Attention sáº½ <strong>chia nhá»</strong> tá»«ng ma tráº­n trong cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$ thÃ nh $h$ pháº§n. VÃ­ dá»¥, vá»›i $Q \in \mathbb{R}^{length_q \times d_q}$ thÃ¬ ta sáº½ cÃ³ $h$ ma tráº­n $Q_i \in \mathbb{R}^{length_q \times (d_k / h)}$ (hay lÃ  chia thÃ nh $h$ heads). Sau Ä‘Ã³, Scale Dot-Product Attention sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n trÃªn $h$ bá»™ ba ma tráº­n $(Q_i, K_i, V_i)$ vÃ  cÃ¡c káº¿t quáº£ sáº½ Ä‘Æ°á»£c tá»•ng há»£p láº¡i. Ta cÃ³ thá»ƒ hÃ¬nh dung ráº±ng má»—i pháº§n nhá» cá»§a cÃ¡c ma tráº­n $Q$, $K$ vÃ  $V$ sáº½ cá»‘ gáº¯ng biá»ƒu diá»…n Ä‘áº·c trÆ°ng cá»§a cÃ¡c tá»« á»Ÿ má»™t phÆ°Æ¡ng diá»‡n nÃ o Ä‘Ã³.</p>
<p>QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Multi-Head Attention Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. CÃ³ má»™t sá»‘ lÆ°u Ã½ nhÆ° sau:</p>
<ul>
<li>Vá»›i Multi-Head Attention trong Transformer, ta sáº½ xÃ©t $length_q = length_k = length_v = n$ vá»›i $n$ lÃ  Ä‘á»™ dÃ i cá»§a cÃ¢u Ä‘áº§u vÃ o</li>
<li>$d_q = d_k = d_v = d_{model}$ (Ä‘Ã¢y lÃ  sá»‘ chiá»u cá»§a embedding vector cá»§a cÃ¡c tá»«, sau khi bá»• sung thÃ´ng tin vá» Positional Encoding trong pháº§n \ref{sec:pos-enc}).</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 350px;" src='./images/multi-head-attn.png'>
<p align="center" style="margin: 0; color: #888;">QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n trong Multi-Head Attention</p>
</div>
<p>Äáº·t $d_x = d_{model} / h$. Vá» máº·t cÃ´ng thá»©c, ma tráº­n káº¿t quáº£ cá»§a Multi-Head Attention vá»›i $h$ head cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh báº±ng
$$
\begin{equation*}
\begin{aligned}
\text{MultiHead}(Q, K, V) &amp;= \text{Concat}(\text{head}_1; \dots; \text{head}_h)W^O \\
\text{vá»›i head}_i &amp;= \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
\end{aligned}
\end{equation*}
$$
, trong Ä‘Ã³ $Q, K, V \in \mathbb{R}^{n \times d_{model}}$, cÃ¡c ma tráº­n trá»ng sá»‘ $W^Q_i, W^V_i, W^K_i \in \mathbb{R}^{d_{model} \times d_x}$, káº¿t quáº£ Scale Dot-Product Attention $\text{head}_i \in \mathbb{R}^{n \times d_x}$ vÃ  $W^O \in \mathbb{R}^{d_{model} \times d_{model}}$. NhÆ° váº­y, ma tráº­n káº¿t quáº£ cá»§a Multi-Head Attention thuá»™c $\mathbb{R}^{n \times d_{model}}$.</p>
<h3 id="masked-multi-head-attention">Masked-Multi Head Attention</h3>
<p>Masked-Multi Head Attention lÃ  má»™t biáº¿n thá»ƒ cá»§a Multi-Head Attention vÃ  nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng trong Decoder cá»§a Transformer. Má»¥c Ä‘Ã­ch cá»§a Masked-Multi Head Attention lÃ  <strong>ngÄƒn cháº·n quÃ¡ trÃ¬nh Attention táº¡i má»™t sá»‘ vá»‹ trÃ­ trong cÃ¢u</strong>. Ta cÃ³ thá»ƒ gá»i phÃ©p tÃ­nh Attention á»Ÿ trong Masked-Multi Head Attention lÃ  Masked Scale Dot-Product Attention, vÃ  Self-Attention táº¡i Ä‘Ã¢y cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i lÃ  Masked Self-Attention.</p>
<p>LÃ½ do ta cáº§n Ä‘áº¿n thÃ nh pháº§n nÃ y á»Ÿ Decoder lÃ  Ä‘á»ƒ trÃ¡nh viá»‡c mÃ´ hÃ¬nh &ldquo;nhÃ¬n tháº¥y&rdquo; Ä‘Æ°á»£c cÃ¡c tá»« á»Ÿ phÃ­a sau tá»« hiá»‡n táº¡i khi nÃ³ Ä‘ang Ä‘Æ°a ra dá»± Ä‘oÃ¡n khi ta thá»±c hiá»‡n Self-Attention táº¡i Decoder. Minh há»a cho Ã½ tÆ°á»Ÿng nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 200px;" src='./images/masked-attn-moti.png'>
<p align="center" style="margin: 0; color: #888;">Minh há»a Ã½ tÆ°á»Ÿng cá»§a Masked Self-Attention</p>
</div>
<p><strong>LÆ°u Ã½.</strong> Báº£n cháº¥t cá»§a phÃ©p toÃ¡n lÃ  ta sáº½ lÃ m cho giÃ¡ trá»‹ attention weights Ä‘áº¿n cÃ¡c tá»« nÃªn Ä‘Æ°á»£c bá» qua thÃ nh má»™t giÃ¡ trá»‹ ráº¥t nhá» Ä‘á»ƒ nÃ³ háº§u nhÆ° khÃ´ng thá»ƒ tÃ¡c Ä‘á»™ng gÃ¬ Ä‘áº¿n káº¿t quáº£ chung cá»§a quÃ¡ trÃ¬nh Attention.</p>
<ul>
<li>Äá»ƒ thá»±c hiá»‡n Ä‘Æ°á»£c Ä‘iá»u Ä‘Ã³, ta sáº½ dÃ¹ng má»™t máº·t náº¡ (mask) Ä‘á»ƒ Ä‘Ã¡nh dáº¥u nhá»¯ng tá»« bá»‹ bá» qua khi Ä‘ang xÃ©t Ä‘áº¿n má»™t tá»« nÃ o Ä‘Ã³ trong cÃ¢u.</li>
<li>VÃ­ dá»¥, vá»›i cÃ¢u &ldquo;TÃ´i Ä‘i há»c á»Ÿ HÃ  Ná»™i&rdquo;, ta xÃ©t tá»« &ldquo;Ä‘i&rdquo; thÃ¬ giÃ¡ trá»‹ cá»§a mask sáº½ lÃ  [0, 0, 1, 1, 1, 1], vá»›i pháº§n tá»­ báº±ng 1 cÃ³ nghÄ©a lÃ  giÃ¡ trá»‹ táº¡i thÃ nh pháº§n Ä‘Ã³ bá»‹ bá» qua. LÃºc Ä‘Ã³, trÆ°á»›c khi tÃ­nh attention weights, ta kiá»ƒm tra mask xem giÃ¡ trá»‹ nÃ o báº±ng 1 thÃ¬ gÃ¡n cho pháº§n tá»­ tÆ°Æ¡ng á»©ng á»Ÿ Ä‘Ã³ lÃ  $-\infty$. Khi Ä‘Ã³, sau phÃ©p toÃ¡n softmax, giÃ¡ trá»‹ attention weight táº¡i Ä‘Ã³ sáº½ xáº¥p xá»‰ 0.</li>
</ul>
<p>NhÆ° váº­y, trong Masked Multi-Head Attention, quÃ¡ trÃ¬nh tÃ­nh toÃ¡n cá»§a Scale Dot-Product Attention sáº½ bá»• sung thÃªm má»™t pháº§n kiá»ƒm tra mask vÃ  gÃ¡n láº¡i giÃ¡ trá»‹ trÆ°á»›c khi tÃ­nh softmax. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 300px;" src='./images/scale-dot-product-attn-masked.png'>
<p align="center" style="margin: 0; color: #888;">QuÃ¡ trÃ¬nh tÃ­nh toÃ¡n trong Scale Dot-Product Attention khi thá»±c hiá»‡n thÃªm thao tÃ¡c kiá»ƒm tra mask</p>
</div>
<h2 id="layer-normalization">Layer Normalization</h2>
<p>Trong cÃ¡c mÃ´ hÃ¬nh Deep Learning, ta thÆ°á»ng tháº¥y sá»± xuáº¥t hiá»‡n cá»§a cÃ¡c normalization layer nháº±m cáº£i thiá»‡n kháº£ nÄƒng há»™i tá»¥ cá»§a mÃ´ hÃ¬nh. Hai loáº¡i normalization layer thÆ°á»ng tháº¥y nháº¥t lÃ  <strong>Batch Normalization</strong> (chuáº©n hÃ³a theo batch) vÃ  <strong>Layer Normalization</strong> (chuáº©n hÃ³a theo tá»«ng máº«u). Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh thuá»™c bÃ i toÃ¡n NLP nÃ³i chung vÃ  Transformer nÃ³i riÃªng thÃ¬ loáº¡i layer Ä‘Æ°á»£c sá»­ dá»¥ng thÆ°á»ng lÃ  Layer Normalization. Sá»± khÃ¡c nhau giá»¯a hai loáº¡i layer nÃ y khi Ã¡p dá»¥ng vÃ o bÃ i toÃ¡n ngÃ´n ngá»¯ Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i, trong Ä‘Ã³:</p>
<ul>
<li><strong>Layer Normalization:</strong> Ta chuáº©n hÃ³a vector Ä‘áº·c trÆ°ng cá»§a tá»«ng vá»‹ trÃ­ trÃªn tá»«ng máº«u má»™t (vá»›i sá»‘ vá»‹ trÃ­ báº±ng vá»›i Ä‘á»™ dÃ i cá»§a cÃ¢u). Má»i vá»‹ trÃ­ cá»§a cÃ¹ng má»™t máº«u sáº½ sá»­ dá»¥ng chung má»™t bá»™ tham sá»‘ gain vÃ  bias. NhÆ° váº­y, viá»‡c chuáº©n hÃ³a cá»§a tá»«ng máº«u sáº½ Ä‘á»™c láº­p vá»›i nhau.</li>
<li><strong>Batch Normalization:</strong> Äá»‘i vá»›i loáº¡i layer nÃ y, ta sáº½ chuáº©n hÃ³a tá»«ng pháº§n tá»­ cá»§a vector Ä‘áº·c trÆ°ng táº¡i tá»«ng vá»‹ trÃ­ trong cÃ¢u Ä‘áº§u vÃ o vÃ  viá»‡c tÃ­nh toÃ¡n Ä‘Æ°á»£c dá»±a theo toÃ n bá»™ cÃ¡c máº«u trong cÃ¹ng batch.</li>
</ul>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 300px;" src='./images/layer_norm.png'>
<p align="center" style="margin: 0; color: #888;">Layer Normalization vÃ  Batch Normalization khi Ã¡p dá»¥ng vÃ o bÃ i toÃ¡n ngÃ´n ngá»¯</p>
</div>
<p>Qua sá»± khÃ¡c biá»‡t Ä‘Ã³, ta cÃ³ thá»ƒ tháº¥y ráº±ng Batch Normalization khÃ´ng nÃªn Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ o cÃ¡c bÃ i toÃ¡n NLP vÃ¬ váº¥n Ä‘á» sá»± khÃ¡c nhau vá» Ä‘á»™ dÃ i tháº­t sá»± cá»§a cÃ¡c cÃ¢u trong cÃ¹ng má»™t batch sáº½ áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ chuáº©n hÃ³a (dÃ¹ ta Ä‘Ã£ sá»­ dá»¥ng kÄ© thuáº­t padding Ä‘á»ƒ Ä‘Æ°a cÃ¡c cÃ¢u Ä‘Ã³ vá» cÃ¹ng má»™t Ä‘á»™ dÃ i nhÆ°ng cÃ¡c vá»‹ trÃ­ Ä‘Æ°á»£c padding láº¡i cÃ³ vector Ä‘áº·c trÆ°ng vá»›i giÃ¡ trá»‹ khÃ¡ vÃ´ nghÄ©a).</p>
<ul>
<li>Cá»¥ thá»ƒ hÆ¡n, náº¿u trong cÃ¹ng má»™t batch, cÃ³ má»™t cÃ¢u cÃ³ Ä‘á»™ dÃ i lá»›n vÃ  nhiá»u cÃ¢u cÃ³ Ä‘á»™ dÃ i nhá» thÃ¬ Ä‘áº·c trÆ°ng cá»§a cÃ¡c tá»« á»Ÿ vá»‹ trÃ­ phÃ­a sau cá»§a cÃ¢u dÃ i hÆ¡n Ä‘Ã³ sáº½ cÃ³ kháº£ nÄƒng cao bá»‹ máº¥t Ä‘i khi ta Ã¡p dá»¥ng Batch Normalization.</li>
</ul>
<h2 id="feed-forward-network-vÃ -skip-connection">Feed Forward Network vÃ  skip connection</h2>
<p>BÃªn cáº¡nh cÃ¡c quÃ¡ trÃ¬nh tÃ­nh toÃ¡n Attention, cÃ¡c tÃ¡c giáº£ cá»§a Transformer sá»­ dá»¥ng thÃªm má»™t sá»‘ <strong>Feed Forward Network</strong> (gá»“m cÃ¡c fully connected layer) vÃ  ká»¹ thuáº­t <strong>Skip connection</strong> Ä‘á»ƒ tÄƒng thÃªm kháº£ nÄƒng há»c cÃ¡c Ä‘áº·c trÆ°ng cá»§a mÃ´ hÃ¬nh (cÃ¡c Ã´ mÃ u xanh dÆ°Æ¡ng trong hÃ¬nh á»Ÿ pháº§n <a class="link" href="#overall" >nÃ y</a>).</p>
<p>Äáº§u tiÃªn, Feed Forward Network (FFN) trong Transformer sá»­ dá»¥ng 2 fully connected layers vá»›i sá»‘ unit láº§n lÆ°á»£t lÃ  $d_{ff}$ vÃ  $d_{model}$ cÃ¹ng activation function ReLU trong layer Ä‘áº§u tiÃªn. Input cá»§a FFN lÃ  má»™t ma tráº­n thuá»™c $\mathbb{R}^{n \times d_{model}}$. NhÆ° váº­y, ta cÃ³ thá»ƒ biá»ƒu diá»…n káº¿t quáº£ Ä‘áº§u ra cá»§a FFN theo cÃ´ng thá»©c
$$
\begin{equation*}
FFN(X) = \max(0, XW_1 + b_1) W_2 + b_2
\end{equation*}
$$
, vá»›i hai ma tráº­n trá»ng sá»‘ $W_1 \in \mathbb{R}^{d_{model \times d_{ff}}}$ vÃ  $W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$ vÃ  $b_1$, $b_2$ lÃ  cÃ¡c bias vector. Do Ä‘Ã³ $FFN(X) \in \mathbb{R}^{n \times d_{model}}$.</p>
<p>NgoÃ i ra, <strong>skip connection</strong> lÃ  má»™t Ã½ tÆ°á»Ÿng ráº¥t hay vÃ  phá»• biáº¿n trong Deep Learning ká»ƒ tá»« khi nÃ³ Ä‘Æ°á»£c Ã¡p dá»¥ng thÃ nh cÃ´ng trong mÃ´ hÃ¬nh ResNet. Skip connection cÃ³ thá»ƒ giÃºp cho gradient Ä‘Æ°á»£c lan truyá»n tá»‘t hÆ¡n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh, tá»« Ä‘Ã³ gÃ³p pháº§n lÃ m giáº£m hiá»‡n tÆ°á»£ng vanishing gradient. MÃ¬nh Ä‘Ã£ Ä‘á» cáº­p Ä‘áº¿n Ã½ tÆ°á»Ÿng nÃ y trong bÃ i viáº¿t vá» <a class="link" href="https://htrvu.github.io/post/resnet/" >ResNet</a>.</p>
<p>Trong kiáº¿n trÃºc Transformer, cÃ¡c tÃ¡c giáº£ Ä‘Ã£ Ã¡p dá»¥ng skip connection cÃ¹ng vá»›i Layer Normalization á»Ÿ ráº¥t nhiá»u vá»‹ trÃ­ (cÃ¡c khá»‘i &ldquo;Add &amp; Norm&rdquo; trong hÃ¬nh á»Ÿ pháº§n <a class="link" href="#overall" >nÃ y</a>). Ta cÃ³ thá»ƒ biá»ƒu diá»…n output cá»§a cÃ¡c khá»‘i Ä‘Ã³ á»Ÿ dáº¡ng
$$\text{LayerNorm}(X + \text{Sublayer}(X)))$$
, vá»›i $\text{Sublayer}$ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘áº¡i diá»‡n cho nhá»¯ng thÃ nh pháº§n á»Ÿ phÃ­a trÆ°á»›c cÃ¡c khá»‘i Ä‘Ã³.</p>
<p><strong>LÆ°u Ã½.</strong> Má»™t hÆ°á»›ng giáº£i thÃ­ch cho chi tiáº¿t Feed Forward Network chá»‰ Ã¡p dá»¥ng activation function ReLU cho layer Ä‘áº§u tiÃªn lÃ  vÃ¬ ngay sau Ä‘Ã³ ta Ä‘Ã£ sá»­ dá»¥ng skip connection. ÄÃ¢y lÃ  má»™t kÄ© thuáº­t thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng khi lÃ m viá»‡c vá»›i skip connection.</p>
<ul>
<li>Náº¿u ta sá»­ dá»¥ng activation function ReLU rá»“i sau Ä‘Ã³ Ã¡p dá»¥ng skip connection thÃ¬ cÃ³ thá»ƒ nÃ³i lÃ  giÃ¡ trá»‹ cÃ¡c pháº§n tá»­ trong ma tráº­n Ä‘áº§u vÃ o sáº½ luÃ´n khÃ´ng giáº£m, vÃ  hiá»‡u á»©ng nÃ y cÃ³ thá»ƒ sáº½ cÃ³ áº£nh hÆ°á»Ÿng khÃ´ng tá»‘t Ä‘áº¿n mÃ´ hÃ¬nh.</li>
</ul>
<h2 id="encoder">Encoder</h2>
<p><span id='sec:encoder'></span></p>
<p>Sau khi Ä‘Ã£ trÃ¬nh bÃ y vá» cÃ¡c thÃ nh pháº§n quan trá»ng trong kiáº¿n trÃºc cá»§a Transformer, ta sáº½ Ä‘i vÃ o cÃ¡c nhÃ¡nh chÃ­nh cá»§a kiáº¿n trÃºc vÃ  nhÃ¡nh Ä‘áº§u tiÃªn lÃ  Encoder. Kiáº¿n trÃºc cá»§a Encoder Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i:</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 350px;" src='./images/encoder.png'>
<p align="center" style="margin: 0; color: #888;">Layer Kiáº¿n trÃºc cá»§a Transformer Encoder</p>
</div>
<p>ThÃ nh pháº§n Encoder trong Transformer lÃ  sá»± káº¿t há»£p cá»§a Word Embedding, Positional Encoding vÃ  má»™t dÃ£y gá»“m $N$ <strong>Encoder Layer</strong> liÃªn tiáº¿p nhau. Trong Ä‘Ã³, Encoder Layer bao gá»“m nhiá»u thÃ nh pháº§n nhÆ° Multi-Head Attention, Feed Forward, skip connection vÃ  Layer Normalization.</p>
<p>Cá»¥ thá»ƒ hÆ¡n, trong bÃ i toÃ¡n Machine Translation, input cá»§a Encoder sáº½ lÃ  cÃ¢u vÄƒn thuá»™c ngÃ´n ngá»¯ nguá»“n gá»“m $L$ tá»«. Trong Ä‘Ã³, cÃ¡c &ldquo;tá»«&rdquo; trong cÃ¢u lÃºc nÃ y lÃ  cÃ¡c con sá»‘ á»©ng vá»›i vá»‹ trÃ­ cá»§a tá»« Ä‘Ã³ trong tá»« Ä‘iá»ƒn cá»§a ngÃ´n ngá»¯ nguá»“n. Khi Ä‘Ã³:</p>
<ul>
<li>Sau khi qua Word Embedding vÃ  Positional Encoding, ta Ä‘Æ°á»£c má»™t ma tráº­n $X \in \mathbb{R}^{L \times d_{model}}$.</li>
<li>Táº¡i má»—i Encoder Layer, tá»« ma tráº­n input $X&rsquo; \in \mathbb{R}^{N \times d_{model}}$, ta sáº½ sá»­ dá»¥ng ba ma tráº­n trá»ng sá»‘ $W_Q, W_K, W_V$ Ä‘á»ƒ tÃ­nh ra $Q$ (Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ pháº§n , $K$, $V$ tá»« $X&rsquo;$ vÃ  sau Ä‘Ã³ báº¯t Ä‘áº§u Ä‘i qua cÃ¡c thÃ nh pháº§n trong Ä‘Ã³. Ma tráº­n output cá»§a Encoder Layer cÅ©ng sáº½ lÃ  má»™t ma tráº­n thuá»™c $\mathbb{R}^{L \times d_{model}}$.</li>
</ul>
<p><strong>LÆ°u Ã½.</strong> Encoder output sáº½ lÃ  má»™t ma tráº­n thuá»™c $\mathbb{R}^{L \times d_{model}}$, má»™t ma tráº­n cÃ³ shape giá»‘ng vá»›i ma tráº­n Ä‘áº§u vÃ o $X$ cá»§a Encoder. NhÆ° váº­y, ta cÃ³ thá»ƒ hÃ¬nh dung ráº±ng Encoder Ä‘ang lÃ m nhiá»‡m vá»¥ lÃ  bá»• sung thÃªm nhá»¯ng Ä‘áº·c trÆ°ng quan trá»ng vÃ o embedding vector ban Ä‘áº§u cá»§a cÃ¡c tá»« trong cÃ¢u.</p>
<p>Äá»‘i vá»›i Encoder output, ta sáº½ sá»­ dá»¥ng ma tráº­n nÃ y Ä‘á»ƒ tham gia vÃ o phÃ©p tÃ­nh Cross-Attention trong Decoder.</p>
<h2 id="decoder">Decoder</h2>
<p><span id='sec:decoder'></span></p>
<h3 id="tá»•ng-quan-vá»-kiáº¿n-trÃºc">Tá»•ng quan vá» kiáº¿n trÃºc</h3>
<p>Kiáº¿n trÃºc cá»§a Decoder Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i. Decoder cÅ©ng cÃ³ cÃ¡ch tá»• chá»©c khÃ¡ tÆ°Æ¡ng tá»± Encoder khi ta báº¯t Ä‘áº§u báº±ng Word Embedding vÃ  Positional Encoding, sau Ä‘Ã³ lÃ  dÃ£y gá»“m $N$ <strong>Decoder Layer</strong> liÃªn tiáº¿p nhau. Pháº§n cuá»‘i cá»§a Decoder lÃ  má»™t fully connected layer kÃ¨m theo hÃ m softmax Ä‘á»ƒ ta chá»n ra tá»« phÃ¹ há»£p nháº¥t lÃ m output cá»§a mÃ´ hÃ¬nh Ä‘á»‘i vá»›i vá»‹ trÃ­ hiá»‡n táº¡i trong cÃ¢u thuá»™c ngÃ´n ngá»¯ Ä‘Ã­ch.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 500px;" src='./images/decoder.png'>
<p align="center" style="margin: 0; color: #888;">Kiáº¿n trÃºc cá»§a Transformer Decoder</p>
</div>
<p>Ta cáº§n Ä‘á»ƒ Ã½ ráº±ng, trong Decoder Layer, ta sáº½ thá»±c hiá»‡n cáº£ hai phÃ©p tÃ­nh Self-Attention vÃ  Cross-Attention. Trong Ä‘Ã³:</p>
<ul>
<li>Self-Attention Ä‘Æ°á»£c thá»±c hiá»‡n trÆ°á»›c Ä‘á»ƒ tiáº¿n hÃ nh Attention Ä‘áº¿n cÃ¡c vá»‹ trÃ­ á»Ÿ phÃ­a trÆ°á»›c vá»‹ trÃ­ hiá»‡n táº¡i trong cÃ¢u, tá»©c lÃ  ta Ä‘ang sá»­ dá»¥ng Masked Multi-Head Attention.</li>
<li>Sau Ä‘Ã³, Cross-Attention Ä‘Æ°á»£c thá»±c hiá»‡n vá»›i hai thÃ nh pháº§n $K$ vÃ  $V$ Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« Encoder output. LÃºc nÃ y thÃ¬ ta Ä‘ang thá»±c hiá»‡n Attention Ä‘áº¿n toÃ n bá»™ cÃ¡c vá»‹ trÃ­ trong Encoder output nÃªn Multi-Head Attention Ä‘Æ°á»£c sá»­ dá»¥ng.</li>
</ul>
<p>NgoÃ i ra, ta sáº½ Ä‘áº·t sá»‘ lÆ°á»£ng tá»« trong cÃ¢u nguá»“n vÃ  cÃ¢u Ä‘Ã­ch cá»§a mÃ´ hÃ¬nh lÃ  báº±ng nhau vÃ  báº±ng $L$ (trong trÆ°á»ng há»£p cÃ¢u nÃ o ngáº¯n hÆ¡n thÃ¬ ta sáº½ sá»­ dá»¥ng ká»¹ thuáº­t padding Ä‘á»ƒ thÃªm cÃ¡c tá»« vÃ o). Do Ä‘Ã³:</p>
<ul>
<li>TrÆ°á»›c khi Ä‘áº¿n vá»›i fully connected layer cuá»‘i cÃ¹ng cá»§a Decoder Ä‘á»ƒ tiáº¿n hÃ nh phÃ¢n lá»›p, ma tráº­n output ta nháº­n Ä‘Æ°á»£c cÅ©ng sáº½ thuá»™c $\mathbb{R}^{L \times d_{model}}$.</li>
<li>Sá»‘ chiá»u cá»§a output vector cá»§a fully connected layer cuá»‘i cÃ¹ng sáº½ báº±ng vá»›i kÃ­ch thÆ°á»›c táº­p tá»« Ä‘iá»ƒn cá»§a ngÃ´n ngá»¯ Ä‘Ã­ch.</li>
</ul>
<h3 id="decoder-trong-quÃ¡-trÃ¬nh-huáº¥n-luyá»‡n">Decoder trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n</h3>
<p>Trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh, ta sáº½ táº¡o ra cÃ¡c cáº·p (input, ground truth) cá»§a Decoder theo cÃ¡ch khÃ¡ dáº·c biá»‡t. Äáº§u tiÃªn, ta sáº½ thÃªm cÃ¡c tá»« Ä‘Ã¡nh dáº¥u cho viá»‡c &ldquo;báº¯t Ä‘áº§u&rdquo; vÃ  &ldquo;káº¿t thÃºc&rdquo; cá»§a quÃ¡ trÃ¬nh dá»‹ch. Ta gá»i Ä‘Ã¢y lÃ  cÃ¡c tá»« &ldquo;<start>&rdquo; vÃ  &ldquo;<end>&rdquo;. CÃ¡c cáº·p (input, ground truth) dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n Decoder sáº½ Ä‘Æ°á»£c táº¡o ra báº±ng cÃ¡ch &ldquo;shifted right&rdquo; má»™t cÃ¢u thuá»™c ngÃ´n ngá»¯ Ä‘Ã­ch. VÃ­ dá»¥, vá»›i cÃ¢u Tiáº¿ng Viá»‡t lÃ  &ldquo;tÃ´i Ä‘i há»c&rdquo;, ta sáº½ cÃ³ cÃ¡c cáº·p (input, ground truth) tÆ°Æ¡ng á»©ng nhÆ° sau:</p>
<ul>
<li>Input: &ldquo;<start>&rdquo;, &ldquo;tÃ´i&rdquo;, &ldquo;Ä‘i&rdquo;, &ldquo;há»c&rdquo;.</li>
<li>Ground truth: &ldquo;tÃ´i&rdquo;, &ldquo;Ä‘i&rdquo;, &ldquo;há»c&rdquo;, &ldquo;<end>&rdquo;</li>
</ul>
<p>NgoÃ i ra, cho dÃ¹ Decoder cÃ³ dá»± Ä‘oÃ¡n ra tá»« nÃ o á»Ÿ vá»‹ trÃ­ $t$ cá»§a cÃ¢u Ä‘Ã­ch Ä‘i ná»¯a thÃ¬ input cá»§a Decoder á»Ÿ vá»‹ trÃ­ $t+1$ cÅ©ng luÃ´n lÃ  má»™t tá»« Ä‘Ãºng (tá»©c lÃ  ground truth cá»§a vá»‹ trÃ­ $t$). Ká»¹ thuáº­t nÃ y gá»i lÃ  <strong>Teacher Forcing</strong> vÃ  nÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng ráº¥t nhiá»u trong cÃ¡c bÃ i toÃ¡n NLP. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c minh há»a trong hÃ¬nh bÃªn dÆ°á»›i.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 400px;" src='./images/teacher-forcing.png'>
<p align="center" style="margin: 0; color: #888;">Ká»¹ thuáº­t Teacher Forcing vá»›i Decoder trong huáº¥n luyá»‡n Transformer</p>
</div>
<h3 id="decoder-trong-quÃ¡-trÃ¬nh-dá»±-Ä‘oÃ¡n">Decoder trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n</h3>
<p>Äá»‘i vá»›i quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n (hay lÃ  dá»‹ch) cá»§a Transformer, Decoder sáº½ báº¯t Ä‘áº§u vá»›i má»™t tá»« lÃ  &ldquo;&lt;start&gt;&rdquo; vÃ  quÃ¡ trÃ¬nh dá»‹ch sáº½ tiáº¿p tá»¥c cho Ä‘áº¿n khi mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ra tá»« &ldquo;&lt;end&gt;&rdquo;. HÆ¡n ná»¯a, tá»« Ä‘Æ°á»£c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ra á»Ÿ vá»‹ trÃ­ $t$ sáº½ Ä‘Æ°á»£c dÃ¹ng lÃ m input cá»§a Decoder cho vá»‹ trÃ­ $t+1$. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh bÃªn dÆ°á»›i.</p>
<div style="display: flex; flex-direction: column; align-items: center;">
<img style="max-height: 400px;" src='./images/decoder_infer.png'>
<p align="center" style="margin: 0; color: #888;">Decoder trong quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n</p>
</div>
<h2 id="káº¿t-luáº­n">Káº¿t luáº­n</h2>
<p>NhÆ° váº­y, sau má»™t bÃ i viáº¿t ráº¥t dÃ i thÃ¬ mÃ¬nh Ä‘Ã£ trÃ¬nh bÃ y vá» Transformer vá»›i khÃ¡ nhiá»u phÃ¢n tÃ­ch vÃ o Ã½ tÆ°á»Ÿng vÃ  báº£n cháº¥t cá»§a cÃ¡c thÃ nh pháº§n. Tá»« sá»± thÃ nh cÃ´ng cá»§a Transformer trong Machine Translation, má»™t ká»· nguyÃªn má»›i tháº­t sá»± Ä‘Ã£ má»Ÿ ra Ä‘á»‘i vá»›i NLP nÃ³i riÃªng vÃ  Deep Learning nÃ³i chung:</p>
<ul>
<li>CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° ChatGPT, Bard,&hellip; Ä‘á»u dá»±a trÃªn ná»n táº£ng kiáº¿n trÃºc cá»§a Transformer. RÃµ hÆ¡n má»™t chÃºt thÃ¬ chÃºng sáº½ sá»­ dá»¥ng Transformer Decoder vÃ  sáº½ khÃ´ng dÃ¹ng Ä‘áº¿n Cross Attention ğŸ˜‰</li>
<li>Äá»‘i vá»›i cÃ¡c bÃ i toÃ¡n thuá»™c lÄ©nh vá»±c CV, hay lÃ  multi-model nhÆ° text-to-image (Stable Diffusion,&hellip;) thÃ¬ cÅ©ng Ä‘á»u Ä‘ang táº­n dá»¥ng sá»©c máº¡nh cá»§a Transformer vÃ  cÃ¡c thÃ nh pháº§n cá»§a chÃºng, Ä‘áº·c biá»‡t lÃ  Cross Attention.</li>
</ul>
<h2 id="tÃ i-liá»‡u-tham-kháº£o">TÃ i liá»‡u tham kháº£o</h2>
<ul>
<li>Vaswani, Ashish, et al. &ldquo;Attention is all you need.&rdquo; Advances in neural information processing systems 30 (2017).</li>
<li><a class="link" href="https://lilianweng.github.io/posts/2018-06-24-attention/"  target="_blank" rel="noopener"
    >Weng, Lilian, &ldquo;Attention? Attention!&rdquo;</a></li>
<li><a class="link" href="https://pbcquoc.github.io/transformer/"  target="_blank" rel="noopener"
    >Pháº¡m BÃ¡ CÆ°á»ng Quá»‘c, &ldquo;TÃ¬m hiá»ƒu mÃ´ hÃ¬nh Transformer - NgÆ°Æ¡i KhÃ´ng Pháº£i LÃ  Anh HÃ¹ng, NgÆ°Æ¡i LÃ  QuÃ¡i Váº­t Nhiá»u Äáº§u&rdquo;</a></li>
<li>Shen, Sheng, et al. &ldquo;Powernorm: Rethinking batch normalization in transformers.&rdquo; International Conference on Machine Learning. PMLR, 2020.</li>
<li><a class="link" href="https://drive.google.com/file/d/1y8YxaJwjjnhdpYeLWOTq27PIEInoOLHj/view"  target="_blank" rel="noopener"
    >ProtonX, Transformer - Encoder</a></li>
<li><a class="link" href="https://drive.google.com/file/d/1zuQah_XnVihdAJ72zy3Wc_cFdPssH4uS/view"  target="_blank" rel="noopener"
    >ProtonX, Transformer - Decoder</a></li>
<li>Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. &ldquo;Effective approaches to attention-based neural machine translation.&rdquo;, arXiv preprint arXiv:1508.04025 (2015).</li>
<li><a class="link" href="https://notesonai.com/Attention&#43;Mechanism"  target="_blank" rel="noopener"
    >Notes On AI, &ldquo;Attention Machenism&rdquo;</a></li>
<li><a class="link" href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"  target="_blank" rel="noopener"
    >Sebastian Raschka, &ldquo;Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch&rdquo;</a></li>
<li><a class="link" href="http://jalammar.github.io/illustrated-transformer/"  target="_blank" rel="noopener"
    >Jay Alammar, &ldquo;The Illustrated Transformer&rdquo;</a></li>
<li><a class="link" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html"  target="_blank" rel="noopener"
    >Dive Into Deep Learning, &ldquo;Beam Search&rdquo;</a></li>
</ul>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/nlp/">nlp</a>
        
            <a href="/tags/attention/">attention</a>
        
            <a href="/tags/transformer/">transformer</a>
        
            <a href="/tags/self-attention/">self-attention</a>
        
            <a href="/tags/cross-attention/">cross-attention</a>
        
    </section>


    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css"integrity="sha256-J&#43;iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s="crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js"integrity="sha256-InsNdER1b2xUewP&#43;pKCUJpkhiqwHgqiPXDlIk7GzBu4="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js"integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI="crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">Related content</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/post/mt_seq2seq/">
        
        

        <div class="article-details">
            <h2 class="article-title">BÃ i toÃ¡n Machine Translation, mÃ´ hÃ¬nh Sequence to Sequence vÃ  Ä‘á»™ Ä‘o BLEU</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/post/deep-rnn_birnn/">
        
        

        <div class="article-details">
            <h2 class="article-title">Deep RNN vÃ  Bidirectional RNN</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/post/lstm-gru/">
        
        

        <div class="article-details">
            <h2 class="article-title">Long Short-Term Memory (LSTM) vÃ  Gated Recurrent Unit (GRU)</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/post/word-embedding/">
        
        

        <div class="article-details">
            <h2 class="article-title">Word Embedding</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/post/rnn/">
        
        

        <div class="article-details">
            <h2 class="article-title">Recurrent Neural Network (RNN)</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

<div id="disqus_thread"></div>

<p><b>LÆ°u Ã½.</b> Náº¿u pháº§n Comment khÃ´ng load ra Ä‘Æ°á»£c thÃ¬ cÃ¡c báº¡n vÃ o DNS setting cá»§a Wifi/LAN vÃ  Ä‘á»•i thÃ nh "8.8.8.8" nhÃ© (server cá»§a Google)!</p>


<script>
    

    

    (function() { 
    var d = document, s = d.createElement('script');
    s.src = 'https://htrvu-blog.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
        2023 Trong-Vu Hoang
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.16.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>

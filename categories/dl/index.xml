<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deep Learning on Hoang Trong Vu</title>
        <link>https://htrvu.github.io/categories/dl/</link>
        <description>Recent content in Deep Learning on Hoang Trong Vu</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Wed, 15 Feb 2023 21:42:44 +0700</lastBuildDate><atom:link href="https://htrvu.github.io/categories/dl/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Recurrent Neural Network</title>
        <link>https://htrvu.github.io/post/rnn/</link>
        <pubDate>Wed, 15 Feb 2023 21:42:44 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/rnn/</guid>
        <description>ÄÃ¢y lÃ  bÃ i viáº¿t Ä‘áº§u tiÃªn cá»§a mÃ¬nh trong lÄ©nh vá»±c Natural Language Processing (Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn). NÃ³ sáº½ khÃ¡ dÃ i vÃ¬ mÃ¬nh Ä‘Ã£ trÃ¬nh bÃ y ká»¹ quÃ¡ trÃ¬nh back-propagation. Mong cÃ¡c báº¡n Ä‘á»c háº¿t nhÃ©! ğŸ˜€
SÆ¡ lÆ°á»£c vá» Natural Language Processing BÃªn cáº¡nh Computer Vision (CV - Thá»‹ giÃ¡c mÃ¡y tÃ­nh) thÃ¬ Natural Language Processing (NLP - Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn) cÅ©ng lÃ  má»™t máº£ng ráº¥t quan trá»ng vÃ  Ä‘Æ°á»£c nghiÃªn cá»©u rá»™ng rÃ£i trong Deep Learning.</description>
        </item>
        <item>
        <title>EfficientNet (2020)</title>
        <link>https://htrvu.github.io/post/efficientnet/</link>
        <pubDate>Wed, 15 Feb 2023 11:17:46 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/efficientnet/</guid>
        <description>Giá»›i thiá»‡u Ta biáº¿t ráº±ng, háº§u háº¿t cÃ¡c mÃ´ hÃ¬nh CNN thÆ°á»ng Ä‘Æ°á»£c xÃ¢y dá»±ng tá»« má»™t phiÃªn báº£n ban Ä‘áº§u (cÃ³ thá»ƒ lÃ  dá»±a theo má»™t nguá»“n tÃ i nguyÃªn nÃ o Ä‘Ã³), sau Ä‘Ã³ chÃºng Ä‘Æ°á»£c scale dáº§n lÃªn Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n, vÃ  táº¥t nhiÃªn lÃ  Ä‘á»™ phá»©c táº¡p cÅ©ng tÄƒng theo
VÃ­ dá»¥: Vá»›i ResNet thÃ¬ ta cÃ³ ResNet18 cho Ä‘áº¿n ResNet152, DenseNet thÃ¬ DenseNet121 cho Ä‘áº¿n 201, MobileNet thÃ¬ ta cÃ³ siÃªu tham sá»‘ width multiplier Ä‘á»ƒ Ä‘iá»u chá»‰nh sá»‘ channel trong tá»«ng layer vÃ  resolutiom multiplier Ä‘á»ƒ Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c táº¡i cÃ¡c layer,â€¦ Nhá»¯ng cÃ¡ch lÃ m Ä‘Ã³ gá»i lÃ  model scaling.</description>
        </item>
        <item>
        <title>MobileNet V2 (2019)</title>
        <link>https://htrvu.github.io/post/mobilenet_v2/</link>
        <pubDate>Mon, 13 Feb 2023 11:32:55 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/mobilenet_v2/</guid>
        <description>Giá»›i thiá»‡u Tá»« sá»± thÃ nh cÃ´ng cá»§a MobileNet (2017) trong viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh Deep Learning trÃªn cÃ¡c thiáº¿t bá»‹ biÃªn (smartphone, embedded,â€¦) nhá» vÃ o viá»‡c sá»­ dá»¥ng hiá»‡u quáº£ phÃ©p toÃ¡n depthwise separable convolution, nhiá»u nghiÃªn cá»©u dá»±a trÃªn hÆ°á»›ng phÃ¡t triá»ƒn nÃ y Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh.
Dá»±a theo cÃ¡c â€œkinh nghiá»‡mâ€ cÃ³ Ä‘Æ°á»£c cá»§a báº£n thÃ¢n, nhÃ¬n vÃ o MobileNet thÃ¬ ta sáº½ tháº¥y ngay ráº±ng, nÃ³ chÆ°a cÃ³ cÃ¡i skip connection nÃ o cáº£ ğŸ˜€ ÄÃºng z, skip connection Ä‘Ã£ cho tháº¥y Ä‘Æ°á»£c sá»± hiá»‡u quáº£ cá»§a mÃ¬nh trong cÃ¡c mÃ´ hÃ¬nh nhÆ° ResNet, Inception-ResNet, DenseNet,â€¦ táº¡i sao ta khÃ´ng thá»­ thÃªm vÃ o MobileNet?</description>
        </item>
        <item>
        <title>DenseNet (2018)</title>
        <link>https://htrvu.github.io/post/densenet/</link>
        <pubDate>Sat, 11 Feb 2023 18:09:08 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/densenet/</guid>
        <description>Skip connection vÃ  concatenate TrÆ°á»›c Ä‘Ã³, kiáº¿n trÃºc ResNet Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ  nÃ³ Ä‘Ã£ cho tháº¥y Ä‘Æ°á»£c sá»©c máº¡nh cá»§a cÃ¡c skip connection khi chÃºng Ä‘Æ°á»£c thÃªm vÃ o cÃ¡c mÃ´ hÃ¬nh tá»« sÃ¢u cho Ä‘áº¿n ráº¥t sÃ¢u (vÃ­ dá»¥ nhÆ° ResNet152). Ta tháº¥y ráº±ng nhá»¯ng kiáº¿n trÃºc Ã¡p dá»¥ng skip connection trÆ°á»›c Ä‘Ã¢y Ä‘á»u cÃ³ má»™t Ä‘iá»ƒm chung lÃ  trong má»™t block thÃ¬ ta sáº½ cÃ³ nhá»¯ng Ä‘iá»ƒm ná»‘i 1 feature map vÃ o lÃ m input cá»§a má»™t layer sau Ä‘Ã³, vÃ  chÃºng Ä‘á»u sá»­ dá»¥ng phÃ©p toÃ¡n cá»™ng.</description>
        </item>
        <item>
        <title>CAM, Grad-CAM vÃ  Score-CAM trong CNN</title>
        <link>https://htrvu.github.io/post/cam/</link>
        <pubDate>Thu, 09 Feb 2023 18:30:41 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/cam/</guid>
        <description>Giá»›i thiá»‡u Class Activation Map (CAM) lÃ  phÆ°Æ¡ng phÃ¡p phá»• biáº¿n trong viá»‡c giáº£i thÃ­ch sá»± hoáº¡t Ä‘á»™ng cá»§a CNN. NÃ³ cho ta biáº¿t ráº±ng CNN sáº½ táº­p trung vÃ o nhá»¯ng pháº§n nÃ o cá»§a áº£nh input Ä‘á»ƒ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t áº£nh Ä‘Ã³ thá»¥Ã´c vá» má»™t class nÃ o Ä‘Ã³. ThÃ´ng thÆ°á»ng, CAM cÃ²n Ä‘Æ°á»£c gá»i lÃ  Attention Map.
Äá»ƒ dá»… hÃ¬nh dung hÆ¡n vá» CAM, ta cÃ³ 2 vÃ­ dá»¥ nhÆ° sau:</description>
        </item>
        <item>
        <title>MobileNet (2017)</title>
        <link>https://htrvu.github.io/post/mobilenet/</link>
        <pubDate>Wed, 08 Feb 2023 18:13:00 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/mobilenet/</guid>
        <description>Giá»›i thiá»‡u Qua cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c giá»›i thiá»‡u nhÆ° VGG, GoogLeNet hay ResNet thÃ¬ ta tháº¥y ráº±ng chÃºng Ä‘á»u Ä‘Æ°á»£c phÃ¡t triá»ƒn theo hÆ°á»›ng tÄƒng dáº§n Ä‘á»™ sÃ¢u vÃ  Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n cá»§a mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n, ká»ƒ tá»« khi AlexNet Ä‘Æ°á»£c cÃ´ng bá»‘. Sá»‘ lÆ°á»£ng tham sá»‘ cá»§a chÃºng lÃ  ráº¥t lá»›n.
Tuy nhiÃªn, cÃ¡c á»©ng dá»¥ng AI trong thá»±c táº¿ nhÆ° robotics, xe tá»± hÃ nh thÃ¬ cÃ¡c phÃ©p tÃ­nh toÃ¡n cá»§a mÃ´ hÃ¬nh cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n trong má»™t khoáº£ng thá»i gian giá»›i háº¡n, cÃ¹ng vá»›i tÃ i nguyÃªn pháº§n cá»©ng háº¡n cháº¿.</description>
        </item>
        <item>
        <title>Inception-Reset (2016)</title>
        <link>https://htrvu.github.io/post/inception-resnet/</link>
        <pubDate>Wed, 08 Feb 2023 18:03:46 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/inception-resnet/</guid>
        <description>Giá»›i thiá»‡u CÃ¡c mÃ´ hÃ¬nh thuá»™c há» Inception-ResNet Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a trÃªn Ã½ tÆ°á»Ÿng lÃ  káº¿t há»£p skip connection vÃ o cÃ¡c Inception block (cÃ¡c Ã½ tÆ°á»Ÿng tá»« ResNet vÃ  GoogLeNet). VÃ¬ paper nÃ y chá»‰ mang tÃ­nh thá»±c nghiá»‡m lÃ  chÃ­nh nÃªn mÃ¬nh sáº½ khÃ´ng trÃ¬nh bÃ y chi tiáº¿t ğŸ‘€.
Kiáº¿n trÃºc mÃ´ hÃ¬nh Inception-ResNet V1 Vá» máº·t tá»•ng quan, Inception-ResNet V1 cÃ³ kiáº¿n trÃºc nhÆ° sau:
Kiáº¿n trÃºc Inception-ResNet V1</description>
        </item>
        <item>
        <title>Resnet (2015)</title>
        <link>https://htrvu.github.io/post/resnet/</link>
        <pubDate>Wed, 08 Feb 2023 17:41:09 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/resnet/</guid>
        <description>Giá»›i thiá»‡u Ta biáº¿t ráº±ng, viá»‡c táº¡o ra cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n (nhiá»u layer) chÆ°a cháº¯c Ä‘Ã£ mang láº¡i hiá»‡u quáº£ tá»‘t hÆ¡n nhá»¯ng mÃ´ hÃ¬nh â€œcáº¡nâ€ hÆ¡n. VÃ­ dá»¥, vá»›i táº­p CIFAR10 thÃ¬ ta cÃ³ má»™t káº¿t quáº£ thá»­ nghiá»‡m cho tháº¥y ráº±ng mÃ´ hÃ¬nh sÃ¢u hÆ¡n láº¡i cÃ³ Ä‘á»™ hiá»‡u quáº£ kÃ©m hÆ¡n:
Äá»‘i vá»›i viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ sÃ¢u lá»›n thÃ¬ ta cÃ³ thá»ƒ sáº½ bá»‹ gáº·p pháº£i cÃ¡c váº¥n Ä‘á» sau:</description>
        </item>
        <item>
        <title>GoogLeNet - Inception V1 (2014)</title>
        <link>https://htrvu.github.io/post/googlenet/</link>
        <pubDate>Wed, 08 Feb 2023 01:59:53 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/googlenet/</guid>
        <description>CÃ¡ nhÃ¢n mÃ¬nh tháº¥y GoogLeNet lÃ  má»™t paper khÃ³ Ä‘á»c. Khi viáº¿t ra bÃ i nÃ y thÃ¬ mÃ¬nh váº«n Ä‘ang cáº£m tháº¥y hÆ¡i lÃº vá» ná»™i dung cá»§a nÃ³ ğŸ˜€
Giá»›i thiá»‡u Tá»« khi AlexNet Ä‘Æ°á»£c cÃ´ng bá»‘ vÃ o nÄƒm 2012 vÃ  Ä‘áº·t ná»n táº£ng cho cÃ¡c máº¡ng Deep CNN, GoogLeNet, hay Inception V1 (2014), lÃ  má»™t trong nhá»¯ng kiáº¿n trÃºc cÃ³ cÃ¡ch thiáº¿t káº¿ ráº¥t thÃº vá»‹ khi nÃ³ táº­n dá»¥ng hiá»‡u quáº£ cÃ¡c conv layer, Ä‘áº·t ná»n mÃ³ng cho nhiá»u mÃ´ hÃ¬nh sau nÃ y.</description>
        </item>
        <item>
        <title>VGG (2014)</title>
        <link>https://htrvu.github.io/post/vgg/</link>
        <pubDate>Wed, 08 Feb 2023 01:52:02 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/vgg/</guid>
        <description>Giá»›i thiá»‡u Dá»±a trÃªn sá»± thÃ nh cÃ´ng cá»§a AlexNet vÃ o nÄƒm 2012, nhiá»u nghiÃªn cá»©u Ä‘Ã£ Ä‘Æ°á»£c tiáº¿n hÃ nh nháº±m tÃ¬m ra cÃ¡c phÆ°Æ¡ng phÃ¡p hay kiáº¿n trÃºc má»›i Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t hÆ¡n, vÃ­ dá»¥ nhÆ°:
Thay Ä‘á»•i (tÄƒng, giáº£m) kÃ­ch thÆ°á»›c cá»§a conv filter Thay Ä‘á»•i stride, padding cá»§a conv layer Train vÃ  test trÃªn cÃ¡c input vá»›i nhiá»u Ä‘á»™ phÃ¢n giáº£i (resolution) áº£nh khÃ¡c nhau Trong nÄƒm 2014, VGG lÃ  má»™t trong nhá»¯ng káº¿t quáº£ nghiÃªn cá»©u ná»•i báº­t nháº¥t, vÃ  nÃ³ táº­p trung vÃ o má»™t váº¥n Ä‘á» khÃ¡c vá»›i cÃ¡c hÆ°á»›ng trÃªn lÃ  Ä‘á»™ sÃ¢u (depth, hay lÃ  sá»‘ layer) cá»§a mÃ´ hÃ¬nh.</description>
        </item>
        
    </channel>
</rss>

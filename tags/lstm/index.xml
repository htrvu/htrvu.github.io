<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>lstm on Hoang Trong Vu</title>
        <link>https://htrvu.github.io/tags/lstm/</link>
        <description>Recent content in lstm on Hoang Trong Vu</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 19 Mar 2023 00:25:41 +0700</lastBuildDate><atom:link href="https://htrvu.github.io/tags/lstm/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Bài toán Machine Translation, mô hình Sequence to Sequence và độ đo BLEU</title>
        <link>https://htrvu.github.io/post/mt_seq2seq/</link>
        <pubDate>Sun, 19 Mar 2023 00:25:41 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/mt_seq2seq/</guid>
        <description>Bài toán Machine Translation Giới thiệu Machine Translation (dịch máy) là bài toán rất phổ biến trong lĩnh vực NLP. Sản phẩm Google Dịch mà chúng ta vẫn dùng hằng ngày chính là một mô hình dịch máy khá tốt và nó được huấn luyện bởi Google 😀
Thật ra bài toán Machine Translation đã ra đời từ rất lâu. Tất nhiên rồi, vì nó đóng vai trò rất quan trọng trong giao tiếp.</description>
        </item>
        <item>
        <title>Deep RNN và Bidirectional RNN</title>
        <link>https://htrvu.github.io/post/deep-rnn_birnn/</link>
        <pubDate>Fri, 24 Feb 2023 13:48:53 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/deep-rnn_birnn/</guid>
        <description>Giới thiệu Qua các bài viết về RNN truyền thống, LSTM và GRU thì mình đều trình bày về các mô hình với duy nhất một cell trong kiến trúc (recurrent cell, LSTM cell hoặc là GRU cell). Ngoài ra, ta thấy các hidden state cũng được truyền theo một hướng cố định là từ trái sang phải (thời điểm $t$ đến thời điểm $t + 1$). Nếu bỏ qua chi tiết về các &amp;ldquo;thời điểm&amp;rdquo; thì nhìn chúng sẽ không khác gì một mô hình MLP cơ bản trong Machine Learning.</description>
        </item>
        <item>
        <title>Long Short-Term Memory (LSTM) và Gated Recurrent Unit (GRU)</title>
        <link>https://htrvu.github.io/post/lstm-gru/</link>
        <pubDate>Fri, 24 Feb 2023 00:51:43 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/lstm-gru/</guid>
        <description>Vanishing gradient và long-term, short-term dependency Trong bài viết về mô hình RNN truyền thống, mình đã có đề cập đến vấn đề vanishing gradient của nó dựa vào công thức của quá trình BPPT (Back-propagation Through Time). Hệ quả của vấn đề này là RNN gặp khó khăn trong việc ghi nhớ thông tin trong những câu có nhiều từ.
Để minh họa rõ hơn về hệ quả của vanishing gradient, ta xét ví dụ với mô hình RNN dùng để sinh ra văn bản.</description>
        </item>
        
    </channel>
</rss>

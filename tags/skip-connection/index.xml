<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>skip connection on Hoang Trong Vu</title>
        <link>https://htrvu.github.io/tags/skip-connection/</link>
        <description>Recent content in skip connection on Hoang Trong Vu</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 13 Feb 2023 11:32:55 +0700</lastBuildDate><atom:link href="https://htrvu.github.io/tags/skip-connection/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>MobileNet V2 (2019)</title>
        <link>https://htrvu.github.io/post/mobilenet_v2/</link>
        <pubDate>Mon, 13 Feb 2023 11:32:55 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/mobilenet_v2/</guid>
        <description>Giới thiệu Từ sự thành công của MobileNet (2017) trong việc triển khai các mô hình Deep Learning trên các thiết bị biên (smartphone, embedded,…) nhờ vào việc sử dụng hiệu quả phép toán depthwise separable convolution, nhiều nghiên cứu dựa trên hướng phát triển này đã được tiến hành.
Dựa theo các “kinh nghiệm” có được của bản thân, nhìn vào MobileNet thì ta sẽ thấy ngay rằng, nó chưa có cái skip connection nào cả 😀 Đúng z, skip connection đã cho thấy được sự hiệu quả của mình trong các mô hình như ResNet, Inception-ResNet, DenseNet,… tại sao ta không thử thêm vào MobileNet?</description>
        </item>
        <item>
        <title>DenseNet (2018)</title>
        <link>https://htrvu.github.io/post/densenet/</link>
        <pubDate>Sat, 11 Feb 2023 18:09:08 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/densenet/</guid>
        <description>Skip connection và concatenate Trước đó, kiến trúc ResNet được công bố và nó đã cho thấy được sức mạnh của các skip connection khi chúng được thêm vào các mô hình từ sâu cho đến rất sâu (ví dụ như ResNet152). Ta thấy rằng những kiến trúc áp dụng skip connection trước đây đều có một điểm chung là trong một block thì ta sẽ có những điểm nối 1 feature map vào làm input của một layer sau đó, và chúng đều sử dụng phép toán cộng.</description>
        </item>
        <item>
        <title>Inception-Reset (2016)</title>
        <link>https://htrvu.github.io/post/inception-resnet/</link>
        <pubDate>Wed, 08 Feb 2023 18:03:46 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/inception-resnet/</guid>
        <description>Giới thiệu Các mô hình thuộc họ Inception-ResNet được phát triển dựa trên ý tưởng là kết hợp skip connection vào các Inception block (các ý tưởng từ ResNet và GoogLeNet). Vì paper này chỉ mang tính thực nghiệm là chính nên mình sẽ không trình bày chi tiết 👀.
Kiến trúc mô hình Inception-ResNet V1 Về mặt tổng quan, Inception-ResNet V1 có kiến trúc như sau:
Kiến trúc Inception-ResNet V1</description>
        </item>
        <item>
        <title>Resnet (2015)</title>
        <link>https://htrvu.github.io/post/resnet/</link>
        <pubDate>Wed, 08 Feb 2023 17:41:09 +0700</pubDate>
        
        <guid>https://htrvu.github.io/post/resnet/</guid>
        <description>Giới thiệu Ta biết rằng, việc tạo ra các mô hình có độ sâu lớn (nhiều layer) chưa chắc đã mang lại hiệu quả tốt hơn những mô hình “cạn” hơn. Ví dụ, với tập CIFAR10 thì ta có một kết quả thử nghiệm cho thấy rằng mô hình sâu hơn lại có độ hiệu quả kém hơn:
Đối với việc huấn luyện các mô hình có độ sâu lớn thì ta có thể sẽ bị gặp phải các vấn đề sau:</description>
        </item>
        
    </channel>
</rss>
